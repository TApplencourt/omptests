
; __CLANG_OFFLOAD_BUNDLE____START__ openmp-nvptx64-nvidia-cuda
; ModuleID = '/localhd/abataev/development/build_ibm/llvm/projects/openmp/libomptarget/omptests/nt-target-teams-distribute-parallel-for/test.c'
source_filename = "/localhd/abataev/development/build_ibm/llvm/projects/openmp/libomptarget/omptests/nt-target-teams-distribute-parallel-for/test.c"
target datalayout = "e-i64:64-i128:128-v16:16-v32:32-n16:32:64"
target triple = "nvptx64-nvidia-cuda"

%struct.ident_t = type { i32, i32, i32, i32, i8* }

@.str = private unnamed_addr constant [23 x i8] c";unknown;unknown;0;0;;\00", align 1
@0 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2, i32 0, i32 0, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str, i32 0, i32 0) }, align 8
@__omp_offloading_802_14c1929_check_offloading_l22_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l45_exec_mode = weak constant i8 1
@1 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2050, i32 0, i32 0, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str, i32 0, i32 0) }, align 8
@2 = private unnamed_addr constant %struct.ident_t { i32 0, i32 514, i32 0, i32 0, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str, i32 0, i32 0) }, align 8
@__omp_offloading_802_14c18b2_main_l66_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l86_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l106_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l126_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l146_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l166_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l196_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l216_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l236_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l256_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l276_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l296_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l326_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l346_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l366_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l386_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l406_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l426_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l461_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l484_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l507_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l530_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l553_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l576_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l612_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l637_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l662_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l687_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l712_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l737_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l883_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l905_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l927_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l949_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l971_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l993_exec_mode = weak constant i8 1
@__omp_offloading_802_14c18b2_main_l1036_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1070_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1105_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1140_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1177_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1213_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1248_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1283_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1318_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1354_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1391_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1428_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1465_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1502_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1539_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1577_exec_mode = weak constant i8 0
@__omp_offloading_802_14c18b2_main_l1613_exec_mode = weak constant i8 0
@llvm.compiler.used = appending global [55 x i8*] [i8* @__omp_offloading_802_14c18b2_main_l1036_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l106_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1070_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1105_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1140_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1177_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1213_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1248_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l126_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1283_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1318_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1354_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1391_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1428_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1465_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l146_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1502_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1539_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1577_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l1613_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l166_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l196_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l216_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l236_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l256_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l276_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l296_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l326_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l346_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l366_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l386_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l406_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l426_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l45_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l461_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l484_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l507_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l530_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l553_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l576_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l612_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l637_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l662_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l66_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l687_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l712_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l737_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l86_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l883_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l905_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l927_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l949_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l971_exec_mode, i8* @__omp_offloading_802_14c18b2_main_l993_exec_mode, i8* @__omp_offloading_802_14c1929_check_offloading_l22_exec_mode], section "llvm.metadata"

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c1929_check_offloading_l22([1 x i32]* dereferenceable(4) %A) local_unnamed_addr #0 {
entry:
  %work_fn.i = alloca i8*, align 8
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %0 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %0, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %1 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %1)
  %2 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %3 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %4 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %4, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c1929_check_offloading_l22_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %3, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %5 = bitcast i8* %4 to void (i16, i32)*
  call void %5(i16 0, i32 %2) #4
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.execute.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c1929_check_offloading_l22_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %1)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %6 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %6, -32
  %7 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %7, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %arrayidx = getelementptr inbounds [1 x i32], [1 x i32]* %A, i64 0, i64 0
  store i32 0, i32* %arrayidx, align 4, !tbaa !120
  tail call void @__kmpc_kernel_deinit(i16 1) #4
  tail call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %.master, %.mastercheck, %__omp_offloading_802_14c1929_check_offloading_l22_worker.exit
  ret void
}

; Function Attrs: nounwind readnone
declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() #1

; Function Attrs: nounwind readnone
declare i32 @llvm.nvvm.read.ptx.sreg.ntid.x() #1

declare void @__kmpc_kernel_init(i32, i16) local_unnamed_addr

declare void @__kmpc_data_sharing_init_stack() local_unnamed_addr

declare void @__kmpc_kernel_deinit(i16) local_unnamed_addr

; Function Attrs: convergent nounwind
declare void @llvm.nvvm.barrier0() #2

declare i1 @__kmpc_kernel_parallel(i8**, i16) local_unnamed_addr

declare i32 @__kmpc_global_thread_num(%struct.ident_t*) local_unnamed_addr

declare void @__kmpc_kernel_end_parallel() local_unnamed_addr

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l45(i32* dereferenceable(4) %cpuExec) local_unnamed_addr #0 {
entry:
  %work_fn.i = alloca i8*, align 8
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %0 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %0, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %1 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %1)
  %2 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %3 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %4 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %4, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l45_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %3, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %5 = bitcast i8* %4 to void (i16, i32)*
  call void %5(i16 0, i32 %2) #4
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.execute.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l45_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %1)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %6 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %6, -32
  %7 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %7, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  store i32 0, i32* %cpuExec, align 4, !tbaa !120
  tail call void @__kmpc_kernel_deinit(i16 1) #4
  tail call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %.master, %.mastercheck, %__omp_offloading_802_14c18b2_main_l45_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l66([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l66_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__1_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %32 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %33 = icmp slt i32 %32, 3071
  %cond.i.i.i = select i1 %33, i32 %32, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp31.i.i.i = icmp sgt i32 %34, %cond.i.i.i
  br i1 %cmp31.i.i.i, label %__omp_outlined__1_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add19.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %35 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %add7.i.i.i = fadd double %35, %36
  %arrayidx9.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %add10.i.i.i = fadd double %37, %add7.i.i.i
  store double %add10.i.i.i, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %38 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %arrayidx14.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx14.i.i.i, align 8, !tbaa !126
  %add15.i.i.i = fadd double %38, %39
  %arrayidx17.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add18.i.i.i = fadd double %40, %add15.i.i.i
  store double %add18.i.i.i, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add19.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp3.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %cond.i.i.i
  br i1 %cmp3.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__1_wrapper.exit.i

__omp_outlined__1_wrapper.exit.i:                 ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__1_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l66_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 2) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__1_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %74 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %75 = icmp slt i32 %74, 3071
  %cond.i.i = select i1 %75, i32 %74, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp31.i.i = icmp sgt i32 %76, %cond.i.i
  br i1 %cmp31.i.i, label %__omp_outlined__1.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add19.i.i, %omp.inner.for.body.i.i ], [ %76, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %77 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %add7.i.i = fadd double %77, %78
  %arrayidx9.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx9.i.i, align 8, !tbaa !126
  %add10.i.i = fadd double %79, %add7.i.i
  store double %add10.i.i, double* %arrayidx9.i.i, align 8, !tbaa !126
  %80 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %arrayidx14.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx14.i.i, align 8, !tbaa !126
  %add15.i.i = fadd double %80, %81
  %arrayidx17.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add18.i.i = fadd double %82, %add15.i.i
  store double %add18.i.i, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add19.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp3.i.i = icmp slt i32 %.omp.iv.02.i.i, %cond.i.i
  br i1 %cmp3.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__1.exit.i

__omp_outlined__1.exit.i:                         ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__1.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %84
  br i1 %cmp5.i, label %__omp_outlined__.exit, label %omp.inner.for.body.i

__omp_outlined__.exit:                            ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l66_worker.exit
  ret void
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64, i8* nocapture) #3

declare void @__kmpc_for_static_init_4(%struct.ident_t*, i32, i32, i32*, i32*, i32*, i32*, i32, i32) local_unnamed_addr

declare void @__kmpc_for_static_fini(%struct.ident_t*, i32) local_unnamed_addr

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64, i8* nocapture) #3

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__1_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %27 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %28 = icmp slt i32 %27, 3071
  %cond.i = select i1 %28, i32 %27, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp31.i = icmp sgt i32 %29, %cond.i
  br i1 %cmp31.i, label %__omp_outlined__1.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add19.i, %omp.inner.for.body.i ], [ %29, %entry ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %30 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %add7.i = fadd double %30, %31
  %arrayidx9.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx9.i, align 8, !tbaa !126
  %add10.i = fadd double %32, %add7.i
  store double %add10.i, double* %arrayidx9.i, align 8, !tbaa !126
  %33 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %arrayidx14.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx14.i, align 8, !tbaa !126
  %add15.i = fadd double %33, %34
  %arrayidx17.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx17.i, align 8, !tbaa !126
  %add18.i = fadd double %35, %add15.i
  store double %add18.i, double* %arrayidx17.i, align 8, !tbaa !126
  %add19.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp3.i = icmp slt i32 %.omp.iv.02.i, %cond.i
  br i1 %cmp3.i, label %omp.inner.for.body.i, label %__omp_outlined__1.exit

__omp_outlined__1.exit:                           ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

declare void @__kmpc_get_shared_variables(i8***) local_unnamed_addr

declare void @__kmpc_push_num_threads(%struct.ident_t*, i32, i32) local_unnamed_addr

declare void @__kmpc_push_proc_bind(%struct.ident_t*, i32, i32) local_unnamed_addr

declare void @__kmpc_kernel_prepare_parallel(i8*, i16) local_unnamed_addr

declare void @__kmpc_begin_sharing_variables(i8***, i64) local_unnamed_addr

declare void @__kmpc_end_sharing_variables() local_unnamed_addr

declare void @__kmpc_serialized_parallel(%struct.ident_t*, i32) local_unnamed_addr

declare void @__kmpc_end_serialized_parallel(%struct.ident_t*, i32) local_unnamed_addr

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l86([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l86_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__3_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 38, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__3_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__3_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !128
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !128

__omp_outlined__3_wrapper.exit.i:                 ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__3_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l86_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__2.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 2) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__3_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 38, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__3.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__3.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !128
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !128

__omp_outlined__3.exit.i:                         ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__3.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__2.exit, label %omp.inner.for.body.i

__omp_outlined__2.exit:                           ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__2.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l86_worker.exit
  ret void
}

declare void @__kmpc_dispatch_init_4(%struct.ident_t*, i32, i32, i32, i32, i32, i32) local_unnamed_addr

declare i32 @__kmpc_dispatch_next_4(%struct.ident_t*, i32, i32*, i32*, i32*, i32*) local_unnamed_addr

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__3_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 38, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__3.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__3.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !128
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !128
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !128

__omp_outlined__3.exit:                           ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l106([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l106_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__5_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 35, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__5_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__5_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !129
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !129

__omp_outlined__5_wrapper.exit.i:                 ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__5_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l106_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__4.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 2) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__5_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 35, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__5.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__5.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !129
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !129

__omp_outlined__5.exit.i:                         ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__5.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__4.exit, label %omp.inner.for.body.i

__omp_outlined__4.exit:                           ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__4.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l106_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__5_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 35, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__5.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__5.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !129
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !129
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !129

__omp_outlined__5.exit:                           ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l126([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l126_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__7_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 36, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__7_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__7_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !130
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !130

__omp_outlined__7_wrapper.exit.i:                 ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__7_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l126_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__6.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 2) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__7_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 36, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__7.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__7.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !130
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !130

__omp_outlined__7.exit.i:                         ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__7.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__6.exit, label %omp.inner.for.body.i

__omp_outlined__6.exit:                           ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__6.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l126_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__7_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 36, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__7.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__7.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !130
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !130
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !130

__omp_outlined__7.exit:                           ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l146([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l146_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__9_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 37, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__9_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__9_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !131
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !131

__omp_outlined__9_wrapper.exit.i:                 ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__9_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l146_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__8.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 2) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__9_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 37, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__9.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__9.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !131
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !131

__omp_outlined__9.exit.i:                         ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__9.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__8.exit, label %omp.inner.for.body.i

__omp_outlined__8.exit:                           ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__8.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l146_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__9_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 37, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__9.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__9.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !131
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !131
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !131

__omp_outlined__9.exit:                           ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l166([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l166_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__11_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %32 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %33 = icmp slt i32 %32, 3071
  %cond.i.i.i = select i1 %33, i32 %32, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp31.i.i.i = icmp sgt i32 %34, %cond.i.i.i
  br i1 %cmp31.i.i.i, label %__omp_outlined__11_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add19.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %35 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %add7.i.i.i = fadd double %35, %36
  %arrayidx9.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %add10.i.i.i = fadd double %37, %add7.i.i.i
  store double %add10.i.i.i, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %38 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %arrayidx14.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx14.i.i.i, align 8, !tbaa !126
  %add15.i.i.i = fadd double %38, %39
  %arrayidx17.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add18.i.i.i = fadd double %40, %add15.i.i.i
  store double %add18.i.i.i, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add19.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp3.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %cond.i.i.i
  br i1 %cmp3.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__11_wrapper.exit.i

__omp_outlined__11_wrapper.exit.i:                ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__11_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l166_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__10.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 2) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__11_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %74 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %75 = icmp slt i32 %74, 3071
  %cond.i.i = select i1 %75, i32 %74, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp31.i.i = icmp sgt i32 %76, %cond.i.i
  br i1 %cmp31.i.i, label %__omp_outlined__11.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add19.i.i, %omp.inner.for.body.i.i ], [ %76, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %77 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %add7.i.i = fadd double %77, %78
  %arrayidx9.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx9.i.i, align 8, !tbaa !126
  %add10.i.i = fadd double %79, %add7.i.i
  store double %add10.i.i, double* %arrayidx9.i.i, align 8, !tbaa !126
  %80 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %arrayidx14.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx14.i.i, align 8, !tbaa !126
  %add15.i.i = fadd double %80, %81
  %arrayidx17.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add18.i.i = fadd double %82, %add15.i.i
  store double %add18.i.i, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add19.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp3.i.i = icmp slt i32 %.omp.iv.02.i.i, %cond.i.i
  br i1 %cmp3.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__11.exit.i

__omp_outlined__11.exit.i:                        ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__11.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %84
  br i1 %cmp5.i, label %__omp_outlined__10.exit, label %omp.inner.for.body.i

__omp_outlined__10.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__10.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l166_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__11_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %27 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %28 = icmp slt i32 %27, 3071
  %cond.i = select i1 %28, i32 %27, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp31.i = icmp sgt i32 %29, %cond.i
  br i1 %cmp31.i, label %__omp_outlined__11.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add19.i, %omp.inner.for.body.i ], [ %29, %entry ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %30 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %add7.i = fadd double %30, %31
  %arrayidx9.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx9.i, align 8, !tbaa !126
  %add10.i = fadd double %32, %add7.i
  store double %add10.i, double* %arrayidx9.i, align 8, !tbaa !126
  %33 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %arrayidx14.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx14.i, align 8, !tbaa !126
  %add15.i = fadd double %33, %34
  %arrayidx17.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx17.i, align 8, !tbaa !126
  %add18.i = fadd double %35, %add15.i
  store double %add18.i, double* %arrayidx17.i, align 8, !tbaa !126
  %add19.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp3.i = icmp slt i32 %.omp.iv.02.i, %cond.i
  br i1 %cmp3.i, label %omp.inner.for.body.i, label %__omp_outlined__11.exit

__omp_outlined__11.exit:                          ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l196([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l196_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__13_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %32 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %33 = icmp slt i32 %32, 3071
  %cond.i.i.i = select i1 %33, i32 %32, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp31.i.i.i = icmp sgt i32 %34, %cond.i.i.i
  br i1 %cmp31.i.i.i, label %__omp_outlined__13_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add19.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %35 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %add7.i.i.i = fadd double %35, %36
  %arrayidx9.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %add10.i.i.i = fadd double %37, %add7.i.i.i
  store double %add10.i.i.i, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %38 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %arrayidx14.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx14.i.i.i, align 8, !tbaa !126
  %add15.i.i.i = fadd double %38, %39
  %arrayidx17.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add18.i.i.i = fadd double %40, %add15.i.i.i
  store double %add18.i.i.i, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add19.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp3.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %cond.i.i.i
  br i1 %cmp3.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__13_wrapper.exit.i

__omp_outlined__13_wrapper.exit.i:                ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__13_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l196_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__12.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 3) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__13_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %74 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %75 = icmp slt i32 %74, 3071
  %cond.i.i = select i1 %75, i32 %74, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp31.i.i = icmp sgt i32 %76, %cond.i.i
  br i1 %cmp31.i.i, label %__omp_outlined__13.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add19.i.i, %omp.inner.for.body.i.i ], [ %76, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %77 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %add7.i.i = fadd double %77, %78
  %arrayidx9.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx9.i.i, align 8, !tbaa !126
  %add10.i.i = fadd double %79, %add7.i.i
  store double %add10.i.i, double* %arrayidx9.i.i, align 8, !tbaa !126
  %80 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %arrayidx14.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx14.i.i, align 8, !tbaa !126
  %add15.i.i = fadd double %80, %81
  %arrayidx17.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add18.i.i = fadd double %82, %add15.i.i
  store double %add18.i.i, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add19.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp3.i.i = icmp slt i32 %.omp.iv.02.i.i, %cond.i.i
  br i1 %cmp3.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__13.exit.i

__omp_outlined__13.exit.i:                        ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__13.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %84
  br i1 %cmp5.i, label %__omp_outlined__12.exit, label %omp.inner.for.body.i

__omp_outlined__12.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__12.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l196_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__13_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %27 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %28 = icmp slt i32 %27, 3071
  %cond.i = select i1 %28, i32 %27, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp31.i = icmp sgt i32 %29, %cond.i
  br i1 %cmp31.i, label %__omp_outlined__13.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add19.i, %omp.inner.for.body.i ], [ %29, %entry ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %30 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %add7.i = fadd double %30, %31
  %arrayidx9.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx9.i, align 8, !tbaa !126
  %add10.i = fadd double %32, %add7.i
  store double %add10.i, double* %arrayidx9.i, align 8, !tbaa !126
  %33 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %arrayidx14.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx14.i, align 8, !tbaa !126
  %add15.i = fadd double %33, %34
  %arrayidx17.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx17.i, align 8, !tbaa !126
  %add18.i = fadd double %35, %add15.i
  store double %add18.i, double* %arrayidx17.i, align 8, !tbaa !126
  %add19.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp3.i = icmp slt i32 %.omp.iv.02.i, %cond.i
  br i1 %cmp3.i, label %omp.inner.for.body.i, label %__omp_outlined__13.exit

__omp_outlined__13.exit:                          ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l216([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l216_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__15_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 38, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__15_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__15_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !132
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !132

__omp_outlined__15_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__15_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l216_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__14.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 3) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__15_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 38, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__15.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__15.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !132
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !132

__omp_outlined__15.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__15.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__14.exit, label %omp.inner.for.body.i

__omp_outlined__14.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__14.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l216_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__15_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 38, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__15.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__15.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !132
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !132
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !132

__omp_outlined__15.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l236([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l236_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__17_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 35, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__17_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__17_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !133
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !133

__omp_outlined__17_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__17_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l236_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__16.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 3) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__17_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 35, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__17.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__17.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !133
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !133

__omp_outlined__17.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__17.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__16.exit, label %omp.inner.for.body.i

__omp_outlined__16.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__16.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l236_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__17_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 35, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__17.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__17.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !133
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !133
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !133

__omp_outlined__17.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l256([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l256_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__19_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 36, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__19_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__19_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !134
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !134

__omp_outlined__19_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__19_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l256_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__18.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 3) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__19_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 36, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__19.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__19.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !134
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !134

__omp_outlined__19.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__19.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__18.exit, label %omp.inner.for.body.i

__omp_outlined__18.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__18.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l256_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__19_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 36, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__19.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__19.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !134
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !134
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !134

__omp_outlined__19.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l276([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l276_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__21_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 37, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__21_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__21_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !135
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !135

__omp_outlined__21_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__21_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l276_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__20.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 3) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__21_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 37, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__21.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__21.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !135
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !135

__omp_outlined__21.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__21.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__20.exit, label %omp.inner.for.body.i

__omp_outlined__20.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__20.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l276_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__21_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 37, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__21.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__21.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !135
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !135
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !135

__omp_outlined__21.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l296([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l296_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__23_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %32 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %33 = icmp slt i32 %32, 3071
  %cond.i.i.i = select i1 %33, i32 %32, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp31.i.i.i = icmp sgt i32 %34, %cond.i.i.i
  br i1 %cmp31.i.i.i, label %__omp_outlined__23_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add19.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %35 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %add7.i.i.i = fadd double %35, %36
  %arrayidx9.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %add10.i.i.i = fadd double %37, %add7.i.i.i
  store double %add10.i.i.i, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %38 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %arrayidx14.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx14.i.i.i, align 8, !tbaa !126
  %add15.i.i.i = fadd double %38, %39
  %arrayidx17.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add18.i.i.i = fadd double %40, %add15.i.i.i
  store double %add18.i.i.i, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add19.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp3.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %cond.i.i.i
  br i1 %cmp3.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__23_wrapper.exit.i

__omp_outlined__23_wrapper.exit.i:                ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__23_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l296_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__22.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 3) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__23_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %74 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %75 = icmp slt i32 %74, 3071
  %cond.i.i = select i1 %75, i32 %74, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp31.i.i = icmp sgt i32 %76, %cond.i.i
  br i1 %cmp31.i.i, label %__omp_outlined__23.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add19.i.i, %omp.inner.for.body.i.i ], [ %76, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %77 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %add7.i.i = fadd double %77, %78
  %arrayidx9.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx9.i.i, align 8, !tbaa !126
  %add10.i.i = fadd double %79, %add7.i.i
  store double %add10.i.i, double* %arrayidx9.i.i, align 8, !tbaa !126
  %80 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %arrayidx14.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx14.i.i, align 8, !tbaa !126
  %add15.i.i = fadd double %80, %81
  %arrayidx17.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add18.i.i = fadd double %82, %add15.i.i
  store double %add18.i.i, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add19.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp3.i.i = icmp slt i32 %.omp.iv.02.i.i, %cond.i.i
  br i1 %cmp3.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__23.exit.i

__omp_outlined__23.exit.i:                        ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__23.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %84
  br i1 %cmp5.i, label %__omp_outlined__22.exit, label %omp.inner.for.body.i

__omp_outlined__22.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__22.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l296_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__23_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %27 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %28 = icmp slt i32 %27, 3071
  %cond.i = select i1 %28, i32 %27, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp31.i = icmp sgt i32 %29, %cond.i
  br i1 %cmp31.i, label %__omp_outlined__23.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add19.i, %omp.inner.for.body.i ], [ %29, %entry ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %30 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %add7.i = fadd double %30, %31
  %arrayidx9.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx9.i, align 8, !tbaa !126
  %add10.i = fadd double %32, %add7.i
  store double %add10.i, double* %arrayidx9.i, align 8, !tbaa !126
  %33 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %arrayidx14.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx14.i, align 8, !tbaa !126
  %add15.i = fadd double %33, %34
  %arrayidx17.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx17.i, align 8, !tbaa !126
  %add18.i = fadd double %35, %add15.i
  store double %add18.i, double* %arrayidx17.i, align 8, !tbaa !126
  %add19.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp3.i = icmp slt i32 %.omp.iv.02.i, %cond.i
  br i1 %cmp3.i, label %omp.inner.for.body.i, label %__omp_outlined__23.exit

__omp_outlined__23.exit:                          ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l326([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l326_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__25_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %32 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %33 = icmp slt i32 %32, 3071
  %cond.i.i.i = select i1 %33, i32 %32, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp31.i.i.i = icmp sgt i32 %34, %cond.i.i.i
  br i1 %cmp31.i.i.i, label %__omp_outlined__25_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add19.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %35 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %add7.i.i.i = fadd double %35, %36
  %arrayidx9.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %add10.i.i.i = fadd double %37, %add7.i.i.i
  store double %add10.i.i.i, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %38 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %arrayidx14.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx14.i.i.i, align 8, !tbaa !126
  %add15.i.i.i = fadd double %38, %39
  %arrayidx17.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add18.i.i.i = fadd double %40, %add15.i.i.i
  store double %add18.i.i.i, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add19.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp3.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %cond.i.i.i
  br i1 %cmp3.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__25_wrapper.exit.i

__omp_outlined__25_wrapper.exit.i:                ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__25_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l326_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__24.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 4) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__25_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %74 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %75 = icmp slt i32 %74, 3071
  %cond.i.i = select i1 %75, i32 %74, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp31.i.i = icmp sgt i32 %76, %cond.i.i
  br i1 %cmp31.i.i, label %__omp_outlined__25.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add19.i.i, %omp.inner.for.body.i.i ], [ %76, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %77 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %add7.i.i = fadd double %77, %78
  %arrayidx9.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx9.i.i, align 8, !tbaa !126
  %add10.i.i = fadd double %79, %add7.i.i
  store double %add10.i.i, double* %arrayidx9.i.i, align 8, !tbaa !126
  %80 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %arrayidx14.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx14.i.i, align 8, !tbaa !126
  %add15.i.i = fadd double %80, %81
  %arrayidx17.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add18.i.i = fadd double %82, %add15.i.i
  store double %add18.i.i, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add19.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp3.i.i = icmp slt i32 %.omp.iv.02.i.i, %cond.i.i
  br i1 %cmp3.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__25.exit.i

__omp_outlined__25.exit.i:                        ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__25.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %84
  br i1 %cmp5.i, label %__omp_outlined__24.exit, label %omp.inner.for.body.i

__omp_outlined__24.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__24.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l326_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__25_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %27 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %28 = icmp slt i32 %27, 3071
  %cond.i = select i1 %28, i32 %27, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp31.i = icmp sgt i32 %29, %cond.i
  br i1 %cmp31.i, label %__omp_outlined__25.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add19.i, %omp.inner.for.body.i ], [ %29, %entry ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %30 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %add7.i = fadd double %30, %31
  %arrayidx9.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx9.i, align 8, !tbaa !126
  %add10.i = fadd double %32, %add7.i
  store double %add10.i, double* %arrayidx9.i, align 8, !tbaa !126
  %33 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %arrayidx14.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx14.i, align 8, !tbaa !126
  %add15.i = fadd double %33, %34
  %arrayidx17.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx17.i, align 8, !tbaa !126
  %add18.i = fadd double %35, %add15.i
  store double %add18.i, double* %arrayidx17.i, align 8, !tbaa !126
  %add19.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp3.i = icmp slt i32 %.omp.iv.02.i, %cond.i
  br i1 %cmp3.i, label %omp.inner.for.body.i, label %__omp_outlined__25.exit

__omp_outlined__25.exit:                          ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l346([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l346_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__27_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 38, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__27_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__27_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !136
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !136

__omp_outlined__27_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__27_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l346_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__26.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 4) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__27_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 38, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__27.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__27.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !136
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !136

__omp_outlined__27.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__27.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__26.exit, label %omp.inner.for.body.i

__omp_outlined__26.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__26.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l346_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__27_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 38, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__27.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__27.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !136
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !136
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !136

__omp_outlined__27.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l366([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l366_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__29_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 35, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__29_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__29_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !137
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !137

__omp_outlined__29_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__29_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l366_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__28.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 4) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__29_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 35, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__29.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__29.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !137
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !137

__omp_outlined__29.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__29.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__28.exit, label %omp.inner.for.body.i

__omp_outlined__28.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__28.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l366_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__29_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 35, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__29.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__29.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !137
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !137
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !137

__omp_outlined__29.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l386([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l386_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__31_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 36, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__31_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__31_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !138
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !138

__omp_outlined__31_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__31_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l386_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__30.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 4) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__31_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 36, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__31.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__31.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !138
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !138

__omp_outlined__31.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__31.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__30.exit, label %omp.inner.for.body.i

__omp_outlined__30.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__30.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l386_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__31_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 36, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__31.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__31.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !138
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !138
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !138

__omp_outlined__31.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l406([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l406_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__33_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 37, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__33_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__33_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !139
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add5.i.i.i = fadd double %36, %37
  %arrayidx7.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add8.i.i.i = fadd double %38, %add5.i.i.i
  store double %add8.i.i.i, double* %arrayidx7.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %39 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add13.i.i.i = fadd double %39, %40
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add16.i.i.i = fadd double %41, %add13.i.i.i
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !139

__omp_outlined__33_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__33_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l406_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__32.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 4) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__33_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 37, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__33.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__33.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !139
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add5.i.i = fadd double %79, %80
  %arrayidx7.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add8.i.i = fadd double %81, %add5.i.i
  store double %add8.i.i, double* %arrayidx7.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %82 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add13.i.i = fadd double %82, %83
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add16.i.i = fadd double %84, %add13.i.i
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !139

__omp_outlined__33.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__33.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %86
  br i1 %cmp5.i, label %__omp_outlined__32.exit, label %omp.inner.for.body.i

__omp_outlined__32.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__32.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l406_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__33_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 37, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__33.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__33.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !139
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add5.i = fadd double %31, %32
  %arrayidx7.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add8.i = fadd double %33, %add5.i
  store double %add8.i, double* %arrayidx7.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %34 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add13.i = fadd double %34, %35
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add16.i = fadd double %36, %add13.i
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !139
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !139

__omp_outlined__33.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l426([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l426_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__35_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %32 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %33 = icmp slt i32 %32, 3071
  %cond.i.i.i = select i1 %33, i32 %32, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp31.i.i.i = icmp sgt i32 %34, %cond.i.i.i
  br i1 %cmp31.i.i.i, label %__omp_outlined__35_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add19.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %35 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %add7.i.i.i = fadd double %35, %36
  %arrayidx9.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %add10.i.i.i = fadd double %37, %add7.i.i.i
  store double %add10.i.i.i, double* %arrayidx9.i.i.i, align 8, !tbaa !126
  %38 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %arrayidx14.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx14.i.i.i, align 8, !tbaa !126
  %add15.i.i.i = fadd double %38, %39
  %arrayidx17.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add18.i.i.i = fadd double %40, %add15.i.i.i
  store double %add18.i.i.i, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add19.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp3.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %cond.i.i.i
  br i1 %cmp3.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__35_wrapper.exit.i

__omp_outlined__35_wrapper.exit.i:                ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__35_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l426_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__34.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @0, i32 %0, i32 4) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__35_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %74 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %75 = icmp slt i32 %74, 3071
  %cond.i.i = select i1 %75, i32 %74, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp31.i.i = icmp sgt i32 %76, %cond.i.i
  br i1 %cmp31.i.i, label %__omp_outlined__35.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add19.i.i, %omp.inner.for.body.i.i ], [ %76, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %77 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %add7.i.i = fadd double %77, %78
  %arrayidx9.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx9.i.i, align 8, !tbaa !126
  %add10.i.i = fadd double %79, %add7.i.i
  store double %add10.i.i, double* %arrayidx9.i.i, align 8, !tbaa !126
  %80 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %arrayidx14.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx14.i.i, align 8, !tbaa !126
  %add15.i.i = fadd double %80, %81
  %arrayidx17.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add18.i.i = fadd double %82, %add15.i.i
  store double %add18.i.i, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add19.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp3.i.i = icmp slt i32 %.omp.iv.02.i.i, %cond.i.i
  br i1 %cmp3.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__35.exit.i

__omp_outlined__35.exit.i:                        ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__35.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %84
  br i1 %cmp5.i, label %__omp_outlined__34.exit, label %omp.inner.for.body.i

__omp_outlined__34.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__34.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l426_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__35_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %27 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %28 = icmp slt i32 %27, 3071
  %cond.i = select i1 %28, i32 %27, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp31.i = icmp sgt i32 %29, %cond.i
  br i1 %cmp31.i, label %__omp_outlined__35.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add19.i, %omp.inner.for.body.i ], [ %29, %entry ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %30 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %add7.i = fadd double %30, %31
  %arrayidx9.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx9.i, align 8, !tbaa !126
  %add10.i = fadd double %32, %add7.i
  store double %add10.i, double* %arrayidx9.i, align 8, !tbaa !126
  %33 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %arrayidx14.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx14.i, align 8, !tbaa !126
  %add15.i = fadd double %33, %34
  %arrayidx17.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx17.i, align 8, !tbaa !126
  %add18.i = fadd double %35, %add15.i
  store double %add18.i, double* %arrayidx17.i, align 8, !tbaa !126
  %add19.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp3.i = icmp slt i32 %.omp.iv.02.i, %cond.i
  br i1 %cmp3.i, label %omp.inner.for.body.i, label %__omp_outlined__35.exit

__omp_outlined__35.exit:                          ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l461([3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l461_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__37_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %32 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %33 = icmp slt i32 %32, 3071
  %cond.i.i.i = select i1 %33, i32 %32, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp31.i.i.i = icmp sgt i32 %34, %cond.i.i.i
  br i1 %cmp31.i.i.i, label %__omp_outlined__37_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add19.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %35 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %add7.i.i.i = fadd double %35, %36
  %arrayidx11.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx11.i.i.i, align 8, !tbaa !126
  %add12.i.i.i = fadd double %36, %37
  %arrayidx14.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx14.i.i.i, align 8, !tbaa !126
  %add15.i.i.i = fadd double %add7.i.i.i, %38
  store double %add15.i.i.i, double* %arrayidx14.i.i.i, align 8, !tbaa !126
  %arrayidx17.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add18.i.i.i = fadd double %add12.i.i.i, %39
  store double %add18.i.i.i, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add19.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp3.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %cond.i.i.i
  br i1 %cmp3.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__37_wrapper.exit.i

__omp_outlined__37_wrapper.exit.i:                ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %40 = bitcast i8* %10 to void (i16, i32)*
  call void %40(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__37_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l461_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %41 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %41, -32
  %42 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %42, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %43 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %43)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %44 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %44) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %45 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %48 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %49 = icmp slt i32 %48, 3071
  %cond.i = select i1 %49, i32 %48, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %50, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__36.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %51 = bitcast i32* %.omp.lb.i.i to i8*
  %52 = bitcast i32* %.omp.ub.i.i to i8*
  %53 = bitcast i32* %.omp.stride.i.i to i8*
  %54 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %50, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %55 = load i32, i32* %.omp.comb.lb.i, align 4
  %56 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %57 = zext i32 %56 to i64
  %58 = zext i32 %55 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__37_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %59 = load i8**, i8*** %shared_arg_refs.i, align 8
  %60 = inttoptr i64 %58 to i8*
  store i8* %60, i8** %59, align 8, !tbaa !124
  %61 = getelementptr inbounds i8*, i8** %59, i64 1
  %62 = inttoptr i64 %57 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %59, i64 2
  %64 = bitcast i8** %63 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %64, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %59, i64 3
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %59, i64 4
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %59, i64 5
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %59, i64 6
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %51) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  store i32 %55, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %56, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %73 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %74 = icmp slt i32 %73, 3071
  %cond.i.i = select i1 %74, i32 %73, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %75 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp31.i.i = icmp sgt i32 %75, %cond.i.i
  br i1 %cmp31.i.i, label %__omp_outlined__37.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add19.i.i, %omp.inner.for.body.i.i ], [ %75, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %76 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %77 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %add7.i.i = fadd double %76, %77
  %arrayidx11.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx11.i.i, align 8, !tbaa !126
  %add12.i.i = fadd double %77, %78
  %arrayidx14.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx14.i.i, align 8, !tbaa !126
  %add15.i.i = fadd double %add7.i.i, %79
  store double %add15.i.i, double* %arrayidx14.i.i, align 8, !tbaa !126
  %arrayidx17.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add18.i.i = fadd double %add12.i.i, %80
  store double %add18.i.i, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add19.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp3.i.i = icmp slt i32 %.omp.iv.02.i.i, %cond.i.i
  br i1 %cmp3.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__37.exit.i

__omp_outlined__37.exit.i:                        ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %51) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__37.exit.i, %omp_if.then.i
  %81 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %81, %.omp.iv.03.i
  %82 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %82
  br i1 %cmp5.i, label %__omp_outlined__36.exit, label %omp.inner.for.body.i

__omp_outlined__36.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %44) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %43)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__36.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l461_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__37_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %27 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %28 = icmp slt i32 %27, 3071
  %cond.i = select i1 %28, i32 %27, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp31.i = icmp sgt i32 %29, %cond.i
  br i1 %cmp31.i, label %__omp_outlined__37.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add19.i, %omp.inner.for.body.i ], [ %29, %entry ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %30 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %add7.i = fadd double %30, %31
  %arrayidx11.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx11.i, align 8, !tbaa !126
  %add12.i = fadd double %31, %32
  %arrayidx14.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx14.i, align 8, !tbaa !126
  %add15.i = fadd double %add7.i, %33
  store double %add15.i, double* %arrayidx14.i, align 8, !tbaa !126
  %arrayidx17.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx17.i, align 8, !tbaa !126
  %add18.i = fadd double %add12.i, %34
  store double %add18.i, double* %arrayidx17.i, align 8, !tbaa !126
  %add19.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp3.i = icmp slt i32 %.omp.iv.02.i, %cond.i
  br i1 %cmp3.i, label %omp.inner.for.body.i, label %__omp_outlined__37.exit

__omp_outlined__37.exit:                          ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l484([3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l484_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__39_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 38, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__39_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__39_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !140
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add5.i.i.i = fadd double %36, %37
  %arrayidx9.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx9.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add10.i.i.i = fadd double %37, %38
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add13.i.i.i = fadd double %add5.i.i.i, %39
  store double %add13.i.i.i, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add16.i.i.i = fadd double %add10.i.i.i, %40
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !140

__omp_outlined__39_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__39_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l484_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__38.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__39_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 38, i32 %56, i32 %57, i32 1, i32 1) #4
  %74 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %74, 0
  br i1 %tobool3.i.i, label %__omp_outlined__39.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %75, 0
  br i1 %tobool.i.i, label %__omp_outlined__39.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %77 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !140
  %cmp1.i.i = icmp sgt i32 %76, %77
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %76, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add5.i.i = fadd double %78, %79
  %arrayidx9.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx9.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add10.i.i = fadd double %79, %80
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add13.i.i = fadd double %add5.i.i, %81
  store double %add13.i.i, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add16.i.i = fadd double %add10.i.i, %82
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %77
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !140

__omp_outlined__39.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__39.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %84
  br i1 %cmp5.i, label %__omp_outlined__38.exit, label %omp.inner.for.body.i

__omp_outlined__38.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__38.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l484_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__39_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 38, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__39.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__39.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !140
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add5.i = fadd double %31, %32
  %arrayidx9.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx9.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add10.i = fadd double %32, %33
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add13.i = fadd double %add5.i, %34
  store double %add13.i, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add16.i = fadd double %add10.i, %35
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !140
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !140

__omp_outlined__39.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l507([3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l507_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__41_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 35, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__41_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__41_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !141
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add5.i.i.i = fadd double %36, %37
  %arrayidx9.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx9.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add10.i.i.i = fadd double %37, %38
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add13.i.i.i = fadd double %add5.i.i.i, %39
  store double %add13.i.i.i, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add16.i.i.i = fadd double %add10.i.i.i, %40
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !141

__omp_outlined__41_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__41_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l507_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__40.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__41_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 35, i32 %56, i32 %57, i32 1, i32 1) #4
  %74 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %74, 0
  br i1 %tobool3.i.i, label %__omp_outlined__41.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %75, 0
  br i1 %tobool.i.i, label %__omp_outlined__41.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %77 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !141
  %cmp1.i.i = icmp sgt i32 %76, %77
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %76, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add5.i.i = fadd double %78, %79
  %arrayidx9.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx9.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add10.i.i = fadd double %79, %80
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add13.i.i = fadd double %add5.i.i, %81
  store double %add13.i.i, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add16.i.i = fadd double %add10.i.i, %82
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %77
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !141

__omp_outlined__41.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__41.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %84
  br i1 %cmp5.i, label %__omp_outlined__40.exit, label %omp.inner.for.body.i

__omp_outlined__40.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__40.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l507_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__41_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 35, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__41.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__41.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !141
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add5.i = fadd double %31, %32
  %arrayidx9.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx9.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add10.i = fadd double %32, %33
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add13.i = fadd double %add5.i, %34
  store double %add13.i, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add16.i = fadd double %add10.i, %35
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !141
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !141

__omp_outlined__41.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l530([3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l530_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__43_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 36, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__43_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__43_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !142
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add5.i.i.i = fadd double %36, %37
  %arrayidx9.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx9.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add10.i.i.i = fadd double %37, %38
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add13.i.i.i = fadd double %add5.i.i.i, %39
  store double %add13.i.i.i, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add16.i.i.i = fadd double %add10.i.i.i, %40
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !142

__omp_outlined__43_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__43_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l530_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__42.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__43_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 36, i32 %56, i32 %57, i32 1, i32 1) #4
  %74 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %74, 0
  br i1 %tobool3.i.i, label %__omp_outlined__43.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %75, 0
  br i1 %tobool.i.i, label %__omp_outlined__43.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %77 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !142
  %cmp1.i.i = icmp sgt i32 %76, %77
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %76, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add5.i.i = fadd double %78, %79
  %arrayidx9.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx9.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add10.i.i = fadd double %79, %80
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add13.i.i = fadd double %add5.i.i, %81
  store double %add13.i.i, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add16.i.i = fadd double %add10.i.i, %82
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %77
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !142

__omp_outlined__43.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__43.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %84
  br i1 %cmp5.i, label %__omp_outlined__42.exit, label %omp.inner.for.body.i

__omp_outlined__42.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__42.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l530_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__43_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 36, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__43.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__43.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !142
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add5.i = fadd double %31, %32
  %arrayidx9.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx9.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add10.i = fadd double %32, %33
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add13.i = fadd double %add5.i, %34
  store double %add13.i, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add16.i = fadd double %add10.i, %35
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !142
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !142

__omp_outlined__43.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l553([3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l553_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__45_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 37, i32 %conv.i.i.i, i32 %conv1.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__45_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__45_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !143
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add17.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %arrayidx4.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx4.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add5.i.i.i = fadd double %36, %37
  %arrayidx9.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx9.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add10.i.i.i = fadd double %37, %38
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add13.i.i.i = fadd double %add5.i.i.i, %39
  store double %add13.i.i.i, double* %arrayidx12.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add16.i.i.i = fadd double %add10.i.i.i, %40
  store double %add16.i.i.i, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add17.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !143

__omp_outlined__45_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__45_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l553_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__44.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__45_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 37, i32 %56, i32 %57, i32 1, i32 1) #4
  %74 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %74, 0
  br i1 %tobool3.i.i, label %__omp_outlined__45.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %75, 0
  br i1 %tobool.i.i, label %__omp_outlined__45.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %77 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !143
  %cmp1.i.i = icmp sgt i32 %76, %77
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add17.i.i, %omp.inner.for.body.i.i ], [ %76, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %arrayidx4.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx4.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add5.i.i = fadd double %78, %79
  %arrayidx9.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx9.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add10.i.i = fadd double %79, %80
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add13.i.i = fadd double %add5.i.i, %81
  store double %add13.i.i, double* %arrayidx12.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add16.i.i = fadd double %add10.i.i, %82
  store double %add16.i.i, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add17.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %77
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !143

__omp_outlined__45.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__45.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %84
  br i1 %cmp5.i, label %__omp_outlined__44.exit, label %omp.inner.for.body.i

__omp_outlined__44.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__44.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l553_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__45_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 37, i32 %conv.i, i32 %conv1.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__45.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__45.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !143
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add17.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %arrayidx4.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx4.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add5.i = fadd double %31, %32
  %arrayidx9.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx9.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add10.i = fadd double %32, %33
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add13.i = fadd double %add5.i, %34
  store double %add13.i, double* %arrayidx12.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add16.i = fadd double %add10.i, %35
  store double %add16.i, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !143
  %add17.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !143

__omp_outlined__45.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l576([3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l576_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__47_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv1.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv1.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %32 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %33 = icmp slt i32 %32, 3071
  %cond.i.i.i = select i1 %33, i32 %32, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp31.i.i.i = icmp sgt i32 %34, %cond.i.i.i
  br i1 %cmp31.i.i.i, label %__omp_outlined__47_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add19.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %35 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126
  %add7.i.i.i = fadd double %35, %36
  %arrayidx11.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx11.i.i.i, align 8, !tbaa !126
  %add12.i.i.i = fadd double %36, %37
  %arrayidx14.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx14.i.i.i, align 8, !tbaa !126
  %add15.i.i.i = fadd double %add7.i.i.i, %38
  store double %add15.i.i.i, double* %arrayidx14.i.i.i, align 8, !tbaa !126
  %arrayidx17.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add18.i.i.i = fadd double %add12.i.i.i, %39
  store double %add18.i.i.i, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add19.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp3.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %cond.i.i.i
  br i1 %cmp3.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__47_wrapper.exit.i

__omp_outlined__47_wrapper.exit.i:                ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %40 = bitcast i8* %10 to void (i16, i32)*
  call void %40(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__47_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l576_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %41 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %41, -32
  %42 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %42, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %43 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %43)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %44 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %44) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %45 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %48 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %49 = icmp slt i32 %48, 3071
  %cond.i = select i1 %49, i32 %48, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %50, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__46.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %51 = bitcast i32* %.omp.lb.i.i to i8*
  %52 = bitcast i32* %.omp.ub.i.i to i8*
  %53 = bitcast i32* %.omp.stride.i.i to i8*
  %54 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %50, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %55 = load i32, i32* %.omp.comb.lb.i, align 4
  %56 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %57 = zext i32 %56 to i64
  %58 = zext i32 %55 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__47_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %59 = load i8**, i8*** %shared_arg_refs.i, align 8
  %60 = inttoptr i64 %58 to i8*
  store i8* %60, i8** %59, align 8, !tbaa !124
  %61 = getelementptr inbounds i8*, i8** %59, i64 1
  %62 = inttoptr i64 %57 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %59, i64 2
  %64 = bitcast i8** %63 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %64, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %59, i64 3
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %59, i64 4
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %59, i64 5
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %59, i64 6
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %51) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  store i32 %55, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %56, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %73 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %74 = icmp slt i32 %73, 3071
  %cond.i.i = select i1 %74, i32 %73, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %75 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp31.i.i = icmp sgt i32 %75, %cond.i.i
  br i1 %cmp31.i.i, label %__omp_outlined__47.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add19.i.i, %omp.inner.for.body.i.i ], [ %75, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %76 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %77 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126
  %add7.i.i = fadd double %76, %77
  %arrayidx11.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx11.i.i, align 8, !tbaa !126
  %add12.i.i = fadd double %77, %78
  %arrayidx14.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx14.i.i, align 8, !tbaa !126
  %add15.i.i = fadd double %add7.i.i, %79
  store double %add15.i.i, double* %arrayidx14.i.i, align 8, !tbaa !126
  %arrayidx17.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add18.i.i = fadd double %add12.i.i, %80
  store double %add18.i.i, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add19.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp3.i.i = icmp slt i32 %.omp.iv.02.i.i, %cond.i.i
  br i1 %cmp3.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__47.exit.i

__omp_outlined__47.exit.i:                        ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %51) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__47.exit.i, %omp_if.then.i
  %81 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %81, %.omp.iv.03.i
  %82 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp5.i = icmp sgt i32 %add.i, %82
  br i1 %cmp5.i, label %__omp_outlined__46.exit, label %omp.inner.for.body.i

__omp_outlined__46.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %44) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %43)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__46.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l576_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__47_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv1.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv1.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %27 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %28 = icmp slt i32 %27, 3071
  %cond.i = select i1 %28, i32 %27, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp31.i = icmp sgt i32 %29, %cond.i
  br i1 %cmp31.i, label %__omp_outlined__47.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add19.i, %omp.inner.for.body.i ], [ %29, %entry ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %30 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx6.i, align 8, !tbaa !126
  %add7.i = fadd double %30, %31
  %arrayidx11.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx11.i, align 8, !tbaa !126
  %add12.i = fadd double %31, %32
  %arrayidx14.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx14.i, align 8, !tbaa !126
  %add15.i = fadd double %add7.i, %33
  store double %add15.i, double* %arrayidx14.i, align 8, !tbaa !126
  %arrayidx17.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx17.i, align 8, !tbaa !126
  %add18.i = fadd double %add12.i, %34
  store double %add18.i, double* %arrayidx17.i, align 8, !tbaa !126
  %add19.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp3.i = icmp slt i32 %.omp.iv.02.i, %cond.i
  br i1 %cmp3.i, label %omp.inner.for.body.i, label %__omp_outlined__47.exit

__omp_outlined__47.exit:                          ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l612([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l612_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__49_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  %32 = getelementptr inbounds i8*, i8** %11, i64 7
  %33 = bitcast i8** %32 to [3072 x double]**
  %34 = load [3072 x double]*, [3072 x double]** %33, align 8, !tbaa !124
  %35 = getelementptr inbounds i8*, i8** %11, i64 8
  %36 = bitcast i8** %35 to i64*
  %37 = load i64, i64* %36, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv2.i.i.i = trunc i64 %13 to i32
  %conv3.i.i.i = trunc i64 %16 to i32
  store i32 %conv2.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv3.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %38 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %39 = icmp slt i32 %38, 3071
  %cond.i.i.i = select i1 %39, i32 %38, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %40 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp52.i.i.i = icmp sgt i32 %40, %cond.i.i.i
  br i1 %cmp52.i.i.i, label %__omp_outlined__49_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %p.addr.05.i.i.i = phi i64 [ %spec.select1.i.i.i, %omp.inner.for.body.i.i.i ], [ %28, %.execute.fn.i ]
  %.omp.iv.04.i.i.i = phi i32 [ %add27.i.i.i, %omp.inner.for.body.i.i.i ], [ %40, %.execute.fn.i ]
  %q.addr.03.i.i.i = phi i64 [ %spec.select.i.i.i, %omp.inner.for.body.i.i.i ], [ %37, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.04.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx8.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %42 = load double, double* %arrayidx8.i.i.i, align 8, !tbaa !126
  %add9.i.i.i = fadd double %41, %42
  %43 = bitcast i64 %p.addr.05.i.i.i to double
  %add10.i.i.i = fadd double %add9.i.i.i, %43
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %44 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126
  %add13.i.i.i = fadd double %44, %add10.i.i.i
  store double %add13.i.i.i, double* %arrayidx12.i.i.i, align 8, !tbaa !126
  %45 = load double, double* %arrayidx8.i.i.i, align 8, !tbaa !126
  %arrayidx17.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %34, i64 0, i64 %idxprom.i.i.i
  %46 = load double, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add18.i.i.i = fadd double %45, %46
  %47 = bitcast i64 %q.addr.03.i.i.i to double
  %add19.i.i.i = fadd double %add18.i.i.i, %47
  %arrayidx21.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %48 = load double, double* %arrayidx21.i.i.i, align 8, !tbaa !126
  %add22.i.i.i = fadd double %48, %add19.i.i.i
  store double %add22.i.i.i, double* %arrayidx21.i.i.i, align 8, !tbaa !126
  %cmp23.i.i.i = icmp eq i32 %.omp.iv.04.i.i.i, 3071
  %add25.i.i.i = fadd double %43, 6.000000e+00
  %49 = bitcast double %add25.i.i.i to i64
  %add26.i.i.i = fadd double %47, 9.000000e+00
  %50 = bitcast double %add26.i.i.i to i64
  %spec.select.i.i.i = select i1 %cmp23.i.i.i, i64 %50, i64 %q.addr.03.i.i.i
  %spec.select1.i.i.i = select i1 %cmp23.i.i.i, i64 %49, i64 %p.addr.05.i.i.i
  %add27.i.i.i = add nsw i32 %.omp.iv.04.i.i.i, 1
  %cmp5.i.i.i = icmp slt i32 %.omp.iv.04.i.i.i, %cond.i.i.i
  br i1 %cmp5.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__49_wrapper.exit.i

__omp_outlined__49_wrapper.exit.i:                ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %51 = bitcast i8* %10 to void (i16, i32)*
  call void %51(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__49_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l612_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %52 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %52, -32
  %53 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %53, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %54 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %54)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %55 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %56 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %57 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %57) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %58 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %58) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %59 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %60 = icmp slt i32 %59, 3071
  %cond.i = select i1 %60, i32 %59, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %61 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp72.i = icmp sgt i32 %61, %cond.i
  br i1 %cmp72.i, label %__omp_outlined__48.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %62 = bitcast i32* %.omp.lb.i.i to i8*
  %63 = bitcast i32* %.omp.ub.i.i to i8*
  %64 = bitcast i32* %.omp.stride.i.i to i8*
  %65 = bitcast i32* %.omp.is_last.i.i to i8*
  %66 = inttoptr i64 %p to i8*
  %67 = inttoptr i64 %q to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %61, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %68 = load i32, i32* %.omp.comb.lb.i, align 4
  %69 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %70 = zext i32 %69 to i64
  %71 = zext i32 %68 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__49_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 9) #4
  %72 = load i8**, i8*** %shared_arg_refs.i, align 8
  %73 = inttoptr i64 %71 to i8*
  store i8* %73, i8** %72, align 8, !tbaa !124
  %74 = getelementptr inbounds i8*, i8** %72, i64 1
  %75 = inttoptr i64 %70 to i8*
  store i8* %75, i8** %74, align 8, !tbaa !124
  %76 = getelementptr inbounds i8*, i8** %72, i64 2
  %77 = bitcast i8** %76 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %77, align 8, !tbaa !124
  %78 = getelementptr inbounds i8*, i8** %72, i64 3
  %79 = bitcast i8** %78 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %79, align 8, !tbaa !124
  %80 = getelementptr inbounds i8*, i8** %72, i64 4
  %81 = bitcast i8** %80 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %81, align 8, !tbaa !124
  %82 = getelementptr inbounds i8*, i8** %72, i64 5
  store i8* %66, i8** %82, align 8, !tbaa !124
  %83 = getelementptr inbounds i8*, i8** %72, i64 6
  %84 = bitcast i8** %83 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %84, align 8, !tbaa !124
  %85 = getelementptr inbounds i8*, i8** %72, i64 7
  %86 = bitcast i8** %85 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %86, align 8, !tbaa !124
  %87 = getelementptr inbounds i8*, i8** %72, i64 8
  store i8* %67, i8** %87, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %62) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %63) #4
  store i32 %68, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %69, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %64) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %65) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %88 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %89 = icmp slt i32 %88, 3071
  %cond.i.i = select i1 %89, i32 %88, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %90 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp52.i.i = icmp sgt i32 %90, %cond.i.i
  br i1 %cmp52.i.i, label %__omp_outlined__49.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %p.addr.05.i.i = phi i64 [ %spec.select1.i.i, %omp.inner.for.body.i.i ], [ %p, %omp_if.else.i ]
  %.omp.iv.04.i.i = phi i32 [ %add27.i.i, %omp.inner.for.body.i.i ], [ %90, %omp_if.else.i ]
  %q.addr.03.i.i = phi i64 [ %spec.select.i.i, %omp.inner.for.body.i.i ], [ %q, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.04.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %91 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx8.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %92 = load double, double* %arrayidx8.i.i, align 8, !tbaa !126
  %add9.i.i = fadd double %91, %92
  %93 = bitcast i64 %p.addr.05.i.i to double
  %add10.i.i = fadd double %add9.i.i, %93
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %94 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126
  %add13.i.i = fadd double %94, %add10.i.i
  store double %add13.i.i, double* %arrayidx12.i.i, align 8, !tbaa !126
  %95 = load double, double* %arrayidx8.i.i, align 8, !tbaa !126
  %arrayidx17.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %96 = load double, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add18.i.i = fadd double %95, %96
  %97 = bitcast i64 %q.addr.03.i.i to double
  %add19.i.i = fadd double %add18.i.i, %97
  %arrayidx21.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %98 = load double, double* %arrayidx21.i.i, align 8, !tbaa !126
  %add22.i.i = fadd double %98, %add19.i.i
  store double %add22.i.i, double* %arrayidx21.i.i, align 8, !tbaa !126
  %cmp23.i.i = icmp eq i32 %.omp.iv.04.i.i, 3071
  %add25.i.i = fadd double %93, 6.000000e+00
  %99 = bitcast double %add25.i.i to i64
  %add26.i.i = fadd double %97, 9.000000e+00
  %100 = bitcast double %add26.i.i to i64
  %spec.select.i.i = select i1 %cmp23.i.i, i64 %100, i64 %q.addr.03.i.i
  %spec.select1.i.i = select i1 %cmp23.i.i, i64 %99, i64 %p.addr.05.i.i
  %add27.i.i = add nsw i32 %.omp.iv.04.i.i, 1
  %cmp5.i.i = icmp slt i32 %.omp.iv.04.i.i, %cond.i.i
  br i1 %cmp5.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__49.exit.i

__omp_outlined__49.exit.i:                        ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %65) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %64) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %63) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %62) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__49.exit.i, %omp_if.then.i
  %101 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %101, %.omp.iv.03.i
  %102 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp7.i = icmp sgt i32 %add.i, %102
  br i1 %cmp7.i, label %__omp_outlined__48.exit, label %omp.inner.for.body.i

__omp_outlined__48.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %58) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %57) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %54)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__48.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l612_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__49_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to i64*
  %19 = load i64, i64* %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %2, i64 7
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %2, i64 8
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %29) #4
  %30 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %30) #4
  %conv2.i = trunc i64 %4 to i32
  %conv3.i = trunc i64 %7 to i32
  store i32 %conv2.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv3.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %31 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %31) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %32 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %32) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %33 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %34 = icmp slt i32 %33, 3071
  %cond.i = select i1 %34, i32 %33, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %35, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__49.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %p.addr.05.i = phi i64 [ %spec.select1.i, %omp.inner.for.body.i ], [ %19, %entry ]
  %.omp.iv.04.i = phi i32 [ %add27.i, %omp.inner.for.body.i ], [ %35, %entry ]
  %q.addr.03.i = phi i64 [ %spec.select.i, %omp.inner.for.body.i ], [ %28, %entry ]
  %idxprom.i = sext i32 %.omp.iv.04.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx8.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %37 = load double, double* %arrayidx8.i, align 8, !tbaa !126
  %add9.i = fadd double %36, %37
  %38 = bitcast i64 %p.addr.05.i to double
  %add10.i = fadd double %add9.i, %38
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %39 = load double, double* %arrayidx12.i, align 8, !tbaa !126
  %add13.i = fadd double %39, %add10.i
  store double %add13.i, double* %arrayidx12.i, align 8, !tbaa !126
  %40 = load double, double* %arrayidx8.i, align 8, !tbaa !126
  %arrayidx17.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i
  %41 = load double, double* %arrayidx17.i, align 8, !tbaa !126
  %add18.i = fadd double %40, %41
  %42 = bitcast i64 %q.addr.03.i to double
  %add19.i = fadd double %add18.i, %42
  %arrayidx21.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %43 = load double, double* %arrayidx21.i, align 8, !tbaa !126
  %add22.i = fadd double %43, %add19.i
  store double %add22.i, double* %arrayidx21.i, align 8, !tbaa !126
  %cmp23.i = icmp eq i32 %.omp.iv.04.i, 3071
  %add25.i = fadd double %38, 6.000000e+00
  %44 = bitcast double %add25.i to i64
  %add26.i = fadd double %42, 9.000000e+00
  %45 = bitcast double %add26.i to i64
  %spec.select.i = select i1 %cmp23.i, i64 %45, i64 %q.addr.03.i
  %spec.select1.i = select i1 %cmp23.i, i64 %44, i64 %p.addr.05.i
  %add27.i = add nsw i32 %.omp.iv.04.i, 1
  %cmp5.i = icmp slt i32 %.omp.iv.04.i, %cond.i
  br i1 %cmp5.i, label %omp.inner.for.body.i, label %__omp_outlined__49.exit

__omp_outlined__49.exit:                          ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %32) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %31) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %30) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %29) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l637([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l637_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__51_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  %32 = getelementptr inbounds i8*, i8** %11, i64 7
  %33 = bitcast i8** %32 to [3072 x double]**
  %34 = load [3072 x double]*, [3072 x double]** %33, align 8, !tbaa !124
  %35 = getelementptr inbounds i8*, i8** %11, i64 8
  %36 = bitcast i8** %35 to i64*
  %37 = load i64, i64* %36, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv2.i.i.i = trunc i64 %13 to i32
  %conv3.i.i.i = trunc i64 %16 to i32
  store i32 %conv2.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv3.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 38, i32 %conv2.i.i.i, i32 %conv3.i.i.i, i32 1, i32 1) #4
  %38 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool7.i.i.i = icmp eq i32 %38, 0
  br i1 %tobool7.i.i.i, label %__omp_outlined__51_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %q.addr.1.lcssa.i.i.i = phi i64 [ %q.addr.08.i.i.i, %omp.dispatch.body.i.i.i ], [ %spec.select.i.i.i, %omp.inner.for.body.i.i.i ]
  %p.addr.1.lcssa.i.i.i = phi i64 [ %p.addr.09.i.i.i, %omp.dispatch.body.i.i.i ], [ %spec.select1.i.i.i, %omp.inner.for.body.i.i.i ]
  %39 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %39, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__51_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %p.addr.09.i.i.i = phi i64 [ %p.addr.1.lcssa.i.i.i, %omp.dispatch.cond.loopexit.i.i.i ], [ %28, %.execute.fn.i ]
  %q.addr.08.i.i.i = phi i64 [ %q.addr.1.lcssa.i.i.i, %omp.dispatch.cond.loopexit.i.i.i ], [ %37, %.execute.fn.i ]
  %40 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %41 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !144
  %cmp2.i.i.i = icmp sgt i32 %40, %41
  br i1 %cmp2.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %p.addr.15.i.i.i = phi i64 [ %spec.select1.i.i.i, %omp.inner.for.body.i.i.i ], [ %p.addr.09.i.i.i, %omp.dispatch.body.i.i.i ]
  %.omp.iv.04.i.i.i = phi i32 [ %add25.i.i.i, %omp.inner.for.body.i.i.i ], [ %40, %omp.dispatch.body.i.i.i ]
  %q.addr.13.i.i.i = phi i64 [ %spec.select.i.i.i, %omp.inner.for.body.i.i.i ], [ %q.addr.08.i.i.i, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.04.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %42 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %43 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add7.i.i.i = fadd double %42, %43
  %44 = bitcast i64 %p.addr.15.i.i.i to double
  %add8.i.i.i = fadd double %add7.i.i.i, %44
  %arrayidx10.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %45 = load double, double* %arrayidx10.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add11.i.i.i = fadd double %45, %add8.i.i.i
  store double %add11.i.i.i, double* %arrayidx10.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %46 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %34, i64 0, i64 %idxprom.i.i.i
  %47 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add16.i.i.i = fadd double %46, %47
  %48 = bitcast i64 %q.addr.13.i.i.i to double
  %add17.i.i.i = fadd double %add16.i.i.i, %48
  %arrayidx19.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %49 = load double, double* %arrayidx19.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add20.i.i.i = fadd double %49, %add17.i.i.i
  store double %add20.i.i.i, double* %arrayidx19.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %cmp21.i.i.i = icmp eq i32 %.omp.iv.04.i.i.i, 3071
  %add23.i.i.i = fadd double %44, 6.000000e+00
  %50 = bitcast double %add23.i.i.i to i64
  %add24.i.i.i = fadd double %48, 9.000000e+00
  %51 = bitcast double %add24.i.i.i to i64
  %spec.select.i.i.i = select i1 %cmp21.i.i.i, i64 %51, i64 %q.addr.13.i.i.i
  %spec.select1.i.i.i = select i1 %cmp21.i.i.i, i64 %50, i64 %p.addr.15.i.i.i
  %add25.i.i.i = add nsw i32 %.omp.iv.04.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.04.i.i.i, %41
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !144

__omp_outlined__51_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %52 = bitcast i8* %10 to void (i16, i32)*
  call void %52(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__51_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l637_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %53 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %53, -32
  %54 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %54, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %55 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %55)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %56 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %57 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %57) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %58 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %58) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %59 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %59) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %60 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %61 = icmp slt i32 %60, 3071
  %cond.i = select i1 %61, i32 %60, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %62 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp72.i = icmp sgt i32 %62, %cond.i
  br i1 %cmp72.i, label %__omp_outlined__50.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %63 = bitcast i32* %.omp.lb.i.i to i8*
  %64 = bitcast i32* %.omp.ub.i.i to i8*
  %65 = bitcast i32* %.omp.stride.i.i to i8*
  %66 = bitcast i32* %.omp.is_last.i.i to i8*
  %67 = inttoptr i64 %p to i8*
  %68 = inttoptr i64 %q to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %62, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %69 = load i32, i32* %.omp.comb.lb.i, align 4
  %70 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %71 = zext i32 %70 to i64
  %72 = zext i32 %69 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__51_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 9) #4
  %73 = load i8**, i8*** %shared_arg_refs.i, align 8
  %74 = inttoptr i64 %72 to i8*
  store i8* %74, i8** %73, align 8, !tbaa !124
  %75 = getelementptr inbounds i8*, i8** %73, i64 1
  %76 = inttoptr i64 %71 to i8*
  store i8* %76, i8** %75, align 8, !tbaa !124
  %77 = getelementptr inbounds i8*, i8** %73, i64 2
  %78 = bitcast i8** %77 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %78, align 8, !tbaa !124
  %79 = getelementptr inbounds i8*, i8** %73, i64 3
  %80 = bitcast i8** %79 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %80, align 8, !tbaa !124
  %81 = getelementptr inbounds i8*, i8** %73, i64 4
  %82 = bitcast i8** %81 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %82, align 8, !tbaa !124
  %83 = getelementptr inbounds i8*, i8** %73, i64 5
  store i8* %67, i8** %83, align 8, !tbaa !124
  %84 = getelementptr inbounds i8*, i8** %73, i64 6
  %85 = bitcast i8** %84 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %85, align 8, !tbaa !124
  %86 = getelementptr inbounds i8*, i8** %73, i64 7
  %87 = bitcast i8** %86 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %87, align 8, !tbaa !124
  %88 = getelementptr inbounds i8*, i8** %73, i64 8
  store i8* %68, i8** %88, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %63) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %64) #4
  store i32 %69, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %70, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %65) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %66) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 38, i32 %69, i32 %70, i32 1, i32 1) #4
  %89 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool7.i.i = icmp eq i32 %89, 0
  br i1 %tobool7.i.i, label %__omp_outlined__51.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %q.addr.1.lcssa.i.i = phi i64 [ %q.addr.08.i.i, %omp.dispatch.body.i.i ], [ %spec.select.i.i, %omp.inner.for.body.i.i ]
  %p.addr.1.lcssa.i.i = phi i64 [ %p.addr.09.i.i, %omp.dispatch.body.i.i ], [ %spec.select1.i.i, %omp.inner.for.body.i.i ]
  %90 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %90, 0
  br i1 %tobool.i.i, label %__omp_outlined__51.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %p.addr.09.i.i = phi i64 [ %p.addr.1.lcssa.i.i, %omp.dispatch.cond.loopexit.i.i ], [ %p, %omp_if.else.i ]
  %q.addr.08.i.i = phi i64 [ %q.addr.1.lcssa.i.i, %omp.dispatch.cond.loopexit.i.i ], [ %q, %omp_if.else.i ]
  %91 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %92 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !144
  %cmp2.i.i = icmp sgt i32 %91, %92
  br i1 %cmp2.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %p.addr.15.i.i = phi i64 [ %spec.select1.i.i, %omp.inner.for.body.i.i ], [ %p.addr.09.i.i, %omp.dispatch.body.i.i ]
  %.omp.iv.04.i.i = phi i32 [ %add25.i.i, %omp.inner.for.body.i.i ], [ %91, %omp.dispatch.body.i.i ]
  %q.addr.13.i.i = phi i64 [ %spec.select.i.i, %omp.inner.for.body.i.i ], [ %q.addr.08.i.i, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.04.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %93 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %94 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add7.i.i = fadd double %93, %94
  %95 = bitcast i64 %p.addr.15.i.i to double
  %add8.i.i = fadd double %add7.i.i, %95
  %arrayidx10.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %96 = load double, double* %arrayidx10.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add11.i.i = fadd double %96, %add8.i.i
  store double %add11.i.i, double* %arrayidx10.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %97 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %98 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add16.i.i = fadd double %97, %98
  %99 = bitcast i64 %q.addr.13.i.i to double
  %add17.i.i = fadd double %add16.i.i, %99
  %arrayidx19.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %100 = load double, double* %arrayidx19.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add20.i.i = fadd double %100, %add17.i.i
  store double %add20.i.i, double* %arrayidx19.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %cmp21.i.i = icmp eq i32 %.omp.iv.04.i.i, 3071
  %add23.i.i = fadd double %95, 6.000000e+00
  %101 = bitcast double %add23.i.i to i64
  %add24.i.i = fadd double %99, 9.000000e+00
  %102 = bitcast double %add24.i.i to i64
  %spec.select.i.i = select i1 %cmp21.i.i, i64 %102, i64 %q.addr.13.i.i
  %spec.select1.i.i = select i1 %cmp21.i.i, i64 %101, i64 %p.addr.15.i.i
  %add25.i.i = add nsw i32 %.omp.iv.04.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.04.i.i, %92
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !144

__omp_outlined__51.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %66) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %65) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %64) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %63) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__51.exit.i, %omp_if.then.i
  %103 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %103, %.omp.iv.03.i
  %104 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp7.i = icmp sgt i32 %add.i, %104
  br i1 %cmp7.i, label %__omp_outlined__50.exit, label %omp.inner.for.body.i

__omp_outlined__50.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %59) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %58) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %57) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %55)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__50.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l637_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__51_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to i64*
  %19 = load i64, i64* %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %2, i64 7
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %2, i64 8
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %29) #4
  %30 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %30) #4
  %conv2.i = trunc i64 %4 to i32
  %conv3.i = trunc i64 %7 to i32
  store i32 %conv2.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv3.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %31 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %31) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %32 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %32) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 38, i32 %conv2.i, i32 %conv3.i, i32 1, i32 1) #4
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool7.i = icmp eq i32 %33, 0
  br i1 %tobool7.i, label %__omp_outlined__51.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %q.addr.1.lcssa.i = phi i64 [ %q.addr.08.i, %omp.dispatch.body.i ], [ %spec.select.i, %omp.inner.for.body.i ]
  %p.addr.1.lcssa.i = phi i64 [ %p.addr.09.i, %omp.dispatch.body.i ], [ %spec.select1.i, %omp.inner.for.body.i ]
  %34 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %34, 0
  br i1 %tobool.i, label %__omp_outlined__51.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %p.addr.09.i = phi i64 [ %p.addr.1.lcssa.i, %omp.dispatch.cond.loopexit.i ], [ %19, %entry ]
  %q.addr.08.i = phi i64 [ %q.addr.1.lcssa.i, %omp.dispatch.cond.loopexit.i ], [ %28, %entry ]
  %35 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %36 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !144
  %cmp2.i = icmp sgt i32 %35, %36
  br i1 %cmp2.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %p.addr.15.i = phi i64 [ %spec.select1.i, %omp.inner.for.body.i ], [ %p.addr.09.i, %omp.dispatch.body.i ]
  %.omp.iv.04.i = phi i32 [ %add25.i, %omp.inner.for.body.i ], [ %35, %omp.dispatch.body.i ]
  %q.addr.13.i = phi i64 [ %spec.select.i, %omp.inner.for.body.i ], [ %q.addr.08.i, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.04.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %37 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %38 = load double, double* %arrayidx6.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add7.i = fadd double %37, %38
  %39 = bitcast i64 %p.addr.15.i to double
  %add8.i = fadd double %add7.i, %39
  %arrayidx10.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %40 = load double, double* %arrayidx10.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add11.i = fadd double %40, %add8.i
  store double %add11.i, double* %arrayidx10.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %41 = load double, double* %arrayidx6.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i
  %42 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add16.i = fadd double %41, %42
  %43 = bitcast i64 %q.addr.13.i to double
  %add17.i = fadd double %add16.i, %43
  %arrayidx19.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %44 = load double, double* %arrayidx19.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %add20.i = fadd double %44, %add17.i
  store double %add20.i, double* %arrayidx19.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !144
  %cmp21.i = icmp eq i32 %.omp.iv.04.i, 3071
  %add23.i = fadd double %39, 6.000000e+00
  %45 = bitcast double %add23.i to i64
  %add24.i = fadd double %43, 9.000000e+00
  %46 = bitcast double %add24.i to i64
  %spec.select.i = select i1 %cmp21.i, i64 %46, i64 %q.addr.13.i
  %spec.select1.i = select i1 %cmp21.i, i64 %45, i64 %p.addr.15.i
  %add25.i = add nsw i32 %.omp.iv.04.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.04.i, %36
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !144

__omp_outlined__51.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %32) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %31) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %30) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %29) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l662([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l662_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__53_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  %32 = getelementptr inbounds i8*, i8** %11, i64 7
  %33 = bitcast i8** %32 to [3072 x double]**
  %34 = load [3072 x double]*, [3072 x double]** %33, align 8, !tbaa !124
  %35 = getelementptr inbounds i8*, i8** %11, i64 8
  %36 = bitcast i8** %35 to i64*
  %37 = load i64, i64* %36, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv2.i.i.i = trunc i64 %13 to i32
  %conv3.i.i.i = trunc i64 %16 to i32
  store i32 %conv2.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv3.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 35, i32 %conv2.i.i.i, i32 %conv3.i.i.i, i32 1, i32 1) #4
  %38 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool7.i.i.i = icmp eq i32 %38, 0
  br i1 %tobool7.i.i.i, label %__omp_outlined__53_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %q.addr.1.lcssa.i.i.i = phi i64 [ %q.addr.08.i.i.i, %omp.dispatch.body.i.i.i ], [ %spec.select.i.i.i, %omp.inner.for.body.i.i.i ]
  %p.addr.1.lcssa.i.i.i = phi i64 [ %p.addr.09.i.i.i, %omp.dispatch.body.i.i.i ], [ %spec.select1.i.i.i, %omp.inner.for.body.i.i.i ]
  %39 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %39, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__53_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %p.addr.09.i.i.i = phi i64 [ %p.addr.1.lcssa.i.i.i, %omp.dispatch.cond.loopexit.i.i.i ], [ %28, %.execute.fn.i ]
  %q.addr.08.i.i.i = phi i64 [ %q.addr.1.lcssa.i.i.i, %omp.dispatch.cond.loopexit.i.i.i ], [ %37, %.execute.fn.i ]
  %40 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %41 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !145
  %cmp2.i.i.i = icmp sgt i32 %40, %41
  br i1 %cmp2.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %p.addr.15.i.i.i = phi i64 [ %spec.select1.i.i.i, %omp.inner.for.body.i.i.i ], [ %p.addr.09.i.i.i, %omp.dispatch.body.i.i.i ]
  %.omp.iv.04.i.i.i = phi i32 [ %add25.i.i.i, %omp.inner.for.body.i.i.i ], [ %40, %omp.dispatch.body.i.i.i ]
  %q.addr.13.i.i.i = phi i64 [ %spec.select.i.i.i, %omp.inner.for.body.i.i.i ], [ %q.addr.08.i.i.i, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.04.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %42 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %43 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add7.i.i.i = fadd double %42, %43
  %44 = bitcast i64 %p.addr.15.i.i.i to double
  %add8.i.i.i = fadd double %add7.i.i.i, %44
  %arrayidx10.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %45 = load double, double* %arrayidx10.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add11.i.i.i = fadd double %45, %add8.i.i.i
  store double %add11.i.i.i, double* %arrayidx10.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %46 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %34, i64 0, i64 %idxprom.i.i.i
  %47 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add16.i.i.i = fadd double %46, %47
  %48 = bitcast i64 %q.addr.13.i.i.i to double
  %add17.i.i.i = fadd double %add16.i.i.i, %48
  %arrayidx19.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %49 = load double, double* %arrayidx19.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add20.i.i.i = fadd double %49, %add17.i.i.i
  store double %add20.i.i.i, double* %arrayidx19.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %cmp21.i.i.i = icmp eq i32 %.omp.iv.04.i.i.i, 3071
  %add23.i.i.i = fadd double %44, 6.000000e+00
  %50 = bitcast double %add23.i.i.i to i64
  %add24.i.i.i = fadd double %48, 9.000000e+00
  %51 = bitcast double %add24.i.i.i to i64
  %spec.select.i.i.i = select i1 %cmp21.i.i.i, i64 %51, i64 %q.addr.13.i.i.i
  %spec.select1.i.i.i = select i1 %cmp21.i.i.i, i64 %50, i64 %p.addr.15.i.i.i
  %add25.i.i.i = add nsw i32 %.omp.iv.04.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.04.i.i.i, %41
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !145

__omp_outlined__53_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %52 = bitcast i8* %10 to void (i16, i32)*
  call void %52(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__53_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l662_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %53 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %53, -32
  %54 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %54, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %55 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %55)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %56 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %57 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %57) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %58 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %58) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %59 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %59) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %60 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %61 = icmp slt i32 %60, 3071
  %cond.i = select i1 %61, i32 %60, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %62 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp72.i = icmp sgt i32 %62, %cond.i
  br i1 %cmp72.i, label %__omp_outlined__52.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %63 = bitcast i32* %.omp.lb.i.i to i8*
  %64 = bitcast i32* %.omp.ub.i.i to i8*
  %65 = bitcast i32* %.omp.stride.i.i to i8*
  %66 = bitcast i32* %.omp.is_last.i.i to i8*
  %67 = inttoptr i64 %p to i8*
  %68 = inttoptr i64 %q to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %62, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %69 = load i32, i32* %.omp.comb.lb.i, align 4
  %70 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %71 = zext i32 %70 to i64
  %72 = zext i32 %69 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__53_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 9) #4
  %73 = load i8**, i8*** %shared_arg_refs.i, align 8
  %74 = inttoptr i64 %72 to i8*
  store i8* %74, i8** %73, align 8, !tbaa !124
  %75 = getelementptr inbounds i8*, i8** %73, i64 1
  %76 = inttoptr i64 %71 to i8*
  store i8* %76, i8** %75, align 8, !tbaa !124
  %77 = getelementptr inbounds i8*, i8** %73, i64 2
  %78 = bitcast i8** %77 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %78, align 8, !tbaa !124
  %79 = getelementptr inbounds i8*, i8** %73, i64 3
  %80 = bitcast i8** %79 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %80, align 8, !tbaa !124
  %81 = getelementptr inbounds i8*, i8** %73, i64 4
  %82 = bitcast i8** %81 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %82, align 8, !tbaa !124
  %83 = getelementptr inbounds i8*, i8** %73, i64 5
  store i8* %67, i8** %83, align 8, !tbaa !124
  %84 = getelementptr inbounds i8*, i8** %73, i64 6
  %85 = bitcast i8** %84 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %85, align 8, !tbaa !124
  %86 = getelementptr inbounds i8*, i8** %73, i64 7
  %87 = bitcast i8** %86 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %87, align 8, !tbaa !124
  %88 = getelementptr inbounds i8*, i8** %73, i64 8
  store i8* %68, i8** %88, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %63) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %64) #4
  store i32 %69, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %70, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %65) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %66) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 35, i32 %69, i32 %70, i32 1, i32 1) #4
  %89 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool7.i.i = icmp eq i32 %89, 0
  br i1 %tobool7.i.i, label %__omp_outlined__53.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %q.addr.1.lcssa.i.i = phi i64 [ %q.addr.08.i.i, %omp.dispatch.body.i.i ], [ %spec.select.i.i, %omp.inner.for.body.i.i ]
  %p.addr.1.lcssa.i.i = phi i64 [ %p.addr.09.i.i, %omp.dispatch.body.i.i ], [ %spec.select1.i.i, %omp.inner.for.body.i.i ]
  %90 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %90, 0
  br i1 %tobool.i.i, label %__omp_outlined__53.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %p.addr.09.i.i = phi i64 [ %p.addr.1.lcssa.i.i, %omp.dispatch.cond.loopexit.i.i ], [ %p, %omp_if.else.i ]
  %q.addr.08.i.i = phi i64 [ %q.addr.1.lcssa.i.i, %omp.dispatch.cond.loopexit.i.i ], [ %q, %omp_if.else.i ]
  %91 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %92 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !145
  %cmp2.i.i = icmp sgt i32 %91, %92
  br i1 %cmp2.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %p.addr.15.i.i = phi i64 [ %spec.select1.i.i, %omp.inner.for.body.i.i ], [ %p.addr.09.i.i, %omp.dispatch.body.i.i ]
  %.omp.iv.04.i.i = phi i32 [ %add25.i.i, %omp.inner.for.body.i.i ], [ %91, %omp.dispatch.body.i.i ]
  %q.addr.13.i.i = phi i64 [ %spec.select.i.i, %omp.inner.for.body.i.i ], [ %q.addr.08.i.i, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.04.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %93 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %94 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add7.i.i = fadd double %93, %94
  %95 = bitcast i64 %p.addr.15.i.i to double
  %add8.i.i = fadd double %add7.i.i, %95
  %arrayidx10.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %96 = load double, double* %arrayidx10.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add11.i.i = fadd double %96, %add8.i.i
  store double %add11.i.i, double* %arrayidx10.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %97 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %98 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add16.i.i = fadd double %97, %98
  %99 = bitcast i64 %q.addr.13.i.i to double
  %add17.i.i = fadd double %add16.i.i, %99
  %arrayidx19.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %100 = load double, double* %arrayidx19.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add20.i.i = fadd double %100, %add17.i.i
  store double %add20.i.i, double* %arrayidx19.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %cmp21.i.i = icmp eq i32 %.omp.iv.04.i.i, 3071
  %add23.i.i = fadd double %95, 6.000000e+00
  %101 = bitcast double %add23.i.i to i64
  %add24.i.i = fadd double %99, 9.000000e+00
  %102 = bitcast double %add24.i.i to i64
  %spec.select.i.i = select i1 %cmp21.i.i, i64 %102, i64 %q.addr.13.i.i
  %spec.select1.i.i = select i1 %cmp21.i.i, i64 %101, i64 %p.addr.15.i.i
  %add25.i.i = add nsw i32 %.omp.iv.04.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.04.i.i, %92
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !145

__omp_outlined__53.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %66) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %65) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %64) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %63) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__53.exit.i, %omp_if.then.i
  %103 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %103, %.omp.iv.03.i
  %104 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp7.i = icmp sgt i32 %add.i, %104
  br i1 %cmp7.i, label %__omp_outlined__52.exit, label %omp.inner.for.body.i

__omp_outlined__52.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %59) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %58) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %57) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %55)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__52.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l662_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__53_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to i64*
  %19 = load i64, i64* %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %2, i64 7
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %2, i64 8
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %29) #4
  %30 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %30) #4
  %conv2.i = trunc i64 %4 to i32
  %conv3.i = trunc i64 %7 to i32
  store i32 %conv2.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv3.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %31 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %31) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %32 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %32) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 35, i32 %conv2.i, i32 %conv3.i, i32 1, i32 1) #4
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool7.i = icmp eq i32 %33, 0
  br i1 %tobool7.i, label %__omp_outlined__53.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %q.addr.1.lcssa.i = phi i64 [ %q.addr.08.i, %omp.dispatch.body.i ], [ %spec.select.i, %omp.inner.for.body.i ]
  %p.addr.1.lcssa.i = phi i64 [ %p.addr.09.i, %omp.dispatch.body.i ], [ %spec.select1.i, %omp.inner.for.body.i ]
  %34 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %34, 0
  br i1 %tobool.i, label %__omp_outlined__53.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %p.addr.09.i = phi i64 [ %p.addr.1.lcssa.i, %omp.dispatch.cond.loopexit.i ], [ %19, %entry ]
  %q.addr.08.i = phi i64 [ %q.addr.1.lcssa.i, %omp.dispatch.cond.loopexit.i ], [ %28, %entry ]
  %35 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %36 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !145
  %cmp2.i = icmp sgt i32 %35, %36
  br i1 %cmp2.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %p.addr.15.i = phi i64 [ %spec.select1.i, %omp.inner.for.body.i ], [ %p.addr.09.i, %omp.dispatch.body.i ]
  %.omp.iv.04.i = phi i32 [ %add25.i, %omp.inner.for.body.i ], [ %35, %omp.dispatch.body.i ]
  %q.addr.13.i = phi i64 [ %spec.select.i, %omp.inner.for.body.i ], [ %q.addr.08.i, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.04.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %37 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %38 = load double, double* %arrayidx6.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add7.i = fadd double %37, %38
  %39 = bitcast i64 %p.addr.15.i to double
  %add8.i = fadd double %add7.i, %39
  %arrayidx10.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %40 = load double, double* %arrayidx10.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add11.i = fadd double %40, %add8.i
  store double %add11.i, double* %arrayidx10.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %41 = load double, double* %arrayidx6.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i
  %42 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add16.i = fadd double %41, %42
  %43 = bitcast i64 %q.addr.13.i to double
  %add17.i = fadd double %add16.i, %43
  %arrayidx19.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %44 = load double, double* %arrayidx19.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %add20.i = fadd double %44, %add17.i
  store double %add20.i, double* %arrayidx19.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !145
  %cmp21.i = icmp eq i32 %.omp.iv.04.i, 3071
  %add23.i = fadd double %39, 6.000000e+00
  %45 = bitcast double %add23.i to i64
  %add24.i = fadd double %43, 9.000000e+00
  %46 = bitcast double %add24.i to i64
  %spec.select.i = select i1 %cmp21.i, i64 %46, i64 %q.addr.13.i
  %spec.select1.i = select i1 %cmp21.i, i64 %45, i64 %p.addr.15.i
  %add25.i = add nsw i32 %.omp.iv.04.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.04.i, %36
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !145

__omp_outlined__53.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %32) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %31) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %30) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %29) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l687([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l687_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__55_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  %32 = getelementptr inbounds i8*, i8** %11, i64 7
  %33 = bitcast i8** %32 to [3072 x double]**
  %34 = load [3072 x double]*, [3072 x double]** %33, align 8, !tbaa !124
  %35 = getelementptr inbounds i8*, i8** %11, i64 8
  %36 = bitcast i8** %35 to i64*
  %37 = load i64, i64* %36, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv2.i.i.i = trunc i64 %13 to i32
  %conv3.i.i.i = trunc i64 %16 to i32
  store i32 %conv2.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv3.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 36, i32 %conv2.i.i.i, i32 %conv3.i.i.i, i32 1, i32 1) #4
  %38 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool7.i.i.i = icmp eq i32 %38, 0
  br i1 %tobool7.i.i.i, label %__omp_outlined__55_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %q.addr.1.lcssa.i.i.i = phi i64 [ %q.addr.08.i.i.i, %omp.dispatch.body.i.i.i ], [ %spec.select.i.i.i, %omp.inner.for.body.i.i.i ]
  %p.addr.1.lcssa.i.i.i = phi i64 [ %p.addr.09.i.i.i, %omp.dispatch.body.i.i.i ], [ %spec.select1.i.i.i, %omp.inner.for.body.i.i.i ]
  %39 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %39, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__55_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %p.addr.09.i.i.i = phi i64 [ %p.addr.1.lcssa.i.i.i, %omp.dispatch.cond.loopexit.i.i.i ], [ %28, %.execute.fn.i ]
  %q.addr.08.i.i.i = phi i64 [ %q.addr.1.lcssa.i.i.i, %omp.dispatch.cond.loopexit.i.i.i ], [ %37, %.execute.fn.i ]
  %40 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %41 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !146
  %cmp2.i.i.i = icmp sgt i32 %40, %41
  br i1 %cmp2.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %p.addr.15.i.i.i = phi i64 [ %spec.select1.i.i.i, %omp.inner.for.body.i.i.i ], [ %p.addr.09.i.i.i, %omp.dispatch.body.i.i.i ]
  %.omp.iv.04.i.i.i = phi i32 [ %add25.i.i.i, %omp.inner.for.body.i.i.i ], [ %40, %omp.dispatch.body.i.i.i ]
  %q.addr.13.i.i.i = phi i64 [ %spec.select.i.i.i, %omp.inner.for.body.i.i.i ], [ %q.addr.08.i.i.i, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.04.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %42 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %43 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add7.i.i.i = fadd double %42, %43
  %44 = bitcast i64 %p.addr.15.i.i.i to double
  %add8.i.i.i = fadd double %add7.i.i.i, %44
  %arrayidx10.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %45 = load double, double* %arrayidx10.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add11.i.i.i = fadd double %45, %add8.i.i.i
  store double %add11.i.i.i, double* %arrayidx10.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %46 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %34, i64 0, i64 %idxprom.i.i.i
  %47 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add16.i.i.i = fadd double %46, %47
  %48 = bitcast i64 %q.addr.13.i.i.i to double
  %add17.i.i.i = fadd double %add16.i.i.i, %48
  %arrayidx19.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %49 = load double, double* %arrayidx19.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add20.i.i.i = fadd double %49, %add17.i.i.i
  store double %add20.i.i.i, double* %arrayidx19.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %cmp21.i.i.i = icmp eq i32 %.omp.iv.04.i.i.i, 3071
  %add23.i.i.i = fadd double %44, 6.000000e+00
  %50 = bitcast double %add23.i.i.i to i64
  %add24.i.i.i = fadd double %48, 9.000000e+00
  %51 = bitcast double %add24.i.i.i to i64
  %spec.select.i.i.i = select i1 %cmp21.i.i.i, i64 %51, i64 %q.addr.13.i.i.i
  %spec.select1.i.i.i = select i1 %cmp21.i.i.i, i64 %50, i64 %p.addr.15.i.i.i
  %add25.i.i.i = add nsw i32 %.omp.iv.04.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.04.i.i.i, %41
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !146

__omp_outlined__55_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %52 = bitcast i8* %10 to void (i16, i32)*
  call void %52(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__55_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l687_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %53 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %53, -32
  %54 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %54, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %55 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %55)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %56 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %57 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %57) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %58 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %58) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %59 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %59) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %60 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %61 = icmp slt i32 %60, 3071
  %cond.i = select i1 %61, i32 %60, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %62 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp72.i = icmp sgt i32 %62, %cond.i
  br i1 %cmp72.i, label %__omp_outlined__54.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %63 = bitcast i32* %.omp.lb.i.i to i8*
  %64 = bitcast i32* %.omp.ub.i.i to i8*
  %65 = bitcast i32* %.omp.stride.i.i to i8*
  %66 = bitcast i32* %.omp.is_last.i.i to i8*
  %67 = inttoptr i64 %p to i8*
  %68 = inttoptr i64 %q to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %62, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %69 = load i32, i32* %.omp.comb.lb.i, align 4
  %70 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %71 = zext i32 %70 to i64
  %72 = zext i32 %69 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__55_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 9) #4
  %73 = load i8**, i8*** %shared_arg_refs.i, align 8
  %74 = inttoptr i64 %72 to i8*
  store i8* %74, i8** %73, align 8, !tbaa !124
  %75 = getelementptr inbounds i8*, i8** %73, i64 1
  %76 = inttoptr i64 %71 to i8*
  store i8* %76, i8** %75, align 8, !tbaa !124
  %77 = getelementptr inbounds i8*, i8** %73, i64 2
  %78 = bitcast i8** %77 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %78, align 8, !tbaa !124
  %79 = getelementptr inbounds i8*, i8** %73, i64 3
  %80 = bitcast i8** %79 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %80, align 8, !tbaa !124
  %81 = getelementptr inbounds i8*, i8** %73, i64 4
  %82 = bitcast i8** %81 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %82, align 8, !tbaa !124
  %83 = getelementptr inbounds i8*, i8** %73, i64 5
  store i8* %67, i8** %83, align 8, !tbaa !124
  %84 = getelementptr inbounds i8*, i8** %73, i64 6
  %85 = bitcast i8** %84 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %85, align 8, !tbaa !124
  %86 = getelementptr inbounds i8*, i8** %73, i64 7
  %87 = bitcast i8** %86 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %87, align 8, !tbaa !124
  %88 = getelementptr inbounds i8*, i8** %73, i64 8
  store i8* %68, i8** %88, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %63) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %64) #4
  store i32 %69, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %70, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %65) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %66) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 36, i32 %69, i32 %70, i32 1, i32 1) #4
  %89 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool7.i.i = icmp eq i32 %89, 0
  br i1 %tobool7.i.i, label %__omp_outlined__55.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %q.addr.1.lcssa.i.i = phi i64 [ %q.addr.08.i.i, %omp.dispatch.body.i.i ], [ %spec.select.i.i, %omp.inner.for.body.i.i ]
  %p.addr.1.lcssa.i.i = phi i64 [ %p.addr.09.i.i, %omp.dispatch.body.i.i ], [ %spec.select1.i.i, %omp.inner.for.body.i.i ]
  %90 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %90, 0
  br i1 %tobool.i.i, label %__omp_outlined__55.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %p.addr.09.i.i = phi i64 [ %p.addr.1.lcssa.i.i, %omp.dispatch.cond.loopexit.i.i ], [ %p, %omp_if.else.i ]
  %q.addr.08.i.i = phi i64 [ %q.addr.1.lcssa.i.i, %omp.dispatch.cond.loopexit.i.i ], [ %q, %omp_if.else.i ]
  %91 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %92 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !146
  %cmp2.i.i = icmp sgt i32 %91, %92
  br i1 %cmp2.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %p.addr.15.i.i = phi i64 [ %spec.select1.i.i, %omp.inner.for.body.i.i ], [ %p.addr.09.i.i, %omp.dispatch.body.i.i ]
  %.omp.iv.04.i.i = phi i32 [ %add25.i.i, %omp.inner.for.body.i.i ], [ %91, %omp.dispatch.body.i.i ]
  %q.addr.13.i.i = phi i64 [ %spec.select.i.i, %omp.inner.for.body.i.i ], [ %q.addr.08.i.i, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.04.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %93 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %94 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add7.i.i = fadd double %93, %94
  %95 = bitcast i64 %p.addr.15.i.i to double
  %add8.i.i = fadd double %add7.i.i, %95
  %arrayidx10.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %96 = load double, double* %arrayidx10.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add11.i.i = fadd double %96, %add8.i.i
  store double %add11.i.i, double* %arrayidx10.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %97 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %98 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add16.i.i = fadd double %97, %98
  %99 = bitcast i64 %q.addr.13.i.i to double
  %add17.i.i = fadd double %add16.i.i, %99
  %arrayidx19.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %100 = load double, double* %arrayidx19.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add20.i.i = fadd double %100, %add17.i.i
  store double %add20.i.i, double* %arrayidx19.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %cmp21.i.i = icmp eq i32 %.omp.iv.04.i.i, 3071
  %add23.i.i = fadd double %95, 6.000000e+00
  %101 = bitcast double %add23.i.i to i64
  %add24.i.i = fadd double %99, 9.000000e+00
  %102 = bitcast double %add24.i.i to i64
  %spec.select.i.i = select i1 %cmp21.i.i, i64 %102, i64 %q.addr.13.i.i
  %spec.select1.i.i = select i1 %cmp21.i.i, i64 %101, i64 %p.addr.15.i.i
  %add25.i.i = add nsw i32 %.omp.iv.04.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.04.i.i, %92
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !146

__omp_outlined__55.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %66) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %65) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %64) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %63) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__55.exit.i, %omp_if.then.i
  %103 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %103, %.omp.iv.03.i
  %104 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp7.i = icmp sgt i32 %add.i, %104
  br i1 %cmp7.i, label %__omp_outlined__54.exit, label %omp.inner.for.body.i

__omp_outlined__54.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %59) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %58) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %57) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %55)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__54.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l687_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__55_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to i64*
  %19 = load i64, i64* %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %2, i64 7
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %2, i64 8
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %29) #4
  %30 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %30) #4
  %conv2.i = trunc i64 %4 to i32
  %conv3.i = trunc i64 %7 to i32
  store i32 %conv2.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv3.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %31 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %31) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %32 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %32) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 36, i32 %conv2.i, i32 %conv3.i, i32 1, i32 1) #4
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool7.i = icmp eq i32 %33, 0
  br i1 %tobool7.i, label %__omp_outlined__55.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %q.addr.1.lcssa.i = phi i64 [ %q.addr.08.i, %omp.dispatch.body.i ], [ %spec.select.i, %omp.inner.for.body.i ]
  %p.addr.1.lcssa.i = phi i64 [ %p.addr.09.i, %omp.dispatch.body.i ], [ %spec.select1.i, %omp.inner.for.body.i ]
  %34 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %34, 0
  br i1 %tobool.i, label %__omp_outlined__55.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %p.addr.09.i = phi i64 [ %p.addr.1.lcssa.i, %omp.dispatch.cond.loopexit.i ], [ %19, %entry ]
  %q.addr.08.i = phi i64 [ %q.addr.1.lcssa.i, %omp.dispatch.cond.loopexit.i ], [ %28, %entry ]
  %35 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %36 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !146
  %cmp2.i = icmp sgt i32 %35, %36
  br i1 %cmp2.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %p.addr.15.i = phi i64 [ %spec.select1.i, %omp.inner.for.body.i ], [ %p.addr.09.i, %omp.dispatch.body.i ]
  %.omp.iv.04.i = phi i32 [ %add25.i, %omp.inner.for.body.i ], [ %35, %omp.dispatch.body.i ]
  %q.addr.13.i = phi i64 [ %spec.select.i, %omp.inner.for.body.i ], [ %q.addr.08.i, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.04.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %37 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %38 = load double, double* %arrayidx6.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add7.i = fadd double %37, %38
  %39 = bitcast i64 %p.addr.15.i to double
  %add8.i = fadd double %add7.i, %39
  %arrayidx10.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %40 = load double, double* %arrayidx10.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add11.i = fadd double %40, %add8.i
  store double %add11.i, double* %arrayidx10.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %41 = load double, double* %arrayidx6.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i
  %42 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add16.i = fadd double %41, %42
  %43 = bitcast i64 %q.addr.13.i to double
  %add17.i = fadd double %add16.i, %43
  %arrayidx19.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %44 = load double, double* %arrayidx19.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %add20.i = fadd double %44, %add17.i
  store double %add20.i, double* %arrayidx19.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !146
  %cmp21.i = icmp eq i32 %.omp.iv.04.i, 3071
  %add23.i = fadd double %39, 6.000000e+00
  %45 = bitcast double %add23.i to i64
  %add24.i = fadd double %43, 9.000000e+00
  %46 = bitcast double %add24.i to i64
  %spec.select.i = select i1 %cmp21.i, i64 %46, i64 %q.addr.13.i
  %spec.select1.i = select i1 %cmp21.i, i64 %45, i64 %p.addr.15.i
  %add25.i = add nsw i32 %.omp.iv.04.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.04.i, %36
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !146

__omp_outlined__55.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %32) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %31) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %30) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %29) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l712([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l712_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__57_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  %32 = getelementptr inbounds i8*, i8** %11, i64 7
  %33 = bitcast i8** %32 to [3072 x double]**
  %34 = load [3072 x double]*, [3072 x double]** %33, align 8, !tbaa !124
  %35 = getelementptr inbounds i8*, i8** %11, i64 8
  %36 = bitcast i8** %35 to i64*
  %37 = load i64, i64* %36, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv2.i.i.i = trunc i64 %13 to i32
  %conv3.i.i.i = trunc i64 %16 to i32
  store i32 %conv2.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv3.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 37, i32 %conv2.i.i.i, i32 %conv3.i.i.i, i32 1, i32 1) #4
  %38 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool7.i.i.i = icmp eq i32 %38, 0
  br i1 %tobool7.i.i.i, label %__omp_outlined__57_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %q.addr.1.lcssa.i.i.i = phi i64 [ %q.addr.08.i.i.i, %omp.dispatch.body.i.i.i ], [ %spec.select.i.i.i, %omp.inner.for.body.i.i.i ]
  %p.addr.1.lcssa.i.i.i = phi i64 [ %p.addr.09.i.i.i, %omp.dispatch.body.i.i.i ], [ %spec.select1.i.i.i, %omp.inner.for.body.i.i.i ]
  %39 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %39, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__57_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %p.addr.09.i.i.i = phi i64 [ %p.addr.1.lcssa.i.i.i, %omp.dispatch.cond.loopexit.i.i.i ], [ %28, %.execute.fn.i ]
  %q.addr.08.i.i.i = phi i64 [ %q.addr.1.lcssa.i.i.i, %omp.dispatch.cond.loopexit.i.i.i ], [ %37, %.execute.fn.i ]
  %40 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %41 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !147
  %cmp2.i.i.i = icmp sgt i32 %40, %41
  br i1 %cmp2.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %p.addr.15.i.i.i = phi i64 [ %spec.select1.i.i.i, %omp.inner.for.body.i.i.i ], [ %p.addr.09.i.i.i, %omp.dispatch.body.i.i.i ]
  %.omp.iv.04.i.i.i = phi i32 [ %add25.i.i.i, %omp.inner.for.body.i.i.i ], [ %40, %omp.dispatch.body.i.i.i ]
  %q.addr.13.i.i.i = phi i64 [ %spec.select.i.i.i, %omp.inner.for.body.i.i.i ], [ %q.addr.08.i.i.i, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.04.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %42 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %arrayidx6.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %43 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add7.i.i.i = fadd double %42, %43
  %44 = bitcast i64 %p.addr.15.i.i.i to double
  %add8.i.i.i = fadd double %add7.i.i.i, %44
  %arrayidx10.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %45 = load double, double* %arrayidx10.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add11.i.i.i = fadd double %45, %add8.i.i.i
  store double %add11.i.i.i, double* %arrayidx10.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %46 = load double, double* %arrayidx6.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %arrayidx15.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %34, i64 0, i64 %idxprom.i.i.i
  %47 = load double, double* %arrayidx15.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add16.i.i.i = fadd double %46, %47
  %48 = bitcast i64 %q.addr.13.i.i.i to double
  %add17.i.i.i = fadd double %add16.i.i.i, %48
  %arrayidx19.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %49 = load double, double* %arrayidx19.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add20.i.i.i = fadd double %49, %add17.i.i.i
  store double %add20.i.i.i, double* %arrayidx19.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %cmp21.i.i.i = icmp eq i32 %.omp.iv.04.i.i.i, 3071
  %add23.i.i.i = fadd double %44, 6.000000e+00
  %50 = bitcast double %add23.i.i.i to i64
  %add24.i.i.i = fadd double %48, 9.000000e+00
  %51 = bitcast double %add24.i.i.i to i64
  %spec.select.i.i.i = select i1 %cmp21.i.i.i, i64 %51, i64 %q.addr.13.i.i.i
  %spec.select1.i.i.i = select i1 %cmp21.i.i.i, i64 %50, i64 %p.addr.15.i.i.i
  %add25.i.i.i = add nsw i32 %.omp.iv.04.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.04.i.i.i, %41
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !147

__omp_outlined__57_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %52 = bitcast i8* %10 to void (i16, i32)*
  call void %52(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__57_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l712_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %53 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %53, -32
  %54 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %54, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %55 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %55)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %56 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %57 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %57) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %58 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %58) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %59 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %59) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %60 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %61 = icmp slt i32 %60, 3071
  %cond.i = select i1 %61, i32 %60, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %62 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp72.i = icmp sgt i32 %62, %cond.i
  br i1 %cmp72.i, label %__omp_outlined__56.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %63 = bitcast i32* %.omp.lb.i.i to i8*
  %64 = bitcast i32* %.omp.ub.i.i to i8*
  %65 = bitcast i32* %.omp.stride.i.i to i8*
  %66 = bitcast i32* %.omp.is_last.i.i to i8*
  %67 = inttoptr i64 %p to i8*
  %68 = inttoptr i64 %q to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %62, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %69 = load i32, i32* %.omp.comb.lb.i, align 4
  %70 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %71 = zext i32 %70 to i64
  %72 = zext i32 %69 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__57_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 9) #4
  %73 = load i8**, i8*** %shared_arg_refs.i, align 8
  %74 = inttoptr i64 %72 to i8*
  store i8* %74, i8** %73, align 8, !tbaa !124
  %75 = getelementptr inbounds i8*, i8** %73, i64 1
  %76 = inttoptr i64 %71 to i8*
  store i8* %76, i8** %75, align 8, !tbaa !124
  %77 = getelementptr inbounds i8*, i8** %73, i64 2
  %78 = bitcast i8** %77 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %78, align 8, !tbaa !124
  %79 = getelementptr inbounds i8*, i8** %73, i64 3
  %80 = bitcast i8** %79 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %80, align 8, !tbaa !124
  %81 = getelementptr inbounds i8*, i8** %73, i64 4
  %82 = bitcast i8** %81 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %82, align 8, !tbaa !124
  %83 = getelementptr inbounds i8*, i8** %73, i64 5
  store i8* %67, i8** %83, align 8, !tbaa !124
  %84 = getelementptr inbounds i8*, i8** %73, i64 6
  %85 = bitcast i8** %84 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %85, align 8, !tbaa !124
  %86 = getelementptr inbounds i8*, i8** %73, i64 7
  %87 = bitcast i8** %86 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %87, align 8, !tbaa !124
  %88 = getelementptr inbounds i8*, i8** %73, i64 8
  store i8* %68, i8** %88, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %63) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %64) #4
  store i32 %69, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %70, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %65) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %66) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 37, i32 %69, i32 %70, i32 1, i32 1) #4
  %89 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool7.i.i = icmp eq i32 %89, 0
  br i1 %tobool7.i.i, label %__omp_outlined__57.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %q.addr.1.lcssa.i.i = phi i64 [ %q.addr.08.i.i, %omp.dispatch.body.i.i ], [ %spec.select.i.i, %omp.inner.for.body.i.i ]
  %p.addr.1.lcssa.i.i = phi i64 [ %p.addr.09.i.i, %omp.dispatch.body.i.i ], [ %spec.select1.i.i, %omp.inner.for.body.i.i ]
  %90 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %90, 0
  br i1 %tobool.i.i, label %__omp_outlined__57.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %p.addr.09.i.i = phi i64 [ %p.addr.1.lcssa.i.i, %omp.dispatch.cond.loopexit.i.i ], [ %p, %omp_if.else.i ]
  %q.addr.08.i.i = phi i64 [ %q.addr.1.lcssa.i.i, %omp.dispatch.cond.loopexit.i.i ], [ %q, %omp_if.else.i ]
  %91 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %92 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !147
  %cmp2.i.i = icmp sgt i32 %91, %92
  br i1 %cmp2.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %p.addr.15.i.i = phi i64 [ %spec.select1.i.i, %omp.inner.for.body.i.i ], [ %p.addr.09.i.i, %omp.dispatch.body.i.i ]
  %.omp.iv.04.i.i = phi i32 [ %add25.i.i, %omp.inner.for.body.i.i ], [ %91, %omp.dispatch.body.i.i ]
  %q.addr.13.i.i = phi i64 [ %spec.select.i.i, %omp.inner.for.body.i.i ], [ %q.addr.08.i.i, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.04.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %93 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %arrayidx6.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %94 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add7.i.i = fadd double %93, %94
  %95 = bitcast i64 %p.addr.15.i.i to double
  %add8.i.i = fadd double %add7.i.i, %95
  %arrayidx10.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %96 = load double, double* %arrayidx10.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add11.i.i = fadd double %96, %add8.i.i
  store double %add11.i.i, double* %arrayidx10.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %97 = load double, double* %arrayidx6.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %arrayidx15.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %98 = load double, double* %arrayidx15.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add16.i.i = fadd double %97, %98
  %99 = bitcast i64 %q.addr.13.i.i to double
  %add17.i.i = fadd double %add16.i.i, %99
  %arrayidx19.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %100 = load double, double* %arrayidx19.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add20.i.i = fadd double %100, %add17.i.i
  store double %add20.i.i, double* %arrayidx19.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %cmp21.i.i = icmp eq i32 %.omp.iv.04.i.i, 3071
  %add23.i.i = fadd double %95, 6.000000e+00
  %101 = bitcast double %add23.i.i to i64
  %add24.i.i = fadd double %99, 9.000000e+00
  %102 = bitcast double %add24.i.i to i64
  %spec.select.i.i = select i1 %cmp21.i.i, i64 %102, i64 %q.addr.13.i.i
  %spec.select1.i.i = select i1 %cmp21.i.i, i64 %101, i64 %p.addr.15.i.i
  %add25.i.i = add nsw i32 %.omp.iv.04.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.04.i.i, %92
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !147

__omp_outlined__57.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %66) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %65) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %64) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %63) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__57.exit.i, %omp_if.then.i
  %103 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %103, %.omp.iv.03.i
  %104 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp7.i = icmp sgt i32 %add.i, %104
  br i1 %cmp7.i, label %__omp_outlined__56.exit, label %omp.inner.for.body.i

__omp_outlined__56.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %59) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %58) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %57) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %55)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__56.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l712_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__57_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to i64*
  %19 = load i64, i64* %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %2, i64 7
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %2, i64 8
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %29) #4
  %30 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %30) #4
  %conv2.i = trunc i64 %4 to i32
  %conv3.i = trunc i64 %7 to i32
  store i32 %conv2.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv3.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %31 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %31) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %32 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %32) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 37, i32 %conv2.i, i32 %conv3.i, i32 1, i32 1) #4
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool7.i = icmp eq i32 %33, 0
  br i1 %tobool7.i, label %__omp_outlined__57.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %q.addr.1.lcssa.i = phi i64 [ %q.addr.08.i, %omp.dispatch.body.i ], [ %spec.select.i, %omp.inner.for.body.i ]
  %p.addr.1.lcssa.i = phi i64 [ %p.addr.09.i, %omp.dispatch.body.i ], [ %spec.select1.i, %omp.inner.for.body.i ]
  %34 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %34, 0
  br i1 %tobool.i, label %__omp_outlined__57.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %p.addr.09.i = phi i64 [ %p.addr.1.lcssa.i, %omp.dispatch.cond.loopexit.i ], [ %19, %entry ]
  %q.addr.08.i = phi i64 [ %q.addr.1.lcssa.i, %omp.dispatch.cond.loopexit.i ], [ %28, %entry ]
  %35 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %36 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !147
  %cmp2.i = icmp sgt i32 %35, %36
  br i1 %cmp2.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %p.addr.15.i = phi i64 [ %spec.select1.i, %omp.inner.for.body.i ], [ %p.addr.09.i, %omp.dispatch.body.i ]
  %.omp.iv.04.i = phi i32 [ %add25.i, %omp.inner.for.body.i ], [ %35, %omp.dispatch.body.i ]
  %q.addr.13.i = phi i64 [ %spec.select.i, %omp.inner.for.body.i ], [ %q.addr.08.i, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.04.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %37 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %arrayidx6.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %38 = load double, double* %arrayidx6.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add7.i = fadd double %37, %38
  %39 = bitcast i64 %p.addr.15.i to double
  %add8.i = fadd double %add7.i, %39
  %arrayidx10.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %40 = load double, double* %arrayidx10.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add11.i = fadd double %40, %add8.i
  store double %add11.i, double* %arrayidx10.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %41 = load double, double* %arrayidx6.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %arrayidx15.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i
  %42 = load double, double* %arrayidx15.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add16.i = fadd double %41, %42
  %43 = bitcast i64 %q.addr.13.i to double
  %add17.i = fadd double %add16.i, %43
  %arrayidx19.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %44 = load double, double* %arrayidx19.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %add20.i = fadd double %44, %add17.i
  store double %add20.i, double* %arrayidx19.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !147
  %cmp21.i = icmp eq i32 %.omp.iv.04.i, 3071
  %add23.i = fadd double %39, 6.000000e+00
  %45 = bitcast double %add23.i to i64
  %add24.i = fadd double %43, 9.000000e+00
  %46 = bitcast double %add24.i to i64
  %spec.select.i = select i1 %cmp21.i, i64 %46, i64 %q.addr.13.i
  %spec.select1.i = select i1 %cmp21.i, i64 %45, i64 %p.addr.15.i
  %add25.i = add nsw i32 %.omp.iv.04.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.04.i, %36
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !147

__omp_outlined__57.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %32) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %31) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %30) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %29) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l737([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l737_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__59_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  %32 = getelementptr inbounds i8*, i8** %11, i64 7
  %33 = bitcast i8** %32 to [3072 x double]**
  %34 = load [3072 x double]*, [3072 x double]** %33, align 8, !tbaa !124
  %35 = getelementptr inbounds i8*, i8** %11, i64 8
  %36 = bitcast i8** %35 to i64*
  %37 = load i64, i64* %36, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv2.i.i.i = trunc i64 %13 to i32
  %conv3.i.i.i = trunc i64 %16 to i32
  store i32 %conv2.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv3.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %38 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %39 = icmp slt i32 %38, 3071
  %cond.i.i.i = select i1 %39, i32 %38, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %40 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp52.i.i.i = icmp sgt i32 %40, %cond.i.i.i
  br i1 %cmp52.i.i.i, label %__omp_outlined__59_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %p.addr.05.i.i.i = phi i64 [ %spec.select1.i.i.i, %omp.inner.for.body.i.i.i ], [ %28, %.execute.fn.i ]
  %.omp.iv.04.i.i.i = phi i32 [ %add27.i.i.i, %omp.inner.for.body.i.i.i ], [ %40, %.execute.fn.i ]
  %q.addr.03.i.i.i = phi i64 [ %spec.select.i.i.i, %omp.inner.for.body.i.i.i ], [ %37, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.04.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx8.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %42 = load double, double* %arrayidx8.i.i.i, align 8, !tbaa !126
  %add9.i.i.i = fadd double %41, %42
  %43 = bitcast i64 %p.addr.05.i.i.i to double
  %add10.i.i.i = fadd double %add9.i.i.i, %43
  %arrayidx12.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %44 = load double, double* %arrayidx12.i.i.i, align 8, !tbaa !126
  %add13.i.i.i = fadd double %44, %add10.i.i.i
  store double %add13.i.i.i, double* %arrayidx12.i.i.i, align 8, !tbaa !126
  %45 = load double, double* %arrayidx8.i.i.i, align 8, !tbaa !126
  %arrayidx17.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %34, i64 0, i64 %idxprom.i.i.i
  %46 = load double, double* %arrayidx17.i.i.i, align 8, !tbaa !126
  %add18.i.i.i = fadd double %45, %46
  %47 = bitcast i64 %q.addr.03.i.i.i to double
  %add19.i.i.i = fadd double %add18.i.i.i, %47
  %arrayidx21.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %48 = load double, double* %arrayidx21.i.i.i, align 8, !tbaa !126
  %add22.i.i.i = fadd double %48, %add19.i.i.i
  store double %add22.i.i.i, double* %arrayidx21.i.i.i, align 8, !tbaa !126
  %cmp23.i.i.i = icmp eq i32 %.omp.iv.04.i.i.i, 3071
  %add25.i.i.i = fadd double %43, 6.000000e+00
  %49 = bitcast double %add25.i.i.i to i64
  %add26.i.i.i = fadd double %47, 9.000000e+00
  %50 = bitcast double %add26.i.i.i to i64
  %spec.select.i.i.i = select i1 %cmp23.i.i.i, i64 %50, i64 %q.addr.03.i.i.i
  %spec.select1.i.i.i = select i1 %cmp23.i.i.i, i64 %49, i64 %p.addr.05.i.i.i
  %add27.i.i.i = add nsw i32 %.omp.iv.04.i.i.i, 1
  %cmp5.i.i.i = icmp slt i32 %.omp.iv.04.i.i.i, %cond.i.i.i
  br i1 %cmp5.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__59_wrapper.exit.i

__omp_outlined__59_wrapper.exit.i:                ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %51 = bitcast i8* %10 to void (i16, i32)*
  call void %51(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__59_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l737_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %52 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %52, -32
  %53 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %53, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %54 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %54)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %55 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %56 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %57 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %57) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %58 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %58) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %59 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %60 = icmp slt i32 %59, 3071
  %cond.i = select i1 %60, i32 %59, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %61 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp72.i = icmp sgt i32 %61, %cond.i
  br i1 %cmp72.i, label %__omp_outlined__58.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %62 = bitcast i32* %.omp.lb.i.i to i8*
  %63 = bitcast i32* %.omp.ub.i.i to i8*
  %64 = bitcast i32* %.omp.stride.i.i to i8*
  %65 = bitcast i32* %.omp.is_last.i.i to i8*
  %66 = inttoptr i64 %p to i8*
  %67 = inttoptr i64 %q to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %61, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %68 = load i32, i32* %.omp.comb.lb.i, align 4
  %69 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %70 = zext i32 %69 to i64
  %71 = zext i32 %68 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__59_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 9) #4
  %72 = load i8**, i8*** %shared_arg_refs.i, align 8
  %73 = inttoptr i64 %71 to i8*
  store i8* %73, i8** %72, align 8, !tbaa !124
  %74 = getelementptr inbounds i8*, i8** %72, i64 1
  %75 = inttoptr i64 %70 to i8*
  store i8* %75, i8** %74, align 8, !tbaa !124
  %76 = getelementptr inbounds i8*, i8** %72, i64 2
  %77 = bitcast i8** %76 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %77, align 8, !tbaa !124
  %78 = getelementptr inbounds i8*, i8** %72, i64 3
  %79 = bitcast i8** %78 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %79, align 8, !tbaa !124
  %80 = getelementptr inbounds i8*, i8** %72, i64 4
  %81 = bitcast i8** %80 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %81, align 8, !tbaa !124
  %82 = getelementptr inbounds i8*, i8** %72, i64 5
  store i8* %66, i8** %82, align 8, !tbaa !124
  %83 = getelementptr inbounds i8*, i8** %72, i64 6
  %84 = bitcast i8** %83 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %84, align 8, !tbaa !124
  %85 = getelementptr inbounds i8*, i8** %72, i64 7
  %86 = bitcast i8** %85 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %86, align 8, !tbaa !124
  %87 = getelementptr inbounds i8*, i8** %72, i64 8
  store i8* %67, i8** %87, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %62) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %63) #4
  store i32 %68, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %69, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %64) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %65) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %88 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %89 = icmp slt i32 %88, 3071
  %cond.i.i = select i1 %89, i32 %88, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %90 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp52.i.i = icmp sgt i32 %90, %cond.i.i
  br i1 %cmp52.i.i, label %__omp_outlined__59.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %p.addr.05.i.i = phi i64 [ %spec.select1.i.i, %omp.inner.for.body.i.i ], [ %p, %omp_if.else.i ]
  %.omp.iv.04.i.i = phi i32 [ %add27.i.i, %omp.inner.for.body.i.i ], [ %90, %omp_if.else.i ]
  %q.addr.03.i.i = phi i64 [ %spec.select.i.i, %omp.inner.for.body.i.i ], [ %q, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.04.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %91 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx8.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %92 = load double, double* %arrayidx8.i.i, align 8, !tbaa !126
  %add9.i.i = fadd double %91, %92
  %93 = bitcast i64 %p.addr.05.i.i to double
  %add10.i.i = fadd double %add9.i.i, %93
  %arrayidx12.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %94 = load double, double* %arrayidx12.i.i, align 8, !tbaa !126
  %add13.i.i = fadd double %94, %add10.i.i
  store double %add13.i.i, double* %arrayidx12.i.i, align 8, !tbaa !126
  %95 = load double, double* %arrayidx8.i.i, align 8, !tbaa !126
  %arrayidx17.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %96 = load double, double* %arrayidx17.i.i, align 8, !tbaa !126
  %add18.i.i = fadd double %95, %96
  %97 = bitcast i64 %q.addr.03.i.i to double
  %add19.i.i = fadd double %add18.i.i, %97
  %arrayidx21.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %98 = load double, double* %arrayidx21.i.i, align 8, !tbaa !126
  %add22.i.i = fadd double %98, %add19.i.i
  store double %add22.i.i, double* %arrayidx21.i.i, align 8, !tbaa !126
  %cmp23.i.i = icmp eq i32 %.omp.iv.04.i.i, 3071
  %add25.i.i = fadd double %93, 6.000000e+00
  %99 = bitcast double %add25.i.i to i64
  %add26.i.i = fadd double %97, 9.000000e+00
  %100 = bitcast double %add26.i.i to i64
  %spec.select.i.i = select i1 %cmp23.i.i, i64 %100, i64 %q.addr.03.i.i
  %spec.select1.i.i = select i1 %cmp23.i.i, i64 %99, i64 %p.addr.05.i.i
  %add27.i.i = add nsw i32 %.omp.iv.04.i.i, 1
  %cmp5.i.i = icmp slt i32 %.omp.iv.04.i.i, %cond.i.i
  br i1 %cmp5.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__59.exit.i

__omp_outlined__59.exit.i:                        ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %65) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %64) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %63) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %62) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__59.exit.i, %omp_if.then.i
  %101 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %101, %.omp.iv.03.i
  %102 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp7.i = icmp sgt i32 %add.i, %102
  br i1 %cmp7.i, label %__omp_outlined__58.exit, label %omp.inner.for.body.i

__omp_outlined__58.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %58) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %57) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %54)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__58.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l737_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__59_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to i64*
  %19 = load i64, i64* %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %2, i64 7
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %2, i64 8
  %27 = bitcast i8** %26 to i64*
  %28 = load i64, i64* %27, align 8, !tbaa !124
  %29 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %29) #4
  %30 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %30) #4
  %conv2.i = trunc i64 %4 to i32
  %conv3.i = trunc i64 %7 to i32
  store i32 %conv2.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv3.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %31 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %31) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %32 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %32) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %33 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %34 = icmp slt i32 %33, 3071
  %cond.i = select i1 %34, i32 %33, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp52.i = icmp sgt i32 %35, %cond.i
  br i1 %cmp52.i, label %__omp_outlined__59.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %p.addr.05.i = phi i64 [ %spec.select1.i, %omp.inner.for.body.i ], [ %19, %entry ]
  %.omp.iv.04.i = phi i32 [ %add27.i, %omp.inner.for.body.i ], [ %35, %entry ]
  %q.addr.03.i = phi i64 [ %spec.select.i, %omp.inner.for.body.i ], [ %28, %entry ]
  %idxprom.i = sext i32 %.omp.iv.04.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx8.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %37 = load double, double* %arrayidx8.i, align 8, !tbaa !126
  %add9.i = fadd double %36, %37
  %38 = bitcast i64 %p.addr.05.i to double
  %add10.i = fadd double %add9.i, %38
  %arrayidx12.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %39 = load double, double* %arrayidx12.i, align 8, !tbaa !126
  %add13.i = fadd double %39, %add10.i
  store double %add13.i, double* %arrayidx12.i, align 8, !tbaa !126
  %40 = load double, double* %arrayidx8.i, align 8, !tbaa !126
  %arrayidx17.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i
  %41 = load double, double* %arrayidx17.i, align 8, !tbaa !126
  %add18.i = fadd double %40, %41
  %42 = bitcast i64 %q.addr.03.i to double
  %add19.i = fadd double %add18.i, %42
  %arrayidx21.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %43 = load double, double* %arrayidx21.i, align 8, !tbaa !126
  %add22.i = fadd double %43, %add19.i
  store double %add22.i, double* %arrayidx21.i, align 8, !tbaa !126
  %cmp23.i = icmp eq i32 %.omp.iv.04.i, 3071
  %add25.i = fadd double %38, 6.000000e+00
  %44 = bitcast double %add25.i to i64
  %add26.i = fadd double %42, 9.000000e+00
  %45 = bitcast double %add26.i to i64
  %spec.select.i = select i1 %cmp23.i, i64 %45, i64 %q.addr.03.i
  %spec.select1.i = select i1 %cmp23.i, i64 %44, i64 %p.addr.05.i
  %add27.i = add nsw i32 %.omp.iv.04.i, 1
  %cmp5.i = icmp slt i32 %.omp.iv.04.i, %cond.i
  br i1 %cmp5.i, label %omp.inner.for.body.i, label %__omp_outlined__59.exit

__omp_outlined__59.exit:                          ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %32) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %31) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %30) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %29) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l883([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l883_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__61_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv2.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv2.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %32 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %33 = icmp slt i32 %32, 3071
  %cond.i.i.i = select i1 %33, i32 %32, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp41.i.i.i = icmp sgt i32 %34, %cond.i.i.i
  br i1 %cmp41.i.i.i, label %__omp_outlined__61_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add34.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %35 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx13.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx13.i.i.i, align 8, !tbaa !126
  %add14.i.i.i = fadd double %35, %36
  %arrayidx18.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx18.i.i.i, align 8, !tbaa !126
  %add19.i.i.i = fadd double %37, %add14.i.i.i
  store double %add19.i.i.i, double* %arrayidx18.i.i.i, align 8, !tbaa !126
  %38 = load double, double* %arrayidx13.i.i.i, align 8, !tbaa !126
  %arrayidx27.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx27.i.i.i, align 8, !tbaa !126
  %add28.i.i.i = fadd double %38, %39
  %arrayidx32.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx32.i.i.i, align 8, !tbaa !126
  %add33.i.i.i = fadd double %40, %add28.i.i.i
  store double %add33.i.i.i, double* %arrayidx32.i.i.i, align 8, !tbaa !126
  %add34.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp4.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %cond.i.i.i
  br i1 %cmp4.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__61_wrapper.exit.i

__omp_outlined__61_wrapper.exit.i:                ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__61_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l883_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp62.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp62.i, label %__omp_outlined__60.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__61_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %74 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %75 = icmp slt i32 %74, 3071
  %cond.i.i = select i1 %75, i32 %74, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp41.i.i = icmp sgt i32 %76, %cond.i.i
  br i1 %cmp41.i.i, label %__omp_outlined__61.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add34.i.i, %omp.inner.for.body.i.i ], [ %76, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %77 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx13.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx13.i.i, align 8, !tbaa !126
  %add14.i.i = fadd double %77, %78
  %arrayidx18.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx18.i.i, align 8, !tbaa !126
  %add19.i.i = fadd double %79, %add14.i.i
  store double %add19.i.i, double* %arrayidx18.i.i, align 8, !tbaa !126
  %80 = load double, double* %arrayidx13.i.i, align 8, !tbaa !126
  %arrayidx27.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx27.i.i, align 8, !tbaa !126
  %add28.i.i = fadd double %80, %81
  %arrayidx32.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx32.i.i, align 8, !tbaa !126
  %add33.i.i = fadd double %82, %add28.i.i
  store double %add33.i.i, double* %arrayidx32.i.i, align 8, !tbaa !126
  %add34.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp4.i.i = icmp slt i32 %.omp.iv.02.i.i, %cond.i.i
  br i1 %cmp4.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__61.exit.i

__omp_outlined__61.exit.i:                        ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__61.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp6.i = icmp sgt i32 %add.i, %84
  br i1 %cmp6.i, label %__omp_outlined__60.exit, label %omp.inner.for.body.i

__omp_outlined__60.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__60.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l883_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__61_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv2.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv2.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %27 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %28 = icmp slt i32 %27, 3071
  %cond.i = select i1 %28, i32 %27, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp41.i = icmp sgt i32 %29, %cond.i
  br i1 %cmp41.i, label %__omp_outlined__61.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add34.i, %omp.inner.for.body.i ], [ %29, %entry ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %30 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx13.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx13.i, align 8, !tbaa !126
  %add14.i = fadd double %30, %31
  %arrayidx18.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx18.i, align 8, !tbaa !126
  %add19.i = fadd double %32, %add14.i
  store double %add19.i, double* %arrayidx18.i, align 8, !tbaa !126
  %33 = load double, double* %arrayidx13.i, align 8, !tbaa !126
  %arrayidx27.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx27.i, align 8, !tbaa !126
  %add28.i = fadd double %33, %34
  %arrayidx32.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx32.i, align 8, !tbaa !126
  %add33.i = fadd double %35, %add28.i
  store double %add33.i, double* %arrayidx32.i, align 8, !tbaa !126
  %add34.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp4.i = icmp slt i32 %.omp.iv.02.i, %cond.i
  br i1 %cmp4.i, label %omp.inner.for.body.i, label %__omp_outlined__61.exit

__omp_outlined__61.exit:                          ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l905([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l905_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__63_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv2.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv2.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 38, i32 %conv.i.i.i, i32 %conv2.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__63_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__63_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !148
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add32.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %arrayidx11.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx11.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add12.i.i.i = fadd double %36, %37
  %arrayidx16.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx16.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add17.i.i.i = fadd double %38, %add12.i.i.i
  store double %add17.i.i.i, double* %arrayidx16.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %39 = load double, double* %arrayidx11.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %arrayidx25.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx25.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add26.i.i.i = fadd double %39, %40
  %arrayidx30.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx30.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add31.i.i.i = fadd double %41, %add26.i.i.i
  store double %add31.i.i.i, double* %arrayidx30.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add32.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !148

__omp_outlined__63_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__63_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l905_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp62.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp62.i, label %__omp_outlined__62.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__63_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 38, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__63.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__63.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !148
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add32.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %arrayidx11.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx11.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add12.i.i = fadd double %79, %80
  %arrayidx16.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx16.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add17.i.i = fadd double %81, %add12.i.i
  store double %add17.i.i, double* %arrayidx16.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %82 = load double, double* %arrayidx11.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %arrayidx25.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx25.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add26.i.i = fadd double %82, %83
  %arrayidx30.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx30.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add31.i.i = fadd double %84, %add26.i.i
  store double %add31.i.i, double* %arrayidx30.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add32.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !148

__omp_outlined__63.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__63.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp6.i = icmp sgt i32 %add.i, %86
  br i1 %cmp6.i, label %__omp_outlined__62.exit, label %omp.inner.for.body.i

__omp_outlined__62.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__62.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l905_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__63_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv2.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv2.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 38, i32 %conv.i, i32 %conv2.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__63.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__63.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !148
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add32.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %arrayidx11.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx11.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add12.i = fadd double %31, %32
  %arrayidx16.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx16.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add17.i = fadd double %33, %add12.i
  store double %add17.i, double* %arrayidx16.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %34 = load double, double* %arrayidx11.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %arrayidx25.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx25.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add26.i = fadd double %34, %35
  %arrayidx30.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx30.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add31.i = fadd double %36, %add26.i
  store double %add31.i, double* %arrayidx30.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !148
  %add32.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !148

__omp_outlined__63.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l927([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l927_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__65_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv2.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv2.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 35, i32 %conv.i.i.i, i32 %conv2.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__65_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__65_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !149
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add32.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %arrayidx11.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx11.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add12.i.i.i = fadd double %36, %37
  %arrayidx16.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx16.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add17.i.i.i = fadd double %38, %add12.i.i.i
  store double %add17.i.i.i, double* %arrayidx16.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %39 = load double, double* %arrayidx11.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %arrayidx25.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx25.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add26.i.i.i = fadd double %39, %40
  %arrayidx30.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx30.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add31.i.i.i = fadd double %41, %add26.i.i.i
  store double %add31.i.i.i, double* %arrayidx30.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add32.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !149

__omp_outlined__65_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__65_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l927_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp62.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp62.i, label %__omp_outlined__64.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__65_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 35, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__65.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__65.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !149
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add32.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %arrayidx11.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx11.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add12.i.i = fadd double %79, %80
  %arrayidx16.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx16.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add17.i.i = fadd double %81, %add12.i.i
  store double %add17.i.i, double* %arrayidx16.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %82 = load double, double* %arrayidx11.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %arrayidx25.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx25.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add26.i.i = fadd double %82, %83
  %arrayidx30.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx30.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add31.i.i = fadd double %84, %add26.i.i
  store double %add31.i.i, double* %arrayidx30.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add32.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !149

__omp_outlined__65.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__65.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp6.i = icmp sgt i32 %add.i, %86
  br i1 %cmp6.i, label %__omp_outlined__64.exit, label %omp.inner.for.body.i

__omp_outlined__64.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__64.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l927_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__65_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv2.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv2.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 35, i32 %conv.i, i32 %conv2.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__65.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__65.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !149
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add32.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %arrayidx11.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx11.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add12.i = fadd double %31, %32
  %arrayidx16.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx16.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add17.i = fadd double %33, %add12.i
  store double %add17.i, double* %arrayidx16.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %34 = load double, double* %arrayidx11.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %arrayidx25.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx25.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add26.i = fadd double %34, %35
  %arrayidx30.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx30.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add31.i = fadd double %36, %add26.i
  store double %add31.i, double* %arrayidx30.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !149
  %add32.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !149

__omp_outlined__65.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l949([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l949_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__67_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv2.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv2.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 36, i32 %conv.i.i.i, i32 %conv2.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__67_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__67_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !150
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add32.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %arrayidx11.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx11.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add12.i.i.i = fadd double %36, %37
  %arrayidx16.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx16.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add17.i.i.i = fadd double %38, %add12.i.i.i
  store double %add17.i.i.i, double* %arrayidx16.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %39 = load double, double* %arrayidx11.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %arrayidx25.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx25.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add26.i.i.i = fadd double %39, %40
  %arrayidx30.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx30.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add31.i.i.i = fadd double %41, %add26.i.i.i
  store double %add31.i.i.i, double* %arrayidx30.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add32.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !150

__omp_outlined__67_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__67_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l949_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp62.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp62.i, label %__omp_outlined__66.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__67_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 36, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__67.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__67.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !150
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add32.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %arrayidx11.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx11.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add12.i.i = fadd double %79, %80
  %arrayidx16.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx16.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add17.i.i = fadd double %81, %add12.i.i
  store double %add17.i.i, double* %arrayidx16.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %82 = load double, double* %arrayidx11.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %arrayidx25.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx25.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add26.i.i = fadd double %82, %83
  %arrayidx30.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx30.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add31.i.i = fadd double %84, %add26.i.i
  store double %add31.i.i, double* %arrayidx30.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add32.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !150

__omp_outlined__67.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__67.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp6.i = icmp sgt i32 %add.i, %86
  br i1 %cmp6.i, label %__omp_outlined__66.exit, label %omp.inner.for.body.i

__omp_outlined__66.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__66.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l949_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__67_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv2.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv2.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 36, i32 %conv.i, i32 %conv2.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__67.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__67.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !150
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add32.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %arrayidx11.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx11.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add12.i = fadd double %31, %32
  %arrayidx16.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx16.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add17.i = fadd double %33, %add12.i
  store double %add17.i, double* %arrayidx16.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %34 = load double, double* %arrayidx11.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %arrayidx25.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx25.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add26.i = fadd double %34, %35
  %arrayidx30.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx30.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add31.i = fadd double %36, %add26.i
  store double %add31.i, double* %arrayidx30.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !150
  %add32.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !150

__omp_outlined__67.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l971([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l971_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__69_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv2.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv2.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %3, i32 37, i32 %conv.i.i.i, i32 %conv2.i.i.i, i32 1, i32 1) #4
  %32 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool3.i.i.i = icmp eq i32 %32, 0
  br i1 %tobool3.i.i.i, label %__omp_outlined__69_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.cond.loopexit.i.i.i:                 ; preds = %omp.inner.for.body.i.i.i, %omp.dispatch.body.i.i.i
  %33 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %3, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i) #4
  %tobool.i.i.i = icmp eq i32 %33, 0
  br i1 %tobool.i.i.i, label %__omp_outlined__69_wrapper.exit.i, label %omp.dispatch.body.i.i.i

omp.dispatch.body.i.i.i:                          ; preds = %.execute.fn.i, %omp.dispatch.cond.loopexit.i.i.i
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %35 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !151
  %cmp1.i.i.i = icmp sgt i32 %34, %35
  br i1 %cmp1.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %omp.dispatch.body.i.i.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add32.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %omp.dispatch.body.i.i.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %arrayidx11.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx11.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add12.i.i.i = fadd double %36, %37
  %arrayidx16.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %38 = load double, double* %arrayidx16.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add17.i.i.i = fadd double %38, %add12.i.i.i
  store double %add17.i.i.i, double* %arrayidx16.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %39 = load double, double* %arrayidx11.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %arrayidx25.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx25.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add26.i.i.i = fadd double %39, %40
  %arrayidx30.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %41 = load double, double* %arrayidx30.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add31.i.i.i = fadd double %41, %add26.i.i.i
  store double %add31.i.i.i, double* %arrayidx30.i.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add32.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %35
  br i1 %cmp.i.i.i, label %omp.inner.for.body.i.i.i, label %omp.dispatch.cond.loopexit.i.i.i, !llvm.loop !151

__omp_outlined__69_wrapper.exit.i:                ; preds = %omp.dispatch.cond.loopexit.i.i.i, %.execute.fn.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %42 = bitcast i8* %10 to void (i16, i32)*
  call void %42(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__69_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l971_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %43 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %43, -32
  %44 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %44, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %45 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %46 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %49 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %49) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %50 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = icmp slt i32 %50, 3071
  %cond.i = select i1 %51, i32 %50, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %52 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp62.i = icmp sgt i32 %52, %cond.i
  br i1 %cmp62.i, label %__omp_outlined__68.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %53 = bitcast i32* %.omp.lb.i.i to i8*
  %54 = bitcast i32* %.omp.ub.i.i to i8*
  %55 = bitcast i32* %.omp.stride.i.i to i8*
  %56 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %52, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %57 = load i32, i32* %.omp.comb.lb.i, align 4
  %58 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %59 = zext i32 %58 to i64
  %60 = zext i32 %57 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__69_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %61 = load i8**, i8*** %shared_arg_refs.i, align 8
  %62 = inttoptr i64 %60 to i8*
  store i8* %62, i8** %61, align 8, !tbaa !124
  %63 = getelementptr inbounds i8*, i8** %61, i64 1
  %64 = inttoptr i64 %59 to i8*
  store i8* %64, i8** %63, align 8, !tbaa !124
  %65 = getelementptr inbounds i8*, i8** %61, i64 2
  %66 = bitcast i8** %65 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %66, align 8, !tbaa !124
  %67 = getelementptr inbounds i8*, i8** %61, i64 3
  %68 = bitcast i8** %67 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %68, align 8, !tbaa !124
  %69 = getelementptr inbounds i8*, i8** %61, i64 4
  %70 = bitcast i8** %69 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %70, align 8, !tbaa !124
  %71 = getelementptr inbounds i8*, i8** %61, i64 5
  %72 = bitcast i8** %71 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %72, align 8, !tbaa !124
  %73 = getelementptr inbounds i8*, i8** %61, i64 6
  %74 = bitcast i8** %73 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %74, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 %57, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %58, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %56) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 37, i32 %57, i32 %58, i32 1, i32 1) #4
  %75 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool3.i.i = icmp eq i32 %75, 0
  br i1 %tobool3.i.i, label %__omp_outlined__69.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.cond.loopexit.i.i:                   ; preds = %omp.inner.for.body.i.i, %omp.dispatch.body.i.i
  %76 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4
  %tobool.i.i = icmp eq i32 %76, 0
  br i1 %tobool.i.i, label %__omp_outlined__69.exit.i, label %omp.dispatch.body.i.i

omp.dispatch.body.i.i:                            ; preds = %omp_if.else.i, %omp.dispatch.cond.loopexit.i.i
  %77 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %78 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !151
  %cmp1.i.i = icmp sgt i32 %77, %78
  br i1 %cmp1.i.i, label %omp.dispatch.cond.loopexit.i.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp.dispatch.body.i.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add32.i.i, %omp.inner.for.body.i.i ], [ %77, %omp.dispatch.body.i.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %arrayidx11.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %80 = load double, double* %arrayidx11.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add12.i.i = fadd double %79, %80
  %arrayidx16.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx16.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add17.i.i = fadd double %81, %add12.i.i
  store double %add17.i.i, double* %arrayidx16.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %82 = load double, double* %arrayidx11.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %arrayidx25.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %83 = load double, double* %arrayidx25.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add26.i.i = fadd double %82, %83
  %arrayidx30.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %84 = load double, double* %arrayidx30.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add31.i.i = fadd double %84, %add26.i.i
  store double %add31.i.i, double* %arrayidx30.i.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add32.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp.i.i = icmp slt i32 %.omp.iv.02.i.i, %78
  br i1 %cmp.i.i, label %omp.inner.for.body.i.i, label %omp.dispatch.cond.loopexit.i.i, !llvm.loop !151

__omp_outlined__69.exit.i:                        ; preds = %omp.dispatch.cond.loopexit.i.i, %omp_if.else.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %56) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__69.exit.i, %omp_if.then.i
  %85 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %85, %.omp.iv.03.i
  %86 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp6.i = icmp sgt i32 %add.i, %86
  br i1 %cmp6.i, label %__omp_outlined__68.exit, label %omp.inner.for.body.i

__omp_outlined__68.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %49) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__68.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l971_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__69_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv2.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv2.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %1, i32 37, i32 %conv.i, i32 %conv2.i, i32 1, i32 1) #4
  %27 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool3.i = icmp eq i32 %27, 0
  br i1 %tobool3.i, label %__omp_outlined__69.exit, label %omp.dispatch.body.i

omp.dispatch.cond.loopexit.i:                     ; preds = %omp.inner.for.body.i, %omp.dispatch.body.i
  %28 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %1, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i) #4
  %tobool.i = icmp eq i32 %28, 0
  br i1 %tobool.i, label %__omp_outlined__69.exit, label %omp.dispatch.body.i

omp.dispatch.body.i:                              ; preds = %entry, %omp.dispatch.cond.loopexit.i
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %30 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !151
  %cmp1.i = icmp sgt i32 %29, %30
  br i1 %cmp1.i, label %omp.dispatch.cond.loopexit.i, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.dispatch.body.i, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add32.i, %omp.inner.for.body.i ], [ %29, %omp.dispatch.body.i ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %arrayidx11.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx11.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add12.i = fadd double %31, %32
  %arrayidx16.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %33 = load double, double* %arrayidx16.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add17.i = fadd double %33, %add12.i
  store double %add17.i, double* %arrayidx16.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %34 = load double, double* %arrayidx11.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %arrayidx25.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx25.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add26.i = fadd double %34, %35
  %arrayidx30.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %36 = load double, double* %arrayidx30.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add31.i = fadd double %36, %add26.i
  store double %add31.i, double* %arrayidx30.i, align 8, !tbaa !126, !llvm.mem.parallel_loop_access !151
  %add32.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp.i = icmp slt i32 %.omp.iv.02.i, %30
  br i1 %cmp.i, label %omp.inner.for.body.i, label %omp.dispatch.cond.loopexit.i, !llvm.loop !151

__omp_outlined__69.exit:                          ; preds = %omp.dispatch.cond.loopexit.i, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l993([3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %shared_arg_refs.i = alloca i8**, align 8
  %.omp.lb.i.i.i = alloca i32, align 4
  %.omp.ub.i.i.i = alloca i32, align 4
  %.omp.stride.i.i.i = alloca i32, align 4
  %.omp.is_last.i.i.i = alloca i32, align 4
  %global_args.i.i = alloca i8**, align 8
  %work_fn.i = alloca i8*, align 8
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_tid = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x(), !range !118
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  %thread_limit = add nsw i32 %nvptx_num_threads, -32
  %1 = icmp ult i32 %nvptx_tid, %thread_limit
  br i1 %1, label %.worker, label %.mastercheck

.worker:                                          ; preds = %entry
  %2 = bitcast i8** %work_fn.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2)
  %3 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  store i8* null, i8** %work_fn.i, align 8
  %4 = bitcast i8*** %global_args.i.i to i8*
  %5 = bitcast i32* %.omp.lb.i.i.i to i8*
  %6 = bitcast i32* %.omp.ub.i.i.i to i8*
  %7 = bitcast i32* %.omp.stride.i.i.i to i8*
  %8 = bitcast i32* %.omp.is_last.i.i.i to i8*
  br label %.await.work.i

.await.work.i:                                    ; preds = %.barrier.parallel.i, %.worker
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_data_sharing_init_stack() #4
  %9 = call i1 @__kmpc_kernel_parallel(i8** nonnull %work_fn.i, i16 1) #4
  %10 = load i8*, i8** %work_fn.i, align 8
  %should_terminate.i = icmp eq i8* %10, null
  br i1 %should_terminate.i, label %__omp_offloading_802_14c18b2_main_l993_worker.exit, label %.select.workers.i

.select.workers.i:                                ; preds = %.await.work.i
  br i1 %9, label %.execute.parallel.i, label %.barrier.parallel.i

.execute.parallel.i:                              ; preds = %.select.workers.i
  %work_match.i = icmp eq i8* %10, bitcast (void (i16, i32)* @__omp_outlined__71_wrapper to i8*)
  br i1 %work_match.i, label %.execute.fn.i, label %.check.next.i

.execute.fn.i:                                    ; preds = %.execute.parallel.i
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %4) #4
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args.i.i) #4
  %11 = load i8**, i8*** %global_args.i.i, align 8
  %12 = bitcast i8** %11 to i64*
  %13 = load i64, i64* %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %11, i64 1
  %15 = bitcast i8** %14 to i64*
  %16 = load i64, i64* %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %11, i64 2
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %11, i64 3
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = getelementptr inbounds i8*, i8** %11, i64 4
  %24 = bitcast i8** %23 to [3072 x double]**
  %25 = load [3072 x double]*, [3072 x double]** %24, align 8, !tbaa !124
  %26 = getelementptr inbounds i8*, i8** %11, i64 5
  %27 = bitcast i8** %26 to [3072 x double]**
  %28 = load [3072 x double]*, [3072 x double]** %27, align 8, !tbaa !124
  %29 = getelementptr inbounds i8*, i8** %11, i64 6
  %30 = bitcast i8** %29 to [3072 x double]**
  %31 = load [3072 x double]*, [3072 x double]** %30, align 8, !tbaa !124
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %6) #4
  %conv.i.i.i = trunc i64 %13 to i32
  %conv2.i.i.i = trunc i64 %16 to i32
  store i32 %conv.i.i.i, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  store i32 %conv2.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4
  store i32 1, i32* %.omp.stride.i.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4
  store i32 0, i32* %.omp.is_last.i.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %3, i32 34, i32* nonnull %.omp.is_last.i.i.i, i32* nonnull %.omp.lb.i.i.i, i32* nonnull %.omp.ub.i.i.i, i32* nonnull %.omp.stride.i.i.i, i32 1, i32 1) #4
  %32 = load i32, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %33 = icmp slt i32 %32, 3071
  %cond.i.i.i = select i1 %33, i32 %32, i32 3071
  store i32 %cond.i.i.i, i32* %.omp.ub.i.i.i, align 4, !tbaa !120
  %34 = load i32, i32* %.omp.lb.i.i.i, align 4, !tbaa !120
  %cmp41.i.i.i = icmp sgt i32 %34, %cond.i.i.i
  br i1 %cmp41.i.i.i, label %__omp_outlined__71_wrapper.exit.i, label %omp.inner.for.body.i.i.i

omp.inner.for.body.i.i.i:                         ; preds = %.execute.fn.i, %omp.inner.for.body.i.i.i
  %.omp.iv.02.i.i.i = phi i32 [ %add34.i.i.i, %omp.inner.for.body.i.i.i ], [ %34, %.execute.fn.i ]
  %idxprom.i.i.i = sext i32 %.omp.iv.02.i.i.i to i64
  %arrayidx.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i.i.i
  %35 = load double, double* %arrayidx.i.i.i, align 8, !tbaa !126
  %arrayidx13.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %25, i64 0, i64 %idxprom.i.i.i
  %36 = load double, double* %arrayidx13.i.i.i, align 8, !tbaa !126
  %add14.i.i.i = fadd double %35, %36
  %arrayidx18.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i.i.i
  %37 = load double, double* %arrayidx18.i.i.i, align 8, !tbaa !126
  %add19.i.i.i = fadd double %37, %add14.i.i.i
  store double %add19.i.i.i, double* %arrayidx18.i.i.i, align 8, !tbaa !126
  %38 = load double, double* %arrayidx13.i.i.i, align 8, !tbaa !126
  %arrayidx27.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %31, i64 0, i64 %idxprom.i.i.i
  %39 = load double, double* %arrayidx27.i.i.i, align 8, !tbaa !126
  %add28.i.i.i = fadd double %38, %39
  %arrayidx32.i.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %28, i64 0, i64 %idxprom.i.i.i
  %40 = load double, double* %arrayidx32.i.i.i, align 8, !tbaa !126
  %add33.i.i.i = fadd double %40, %add28.i.i.i
  store double %add33.i.i.i, double* %arrayidx32.i.i.i, align 8, !tbaa !126
  %add34.i.i.i = add nsw i32 %.omp.iv.02.i.i.i, 1
  %cmp4.i.i.i = icmp slt i32 %.omp.iv.02.i.i.i, %cond.i.i.i
  br i1 %cmp4.i.i.i, label %omp.inner.for.body.i.i.i, label %__omp_outlined__71_wrapper.exit.i

__omp_outlined__71_wrapper.exit.i:                ; preds = %omp.inner.for.body.i.i.i, %.execute.fn.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %3) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %6) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %4) #4
  br label %.terminate.parallel.i

.check.next.i:                                    ; preds = %.execute.parallel.i
  %41 = bitcast i8* %10 to void (i16, i32)*
  call void %41(i16 0, i32 %3) #4
  br label %.terminate.parallel.i

.terminate.parallel.i:                            ; preds = %.check.next.i, %__omp_outlined__71_wrapper.exit.i
  call void @__kmpc_kernel_end_parallel() #4
  br label %.barrier.parallel.i

.barrier.parallel.i:                              ; preds = %.terminate.parallel.i, %.select.workers.i
  call void @llvm.nvvm.barrier0() #4
  br label %.await.work.i

__omp_offloading_802_14c18b2_main_l993_worker.exit: ; preds = %.await.work.i
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2)
  br label %.exit

.mastercheck:                                     ; preds = %entry
  %42 = add nsw i32 %nvptx_num_threads, -1
  %master_tid = and i32 %42, -32
  %43 = icmp eq i32 %nvptx_tid, %master_tid
  br i1 %43, label %.master, label %.exit

.master:                                          ; preds = %.mastercheck
  tail call void @__kmpc_kernel_init(i32 %thread_limit, i16 1) #4
  tail call void @__kmpc_data_sharing_init_stack() #4
  %44 = bitcast i8*** %shared_arg_refs.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44)
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr.1 to i32
  %45 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %45) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %46 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %46) #4
  store i32 3071, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %47 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %47) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %48 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %48) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %49 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %50 = icmp slt i32 %49, 3071
  %cond.i = select i1 %50, i32 %49, i32 3071
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %51 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp62.i = icmp sgt i32 %51, %cond.i
  br i1 %cmp62.i, label %__omp_outlined__70.exit, label %omp.inner.for.body.lr.ph.i

omp.inner.for.body.lr.ph.i:                       ; preds = %.master
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %tobool.i = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i, 0
  %52 = bitcast i32* %.omp.lb.i.i to i8*
  %53 = bitcast i32* %.omp.ub.i.i to i8*
  %54 = bitcast i32* %.omp.stride.i.i to i8*
  %55 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %omp.inner.for.inc.i, %omp.inner.for.body.lr.ph.i
  %.omp.iv.03.i = phi i32 [ %51, %omp.inner.for.body.lr.ph.i ], [ %add.i, %omp.inner.for.inc.i ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @0, i32 %0, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i) #4
  %56 = load i32, i32* %.omp.comb.lb.i, align 4
  %57 = load i32, i32* %.omp.comb.ub.i, align 4
  br i1 %tobool.i, label %omp_if.else.i, label %omp_if.then.i

omp_if.then.i:                                    ; preds = %omp.inner.for.body.i
  %58 = zext i32 %57 to i64
  %59 = zext i32 %56 to i64
  call void @__kmpc_kernel_prepare_parallel(i8* bitcast (void (i16, i32)* @__omp_outlined__71_wrapper to i8*), i16 1) #4
  call void @__kmpc_begin_sharing_variables(i8*** nonnull %shared_arg_refs.i, i64 7) #4
  %60 = load i8**, i8*** %shared_arg_refs.i, align 8
  %61 = inttoptr i64 %59 to i8*
  store i8* %61, i8** %60, align 8, !tbaa !124
  %62 = getelementptr inbounds i8*, i8** %60, i64 1
  %63 = inttoptr i64 %58 to i8*
  store i8* %63, i8** %62, align 8, !tbaa !124
  %64 = getelementptr inbounds i8*, i8** %60, i64 2
  %65 = bitcast i8** %64 to [3072 x double]**
  store [3072 x double]* %A, [3072 x double]** %65, align 8, !tbaa !124
  %66 = getelementptr inbounds i8*, i8** %60, i64 3
  %67 = bitcast i8** %66 to [3072 x double]**
  store [3072 x double]* %C, [3072 x double]** %67, align 8, !tbaa !124
  %68 = getelementptr inbounds i8*, i8** %60, i64 4
  %69 = bitcast i8** %68 to [3072 x double]**
  store [3072 x double]* %D, [3072 x double]** %69, align 8, !tbaa !124
  %70 = getelementptr inbounds i8*, i8** %60, i64 5
  %71 = bitcast i8** %70 to [3072 x double]**
  store [3072 x double]* %B, [3072 x double]** %71, align 8, !tbaa !124
  %72 = getelementptr inbounds i8*, i8** %60, i64 6
  %73 = bitcast i8** %72 to [3072 x double]**
  store [3072 x double]* %E, [3072 x double]** %73, align 8, !tbaa !124
  call void @llvm.nvvm.barrier0() #4
  call void @llvm.nvvm.barrier0() #4
  call void @__kmpc_end_sharing_variables() #4
  br label %omp.inner.for.inc.i

omp_if.else.i:                                    ; preds = %omp.inner.for.body.i
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %52) #4
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %53) #4
  store i32 %56, i32* %.omp.lb.i.i, align 4, !tbaa !120
  store i32 %57, i32* %.omp.ub.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %54) #4
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %55) #4
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4
  %74 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %75 = icmp slt i32 %74, 3071
  %cond.i.i = select i1 %75, i32 %74, i32 3071
  store i32 %cond.i.i, i32* %.omp.ub.i.i, align 4, !tbaa !120
  %76 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120
  %cmp41.i.i = icmp sgt i32 %76, %cond.i.i
  br i1 %cmp41.i.i, label %__omp_outlined__71.exit.i, label %omp.inner.for.body.i.i

omp.inner.for.body.i.i:                           ; preds = %omp_if.else.i, %omp.inner.for.body.i.i
  %.omp.iv.02.i.i = phi i32 [ %add34.i.i, %omp.inner.for.body.i.i ], [ %76, %omp_if.else.i ]
  %idxprom.i.i = sext i32 %.omp.iv.02.i.i to i64
  %arrayidx.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %idxprom.i.i
  %77 = load double, double* %arrayidx.i.i, align 8, !tbaa !126
  %arrayidx13.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %idxprom.i.i
  %78 = load double, double* %arrayidx13.i.i, align 8, !tbaa !126
  %add14.i.i = fadd double %77, %78
  %arrayidx18.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %idxprom.i.i
  %79 = load double, double* %arrayidx18.i.i, align 8, !tbaa !126
  %add19.i.i = fadd double %79, %add14.i.i
  store double %add19.i.i, double* %arrayidx18.i.i, align 8, !tbaa !126
  %80 = load double, double* %arrayidx13.i.i, align 8, !tbaa !126
  %arrayidx27.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %idxprom.i.i
  %81 = load double, double* %arrayidx27.i.i, align 8, !tbaa !126
  %add28.i.i = fadd double %80, %81
  %arrayidx32.i.i = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %idxprom.i.i
  %82 = load double, double* %arrayidx32.i.i, align 8, !tbaa !126
  %add33.i.i = fadd double %82, %add28.i.i
  store double %add33.i.i, double* %arrayidx32.i.i, align 8, !tbaa !126
  %add34.i.i = add nsw i32 %.omp.iv.02.i.i, 1
  %cmp4.i.i = icmp slt i32 %.omp.iv.02.i.i, %cond.i.i
  br i1 %cmp4.i.i, label %omp.inner.for.body.i.i, label %__omp_outlined__71.exit.i

__omp_outlined__71.exit.i:                        ; preds = %omp.inner.for.body.i.i, %omp_if.else.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %55) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %54) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %53) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %52) #4
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @0, i32 %0) #4
  br label %omp.inner.for.inc.i

omp.inner.for.inc.i:                              ; preds = %__omp_outlined__71.exit.i, %omp_if.then.i
  %83 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add.i = add nsw i32 %83, %.omp.iv.03.i
  %84 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp6.i = icmp sgt i32 %add.i, %84
  br i1 %cmp6.i, label %__omp_outlined__70.exit, label %omp.inner.for.body.i

__omp_outlined__70.exit:                          ; preds = %omp.inner.for.inc.i, %.master
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %48) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %47) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %46) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %45) #4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44)
  call void @__kmpc_kernel_deinit(i16 1) #4
  call void @llvm.nvvm.barrier0()
  br label %.exit

.exit:                                            ; preds = %__omp_outlined__70.exit, %.mastercheck, %__omp_offloading_802_14c18b2_main_l993_worker.exit
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @__omp_outlined__71_wrapper(i16 zeroext, i32) #0 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %global_args = alloca i8**, align 8
  call void @__kmpc_get_shared_variables(i8*** nonnull %global_args) #4
  %2 = load i8**, i8*** %global_args, align 8
  %3 = bitcast i8** %2 to i64*
  %4 = load i64, i64* %3, align 8, !tbaa !124
  %5 = getelementptr inbounds i8*, i8** %2, i64 1
  %6 = bitcast i8** %5 to i64*
  %7 = load i64, i64* %6, align 8, !tbaa !124
  %8 = getelementptr inbounds i8*, i8** %2, i64 2
  %9 = bitcast i8** %8 to [3072 x double]**
  %10 = load [3072 x double]*, [3072 x double]** %9, align 8, !tbaa !124
  %11 = getelementptr inbounds i8*, i8** %2, i64 3
  %12 = bitcast i8** %11 to [3072 x double]**
  %13 = load [3072 x double]*, [3072 x double]** %12, align 8, !tbaa !124
  %14 = getelementptr inbounds i8*, i8** %2, i64 4
  %15 = bitcast i8** %14 to [3072 x double]**
  %16 = load [3072 x double]*, [3072 x double]** %15, align 8, !tbaa !124
  %17 = getelementptr inbounds i8*, i8** %2, i64 5
  %18 = bitcast i8** %17 to [3072 x double]**
  %19 = load [3072 x double]*, [3072 x double]** %18, align 8, !tbaa !124
  %20 = getelementptr inbounds i8*, i8** %2, i64 6
  %21 = bitcast i8** %20 to [3072 x double]**
  %22 = load [3072 x double]*, [3072 x double]** %21, align 8, !tbaa !124
  %23 = bitcast i32* %.omp.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %23) #4
  %24 = bitcast i32* %.omp.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %24) #4
  %conv.i = trunc i64 %4 to i32
  %conv2.i = trunc i64 %7 to i32
  store i32 %conv.i, i32* %.omp.lb.i, align 4, !tbaa !120
  store i32 %conv2.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %25 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %25) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %26 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %26) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %1, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %27 = load i32, i32* %.omp.ub.i, align 4, !tbaa !120
  %28 = icmp slt i32 %27, 3071
  %cond.i = select i1 %28, i32 %27, i32 3071
  store i32 %cond.i, i32* %.omp.ub.i, align 4, !tbaa !120
  %29 = load i32, i32* %.omp.lb.i, align 4, !tbaa !120
  %cmp41.i = icmp sgt i32 %29, %cond.i
  br i1 %cmp41.i, label %__omp_outlined__71.exit, label %omp.inner.for.body.i

omp.inner.for.body.i:                             ; preds = %entry, %omp.inner.for.body.i
  %.omp.iv.02.i = phi i32 [ %add34.i, %omp.inner.for.body.i ], [ %29, %entry ]
  %idxprom.i = sext i32 %.omp.iv.02.i to i64
  %arrayidx.i = getelementptr inbounds [3072 x double], [3072 x double]* %13, i64 0, i64 %idxprom.i
  %30 = load double, double* %arrayidx.i, align 8, !tbaa !126
  %arrayidx13.i = getelementptr inbounds [3072 x double], [3072 x double]* %16, i64 0, i64 %idxprom.i
  %31 = load double, double* %arrayidx13.i, align 8, !tbaa !126
  %add14.i = fadd double %30, %31
  %arrayidx18.i = getelementptr inbounds [3072 x double], [3072 x double]* %10, i64 0, i64 %idxprom.i
  %32 = load double, double* %arrayidx18.i, align 8, !tbaa !126
  %add19.i = fadd double %32, %add14.i
  store double %add19.i, double* %arrayidx18.i, align 8, !tbaa !126
  %33 = load double, double* %arrayidx13.i, align 8, !tbaa !126
  %arrayidx27.i = getelementptr inbounds [3072 x double], [3072 x double]* %22, i64 0, i64 %idxprom.i
  %34 = load double, double* %arrayidx27.i, align 8, !tbaa !126
  %add28.i = fadd double %33, %34
  %arrayidx32.i = getelementptr inbounds [3072 x double], [3072 x double]* %19, i64 0, i64 %idxprom.i
  %35 = load double, double* %arrayidx32.i, align 8, !tbaa !126
  %add33.i = fadd double %35, %add28.i
  store double %add33.i, double* %arrayidx32.i, align 8, !tbaa !126
  %add34.i = add nsw i32 %.omp.iv.02.i, 1
  %cmp4.i = icmp slt i32 %.omp.iv.02.i, %cond.i
  br i1 %cmp4.i, label %omp.inner.for.body.i, label %__omp_outlined__71.exit

__omp_outlined__71.exit:                          ; preds = %omp.inner.for.body.i, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %1) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %26) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %25) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %24) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %23) #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1036(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub2.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre4.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre5.i = bitcast i32* %.omp.stride.i to i8*
  %.pre7.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre9.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__72.exit

omp.precond.then.i:                               ; preds = %entry
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub2.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp6.i = icmp sgt i32 %5, %sub2.i
  %cond.i = select i1 %cmp6.i, i32 %sub2.i, i32 %5
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp81.i = icmp sgt i32 %6, %cond.i
  br i1 %cmp81.i, label %omp.loop.exit.i, label %omp.inner.for.body.us.preheader.i

omp.inner.for.body.us.preheader.i:                ; preds = %omp.precond.then.i
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.us.i

omp.inner.for.body.us.i:                          ; preds = %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i, %omp.inner.for.body.us.preheader.i
  %11 = phi i32 [ %.pre.i, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  %12 = phi i32 [ %19, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %cond.i, %omp.inner.for.body.us.preheader.i ]
  %.omp.iv.02.us.i = phi i32 [ %add11.us.i, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !152
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !152
  store i32 %11, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !152
  store i32 %12, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !152
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !152
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !152
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !152
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !152
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4, !noalias !152
  %13 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !152
  %cmp8.i.us.i = icmp sgt i32 %13, %sub2.i
  %cond.i.us.i = select i1 %cmp8.i.us.i, i32 %sub2.i, i32 %13
  store i32 %cond.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !152
  %14 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !152
  %cmp101.i.us.i = icmp sgt i32 %14, %cond.i.us.i
  br i1 %cmp101.i.us.i, label %omp.loop.exit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.inner.for.body.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add19.i.us.i, %omp.inner.for.body.i.us.i ], [ %14, %omp.inner.for.body.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %15 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !152
  %arrayidx14.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %16 = load double, double* %arrayidx14.i.us.i, align 8, !tbaa !126, !noalias !152
  %add15.i.us.i = fadd double %15, %16
  %arrayidx17.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %17 = load double, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !152
  %add18.i.us.i = fadd double %17, %add15.i.us.i
  store double %add18.i.us.i, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !152
  %add19.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp10.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %cond.i.us.i
  br i1 %cmp10.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.loop.exit.i.us.i

omp.loop.exit.i.us.i:                             ; preds = %omp.inner.for.body.i.us.i, %omp.inner.for.body.us.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4, !noalias !152
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !152
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !152
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !152
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !152
  %18 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add11.us.i = add nsw i32 %18, %.omp.iv.02.us.i
  %19 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp8.us.i = icmp sgt i32 %add11.us.i, %19
  br i1 %cmp8.us.i, label %omp.loop.exit.i, label %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i

omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i: ; preds = %omp.loop.exit.i.us.i
  %.pre.i = load i32, i32* %.omp.comb.lb.i, align 4
  br label %omp.inner.for.body.us.i

omp.loop.exit.i:                                  ; preds = %omp.loop.exit.i.us.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__72.exit

__omp_outlined__72.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.loop.exit.i
  %.pre-phi10.i = phi i8* [ %.pre9.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.loop.exit.i ]
  %.pre-phi8.i = phi i8* [ %.pre7.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.loop.exit.i ]
  %.pre-phi6.i = phi i8* [ %.pre5.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.loop.exit.i ]
  %.pre-phi.i = phi i8* [ %.pre4.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.loop.exit.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi6.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi8.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi10.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

declare void @__kmpc_spmd_kernel_init(i32, i16, i16) local_unnamed_addr

declare void @__kmpc_spmd_kernel_deinit() local_unnamed_addr

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1070(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub2.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre4.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre5.i = bitcast i32* %.omp.stride.i to i8*
  %.pre7.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre9.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__74.exit

omp.precond.then.i:                               ; preds = %entry
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub2.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp6.i = icmp sgt i32 %5, %sub2.i
  %cond.i = select i1 %cmp6.i, i32 %sub2.i, i32 %5
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp81.i = icmp sgt i32 %6, %cond.i
  br i1 %cmp81.i, label %omp.loop.exit.i, label %omp.inner.for.body.us.preheader.i

omp.inner.for.body.us.preheader.i:                ; preds = %omp.precond.then.i
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.us.i

omp.inner.for.body.us.i:                          ; preds = %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i, %omp.inner.for.body.us.preheader.i
  %11 = phi i32 [ %.pre.i, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  %12 = phi i32 [ %19, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %cond.i, %omp.inner.for.body.us.preheader.i ]
  %.omp.iv.02.us.i = phi i32 [ %add11.us.i, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !155
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !155
  store i32 %11, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !155
  store i32 %12, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !155
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !155
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !155
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !155
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !155
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4, !noalias !155
  %13 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !155
  %cmp8.i.us.i = icmp sgt i32 %13, %sub2.i
  %cond.i.us.i = select i1 %cmp8.i.us.i, i32 %sub2.i, i32 %13
  store i32 %cond.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !155
  %14 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !155
  %cmp101.i.us.i = icmp sgt i32 %14, %cond.i.us.i
  br i1 %cmp101.i.us.i, label %omp.loop.exit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.inner.for.body.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add19.i.us.i, %omp.inner.for.body.i.us.i ], [ %14, %omp.inner.for.body.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %15 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !155
  %arrayidx14.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %16 = load double, double* %arrayidx14.i.us.i, align 8, !tbaa !126, !noalias !155
  %add15.i.us.i = fadd double %15, %16
  %arrayidx17.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %17 = load double, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !155
  %add18.i.us.i = fadd double %17, %add15.i.us.i
  store double %add18.i.us.i, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !155
  %add19.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp10.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %cond.i.us.i
  br i1 %cmp10.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.loop.exit.i.us.i

omp.loop.exit.i.us.i:                             ; preds = %omp.inner.for.body.i.us.i, %omp.inner.for.body.us.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4, !noalias !155
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !155
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !155
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !155
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !155
  %18 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add11.us.i = add nsw i32 %18, %.omp.iv.02.us.i
  %19 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp8.us.i = icmp sgt i32 %add11.us.i, %19
  br i1 %cmp8.us.i, label %omp.loop.exit.i, label %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i

omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i: ; preds = %omp.loop.exit.i.us.i
  %.pre.i = load i32, i32* %.omp.comb.lb.i, align 4
  br label %omp.inner.for.body.us.i

omp.loop.exit.i:                                  ; preds = %omp.loop.exit.i.us.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__74.exit

__omp_outlined__74.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.loop.exit.i
  %.pre-phi10.i = phi i8* [ %.pre9.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.loop.exit.i ]
  %.pre-phi8.i = phi i8* [ %.pre7.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.loop.exit.i ]
  %.pre-phi6.i = phi i8* [ %.pre5.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.loop.exit.i ]
  %.pre-phi.i = phi i8* [ %.pre4.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.loop.exit.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi6.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi8.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi10.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1105(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub4.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre4.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre5.i = bitcast i32* %.omp.stride.i to i8*
  %.pre7.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre9.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__76.exit

omp.precond.then.i:                               ; preds = %entry
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub4.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp8.i = icmp sgt i32 %5, %sub4.i
  %cond.i = select i1 %cmp8.i, i32 %sub4.i, i32 %5
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp101.i = icmp sgt i32 %6, %cond.i
  br i1 %cmp101.i, label %omp.loop.exit.i, label %omp.inner.for.body.us.preheader.i

omp.inner.for.body.us.preheader.i:                ; preds = %omp.precond.then.i
  %.capture_expr..addr.sroa.0.0.extract.trunc.i.i = trunc i64 %.capture_expr. to i32
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.us.i

omp.inner.for.body.us.i:                          ; preds = %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i, %omp.inner.for.body.us.preheader.i
  %11 = phi i32 [ %.pre.i, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  %12 = phi i32 [ %22, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %cond.i, %omp.inner.for.body.us.preheader.i ]
  %.omp.iv.02.us.i = phi i32 [ %add14.us.i, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  %13 = zext i32 %12 to i64
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !158
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !158
  store i32 %11, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !158
  store i32 %12, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !158
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !158
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !158
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !158
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !158
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 33, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i.i) #4, !noalias !158
  %14 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !158
  %conv103.i.us.i = sext i32 %14 to i64
  %cmp114.i.us.i = icmp ugt i64 %conv103.i.us.i, %13
  %cond5.i.us.i = select i1 %cmp114.i.us.i, i32 %12, i32 %14
  store i32 %cond5.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !158
  %15 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !158
  %cmp157.i.us.i = icmp sgt i32 %15, %cond5.i.us.i
  br i1 %cmp157.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.inner.for.cond.preheader.lr.ph.i.us.i

omp.inner.for.cond.preheader.lr.ph.i.us.i:        ; preds = %omp.inner.for.body.us.i
  %16 = load i32, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !158
  br label %omp.inner.for.cond.preheader.i.us.i

omp.inner.for.cond.preheader.i.us.i:              ; preds = %omp.dispatch.inc.i.us.i, %omp.inner.for.cond.preheader.lr.ph.i.us.i
  %17 = phi i32 [ %15, %omp.inner.for.cond.preheader.lr.ph.i.us.i ], [ %add27.i.us.i, %omp.dispatch.inc.i.us.i ]
  %conv148.i.us.i = phi i32 [ %cond5.i.us.i, %omp.inner.for.cond.preheader.lr.ph.i.us.i ], [ %cond.i.us.i, %omp.dispatch.inc.i.us.i ]
  %cmp171.i.us.i = icmp sgt i32 %17, %conv148.i.us.i
  br i1 %cmp171.i.us.i, label %omp.dispatch.inc.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.inner.for.cond.preheader.i.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add26.i.us.i, %omp.inner.for.body.i.us.i ], [ %17, %omp.inner.for.cond.preheader.i.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !158
  %arrayidx21.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %19 = load double, double* %arrayidx21.i.us.i, align 8, !tbaa !126, !noalias !158
  %add22.i.us.i = fadd double %18, %19
  %arrayidx24.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %20 = load double, double* %arrayidx24.i.us.i, align 8, !tbaa !126, !noalias !158
  %add25.i.us.i = fadd double %20, %add22.i.us.i
  store double %add25.i.us.i, double* %arrayidx24.i.us.i, align 8, !tbaa !126, !noalias !158
  %add26.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp17.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %conv148.i.us.i
  br i1 %cmp17.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.dispatch.inc.i.us.i

omp.dispatch.inc.i.us.i:                          ; preds = %omp.inner.for.body.i.us.i, %omp.inner.for.cond.preheader.i.us.i
  %add27.i.us.i = add nsw i32 %17, %16
  %add28.i.us.i = add nsw i32 %conv148.i.us.i, %16
  %conv10.i.us.i = sext i32 %add28.i.us.i to i64
  %cmp11.i.us.i = icmp ugt i64 %conv10.i.us.i, %13
  %cond.i.us.i = select i1 %cmp11.i.us.i, i32 %12, i32 %add28.i.us.i
  %cmp15.i.us.i = icmp sgt i32 %add27.i.us.i, %cond.i.us.i
  br i1 %cmp15.i.us.i, label %omp.dispatch.cond.omp.dispatch.end_crit_edge.i.us.i, label %omp.inner.for.cond.preheader.i.us.i

omp.dispatch.cond.omp.dispatch.end_crit_edge.i.us.i: ; preds = %omp.dispatch.inc.i.us.i
  store i32 %add27.i.us.i, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !158
  store i32 %cond.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !158
  br label %omp.dispatch.end.i.us.i

omp.dispatch.end.i.us.i:                          ; preds = %omp.dispatch.cond.omp.dispatch.end_crit_edge.i.us.i, %omp.inner.for.body.us.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4, !noalias !158
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !158
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !158
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !158
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !158
  %21 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add14.us.i = add nsw i32 %21, %.omp.iv.02.us.i
  %22 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp10.us.i = icmp sgt i32 %add14.us.i, %22
  br i1 %cmp10.us.i, label %omp.loop.exit.i, label %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i

omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i: ; preds = %omp.dispatch.end.i.us.i
  %.pre.i = load i32, i32* %.omp.comb.lb.i, align 4
  br label %omp.inner.for.body.us.i

omp.loop.exit.i:                                  ; preds = %omp.dispatch.end.i.us.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__76.exit

__omp_outlined__76.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.loop.exit.i
  %.pre-phi10.i = phi i8* [ %.pre9.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.loop.exit.i ]
  %.pre-phi8.i = phi i8* [ %.pre7.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.loop.exit.i ]
  %.pre-phi6.i = phi i8* [ %.pre5.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.loop.exit.i ]
  %.pre-phi.i = phi i8* [ %.pre4.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.loop.exit.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi6.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi8.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi10.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1140(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub2.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre4.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre5.i = bitcast i32* %.omp.stride.i to i8*
  %.pre7.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre9.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__78.exit

omp.precond.then.i:                               ; preds = %entry
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub2.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp6.i = icmp sgt i32 %5, %sub2.i
  %cond.i = select i1 %cmp6.i, i32 %sub2.i, i32 %5
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp81.i = icmp sgt i32 %6, %cond.i
  br i1 %cmp81.i, label %omp.loop.exit.i, label %omp.inner.for.body.us.preheader.i

omp.inner.for.body.us.preheader.i:                ; preds = %omp.precond.then.i
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.us.i

omp.inner.for.body.us.i:                          ; preds = %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i, %omp.inner.for.body.us.preheader.i
  %11 = phi i32 [ %.pre.i, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  %12 = phi i32 [ %21, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %cond.i, %omp.inner.for.body.us.preheader.i ]
  %.omp.iv.02.us.i = phi i32 [ %add11.us.i, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !161
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !161
  store i32 %11, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !161
  store i32 %12, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !161
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !161
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !161
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !161
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !161
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 35, i32 %11, i32 %12, i32 1, i32 1) #4, !noalias !161
  %13 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !161
  %tobool3.i.us.i = icmp eq i32 %13, 0
  br i1 %tobool3.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.body.i.us.i:                         ; preds = %omp.inner.for.body.us.i, %omp.dispatch.cond.loopexit.i.us.i
  %14 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !161
  %15 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !161, !llvm.mem.parallel_loop_access !164
  %cmp81.i.us.i = icmp sgt i32 %14, %15
  br i1 %cmp81.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.dispatch.body.i.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add17.i.us.i, %omp.inner.for.body.i.us.i ], [ %14, %omp.dispatch.body.i.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %16 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !161, !llvm.mem.parallel_loop_access !164
  %arrayidx12.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %17 = load double, double* %arrayidx12.i.us.i, align 8, !tbaa !126, !noalias !161, !llvm.mem.parallel_loop_access !164
  %add13.i.us.i = fadd double %16, %17
  %arrayidx15.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx15.i.us.i, align 8, !tbaa !126, !noalias !161, !llvm.mem.parallel_loop_access !164
  %add16.i.us.i = fadd double %18, %add13.i.us.i
  store double %add16.i.us.i, double* %arrayidx15.i.us.i, align 8, !tbaa !126, !noalias !161, !llvm.mem.parallel_loop_access !164
  %add17.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp8.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %15
  br i1 %cmp8.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, !llvm.loop !164

omp.dispatch.cond.loopexit.i.us.i:                ; preds = %omp.inner.for.body.i.us.i, %omp.dispatch.body.i.us.i
  %19 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !161
  %tobool.i.us.i = icmp eq i32 %19, 0
  br i1 %tobool.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.end.i.us.i:                          ; preds = %omp.dispatch.cond.loopexit.i.us.i, %omp.inner.for.body.us.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !161
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !161
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !161
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !161
  %20 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add11.us.i = add nsw i32 %20, %.omp.iv.02.us.i
  %21 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp8.us.i = icmp sgt i32 %add11.us.i, %21
  br i1 %cmp8.us.i, label %omp.loop.exit.i, label %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i

omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i: ; preds = %omp.dispatch.end.i.us.i
  %.pre.i = load i32, i32* %.omp.comb.lb.i, align 4
  br label %omp.inner.for.body.us.i

omp.loop.exit.i:                                  ; preds = %omp.dispatch.end.i.us.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__78.exit

__omp_outlined__78.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.loop.exit.i
  %.pre-phi10.i = phi i8* [ %.pre9.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.loop.exit.i ]
  %.pre-phi8.i = phi i8* [ %.pre7.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.loop.exit.i ]
  %.pre-phi6.i = phi i8* [ %.pre5.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.loop.exit.i ]
  %.pre-phi.i = phi i8* [ %.pre4.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.loop.exit.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi6.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi8.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi10.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1177(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub4.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre4.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre5.i = bitcast i32* %.omp.stride.i to i8*
  %.pre7.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre9.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__80.exit

omp.precond.then.i:                               ; preds = %entry
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub4.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp8.i = icmp sgt i32 %5, %sub4.i
  %cond.i = select i1 %cmp8.i, i32 %sub4.i, i32 %5
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp101.i = icmp sgt i32 %6, %cond.i
  br i1 %cmp101.i, label %omp.loop.exit.i, label %omp.inner.for.body.us.preheader.i

omp.inner.for.body.us.preheader.i:                ; preds = %omp.precond.then.i
  %.capture_expr..addr.sroa.0.0.extract.trunc.i.i = trunc i64 %.capture_expr. to i32
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.us.i

omp.inner.for.body.us.i:                          ; preds = %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i, %omp.inner.for.body.us.preheader.i
  %11 = phi i32 [ %.pre.i, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  %12 = phi i32 [ %21, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %cond.i, %omp.inner.for.body.us.preheader.i ]
  %.omp.iv.02.us.i = phi i32 [ %add14.us.i, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !165
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !165
  store i32 %11, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !165
  store i32 %12, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !165
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !165
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !165
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !165
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !165
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 35, i32 %11, i32 %12, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i.i) #4, !noalias !165
  %13 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !165
  %tobool3.i.us.i = icmp eq i32 %13, 0
  br i1 %tobool3.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.body.i.us.i:                         ; preds = %omp.inner.for.body.us.i, %omp.dispatch.cond.loopexit.i.us.i
  %14 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !165
  %15 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !165, !llvm.mem.parallel_loop_access !168
  %cmp101.i.us.i = icmp sgt i32 %14, %15
  br i1 %cmp101.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.dispatch.body.i.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add19.i.us.i, %omp.inner.for.body.i.us.i ], [ %14, %omp.dispatch.body.i.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %16 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !165, !llvm.mem.parallel_loop_access !168
  %arrayidx14.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %17 = load double, double* %arrayidx14.i.us.i, align 8, !tbaa !126, !noalias !165, !llvm.mem.parallel_loop_access !168
  %add15.i.us.i = fadd double %16, %17
  %arrayidx17.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !165, !llvm.mem.parallel_loop_access !168
  %add18.i.us.i = fadd double %18, %add15.i.us.i
  store double %add18.i.us.i, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !165, !llvm.mem.parallel_loop_access !168
  %add19.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp10.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %15
  br i1 %cmp10.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, !llvm.loop !168

omp.dispatch.cond.loopexit.i.us.i:                ; preds = %omp.inner.for.body.i.us.i, %omp.dispatch.body.i.us.i
  %19 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !165
  %tobool.i.us.i = icmp eq i32 %19, 0
  br i1 %tobool.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.end.i.us.i:                          ; preds = %omp.dispatch.cond.loopexit.i.us.i, %omp.inner.for.body.us.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !165
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !165
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !165
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !165
  %20 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add14.us.i = add nsw i32 %20, %.omp.iv.02.us.i
  %21 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp10.us.i = icmp sgt i32 %add14.us.i, %21
  br i1 %cmp10.us.i, label %omp.loop.exit.i, label %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i

omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i: ; preds = %omp.dispatch.end.i.us.i
  %.pre.i = load i32, i32* %.omp.comb.lb.i, align 4
  br label %omp.inner.for.body.us.i

omp.loop.exit.i:                                  ; preds = %omp.dispatch.end.i.us.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__80.exit

__omp_outlined__80.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.loop.exit.i
  %.pre-phi10.i = phi i8* [ %.pre9.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.loop.exit.i ]
  %.pre-phi8.i = phi i8* [ %.pre7.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.loop.exit.i ]
  %.pre-phi6.i = phi i8* [ %.pre5.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.loop.exit.i ]
  %.pre-phi.i = phi i8* [ %.pre4.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.loop.exit.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi6.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi8.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi10.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1213(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub2.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre4.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre5.i = bitcast i32* %.omp.stride.i to i8*
  %.pre7.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre9.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__82.exit

omp.precond.then.i:                               ; preds = %entry
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub2.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp6.i = icmp sgt i32 %5, %sub2.i
  %cond.i = select i1 %cmp6.i, i32 %sub2.i, i32 %5
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp81.i = icmp sgt i32 %6, %cond.i
  br i1 %cmp81.i, label %omp.loop.exit.i, label %omp.inner.for.body.us.preheader.i

omp.inner.for.body.us.preheader.i:                ; preds = %omp.precond.then.i
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.us.i

omp.inner.for.body.us.i:                          ; preds = %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i, %omp.inner.for.body.us.preheader.i
  %11 = phi i32 [ %.pre.i, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  %12 = phi i32 [ %19, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %cond.i, %omp.inner.for.body.us.preheader.i ]
  %.omp.iv.02.us.i = phi i32 [ %add11.us.i, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !169
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !169
  store i32 %11, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !169
  store i32 %12, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !169
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !169
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !169
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !169
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !169
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4, !noalias !169
  %13 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !169
  %cmp8.i.us.i = icmp sgt i32 %13, %sub2.i
  %cond.i.us.i = select i1 %cmp8.i.us.i, i32 %sub2.i, i32 %13
  store i32 %cond.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !169
  %14 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !169
  %cmp101.i.us.i = icmp sgt i32 %14, %cond.i.us.i
  br i1 %cmp101.i.us.i, label %omp.loop.exit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.inner.for.body.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add19.i.us.i, %omp.inner.for.body.i.us.i ], [ %14, %omp.inner.for.body.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %15 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !169
  %arrayidx14.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %16 = load double, double* %arrayidx14.i.us.i, align 8, !tbaa !126, !noalias !169
  %add15.i.us.i = fadd double %15, %16
  %arrayidx17.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %17 = load double, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !169
  %add18.i.us.i = fadd double %17, %add15.i.us.i
  store double %add18.i.us.i, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !169
  %add19.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp10.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %cond.i.us.i
  br i1 %cmp10.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.loop.exit.i.us.i

omp.loop.exit.i.us.i:                             ; preds = %omp.inner.for.body.i.us.i, %omp.inner.for.body.us.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4, !noalias !169
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !169
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !169
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !169
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !169
  %18 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add11.us.i = add nsw i32 %18, %.omp.iv.02.us.i
  %19 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp8.us.i = icmp sgt i32 %add11.us.i, %19
  br i1 %cmp8.us.i, label %omp.loop.exit.i, label %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i

omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i: ; preds = %omp.loop.exit.i.us.i
  %.pre.i = load i32, i32* %.omp.comb.lb.i, align 4
  br label %omp.inner.for.body.us.i

omp.loop.exit.i:                                  ; preds = %omp.loop.exit.i.us.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__82.exit

__omp_outlined__82.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.loop.exit.i
  %.pre-phi10.i = phi i8* [ %.pre9.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.loop.exit.i ]
  %.pre-phi8.i = phi i8* [ %.pre7.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.loop.exit.i ]
  %.pre-phi6.i = phi i8* [ %.pre5.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.loop.exit.i ]
  %.pre-phi.i = phi i8* [ %.pre4.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.loop.exit.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi6.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi8.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi10.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1248(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub4.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre13.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre14.i = bitcast i32* %.omp.stride.i to i8*
  %.pre16.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre18.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__84.exit

omp.precond.then.i:                               ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub4.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 91, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp85.i = icmp sgt i32 %5, %sub4.i
  %cond6.i = select i1 %cmp85.i, i32 %sub4.i, i32 %5
  store i32 %cond6.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp107.i = icmp sgt i32 %6, %cond6.i
  br i1 %cmp107.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.lr.ph.i

omp.inner.for.cond.preheader.lr.ph.i:             ; preds = %omp.precond.then.i
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.cond.preheader.i

omp.inner.for.cond.preheader.i:                   ; preds = %omp.dispatch.inc.i, %omp.inner.for.cond.preheader.lr.ph.i
  %11 = phi i32 [ %cond6.i, %omp.inner.for.cond.preheader.lr.ph.i ], [ %cond.i, %omp.dispatch.inc.i ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph.i ], [ %add17.i, %omp.dispatch.inc.i ]
  %cmp122.i = icmp sgt i32 %12, %11
  br i1 %cmp122.i, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i, label %omp.inner.for.body.us.i

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i: ; preds = %omp.inner.for.cond.preheader.i
  %.pre.i = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  br label %omp.dispatch.inc.i

omp.inner.for.body.us.i:                          ; preds = %omp.inner.for.cond.preheader.i, %omp.loop.exit.i.us.i
  %13 = phi i32 [ %22, %omp.loop.exit.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  %14 = phi i32 [ %21, %omp.loop.exit.i.us.i ], [ %11, %omp.inner.for.cond.preheader.i ]
  %.omp.iv.03.us.i = phi i32 [ %add16.us.i, %omp.loop.exit.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !172
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !172
  store i32 %13, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !172
  store i32 %14, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !172
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !172
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !172
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !172
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !172
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4, !noalias !172
  %15 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !172
  %cmp10.i.us.i = icmp sgt i32 %15, %sub4.i
  %cond.i.us.i = select i1 %cmp10.i.us.i, i32 %sub4.i, i32 %15
  store i32 %cond.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !172
  %16 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !172
  %cmp121.i.us.i = icmp sgt i32 %16, %cond.i.us.i
  br i1 %cmp121.i.us.i, label %omp.loop.exit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.inner.for.body.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add21.i.us.i, %omp.inner.for.body.i.us.i ], [ %16, %omp.inner.for.body.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %17 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !172
  %arrayidx16.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx16.i.us.i, align 8, !tbaa !126, !noalias !172
  %add17.i.us.i = fadd double %17, %18
  %arrayidx19.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %19 = load double, double* %arrayidx19.i.us.i, align 8, !tbaa !126, !noalias !172
  %add20.i.us.i = fadd double %19, %add17.i.us.i
  store double %add20.i.us.i, double* %arrayidx19.i.us.i, align 8, !tbaa !126, !noalias !172
  %add21.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp12.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %cond.i.us.i
  br i1 %cmp12.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.loop.exit.i.us.i

omp.loop.exit.i.us.i:                             ; preds = %omp.inner.for.body.i.us.i, %omp.inner.for.body.us.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4, !noalias !172
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !172
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !172
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !172
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !172
  %20 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !175
  %add16.us.i = add nsw i32 %20, %.omp.iv.03.us.i
  %21 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !175
  %cmp12.us.i = icmp sgt i32 %add16.us.i, %21
  %22 = load i32, i32* %.omp.comb.lb.i, align 4
  br i1 %cmp12.us.i, label %omp.dispatch.inc.i, label %omp.inner.for.body.us.i, !llvm.loop !175

omp.dispatch.inc.i:                               ; preds = %omp.loop.exit.i.us.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i
  %23 = phi i32 [ %.pre.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %20, %omp.loop.exit.i.us.i ]
  %.lcssa1.i = phi i32 [ %11, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %21, %omp.loop.exit.i.us.i ]
  %.lcssa.i = phi i32 [ %12, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %22, %omp.loop.exit.i.us.i ]
  %add17.i = add nsw i32 %.lcssa.i, %23
  store i32 %add17.i, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %add18.i = add nsw i32 %.lcssa1.i, %23
  %cmp8.i = icmp sgt i32 %add18.i, %sub4.i
  %cond.i = select i1 %cmp8.i, i32 %sub4.i, i32 %add18.i
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp10.i = icmp sgt i32 %add17.i, %cond.i
  br i1 %cmp10.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.i

omp.dispatch.end.i:                               ; preds = %omp.dispatch.inc.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__84.exit

__omp_outlined__84.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.dispatch.end.i
  %.pre-phi19.i = phi i8* [ %.pre18.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.dispatch.end.i ]
  %.pre-phi17.i = phi i8* [ %.pre16.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.dispatch.end.i ]
  %.pre-phi15.i = phi i8* [ %.pre14.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.dispatch.end.i ]
  %.pre-phi.i = phi i8* [ %.pre13.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.dispatch.end.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi15.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi17.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi19.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1283(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub2.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre4.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre5.i = bitcast i32* %.omp.stride.i to i8*
  %.pre7.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre9.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__86.exit

omp.precond.then.i:                               ; preds = %entry
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub2.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp6.i = icmp sgt i32 %5, %sub2.i
  %cond.i = select i1 %cmp6.i, i32 %sub2.i, i32 %5
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp81.i = icmp sgt i32 %6, %cond.i
  br i1 %cmp81.i, label %omp.loop.exit.i, label %omp.inner.for.body.us.preheader.i

omp.inner.for.body.us.preheader.i:                ; preds = %omp.precond.then.i
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.us.i

omp.inner.for.body.us.i:                          ; preds = %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i, %omp.inner.for.body.us.preheader.i
  %11 = phi i32 [ %.pre.i, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  %12 = phi i32 [ %19, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %cond.i, %omp.inner.for.body.us.preheader.i ]
  %.omp.iv.02.us.i = phi i32 [ %add11.us.i, %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !176
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !176
  store i32 %11, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !176
  store i32 %12, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !176
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !176
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !176
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !176
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !176
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4, !noalias !176
  %13 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !176
  %cmp8.i.us.i = icmp sgt i32 %13, %sub2.i
  %cond.i.us.i = select i1 %cmp8.i.us.i, i32 %sub2.i, i32 %13
  store i32 %cond.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !176
  %14 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !176
  %cmp101.i.us.i = icmp sgt i32 %14, %cond.i.us.i
  br i1 %cmp101.i.us.i, label %omp.loop.exit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.inner.for.body.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add19.i.us.i, %omp.inner.for.body.i.us.i ], [ %14, %omp.inner.for.body.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %15 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !176
  %arrayidx14.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %16 = load double, double* %arrayidx14.i.us.i, align 8, !tbaa !126, !noalias !176
  %add15.i.us.i = fadd double %15, %16
  %arrayidx17.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %17 = load double, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !176
  %add18.i.us.i = fadd double %17, %add15.i.us.i
  store double %add18.i.us.i, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !176
  %add19.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp10.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %cond.i.us.i
  br i1 %cmp10.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.loop.exit.i.us.i

omp.loop.exit.i.us.i:                             ; preds = %omp.inner.for.body.i.us.i, %omp.inner.for.body.us.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4, !noalias !176
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !176
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !176
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !176
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !176
  %18 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add11.us.i = add nsw i32 %18, %.omp.iv.02.us.i
  %19 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp8.us.i = icmp sgt i32 %add11.us.i, %19
  br i1 %cmp8.us.i, label %omp.loop.exit.i, label %omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i

omp.loop.exit.i.us.omp.inner.for.body.us_crit_edge.i: ; preds = %omp.loop.exit.i.us.i
  %.pre.i = load i32, i32* %.omp.comb.lb.i, align 4
  br label %omp.inner.for.body.us.i

omp.loop.exit.i:                                  ; preds = %omp.loop.exit.i.us.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__86.exit

__omp_outlined__86.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.loop.exit.i
  %.pre-phi10.i = phi i8* [ %.pre9.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.loop.exit.i ]
  %.pre-phi8.i = phi i8* [ %.pre7.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.loop.exit.i ]
  %.pre-phi6.i = phi i8* [ %.pre5.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.loop.exit.i ]
  %.pre-phi.i = phi i8* [ %.pre4.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.loop.exit.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi6.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi8.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi10.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1318(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub4.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre4.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre5.i = bitcast i32* %.omp.stride.i to i8*
  %.pre7.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre9.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__88.exit

omp.precond.then.i:                               ; preds = %entry
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub4.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 92, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp8.i = icmp sgt i32 %5, %sub4.i
  %cond.i = select i1 %cmp8.i, i32 %sub4.i, i32 %5
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp101.i = icmp sgt i32 %6, %cond.i
  br i1 %cmp101.i, label %omp.loop.exit.i, label %omp.inner.for.body.us.preheader.i

omp.inner.for.body.us.preheader.i:                ; preds = %omp.precond.then.i
  %.capture_expr..addr.sroa.0.0.extract.trunc.i.i = trunc i64 %.capture_expr. to i32
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.body.us.i

omp.inner.for.body.us.i:                          ; preds = %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i, %omp.inner.for.body.us.preheader.i
  %11 = phi i32 [ %.pre.i, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  %12 = phi i32 [ %22, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %cond.i, %omp.inner.for.body.us.preheader.i ]
  %.omp.iv.02.us.i = phi i32 [ %add14.us.i, %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i ], [ %6, %omp.inner.for.body.us.preheader.i ]
  %13 = zext i32 %12 to i64
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !179
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !179
  store i32 %11, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !179
  store i32 %12, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !179
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !179
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !179
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !179
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !179
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 33, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i.i) #4, !noalias !179
  %14 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !179
  %conv103.i.us.i = sext i32 %14 to i64
  %cmp114.i.us.i = icmp ugt i64 %conv103.i.us.i, %13
  %cond5.i.us.i = select i1 %cmp114.i.us.i, i32 %12, i32 %14
  store i32 %cond5.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !179
  %15 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !179
  %cmp157.i.us.i = icmp sgt i32 %15, %cond5.i.us.i
  br i1 %cmp157.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.inner.for.cond.preheader.lr.ph.i.us.i

omp.inner.for.cond.preheader.lr.ph.i.us.i:        ; preds = %omp.inner.for.body.us.i
  %16 = load i32, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !179
  br label %omp.inner.for.cond.preheader.i.us.i

omp.inner.for.cond.preheader.i.us.i:              ; preds = %omp.dispatch.inc.i.us.i, %omp.inner.for.cond.preheader.lr.ph.i.us.i
  %17 = phi i32 [ %15, %omp.inner.for.cond.preheader.lr.ph.i.us.i ], [ %add27.i.us.i, %omp.dispatch.inc.i.us.i ]
  %conv148.i.us.i = phi i32 [ %cond5.i.us.i, %omp.inner.for.cond.preheader.lr.ph.i.us.i ], [ %cond.i.us.i, %omp.dispatch.inc.i.us.i ]
  %cmp171.i.us.i = icmp sgt i32 %17, %conv148.i.us.i
  br i1 %cmp171.i.us.i, label %omp.dispatch.inc.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.inner.for.cond.preheader.i.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add26.i.us.i, %omp.inner.for.body.i.us.i ], [ %17, %omp.inner.for.cond.preheader.i.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !179
  %arrayidx21.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %19 = load double, double* %arrayidx21.i.us.i, align 8, !tbaa !126, !noalias !179
  %add22.i.us.i = fadd double %18, %19
  %arrayidx24.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %20 = load double, double* %arrayidx24.i.us.i, align 8, !tbaa !126, !noalias !179
  %add25.i.us.i = fadd double %20, %add22.i.us.i
  store double %add25.i.us.i, double* %arrayidx24.i.us.i, align 8, !tbaa !126, !noalias !179
  %add26.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp17.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %conv148.i.us.i
  br i1 %cmp17.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.dispatch.inc.i.us.i

omp.dispatch.inc.i.us.i:                          ; preds = %omp.inner.for.body.i.us.i, %omp.inner.for.cond.preheader.i.us.i
  %add27.i.us.i = add nsw i32 %17, %16
  %add28.i.us.i = add nsw i32 %conv148.i.us.i, %16
  %conv10.i.us.i = sext i32 %add28.i.us.i to i64
  %cmp11.i.us.i = icmp ugt i64 %conv10.i.us.i, %13
  %cond.i.us.i = select i1 %cmp11.i.us.i, i32 %12, i32 %add28.i.us.i
  %cmp15.i.us.i = icmp sgt i32 %add27.i.us.i, %cond.i.us.i
  br i1 %cmp15.i.us.i, label %omp.dispatch.cond.omp.dispatch.end_crit_edge.i.us.i, label %omp.inner.for.cond.preheader.i.us.i

omp.dispatch.cond.omp.dispatch.end_crit_edge.i.us.i: ; preds = %omp.dispatch.inc.i.us.i
  store i32 %add27.i.us.i, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !179
  store i32 %cond.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !179
  br label %omp.dispatch.end.i.us.i

omp.dispatch.end.i.us.i:                          ; preds = %omp.dispatch.cond.omp.dispatch.end_crit_edge.i.us.i, %omp.inner.for.body.us.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4, !noalias !179
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !179
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !179
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !179
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !179
  %21 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  %add14.us.i = add nsw i32 %21, %.omp.iv.02.us.i
  %22 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp10.us.i = icmp sgt i32 %add14.us.i, %22
  br i1 %cmp10.us.i, label %omp.loop.exit.i, label %omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i

omp.dispatch.end.i.us.omp.inner.for.body.us_crit_edge.i: ; preds = %omp.dispatch.end.i.us.i
  %.pre.i = load i32, i32* %.omp.comb.lb.i, align 4
  br label %omp.inner.for.body.us.i

omp.loop.exit.i:                                  ; preds = %omp.dispatch.end.i.us.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__88.exit

__omp_outlined__88.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.loop.exit.i
  %.pre-phi10.i = phi i8* [ %.pre9.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.loop.exit.i ]
  %.pre-phi8.i = phi i8* [ %.pre7.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.loop.exit.i ]
  %.pre-phi6.i = phi i8* [ %.pre5.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.loop.exit.i ]
  %.pre-phi.i = phi i8* [ %.pre4.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.loop.exit.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi6.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi8.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi10.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1354(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub4.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre13.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre14.i = bitcast i32* %.omp.stride.i to i8*
  %.pre16.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre18.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__90.exit

omp.precond.then.i:                               ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub4.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 91, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp85.i = icmp sgt i32 %5, %sub4.i
  %cond6.i = select i1 %cmp85.i, i32 %sub4.i, i32 %5
  store i32 %cond6.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp107.i = icmp sgt i32 %6, %cond6.i
  br i1 %cmp107.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.lr.ph.i

omp.inner.for.cond.preheader.lr.ph.i:             ; preds = %omp.precond.then.i
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.cond.preheader.i

omp.inner.for.cond.preheader.i:                   ; preds = %omp.dispatch.inc.i, %omp.inner.for.cond.preheader.lr.ph.i
  %11 = phi i32 [ %cond6.i, %omp.inner.for.cond.preheader.lr.ph.i ], [ %cond.i, %omp.dispatch.inc.i ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph.i ], [ %add17.i, %omp.dispatch.inc.i ]
  %cmp122.i = icmp sgt i32 %12, %11
  br i1 %cmp122.i, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i, label %omp.inner.for.body.us.i

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i: ; preds = %omp.inner.for.cond.preheader.i
  %.pre.i = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  br label %omp.dispatch.inc.i

omp.inner.for.body.us.i:                          ; preds = %omp.inner.for.cond.preheader.i, %omp.loop.exit.i.us.i
  %13 = phi i32 [ %22, %omp.loop.exit.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  %14 = phi i32 [ %21, %omp.loop.exit.i.us.i ], [ %11, %omp.inner.for.cond.preheader.i ]
  %.omp.iv.03.us.i = phi i32 [ %add16.us.i, %omp.loop.exit.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !182
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !182
  store i32 %13, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !182
  store i32 %14, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !182
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !182
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !182
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !182
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !182
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 34, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 1) #4, !noalias !182
  %15 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !182
  %cmp10.i.us.i = icmp sgt i32 %15, %sub4.i
  %cond.i.us.i = select i1 %cmp10.i.us.i, i32 %sub4.i, i32 %15
  store i32 %cond.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !182
  %16 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !182
  %cmp121.i.us.i = icmp sgt i32 %16, %cond.i.us.i
  br i1 %cmp121.i.us.i, label %omp.loop.exit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.inner.for.body.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add21.i.us.i, %omp.inner.for.body.i.us.i ], [ %16, %omp.inner.for.body.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %17 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !182
  %arrayidx16.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx16.i.us.i, align 8, !tbaa !126, !noalias !182
  %add17.i.us.i = fadd double %17, %18
  %arrayidx19.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %19 = load double, double* %arrayidx19.i.us.i, align 8, !tbaa !126, !noalias !182
  %add20.i.us.i = fadd double %19, %add17.i.us.i
  store double %add20.i.us.i, double* %arrayidx19.i.us.i, align 8, !tbaa !126, !noalias !182
  %add21.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp12.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %cond.i.us.i
  br i1 %cmp12.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.loop.exit.i.us.i

omp.loop.exit.i.us.i:                             ; preds = %omp.inner.for.body.i.us.i, %omp.inner.for.body.us.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4, !noalias !182
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !182
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !182
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !182
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !182
  %20 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !185
  %add16.us.i = add nsw i32 %20, %.omp.iv.03.us.i
  %21 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !185
  %cmp12.us.i = icmp sgt i32 %add16.us.i, %21
  %22 = load i32, i32* %.omp.comb.lb.i, align 4
  br i1 %cmp12.us.i, label %omp.dispatch.inc.i, label %omp.inner.for.body.us.i, !llvm.loop !185

omp.dispatch.inc.i:                               ; preds = %omp.loop.exit.i.us.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i
  %23 = phi i32 [ %.pre.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %20, %omp.loop.exit.i.us.i ]
  %.lcssa1.i = phi i32 [ %11, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %21, %omp.loop.exit.i.us.i ]
  %.lcssa.i = phi i32 [ %12, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %22, %omp.loop.exit.i.us.i ]
  %add17.i = add nsw i32 %.lcssa.i, %23
  store i32 %add17.i, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %add18.i = add nsw i32 %.lcssa1.i, %23
  %cmp8.i = icmp sgt i32 %add18.i, %sub4.i
  %cond.i = select i1 %cmp8.i, i32 %sub4.i, i32 %add18.i
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp10.i = icmp sgt i32 %add17.i, %cond.i
  br i1 %cmp10.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.i

omp.dispatch.end.i:                               ; preds = %omp.dispatch.inc.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__90.exit

__omp_outlined__90.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.dispatch.end.i
  %.pre-phi19.i = phi i8* [ %.pre18.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.dispatch.end.i ]
  %.pre-phi17.i = phi i8* [ %.pre16.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.dispatch.end.i ]
  %.pre-phi15.i = phi i8* [ %.pre14.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.dispatch.end.i ]
  %.pre-phi.i = phi i8* [ %.pre13.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.dispatch.end.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi15.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi17.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi19.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1391(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3, i64 %.capture_expr.5) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub7.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre13.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre14.i = bitcast i32* %.omp.stride.i to i8*
  %.pre16.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre18.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__92.exit

omp.precond.then.i:                               ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub7.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 91, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp115.i = icmp sgt i32 %5, %sub7.i
  %cond6.i = select i1 %cmp115.i, i32 %sub7.i, i32 %5
  store i32 %cond6.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp137.i = icmp sgt i32 %6, %cond6.i
  br i1 %cmp137.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.lr.ph.i

omp.inner.for.cond.preheader.lr.ph.i:             ; preds = %omp.precond.then.i
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i.i = trunc i64 %.capture_expr.1 to i32
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.cond.preheader.i

omp.inner.for.cond.preheader.i:                   ; preds = %omp.dispatch.inc.i, %omp.inner.for.cond.preheader.lr.ph.i
  %11 = phi i32 [ %cond6.i, %omp.inner.for.cond.preheader.lr.ph.i ], [ %cond.i, %omp.dispatch.inc.i ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph.i ], [ %add22.i, %omp.dispatch.inc.i ]
  %cmp152.i = icmp sgt i32 %12, %11
  br i1 %cmp152.i, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i, label %omp.inner.for.body.us.i

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i: ; preds = %omp.inner.for.cond.preheader.i
  %.pre.i = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  br label %omp.dispatch.inc.i

omp.inner.for.body.us.i:                          ; preds = %omp.inner.for.cond.preheader.i, %omp.dispatch.end.i.us.i
  %13 = phi i32 [ %25, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  %14 = phi i32 [ %24, %omp.dispatch.end.i.us.i ], [ %11, %omp.inner.for.cond.preheader.i ]
  %.omp.iv.03.us.i = phi i32 [ %add21.us.i, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  %15 = zext i32 %14 to i64
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !186
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !186
  store i32 %13, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !186
  store i32 %14, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !186
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !186
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !186
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !186
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !186
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @2, i32 %0, i32 33, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i, i32 1, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i.i) #4, !noalias !186
  %16 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !186
  %conv133.i.us.i = sext i32 %16 to i64
  %cmp144.i.us.i = icmp ugt i64 %conv133.i.us.i, %15
  %cond5.i.us.i = select i1 %cmp144.i.us.i, i32 %14, i32 %16
  store i32 %cond5.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !186
  %17 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !186
  %cmp187.i.us.i = icmp sgt i32 %17, %cond5.i.us.i
  br i1 %cmp187.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.inner.for.cond.preheader.lr.ph.i.us.i

omp.inner.for.cond.preheader.lr.ph.i.us.i:        ; preds = %omp.inner.for.body.us.i
  %18 = load i32, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !186
  br label %omp.inner.for.cond.preheader.i.us.i

omp.inner.for.cond.preheader.i.us.i:              ; preds = %omp.dispatch.inc.i.us.i, %omp.inner.for.cond.preheader.lr.ph.i.us.i
  %19 = phi i32 [ %17, %omp.inner.for.cond.preheader.lr.ph.i.us.i ], [ %add30.i.us.i, %omp.dispatch.inc.i.us.i ]
  %conv178.i.us.i = phi i32 [ %cond5.i.us.i, %omp.inner.for.cond.preheader.lr.ph.i.us.i ], [ %cond.i.us.i, %omp.dispatch.inc.i.us.i ]
  %cmp201.i.us.i = icmp sgt i32 %19, %conv178.i.us.i
  br i1 %cmp201.i.us.i, label %omp.dispatch.inc.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.inner.for.cond.preheader.i.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add29.i.us.i, %omp.inner.for.body.i.us.i ], [ %19, %omp.inner.for.cond.preheader.i.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %20 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !186
  %arrayidx24.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %21 = load double, double* %arrayidx24.i.us.i, align 8, !tbaa !126, !noalias !186
  %add25.i.us.i = fadd double %20, %21
  %arrayidx27.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %22 = load double, double* %arrayidx27.i.us.i, align 8, !tbaa !126, !noalias !186
  %add28.i.us.i = fadd double %22, %add25.i.us.i
  store double %add28.i.us.i, double* %arrayidx27.i.us.i, align 8, !tbaa !126, !noalias !186
  %add29.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp20.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %conv178.i.us.i
  br i1 %cmp20.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.dispatch.inc.i.us.i

omp.dispatch.inc.i.us.i:                          ; preds = %omp.inner.for.body.i.us.i, %omp.inner.for.cond.preheader.i.us.i
  %add30.i.us.i = add nsw i32 %19, %18
  %add31.i.us.i = add nsw i32 %conv178.i.us.i, %18
  %conv13.i.us.i = sext i32 %add31.i.us.i to i64
  %cmp14.i.us.i = icmp ugt i64 %conv13.i.us.i, %15
  %cond.i.us.i = select i1 %cmp14.i.us.i, i32 %14, i32 %add31.i.us.i
  %cmp18.i.us.i = icmp sgt i32 %add30.i.us.i, %cond.i.us.i
  br i1 %cmp18.i.us.i, label %omp.dispatch.cond.omp.dispatch.end_crit_edge.i.us.i, label %omp.inner.for.cond.preheader.i.us.i

omp.dispatch.cond.omp.dispatch.end_crit_edge.i.us.i: ; preds = %omp.dispatch.inc.i.us.i
  store i32 %add30.i.us.i, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !186
  store i32 %cond.i.us.i, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !186
  br label %omp.dispatch.end.i.us.i

omp.dispatch.end.i.us.i:                          ; preds = %omp.dispatch.cond.omp.dispatch.end_crit_edge.i.us.i, %omp.inner.for.body.us.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4, !noalias !186
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !186
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !186
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !186
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !186
  %23 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !189
  %add21.us.i = add nsw i32 %23, %.omp.iv.03.us.i
  %24 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !189
  %cmp15.us.i = icmp sgt i32 %add21.us.i, %24
  %25 = load i32, i32* %.omp.comb.lb.i, align 4
  br i1 %cmp15.us.i, label %omp.dispatch.inc.i, label %omp.inner.for.body.us.i, !llvm.loop !189

omp.dispatch.inc.i:                               ; preds = %omp.dispatch.end.i.us.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i
  %26 = phi i32 [ %.pre.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %23, %omp.dispatch.end.i.us.i ]
  %.lcssa1.i = phi i32 [ %11, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %24, %omp.dispatch.end.i.us.i ]
  %.lcssa.i = phi i32 [ %12, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %25, %omp.dispatch.end.i.us.i ]
  %add22.i = add nsw i32 %.lcssa.i, %26
  store i32 %add22.i, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %add23.i = add nsw i32 %.lcssa1.i, %26
  %cmp11.i = icmp sgt i32 %add23.i, %sub7.i
  %cond.i = select i1 %cmp11.i, i32 %sub7.i, i32 %add23.i
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp13.i = icmp sgt i32 %add22.i, %cond.i
  br i1 %cmp13.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.i

omp.dispatch.end.i:                               ; preds = %omp.dispatch.inc.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__92.exit

__omp_outlined__92.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.dispatch.end.i
  %.pre-phi19.i = phi i8* [ %.pre18.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.dispatch.end.i ]
  %.pre-phi17.i = phi i8* [ %.pre16.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.dispatch.end.i ]
  %.pre-phi15.i = phi i8* [ %.pre14.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.dispatch.end.i ]
  %.pre-phi.i = phi i8* [ %.pre13.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.dispatch.end.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi15.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi17.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi19.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1428(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub4.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre13.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre14.i = bitcast i32* %.omp.stride.i to i8*
  %.pre16.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre18.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__94.exit

omp.precond.then.i:                               ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub4.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 91, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp85.i = icmp sgt i32 %5, %sub4.i
  %cond6.i = select i1 %cmp85.i, i32 %sub4.i, i32 %5
  store i32 %cond6.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp107.i = icmp sgt i32 %6, %cond6.i
  br i1 %cmp107.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.lr.ph.i

omp.inner.for.cond.preheader.lr.ph.i:             ; preds = %omp.precond.then.i
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.cond.preheader.i

omp.inner.for.cond.preheader.i:                   ; preds = %omp.dispatch.inc.i, %omp.inner.for.cond.preheader.lr.ph.i
  %11 = phi i32 [ %cond6.i, %omp.inner.for.cond.preheader.lr.ph.i ], [ %cond.i, %omp.dispatch.inc.i ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph.i ], [ %add17.i, %omp.dispatch.inc.i ]
  %cmp122.i = icmp sgt i32 %12, %11
  br i1 %cmp122.i, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i, label %omp.inner.for.body.us.i

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i: ; preds = %omp.inner.for.cond.preheader.i
  %.pre.i = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  br label %omp.dispatch.inc.i

omp.inner.for.body.us.i:                          ; preds = %omp.inner.for.cond.preheader.i, %omp.dispatch.end.i.us.i
  %13 = phi i32 [ %24, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  %14 = phi i32 [ %23, %omp.dispatch.end.i.us.i ], [ %11, %omp.inner.for.cond.preheader.i ]
  %.omp.iv.03.us.i = phi i32 [ %add16.us.i, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !190
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !190
  store i32 %13, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !190
  store i32 %14, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !190
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !190
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !190
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !190
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !190
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 35, i32 %13, i32 %14, i32 1, i32 1) #4, !noalias !190
  %15 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !190
  %tobool3.i.us.i = icmp eq i32 %15, 0
  br i1 %tobool3.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.body.i.us.i:                         ; preds = %omp.inner.for.body.us.i, %omp.dispatch.cond.loopexit.i.us.i
  %16 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !190
  %17 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !190, !llvm.mem.parallel_loop_access !193
  %cmp101.i.us.i = icmp sgt i32 %16, %17
  br i1 %cmp101.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.dispatch.body.i.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add19.i.us.i, %omp.inner.for.body.i.us.i ], [ %16, %omp.dispatch.body.i.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !190, !llvm.mem.parallel_loop_access !193
  %arrayidx14.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %19 = load double, double* %arrayidx14.i.us.i, align 8, !tbaa !126, !noalias !190, !llvm.mem.parallel_loop_access !193
  %add15.i.us.i = fadd double %18, %19
  %arrayidx17.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %20 = load double, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !190, !llvm.mem.parallel_loop_access !193
  %add18.i.us.i = fadd double %20, %add15.i.us.i
  store double %add18.i.us.i, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !190, !llvm.mem.parallel_loop_access !193
  %add19.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp10.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %17
  br i1 %cmp10.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, !llvm.loop !193

omp.dispatch.cond.loopexit.i.us.i:                ; preds = %omp.inner.for.body.i.us.i, %omp.dispatch.body.i.us.i
  %21 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !190
  %tobool.i.us.i = icmp eq i32 %21, 0
  br i1 %tobool.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.end.i.us.i:                          ; preds = %omp.dispatch.cond.loopexit.i.us.i, %omp.inner.for.body.us.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !190
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !190
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !190
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !190
  %22 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !194
  %add16.us.i = add nsw i32 %22, %.omp.iv.03.us.i
  %23 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !194
  %cmp12.us.i = icmp sgt i32 %add16.us.i, %23
  %24 = load i32, i32* %.omp.comb.lb.i, align 4
  br i1 %cmp12.us.i, label %omp.dispatch.inc.i, label %omp.inner.for.body.us.i, !llvm.loop !194

omp.dispatch.inc.i:                               ; preds = %omp.dispatch.end.i.us.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i
  %25 = phi i32 [ %.pre.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %22, %omp.dispatch.end.i.us.i ]
  %.lcssa1.i = phi i32 [ %11, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %23, %omp.dispatch.end.i.us.i ]
  %.lcssa.i = phi i32 [ %12, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %24, %omp.dispatch.end.i.us.i ]
  %add17.i = add nsw i32 %.lcssa.i, %25
  store i32 %add17.i, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %add18.i = add nsw i32 %.lcssa1.i, %25
  %cmp8.i = icmp sgt i32 %add18.i, %sub4.i
  %cond.i = select i1 %cmp8.i, i32 %sub4.i, i32 %add18.i
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp10.i = icmp sgt i32 %add17.i, %cond.i
  br i1 %cmp10.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.i

omp.dispatch.end.i:                               ; preds = %omp.dispatch.inc.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__94.exit

__omp_outlined__94.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.dispatch.end.i
  %.pre-phi19.i = phi i8* [ %.pre18.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.dispatch.end.i ]
  %.pre-phi17.i = phi i8* [ %.pre16.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.dispatch.end.i ]
  %.pre-phi15.i = phi i8* [ %.pre14.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.dispatch.end.i ]
  %.pre-phi.i = phi i8* [ %.pre13.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.dispatch.end.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi15.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi17.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi19.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1465(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3, i64 %.capture_expr.5) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub7.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre13.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre14.i = bitcast i32* %.omp.stride.i to i8*
  %.pre16.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre18.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__96.exit

omp.precond.then.i:                               ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub7.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 91, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp115.i = icmp sgt i32 %5, %sub7.i
  %cond6.i = select i1 %cmp115.i, i32 %sub7.i, i32 %5
  store i32 %cond6.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp137.i = icmp sgt i32 %6, %cond6.i
  br i1 %cmp137.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.lr.ph.i

omp.inner.for.cond.preheader.lr.ph.i:             ; preds = %omp.precond.then.i
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i.i = trunc i64 %.capture_expr.1 to i32
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.cond.preheader.i

omp.inner.for.cond.preheader.i:                   ; preds = %omp.dispatch.inc.i, %omp.inner.for.cond.preheader.lr.ph.i
  %11 = phi i32 [ %cond6.i, %omp.inner.for.cond.preheader.lr.ph.i ], [ %cond.i, %omp.dispatch.inc.i ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph.i ], [ %add22.i, %omp.dispatch.inc.i ]
  %cmp152.i = icmp sgt i32 %12, %11
  br i1 %cmp152.i, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i, label %omp.inner.for.body.us.i

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i: ; preds = %omp.inner.for.cond.preheader.i
  %.pre.i = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  br label %omp.dispatch.inc.i

omp.inner.for.body.us.i:                          ; preds = %omp.inner.for.cond.preheader.i, %omp.dispatch.end.i.us.i
  %13 = phi i32 [ %24, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  %14 = phi i32 [ %23, %omp.dispatch.end.i.us.i ], [ %11, %omp.inner.for.cond.preheader.i ]
  %.omp.iv.03.us.i = phi i32 [ %add21.us.i, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !195
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !195
  store i32 %13, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !195
  store i32 %14, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !195
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !195
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !195
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !195
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !195
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 35, i32 %13, i32 %14, i32 1, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i.i) #4, !noalias !195
  %15 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !195
  %tobool3.i.us.i = icmp eq i32 %15, 0
  br i1 %tobool3.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.body.i.us.i:                         ; preds = %omp.inner.for.body.us.i, %omp.dispatch.cond.loopexit.i.us.i
  %16 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !195
  %17 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !195, !llvm.mem.parallel_loop_access !198
  %cmp131.i.us.i = icmp sgt i32 %16, %17
  br i1 %cmp131.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.dispatch.body.i.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add22.i.us.i, %omp.inner.for.body.i.us.i ], [ %16, %omp.dispatch.body.i.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !195, !llvm.mem.parallel_loop_access !198
  %arrayidx17.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %19 = load double, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !195, !llvm.mem.parallel_loop_access !198
  %add18.i.us.i = fadd double %18, %19
  %arrayidx20.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %20 = load double, double* %arrayidx20.i.us.i, align 8, !tbaa !126, !noalias !195, !llvm.mem.parallel_loop_access !198
  %add21.i.us.i = fadd double %20, %add18.i.us.i
  store double %add21.i.us.i, double* %arrayidx20.i.us.i, align 8, !tbaa !126, !noalias !195, !llvm.mem.parallel_loop_access !198
  %add22.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp13.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %17
  br i1 %cmp13.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, !llvm.loop !198

omp.dispatch.cond.loopexit.i.us.i:                ; preds = %omp.inner.for.body.i.us.i, %omp.dispatch.body.i.us.i
  %21 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !195
  %tobool.i.us.i = icmp eq i32 %21, 0
  br i1 %tobool.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.end.i.us.i:                          ; preds = %omp.dispatch.cond.loopexit.i.us.i, %omp.inner.for.body.us.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !195
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !195
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !195
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !195
  %22 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !199
  %add21.us.i = add nsw i32 %22, %.omp.iv.03.us.i
  %23 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !199
  %cmp15.us.i = icmp sgt i32 %add21.us.i, %23
  %24 = load i32, i32* %.omp.comb.lb.i, align 4
  br i1 %cmp15.us.i, label %omp.dispatch.inc.i, label %omp.inner.for.body.us.i, !llvm.loop !199

omp.dispatch.inc.i:                               ; preds = %omp.dispatch.end.i.us.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i
  %25 = phi i32 [ %.pre.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %22, %omp.dispatch.end.i.us.i ]
  %.lcssa1.i = phi i32 [ %11, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %23, %omp.dispatch.end.i.us.i ]
  %.lcssa.i = phi i32 [ %12, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %24, %omp.dispatch.end.i.us.i ]
  %add22.i = add nsw i32 %.lcssa.i, %25
  store i32 %add22.i, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %add23.i = add nsw i32 %.lcssa1.i, %25
  %cmp11.i = icmp sgt i32 %add23.i, %sub7.i
  %cond.i = select i1 %cmp11.i, i32 %sub7.i, i32 %add23.i
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp13.i = icmp sgt i32 %add22.i, %cond.i
  br i1 %cmp13.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.i

omp.dispatch.end.i:                               ; preds = %omp.dispatch.inc.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__96.exit

__omp_outlined__96.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.dispatch.end.i
  %.pre-phi19.i = phi i8* [ %.pre18.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.dispatch.end.i ]
  %.pre-phi17.i = phi i8* [ %.pre16.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.dispatch.end.i ]
  %.pre-phi15.i = phi i8* [ %.pre14.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.dispatch.end.i ]
  %.pre-phi.i = phi i8* [ %.pre13.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.dispatch.end.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi15.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi17.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi19.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1502(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub4.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre13.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre14.i = bitcast i32* %.omp.stride.i to i8*
  %.pre16.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre18.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__98.exit

omp.precond.then.i:                               ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub4.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 91, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp85.i = icmp sgt i32 %5, %sub4.i
  %cond6.i = select i1 %cmp85.i, i32 %sub4.i, i32 %5
  store i32 %cond6.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp107.i = icmp sgt i32 %6, %cond6.i
  br i1 %cmp107.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.lr.ph.i

omp.inner.for.cond.preheader.lr.ph.i:             ; preds = %omp.precond.then.i
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.cond.preheader.i

omp.inner.for.cond.preheader.i:                   ; preds = %omp.dispatch.inc.i, %omp.inner.for.cond.preheader.lr.ph.i
  %11 = phi i32 [ %cond6.i, %omp.inner.for.cond.preheader.lr.ph.i ], [ %cond.i, %omp.dispatch.inc.i ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph.i ], [ %add17.i, %omp.dispatch.inc.i ]
  %cmp122.i = icmp sgt i32 %12, %11
  br i1 %cmp122.i, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i, label %omp.inner.for.body.us.i

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i: ; preds = %omp.inner.for.cond.preheader.i
  %.pre.i = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  br label %omp.dispatch.inc.i

omp.inner.for.body.us.i:                          ; preds = %omp.inner.for.cond.preheader.i, %omp.dispatch.end.i.us.i
  %13 = phi i32 [ %24, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  %14 = phi i32 [ %23, %omp.dispatch.end.i.us.i ], [ %11, %omp.inner.for.cond.preheader.i ]
  %.omp.iv.03.us.i = phi i32 [ %add16.us.i, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !200
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !200
  store i32 %13, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !200
  store i32 %14, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !200
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !200
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !200
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !200
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !200
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 36, i32 %13, i32 %14, i32 1, i32 1) #4, !noalias !200
  %15 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !200
  %tobool3.i.us.i = icmp eq i32 %15, 0
  br i1 %tobool3.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.body.i.us.i:                         ; preds = %omp.inner.for.body.us.i, %omp.dispatch.cond.loopexit.i.us.i
  %16 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !200
  %17 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !200, !llvm.mem.parallel_loop_access !203
  %cmp101.i.us.i = icmp sgt i32 %16, %17
  br i1 %cmp101.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.dispatch.body.i.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add19.i.us.i, %omp.inner.for.body.i.us.i ], [ %16, %omp.dispatch.body.i.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !200, !llvm.mem.parallel_loop_access !203
  %arrayidx14.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %19 = load double, double* %arrayidx14.i.us.i, align 8, !tbaa !126, !noalias !200, !llvm.mem.parallel_loop_access !203
  %add15.i.us.i = fadd double %18, %19
  %arrayidx17.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %20 = load double, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !200, !llvm.mem.parallel_loop_access !203
  %add18.i.us.i = fadd double %20, %add15.i.us.i
  store double %add18.i.us.i, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !200, !llvm.mem.parallel_loop_access !203
  %add19.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp10.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %17
  br i1 %cmp10.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, !llvm.loop !203

omp.dispatch.cond.loopexit.i.us.i:                ; preds = %omp.inner.for.body.i.us.i, %omp.dispatch.body.i.us.i
  %21 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !200
  %tobool.i.us.i = icmp eq i32 %21, 0
  br i1 %tobool.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.end.i.us.i:                          ; preds = %omp.dispatch.cond.loopexit.i.us.i, %omp.inner.for.body.us.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !200
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !200
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !200
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !200
  %22 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !204
  %add16.us.i = add nsw i32 %22, %.omp.iv.03.us.i
  %23 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !204
  %cmp12.us.i = icmp sgt i32 %add16.us.i, %23
  %24 = load i32, i32* %.omp.comb.lb.i, align 4
  br i1 %cmp12.us.i, label %omp.dispatch.inc.i, label %omp.inner.for.body.us.i, !llvm.loop !204

omp.dispatch.inc.i:                               ; preds = %omp.dispatch.end.i.us.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i
  %25 = phi i32 [ %.pre.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %22, %omp.dispatch.end.i.us.i ]
  %.lcssa1.i = phi i32 [ %11, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %23, %omp.dispatch.end.i.us.i ]
  %.lcssa.i = phi i32 [ %12, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %24, %omp.dispatch.end.i.us.i ]
  %add17.i = add nsw i32 %.lcssa.i, %25
  store i32 %add17.i, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %add18.i = add nsw i32 %.lcssa1.i, %25
  %cmp8.i = icmp sgt i32 %add18.i, %sub4.i
  %cond.i = select i1 %cmp8.i, i32 %sub4.i, i32 %add18.i
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp10.i = icmp sgt i32 %add17.i, %cond.i
  br i1 %cmp10.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.i

omp.dispatch.end.i:                               ; preds = %omp.dispatch.inc.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__98.exit

__omp_outlined__98.exit:                          ; preds = %entry.omp.precond.end_crit_edge.i, %omp.dispatch.end.i
  %.pre-phi19.i = phi i8* [ %.pre18.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.dispatch.end.i ]
  %.pre-phi17.i = phi i8* [ %.pre16.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.dispatch.end.i ]
  %.pre-phi15.i = phi i8* [ %.pre14.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.dispatch.end.i ]
  %.pre-phi.i = phi i8* [ %.pre13.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.dispatch.end.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi15.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi17.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi19.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1539(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3, i64 %.capture_expr.5) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub7.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre13.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre14.i = bitcast i32* %.omp.stride.i to i8*
  %.pre16.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre18.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__100.exit

omp.precond.then.i:                               ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub7.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 91, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp115.i = icmp sgt i32 %5, %sub7.i
  %cond6.i = select i1 %cmp115.i, i32 %sub7.i, i32 %5
  store i32 %cond6.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp137.i = icmp sgt i32 %6, %cond6.i
  br i1 %cmp137.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.lr.ph.i

omp.inner.for.cond.preheader.lr.ph.i:             ; preds = %omp.precond.then.i
  %.capture_expr..addr2.sroa.0.0.extract.trunc.i.i = trunc i64 %.capture_expr.1 to i32
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.cond.preheader.i

omp.inner.for.cond.preheader.i:                   ; preds = %omp.dispatch.inc.i, %omp.inner.for.cond.preheader.lr.ph.i
  %11 = phi i32 [ %cond6.i, %omp.inner.for.cond.preheader.lr.ph.i ], [ %cond.i, %omp.dispatch.inc.i ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph.i ], [ %add22.i, %omp.dispatch.inc.i ]
  %cmp152.i = icmp sgt i32 %12, %11
  br i1 %cmp152.i, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i, label %omp.inner.for.body.us.i

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i: ; preds = %omp.inner.for.cond.preheader.i
  %.pre.i = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  br label %omp.dispatch.inc.i

omp.inner.for.body.us.i:                          ; preds = %omp.inner.for.cond.preheader.i, %omp.dispatch.end.i.us.i
  %13 = phi i32 [ %24, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  %14 = phi i32 [ %23, %omp.dispatch.end.i.us.i ], [ %11, %omp.inner.for.cond.preheader.i ]
  %.omp.iv.03.us.i = phi i32 [ %add21.us.i, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !205
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !205
  store i32 %13, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !205
  store i32 %14, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !205
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !205
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !205
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !205
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !205
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 36, i32 %13, i32 %14, i32 1, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc.i.i) #4, !noalias !205
  %15 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !205
  %tobool3.i.us.i = icmp eq i32 %15, 0
  br i1 %tobool3.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.body.i.us.i:                         ; preds = %omp.inner.for.body.us.i, %omp.dispatch.cond.loopexit.i.us.i
  %16 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !205
  %17 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !205, !llvm.mem.parallel_loop_access !208
  %cmp131.i.us.i = icmp sgt i32 %16, %17
  br i1 %cmp131.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.dispatch.body.i.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add22.i.us.i, %omp.inner.for.body.i.us.i ], [ %16, %omp.dispatch.body.i.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !205, !llvm.mem.parallel_loop_access !208
  %arrayidx17.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %19 = load double, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !205, !llvm.mem.parallel_loop_access !208
  %add18.i.us.i = fadd double %18, %19
  %arrayidx20.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %20 = load double, double* %arrayidx20.i.us.i, align 8, !tbaa !126, !noalias !205, !llvm.mem.parallel_loop_access !208
  %add21.i.us.i = fadd double %20, %add18.i.us.i
  store double %add21.i.us.i, double* %arrayidx20.i.us.i, align 8, !tbaa !126, !noalias !205, !llvm.mem.parallel_loop_access !208
  %add22.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp13.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %17
  br i1 %cmp13.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, !llvm.loop !208

omp.dispatch.cond.loopexit.i.us.i:                ; preds = %omp.inner.for.body.i.us.i, %omp.dispatch.body.i.us.i
  %21 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !205
  %tobool.i.us.i = icmp eq i32 %21, 0
  br i1 %tobool.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.end.i.us.i:                          ; preds = %omp.dispatch.cond.loopexit.i.us.i, %omp.inner.for.body.us.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !205
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !205
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !205
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !205
  %22 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !209
  %add21.us.i = add nsw i32 %22, %.omp.iv.03.us.i
  %23 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !209
  %cmp15.us.i = icmp sgt i32 %add21.us.i, %23
  %24 = load i32, i32* %.omp.comb.lb.i, align 4
  br i1 %cmp15.us.i, label %omp.dispatch.inc.i, label %omp.inner.for.body.us.i, !llvm.loop !209

omp.dispatch.inc.i:                               ; preds = %omp.dispatch.end.i.us.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i
  %25 = phi i32 [ %.pre.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %22, %omp.dispatch.end.i.us.i ]
  %.lcssa1.i = phi i32 [ %11, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %23, %omp.dispatch.end.i.us.i ]
  %.lcssa.i = phi i32 [ %12, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %24, %omp.dispatch.end.i.us.i ]
  %add22.i = add nsw i32 %.lcssa.i, %25
  store i32 %add22.i, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %add23.i = add nsw i32 %.lcssa1.i, %25
  %cmp11.i = icmp sgt i32 %add23.i, %sub7.i
  %cond.i = select i1 %cmp11.i, i32 %sub7.i, i32 %add23.i
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp13.i = icmp sgt i32 %add22.i, %cond.i
  br i1 %cmp13.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.i

omp.dispatch.end.i:                               ; preds = %omp.dispatch.inc.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__100.exit

__omp_outlined__100.exit:                         ; preds = %entry.omp.precond.end_crit_edge.i, %omp.dispatch.end.i
  %.pre-phi19.i = phi i8* [ %.pre18.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.dispatch.end.i ]
  %.pre-phi17.i = phi i8* [ %.pre16.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.dispatch.end.i ]
  %.pre-phi15.i = phi i8* [ %.pre14.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.dispatch.end.i ]
  %.pre-phi.i = phi i8* [ %.pre13.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.dispatch.end.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi15.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi17.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi19.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1577(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub4.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre13.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre14.i = bitcast i32* %.omp.stride.i to i8*
  %.pre16.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre18.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__102.exit

omp.precond.then.i:                               ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub4.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 91, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp85.i = icmp sgt i32 %5, %sub4.i
  %cond6.i = select i1 %cmp85.i, i32 %sub4.i, i32 %5
  store i32 %cond6.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp107.i = icmp sgt i32 %6, %cond6.i
  br i1 %cmp107.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.lr.ph.i

omp.inner.for.cond.preheader.lr.ph.i:             ; preds = %omp.precond.then.i
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.cond.preheader.i

omp.inner.for.cond.preheader.i:                   ; preds = %omp.dispatch.inc.i, %omp.inner.for.cond.preheader.lr.ph.i
  %11 = phi i32 [ %cond6.i, %omp.inner.for.cond.preheader.lr.ph.i ], [ %cond.i, %omp.dispatch.inc.i ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph.i ], [ %add17.i, %omp.dispatch.inc.i ]
  %cmp122.i = icmp sgt i32 %12, %11
  br i1 %cmp122.i, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i, label %omp.inner.for.body.us.i

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i: ; preds = %omp.inner.for.cond.preheader.i
  %.pre.i = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  br label %omp.dispatch.inc.i

omp.inner.for.body.us.i:                          ; preds = %omp.inner.for.cond.preheader.i, %omp.dispatch.end.i.us.i
  %13 = phi i32 [ %24, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  %14 = phi i32 [ %23, %omp.dispatch.end.i.us.i ], [ %11, %omp.inner.for.cond.preheader.i ]
  %.omp.iv.03.us.i = phi i32 [ %add16.us.i, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !210
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !210
  store i32 %13, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !210
  store i32 %14, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !210
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !210
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !210
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !210
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !210
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 38, i32 %13, i32 %14, i32 1, i32 1) #4, !noalias !210
  %15 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !210
  %tobool3.i.us.i = icmp eq i32 %15, 0
  br i1 %tobool3.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.body.i.us.i:                         ; preds = %omp.inner.for.body.us.i, %omp.dispatch.cond.loopexit.i.us.i
  %16 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !210
  %17 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !210, !llvm.mem.parallel_loop_access !213
  %cmp101.i.us.i = icmp sgt i32 %16, %17
  br i1 %cmp101.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.dispatch.body.i.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add19.i.us.i, %omp.inner.for.body.i.us.i ], [ %16, %omp.dispatch.body.i.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !210, !llvm.mem.parallel_loop_access !213
  %arrayidx14.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %19 = load double, double* %arrayidx14.i.us.i, align 8, !tbaa !126, !noalias !210, !llvm.mem.parallel_loop_access !213
  %add15.i.us.i = fadd double %18, %19
  %arrayidx17.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %20 = load double, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !210, !llvm.mem.parallel_loop_access !213
  %add18.i.us.i = fadd double %20, %add15.i.us.i
  store double %add18.i.us.i, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !210, !llvm.mem.parallel_loop_access !213
  %add19.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp10.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %17
  br i1 %cmp10.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, !llvm.loop !213

omp.dispatch.cond.loopexit.i.us.i:                ; preds = %omp.inner.for.body.i.us.i, %omp.dispatch.body.i.us.i
  %21 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !210
  %tobool.i.us.i = icmp eq i32 %21, 0
  br i1 %tobool.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.end.i.us.i:                          ; preds = %omp.dispatch.cond.loopexit.i.us.i, %omp.inner.for.body.us.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !210
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !210
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !210
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !210
  %22 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !214
  %add16.us.i = add nsw i32 %22, %.omp.iv.03.us.i
  %23 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !214
  %cmp12.us.i = icmp sgt i32 %add16.us.i, %23
  %24 = load i32, i32* %.omp.comb.lb.i, align 4
  br i1 %cmp12.us.i, label %omp.dispatch.inc.i, label %omp.inner.for.body.us.i, !llvm.loop !214

omp.dispatch.inc.i:                               ; preds = %omp.dispatch.end.i.us.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i
  %25 = phi i32 [ %.pre.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %22, %omp.dispatch.end.i.us.i ]
  %.lcssa1.i = phi i32 [ %11, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %23, %omp.dispatch.end.i.us.i ]
  %.lcssa.i = phi i32 [ %12, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %24, %omp.dispatch.end.i.us.i ]
  %add17.i = add nsw i32 %.lcssa.i, %25
  store i32 %add17.i, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %add18.i = add nsw i32 %.lcssa1.i, %25
  %cmp8.i = icmp sgt i32 %add18.i, %sub4.i
  %cond.i = select i1 %cmp8.i, i32 %sub4.i, i32 %add18.i
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp10.i = icmp sgt i32 %add17.i, %cond.i
  br i1 %cmp10.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.i

omp.dispatch.end.i:                               ; preds = %omp.dispatch.inc.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__102.exit

__omp_outlined__102.exit:                         ; preds = %entry.omp.precond.end_crit_edge.i, %omp.dispatch.end.i
  %.pre-phi19.i = phi i8* [ %.pre18.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.dispatch.end.i ]
  %.pre-phi17.i = phi i8* [ %.pre16.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.dispatch.end.i ]
  %.pre-phi15.i = phi i8* [ %.pre14.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.dispatch.end.i ]
  %.pre-phi.i = phi i8* [ %.pre13.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.dispatch.end.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi15.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi17.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi19.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

; Function Attrs: norecurse nounwind
define weak void @__omp_offloading_802_14c18b2_main_l1613(i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1, i64 %.capture_expr.3) local_unnamed_addr #0 {
entry:
  %.omp.lb.i.i = alloca i32, align 4
  %.omp.ub.i.i = alloca i32, align 4
  %.omp.stride.i.i = alloca i32, align 4
  %.omp.is_last.i.i = alloca i32, align 4
  %.omp.comb.lb.i = alloca i32, align 4
  %.omp.comb.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %0 = tail call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @0) #4
  %nvptx_num_threads = tail call i32 @llvm.nvvm.read.ptx.sreg.ntid.x(), !range !119
  tail call void @__kmpc_spmd_kernel_init(i32 %nvptx_num_threads, i16 1, i16 1) #4
  %n.addr.sroa.0.0.extract.trunc.i = trunc i64 %n to i32
  %sub4.i = add nsw i32 %n.addr.sroa.0.0.extract.trunc.i, -1
  %cmp.i = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc.i, 0
  br i1 %cmp.i, label %omp.precond.then.i, label %entry.omp.precond.end_crit_edge.i

entry.omp.precond.end_crit_edge.i:                ; preds = %entry
  %.pre13.i = bitcast i32* %.omp.is_last.i to i8*
  %.pre14.i = bitcast i32* %.omp.stride.i to i8*
  %.pre16.i = bitcast i32* %.omp.comb.ub.i to i8*
  %.pre18.i = bitcast i32* %.omp.comb.lb.i to i8*
  br label %__omp_outlined__104.exit

omp.precond.then.i:                               ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc.i = trunc i64 %.capture_expr. to i32
  %1 = bitcast i32* %.omp.comb.lb.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #4
  store i32 0, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %2 = bitcast i32* %.omp.comb.ub.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #4
  store i32 %sub4.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %3 = bitcast i32* %.omp.stride.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #4
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !120
  %4 = bitcast i32* %.omp.is_last.i to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %4) #4
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !120
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %0, i32 91, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.comb.lb.i, i32* nonnull %.omp.comb.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc.i) #4
  %5 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp85.i = icmp sgt i32 %5, %sub4.i
  %cond6.i = select i1 %cmp85.i, i32 %sub4.i, i32 %5
  store i32 %cond6.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %6 = load i32, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %cmp107.i = icmp sgt i32 %6, %cond6.i
  br i1 %cmp107.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.lr.ph.i

omp.inner.for.cond.preheader.lr.ph.i:             ; preds = %omp.precond.then.i
  %7 = bitcast i32* %.omp.lb.i.i to i8*
  %8 = bitcast i32* %.omp.ub.i.i to i8*
  %9 = bitcast i32* %.omp.stride.i.i to i8*
  %10 = bitcast i32* %.omp.is_last.i.i to i8*
  br label %omp.inner.for.cond.preheader.i

omp.inner.for.cond.preheader.i:                   ; preds = %omp.dispatch.inc.i, %omp.inner.for.cond.preheader.lr.ph.i
  %11 = phi i32 [ %cond6.i, %omp.inner.for.cond.preheader.lr.ph.i ], [ %cond.i, %omp.dispatch.inc.i ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph.i ], [ %add17.i, %omp.dispatch.inc.i ]
  %cmp122.i = icmp sgt i32 %12, %11
  br i1 %cmp122.i, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i, label %omp.inner.for.body.us.i

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i: ; preds = %omp.inner.for.cond.preheader.i
  %.pre.i = load i32, i32* %.omp.stride.i, align 4, !tbaa !120
  br label %omp.dispatch.inc.i

omp.inner.for.body.us.i:                          ; preds = %omp.inner.for.cond.preheader.i, %omp.dispatch.end.i.us.i
  %13 = phi i32 [ %24, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  %14 = phi i32 [ %23, %omp.dispatch.end.i.us.i ], [ %11, %omp.inner.for.cond.preheader.i ]
  %.omp.iv.03.us.i = phi i32 [ %add16.us.i, %omp.dispatch.end.i.us.i ], [ %12, %omp.inner.for.cond.preheader.i ]
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %7) #4, !noalias !215
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #4, !noalias !215
  store i32 %13, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !215
  store i32 %14, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !215
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #4, !noalias !215
  store i32 1, i32* %.omp.stride.i.i, align 4, !tbaa !120, !noalias !215
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #4, !noalias !215
  store i32 0, i32* %.omp.is_last.i.i, align 4, !tbaa !120, !noalias !215
  call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @0, i32 %0, i32 37, i32 %13, i32 %14, i32 1, i32 1) #4, !noalias !215
  %15 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !215
  %tobool3.i.us.i = icmp eq i32 %15, 0
  br i1 %tobool3.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.body.i.us.i:                         ; preds = %omp.inner.for.body.us.i, %omp.dispatch.cond.loopexit.i.us.i
  %16 = load i32, i32* %.omp.lb.i.i, align 4, !tbaa !120, !noalias !215
  %17 = load i32, i32* %.omp.ub.i.i, align 4, !tbaa !120, !noalias !215, !llvm.mem.parallel_loop_access !218
  %cmp101.i.us.i = icmp sgt i32 %16, %17
  br i1 %cmp101.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, label %omp.inner.for.body.i.us.i

omp.inner.for.body.i.us.i:                        ; preds = %omp.dispatch.body.i.us.i, %omp.inner.for.body.i.us.i
  %.omp.iv.02.i.us.i = phi i32 [ %add19.i.us.i, %omp.inner.for.body.i.us.i ], [ %16, %omp.dispatch.body.i.us.i ]
  %idxprom.i.us.i = sext i32 %.omp.iv.02.i.us.i to i64
  %arrayidx.i.us.i = getelementptr inbounds double, double* %b, i64 %idxprom.i.us.i
  %18 = load double, double* %arrayidx.i.us.i, align 8, !tbaa !126, !noalias !215, !llvm.mem.parallel_loop_access !218
  %arrayidx14.i.us.i = getelementptr inbounds double, double* %c, i64 %idxprom.i.us.i
  %19 = load double, double* %arrayidx14.i.us.i, align 8, !tbaa !126, !noalias !215, !llvm.mem.parallel_loop_access !218
  %add15.i.us.i = fadd double %18, %19
  %arrayidx17.i.us.i = getelementptr inbounds double, double* %a, i64 %idxprom.i.us.i
  %20 = load double, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !215, !llvm.mem.parallel_loop_access !218
  %add18.i.us.i = fadd double %20, %add15.i.us.i
  store double %add18.i.us.i, double* %arrayidx17.i.us.i, align 8, !tbaa !126, !noalias !215, !llvm.mem.parallel_loop_access !218
  %add19.i.us.i = add nsw i32 %.omp.iv.02.i.us.i, 1
  %cmp10.i.us.i = icmp slt i32 %.omp.iv.02.i.us.i, %17
  br i1 %cmp10.i.us.i, label %omp.inner.for.body.i.us.i, label %omp.dispatch.cond.loopexit.i.us.i, !llvm.loop !218

omp.dispatch.cond.loopexit.i.us.i:                ; preds = %omp.inner.for.body.i.us.i, %omp.dispatch.body.i.us.i
  %21 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @0, i32 %0, i32* nonnull %.omp.is_last.i.i, i32* nonnull %.omp.lb.i.i, i32* nonnull %.omp.ub.i.i, i32* nonnull %.omp.stride.i.i) #4, !noalias !215
  %tobool.i.us.i = icmp eq i32 %21, 0
  br i1 %tobool.i.us.i, label %omp.dispatch.end.i.us.i, label %omp.dispatch.body.i.us.i

omp.dispatch.end.i.us.i:                          ; preds = %omp.dispatch.cond.loopexit.i.us.i, %omp.inner.for.body.us.i
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #4, !noalias !215
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #4, !noalias !215
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #4, !noalias !215
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %7) #4, !noalias !215
  %22 = load i32, i32* %.omp.stride.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !219
  %add16.us.i = add nsw i32 %22, %.omp.iv.03.us.i
  %23 = load i32, i32* %.omp.comb.ub.i, align 4, !tbaa !120, !llvm.mem.parallel_loop_access !219
  %cmp12.us.i = icmp sgt i32 %add16.us.i, %23
  %24 = load i32, i32* %.omp.comb.lb.i, align 4
  br i1 %cmp12.us.i, label %omp.dispatch.inc.i, label %omp.inner.for.body.us.i, !llvm.loop !219

omp.dispatch.inc.i:                               ; preds = %omp.dispatch.end.i.us.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i
  %25 = phi i32 [ %.pre.i, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %22, %omp.dispatch.end.i.us.i ]
  %.lcssa1.i = phi i32 [ %11, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %23, %omp.dispatch.end.i.us.i ]
  %.lcssa.i = phi i32 [ %12, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge.i ], [ %24, %omp.dispatch.end.i.us.i ]
  %add17.i = add nsw i32 %.lcssa.i, %25
  store i32 %add17.i, i32* %.omp.comb.lb.i, align 4, !tbaa !120
  %add18.i = add nsw i32 %.lcssa1.i, %25
  %cmp8.i = icmp sgt i32 %add18.i, %sub4.i
  %cond.i = select i1 %cmp8.i, i32 %sub4.i, i32 %add18.i
  store i32 %cond.i, i32* %.omp.comb.ub.i, align 4, !tbaa !120
  %cmp10.i = icmp sgt i32 %add17.i, %cond.i
  br i1 %cmp10.i, label %omp.dispatch.end.i, label %omp.inner.for.cond.preheader.i

omp.dispatch.end.i:                               ; preds = %omp.dispatch.inc.i, %omp.precond.then.i
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @1, i32 %0) #4
  br label %__omp_outlined__104.exit

__omp_outlined__104.exit:                         ; preds = %entry.omp.precond.end_crit_edge.i, %omp.dispatch.end.i
  %.pre-phi19.i = phi i8* [ %.pre18.i, %entry.omp.precond.end_crit_edge.i ], [ %1, %omp.dispatch.end.i ]
  %.pre-phi17.i = phi i8* [ %.pre16.i, %entry.omp.precond.end_crit_edge.i ], [ %2, %omp.dispatch.end.i ]
  %.pre-phi15.i = phi i8* [ %.pre14.i, %entry.omp.precond.end_crit_edge.i ], [ %3, %omp.dispatch.end.i ]
  %.pre-phi.i = phi i8* [ %.pre13.i, %entry.omp.precond.end_crit_edge.i ], [ %4, %omp.dispatch.end.i ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi15.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi17.i) #4
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi19.i) #4
  call void @__kmpc_spmd_kernel_deinit() #4
  ret void
}

attributes #0 = { norecurse nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="sm_35" "target-features"="+ptx42,+sm_35" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { nounwind readnone }
attributes #2 = { convergent nounwind }
attributes #3 = { argmemonly nounwind }
attributes #4 = { nounwind }

!omp_offload.info = !{!0, !1, !2, !3, !4, !5, !6, !7, !8, !9, !10, !11, !12, !13, !14, !15, !16, !17, !18, !19, !20, !21, !22, !23, !24, !25, !26, !27, !28, !29, !30, !31, !32, !33, !34, !35, !36, !37, !38, !39, !40, !41, !42, !43, !44, !45, !46, !47, !48, !49, !50, !51, !52, !53, !54}
!nvvm.annotations = !{!55, !56, !57, !58, !59, !60, !61, !62, !63, !64, !65, !66, !67, !68, !69, !70, !71, !72, !73, !74, !75, !76, !77, !78, !79, !80, !81, !82, !83, !84, !85, !86, !87, !88, !89, !90, !91, !92, !93, !94, !95, !96, !97, !98, !99, !100, !101, !102, !103, !104, !105, !106, !107, !108, !109, !110, !111, !110, !112, !112, !112, !112, !113, !113, !112}
!llvm.module.flags = !{!114, !115}
!llvm.ident = !{!116}
!nvvm.internalize.after.link = !{}
!nvvmir.version = !{!117}

!0 = !{i32 0, i32 2050, i32 21764393, !"check_offloading", i32 22, i32 0}
!1 = !{i32 0, i32 2050, i32 21764274, !"main", i32 256, i32 11}
!2 = !{i32 0, i32 2050, i32 21764274, !"main", i32 45, i32 1}
!3 = !{i32 0, i32 2050, i32 21764274, !"main", i32 346, i32 15}
!4 = !{i32 0, i32 2050, i32 21764274, !"main", i32 737, i32 31}
!5 = !{i32 0, i32 2050, i32 21764274, !"main", i32 993, i32 37}
!6 = !{i32 0, i32 2050, i32 21764274, !"main", i32 66, i32 2}
!7 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1391, i32 48}
!8 = !{i32 0, i32 2050, i32 21764274, !"main", i32 637, i32 27}
!9 = !{i32 0, i32 2050, i32 21764274, !"main", i32 426, i32 19}
!10 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1502, i32 51}
!11 = !{i32 0, i32 2050, i32 21764274, !"main", i32 146, i32 6}
!12 = !{i32 0, i32 2050, i32 21764274, !"main", i32 530, i32 23}
!13 = !{i32 0, i32 2050, i32 21764274, !"main", i32 236, i32 10}
!14 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1177, i32 42}
!15 = !{i32 0, i32 2050, i32 21764274, !"main", i32 326, i32 14}
!16 = !{i32 0, i32 2050, i32 21764274, !"main", i32 883, i32 32}
!17 = !{i32 0, i32 2050, i32 21764274, !"main", i32 461, i32 20}
!18 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1613, i32 54}
!19 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1070, i32 39}
!20 = !{i32 0, i32 2050, i32 21764274, !"main", i32 949, i32 35}
!21 = !{i32 0, i32 2050, i32 21764274, !"main", i32 406, i32 18}
!22 = !{i32 0, i32 2050, i32 21764274, !"main", i32 662, i32 28}
!23 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1354, i32 47}
!24 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1105, i32 40}
!25 = !{i32 0, i32 2050, i32 21764274, !"main", i32 126, i32 5}
!26 = !{i32 0, i32 2050, i32 21764274, !"main", i32 216, i32 9}
!27 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1036, i32 38}
!28 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1465, i32 50}
!29 = !{i32 0, i32 2050, i32 21764274, !"main", i32 576, i32 25}
!30 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1140, i32 41}
!31 = !{i32 0, i32 2050, i32 21764274, !"main", i32 507, i32 22}
!32 = !{i32 0, i32 2050, i32 21764274, !"main", i32 296, i32 13}
!33 = !{i32 0, i32 2050, i32 21764274, !"main", i32 386, i32 17}
!34 = !{i32 0, i32 2050, i32 21764274, !"main", i32 687, i32 29}
!35 = !{i32 0, i32 2050, i32 21764274, !"main", i32 905, i32 33}
!36 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1213, i32 43}
!37 = !{i32 0, i32 2050, i32 21764274, !"main", i32 106, i32 4}
!38 = !{i32 0, i32 2050, i32 21764274, !"main", i32 196, i32 8}
!39 = !{i32 0, i32 2050, i32 21764274, !"main", i32 971, i32 36}
!40 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1248, i32 44}
!41 = !{i32 0, i32 2050, i32 21764274, !"main", i32 276, i32 12}
!42 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1428, i32 49}
!43 = !{i32 0, i32 2050, i32 21764274, !"main", i32 366, i32 16}
!44 = !{i32 0, i32 2050, i32 21764274, !"main", i32 712, i32 30}
!45 = !{i32 0, i32 2050, i32 21764274, !"main", i32 553, i32 24}
!46 = !{i32 0, i32 2050, i32 21764274, !"main", i32 86, i32 3}
!47 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1283, i32 45}
!48 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1539, i32 52}
!49 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1577, i32 53}
!50 = !{i32 0, i32 2050, i32 21764274, !"main", i32 484, i32 21}
!51 = !{i32 0, i32 2050, i32 21764274, !"main", i32 612, i32 26}
!52 = !{i32 0, i32 2050, i32 21764274, !"main", i32 927, i32 34}
!53 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1318, i32 46}
!54 = !{i32 0, i32 2050, i32 21764274, !"main", i32 166, i32 7}
!55 = !{void ([1 x i32]*)* @__omp_offloading_802_14c1929_check_offloading_l22, !"kernel", i32 1}
!56 = !{void (i32*)* @__omp_offloading_802_14c18b2_main_l45, !"kernel", i32 1}
!57 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l66, !"kernel", i32 1}
!58 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l86, !"kernel", i32 1}
!59 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l106, !"kernel", i32 1}
!60 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l126, !"kernel", i32 1}
!61 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l146, !"kernel", i32 1}
!62 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l166, !"kernel", i32 1}
!63 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l196, !"kernel", i32 1}
!64 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l216, !"kernel", i32 1}
!65 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l236, !"kernel", i32 1}
!66 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l256, !"kernel", i32 1}
!67 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l276, !"kernel", i32 1}
!68 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l296, !"kernel", i32 1}
!69 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l326, !"kernel", i32 1}
!70 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l346, !"kernel", i32 1}
!71 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l366, !"kernel", i32 1}
!72 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l386, !"kernel", i32 1}
!73 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l406, !"kernel", i32 1}
!74 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l426, !"kernel", i32 1}
!75 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l461, !"kernel", i32 1}
!76 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l484, !"kernel", i32 1}
!77 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l507, !"kernel", i32 1}
!78 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l530, !"kernel", i32 1}
!79 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l553, !"kernel", i32 1}
!80 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l576, !"kernel", i32 1}
!81 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l612, !"kernel", i32 1}
!82 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l637, !"kernel", i32 1}
!83 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l662, !"kernel", i32 1}
!84 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l687, !"kernel", i32 1}
!85 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l712, !"kernel", i32 1}
!86 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l737, !"kernel", i32 1}
!87 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l883, !"kernel", i32 1}
!88 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l905, !"kernel", i32 1}
!89 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l927, !"kernel", i32 1}
!90 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l949, !"kernel", i32 1}
!91 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l971, !"kernel", i32 1}
!92 = !{void ([3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l993, !"kernel", i32 1}
!93 = !{void (i64, double*, double*, double*, i64, i64)* @__omp_offloading_802_14c18b2_main_l1036, !"kernel", i32 1}
!94 = !{void (i64, double*, double*, double*, i64, i64)* @__omp_offloading_802_14c18b2_main_l1070, !"kernel", i32 1}
!95 = !{void (i64, double*, double*, double*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1105, !"kernel", i32 1}
!96 = !{void (i64, double*, double*, double*, i64, i64)* @__omp_offloading_802_14c18b2_main_l1140, !"kernel", i32 1}
!97 = !{void (i64, double*, double*, double*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1177, !"kernel", i32 1}
!98 = !{void (i64, double*, double*, double*, i64, i64)* @__omp_offloading_802_14c18b2_main_l1213, !"kernel", i32 1}
!99 = !{void (i64, double*, double*, double*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1248, !"kernel", i32 1}
!100 = !{void (i64, double*, double*, double*, i64, i64)* @__omp_offloading_802_14c18b2_main_l1283, !"kernel", i32 1}
!101 = !{void (i64, double*, double*, double*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1318, !"kernel", i32 1}
!102 = !{void (i64, double*, double*, double*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1354, !"kernel", i32 1}
!103 = !{void (i64, double*, double*, double*, i64, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1391, !"kernel", i32 1}
!104 = !{void (i64, double*, double*, double*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1428, !"kernel", i32 1}
!105 = !{void (i64, double*, double*, double*, i64, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1465, !"kernel", i32 1}
!106 = !{void (i64, double*, double*, double*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1502, !"kernel", i32 1}
!107 = !{void (i64, double*, double*, double*, i64, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1539, !"kernel", i32 1}
!108 = !{void (i64, double*, double*, double*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1577, !"kernel", i32 1}
!109 = !{void (i64, double*, double*, double*, i64, i64, i64)* @__omp_offloading_802_14c18b2_main_l1613, !"kernel", i32 1}
!110 = !{null, !"align", i32 8}
!111 = !{null, !"align", i32 8, !"align", i32 65544, !"align", i32 131080}
!112 = !{null, !"align", i32 16}
!113 = !{null, !"align", i32 16, !"align", i32 65552, !"align", i32 131088}
!114 = !{i32 1, !"wchar_size", i32 4}
!115 = !{i32 7, !"PIC Level", i32 2}
!116 = !{!"clang version 7.0.0 (trunk 336676) (llvm/trunk 336675)"}
!117 = !{i32 1, i32 2}
!118 = !{i32 0, i32 1024}
!119 = !{i32 1, i32 1025}
!120 = !{!121, !121, i64 0}
!121 = !{!"int", !122, i64 0}
!122 = !{!"omnipotent char", !123, i64 0}
!123 = !{!"Simple C/C++ TBAA"}
!124 = !{!125, !125, i64 0}
!125 = !{!"any pointer", !122, i64 0}
!126 = !{!127, !127, i64 0}
!127 = !{!"double", !122, i64 0}
!128 = distinct !{!128}
!129 = distinct !{!129}
!130 = distinct !{!130}
!131 = distinct !{!131}
!132 = distinct !{!132}
!133 = distinct !{!133}
!134 = distinct !{!134}
!135 = distinct !{!135}
!136 = distinct !{!136}
!137 = distinct !{!137}
!138 = distinct !{!138}
!139 = distinct !{!139}
!140 = distinct !{!140}
!141 = distinct !{!141}
!142 = distinct !{!142}
!143 = distinct !{!143}
!144 = distinct !{!144}
!145 = distinct !{!145}
!146 = distinct !{!146}
!147 = distinct !{!147}
!148 = distinct !{!148}
!149 = distinct !{!149}
!150 = distinct !{!150}
!151 = distinct !{!151}
!152 = !{!153}
!153 = distinct !{!153, !154, !"__omp_outlined__73: %.global_tid."}
!154 = distinct !{!154, !"__omp_outlined__73"}
!155 = !{!156}
!156 = distinct !{!156, !157, !"__omp_outlined__75: %.global_tid."}
!157 = distinct !{!157, !"__omp_outlined__75"}
!158 = !{!159}
!159 = distinct !{!159, !160, !"__omp_outlined__77: %.global_tid."}
!160 = distinct !{!160, !"__omp_outlined__77"}
!161 = !{!162}
!162 = distinct !{!162, !163, !"__omp_outlined__79: %.global_tid."}
!163 = distinct !{!163, !"__omp_outlined__79"}
!164 = distinct !{!164}
!165 = !{!166}
!166 = distinct !{!166, !167, !"__omp_outlined__81: %.global_tid."}
!167 = distinct !{!167, !"__omp_outlined__81"}
!168 = distinct !{!168}
!169 = !{!170}
!170 = distinct !{!170, !171, !"__omp_outlined__83: %.global_tid."}
!171 = distinct !{!171, !"__omp_outlined__83"}
!172 = !{!173}
!173 = distinct !{!173, !174, !"__omp_outlined__85: %.global_tid."}
!174 = distinct !{!174, !"__omp_outlined__85"}
!175 = distinct !{!175}
!176 = !{!177}
!177 = distinct !{!177, !178, !"__omp_outlined__87: %.global_tid."}
!178 = distinct !{!178, !"__omp_outlined__87"}
!179 = !{!180}
!180 = distinct !{!180, !181, !"__omp_outlined__89: %.global_tid."}
!181 = distinct !{!181, !"__omp_outlined__89"}
!182 = !{!183}
!183 = distinct !{!183, !184, !"__omp_outlined__91: %.global_tid."}
!184 = distinct !{!184, !"__omp_outlined__91"}
!185 = distinct !{!185}
!186 = !{!187}
!187 = distinct !{!187, !188, !"__omp_outlined__93: %.global_tid."}
!188 = distinct !{!188, !"__omp_outlined__93"}
!189 = distinct !{!189}
!190 = !{!191}
!191 = distinct !{!191, !192, !"__omp_outlined__95: %.global_tid."}
!192 = distinct !{!192, !"__omp_outlined__95"}
!193 = distinct !{!193}
!194 = distinct !{!194}
!195 = !{!196}
!196 = distinct !{!196, !197, !"__omp_outlined__97: %.global_tid."}
!197 = distinct !{!197, !"__omp_outlined__97"}
!198 = distinct !{!198}
!199 = distinct !{!199}
!200 = !{!201}
!201 = distinct !{!201, !202, !"__omp_outlined__99: %.global_tid."}
!202 = distinct !{!202, !"__omp_outlined__99"}
!203 = distinct !{!203}
!204 = distinct !{!204}
!205 = !{!206}
!206 = distinct !{!206, !207, !"__omp_outlined__101: %.global_tid."}
!207 = distinct !{!207, !"__omp_outlined__101"}
!208 = distinct !{!208}
!209 = distinct !{!209}
!210 = !{!211}
!211 = distinct !{!211, !212, !"__omp_outlined__103: %.global_tid."}
!212 = distinct !{!212, !"__omp_outlined__103"}
!213 = distinct !{!213}
!214 = distinct !{!214}
!215 = !{!216}
!216 = distinct !{!216, !217, !"__omp_outlined__105: %.global_tid."}
!217 = distinct !{!217, !"__omp_outlined__105"}
!218 = distinct !{!218}
!219 = distinct !{!219}

; __CLANG_OFFLOAD_BUNDLE____END__ openmp-nvptx64-nvidia-cuda

; __CLANG_OFFLOAD_BUNDLE____START__ host-powerpc64le-ibm-linux-gnu
; ModuleID = '/tmp/test-ce8dd3.bc'
source_filename = "/localhd/abataev/development/build_ibm/llvm/projects/openmp/libomptarget/omptests/nt-target-teams-distribute-parallel-for/test.c"
target datalayout = "e-m:e-i64:64-n32:64"
target triple = "powerpc64le-ibm-linux-gnu"

%struct.ident_t = type { i32, i32, i32, i32, i8* }
%struct.__tgt_offload_entry = type { i8*, i8*, i64, i32, i32 }
%struct.__tgt_device_image = type { i8*, i8*, %struct.__tgt_offload_entry*, %struct.__tgt_offload_entry* }
%struct.__tgt_bin_desc = type { i32, %struct.__tgt_device_image*, %struct.__tgt_offload_entry*, %struct.__tgt_offload_entry* }

$.omp_offloading.descriptor_reg = comdat any

@.str = private unnamed_addr constant [19 x i8] c"OMP_TARGET_OFFLOAD\00", align 1
@.str.1 = private unnamed_addr constant [9 x i8] c"DISABLED\00", align 1
@.__omp_offloading_802_14c1929_check_offloading_l22.region_id = weak constant i8 0
@.offload_maptypes = private unnamed_addr constant [1 x i64] [i64 547]
@.__omp_offloading_802_14c18b2_main_l45.region_id = weak constant i8 0
@.offload_sizes.4 = private unnamed_addr constant [1 x i64] [i64 4]
@.offload_maptypes.5 = private unnamed_addr constant [1 x i64] [i64 35]
@.str.6 = private unnamed_addr constant [23 x i8] c";unknown;unknown;0;0;;\00", align 1
@0 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2050, i32 0, i32 0, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str.6, i32 0, i32 0) }, align 8
@1 = private unnamed_addr constant %struct.ident_t { i32 0, i32 514, i32 0, i32 0, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str.6, i32 0, i32 0) }, align 8
@2 = private unnamed_addr constant %struct.ident_t { i32 0, i32 2, i32 0, i32 0, i8* getelementptr inbounds ([23 x i8], [23 x i8]* @.str.6, i32 0, i32 0) }, align 8
@.__omp_offloading_802_14c18b2_main_l66.region_id = weak constant i8 0
@.str.10 = private unnamed_addr constant [54 x i8] c"Failed at %3d with %s, expected %20.15e, got %20.15e\0A\00", align 1
@.str.11 = private unnamed_addr constant [5 x i8] c"S[0]\00", align 1
@.__omp_offloading_802_14c18b2_main_l86.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l106.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l126.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l146.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l166.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l196.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l216.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l236.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l256.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l276.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l296.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l326.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l346.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l366.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l386.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l406.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l426.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l461.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l484.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l507.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l530.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l553.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l576.region_id = weak constant i8 0
@.offload_maptypes.105 = private unnamed_addr constant [8 x i64] [i64 35, i64 35, i64 35, i64 35, i64 35, i64 288, i64 288, i64 288]
@.__omp_offloading_802_14c18b2_main_l612.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l637.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l662.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l687.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l712.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l737.region_id = weak constant i8 0
@.offload_sizes.128 = private unnamed_addr constant [10 x i64] [i64 24576, i64 24576, i64 24576, i64 8, i64 24576, i64 24576, i64 8, i64 4, i64 4, i64 4]
@.offload_maptypes.129 = private unnamed_addr constant [10 x i64] [i64 547, i64 547, i64 547, i64 288, i64 547, i64 547, i64 288, i64 288, i64 288, i64 288]
@.__omp_offloading_802_14c18b2_main_l883.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l905.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l927.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l949.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l971.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l993.region_id = weak constant i8 0
@.offload_sizes.152 = private unnamed_addr constant [8 x i64] [i64 24576, i64 24576, i64 24576, i64 24576, i64 24576, i64 4, i64 4, i64 4]
@.offload_maptypes.153 = private unnamed_addr constant [8 x i64] [i64 547, i64 547, i64 547, i64 547, i64 547, i64 288, i64 288, i64 288]
@.__omp_offloading_802_14c18b2_main_l1036.region_id = weak constant i8 0
@.str.163 = private unnamed_addr constant [51 x i8] c"Error at n = %d, i = %d: host = %lf, device = %lf\0A\00", align 1
@.__omp_offloading_802_14c18b2_main_l1070.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1105.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1140.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1177.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1213.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1248.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1283.region_id = weak constant i8 0
@.offload_sizes.210 = private unnamed_addr constant [6 x i64] [i64 4, i64 0, i64 0, i64 0, i64 4, i64 4]
@.offload_maptypes.211 = private unnamed_addr constant [6 x i64] [i64 288, i64 32, i64 32, i64 32, i64 288, i64 288]
@.__omp_offloading_802_14c18b2_main_l1318.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1354.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1391.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1428.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1465.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1502.region_id = weak constant i8 0
@.__omp_offloading_802_14c18b2_main_l1539.region_id = weak constant i8 0
@.offload_sizes.259 = private unnamed_addr constant [8 x i64] [i64 4, i64 0, i64 0, i64 0, i64 4, i64 4, i64 4, i64 4]
@.offload_maptypes.260 = private unnamed_addr constant [8 x i64] [i64 288, i64 32, i64 32, i64 32, i64 288, i64 288, i64 288, i64 288]
@.__omp_offloading_802_14c18b2_main_l1577.region_id = weak constant i8 0
@.offload_maptypes.270 = private unnamed_addr constant [3 x i64] [i64 33, i64 33, i64 33]
@.__omp_offloading_802_14c18b2_main_l1613.region_id = weak constant i8 0
@.offload_sizes.273 = private unnamed_addr constant [7 x i64] [i64 4, i64 0, i64 0, i64 0, i64 4, i64 4, i64 4]
@.offload_maptypes.274 = private unnamed_addr constant [7 x i64] [i64 288, i64 32, i64 32, i64 32, i64 288, i64 288, i64 288]
@.offload_maptypes.275 = private unnamed_addr constant [1 x i64] [i64 34]
@.offload_sizes.276 = private unnamed_addr constant [3 x i64] [i64 200000, i64 200000, i64 200000]
@.offload_maptypes.277 = private unnamed_addr constant [3 x i64] [i64 32, i64 32, i64 32]
@.omp_offloading.entry_name = internal unnamed_addr constant [50 x i8] c"__omp_offloading_802_14c1929_check_offloading_l22\00"
@.omp_offloading.entry.__omp_offloading_802_14c1929_check_offloading_l22 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c1929_check_offloading_l22.region_id, i8* getelementptr inbounds ([50 x i8], [50 x i8]* @.omp_offloading.entry_name, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.278 = internal unnamed_addr constant [38 x i8] c"__omp_offloading_802_14c18b2_main_l45\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l45 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l45.region_id, i8* getelementptr inbounds ([38 x i8], [38 x i8]* @.omp_offloading.entry_name.278, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.279 = internal unnamed_addr constant [38 x i8] c"__omp_offloading_802_14c18b2_main_l66\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l66 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l66.region_id, i8* getelementptr inbounds ([38 x i8], [38 x i8]* @.omp_offloading.entry_name.279, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.280 = internal unnamed_addr constant [38 x i8] c"__omp_offloading_802_14c18b2_main_l86\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l86 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l86.region_id, i8* getelementptr inbounds ([38 x i8], [38 x i8]* @.omp_offloading.entry_name.280, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.281 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l106\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l106 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l106.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.281, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.282 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l126\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l126 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l126.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.282, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.283 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l146\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l146 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l146.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.283, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.284 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l166\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l166 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l166.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.284, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.285 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l196\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l196 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l196.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.285, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.286 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l216\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l216 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l216.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.286, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.287 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l236\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l236 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l236.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.287, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.288 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l256\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l256 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l256.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.288, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.289 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l276\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l276 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l276.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.289, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.290 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l296\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l296 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l296.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.290, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.291 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l326\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l326 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l326.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.291, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.292 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l346\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l346 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l346.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.292, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.293 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l366\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l366 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l366.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.293, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.294 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l386\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l386 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l386.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.294, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.295 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l406\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l406 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l406.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.295, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.296 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l426\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l426 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l426.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.296, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.297 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l461\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l461 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l461.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.297, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.298 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l484\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l484 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l484.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.298, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.299 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l507\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l507 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l507.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.299, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.300 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l530\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l530 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l530.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.300, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.301 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l553\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l553 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l553.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.301, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.302 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l576\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l576 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l576.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.302, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.303 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l612\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l612 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l612.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.303, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.304 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l637\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l637 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l637.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.304, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.305 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l662\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l662 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l662.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.305, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.306 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l687\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l687 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l687.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.306, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.307 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l712\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l712 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l712.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.307, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.308 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l737\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l737 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l737.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.308, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.309 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l883\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l883 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l883.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.309, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.310 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l905\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l905 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l905.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.310, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.311 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l927\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l927 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l927.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.311, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.312 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l949\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l949 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l949.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.312, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.313 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l971\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l971 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l971.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.313, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.314 = internal unnamed_addr constant [39 x i8] c"__omp_offloading_802_14c18b2_main_l993\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l993 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l993.region_id, i8* getelementptr inbounds ([39 x i8], [39 x i8]* @.omp_offloading.entry_name.314, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.315 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1036\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1036 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1036.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.315, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.316 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1070\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1070 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1070.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.316, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.317 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1105\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1105 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1105.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.317, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.318 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1140\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1140 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1140.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.318, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.319 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1177\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1177 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1177.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.319, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.320 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1213\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1213 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1213.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.320, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.321 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1248\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1248 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1248.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.321, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.322 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1283\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1283 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1283.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.322, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.323 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1318\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1318 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1318.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.323, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.324 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1354\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1354 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1354.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.324, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.325 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1391\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1391 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1391.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.325, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.326 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1428\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1428 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1428.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.326, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.327 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1465\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1465 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1465.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.327, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.328 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1502\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1502 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1502.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.328, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.329 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1539\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1539 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1539.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.329, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.330 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1577\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1577 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1577.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.330, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entry_name.331 = internal unnamed_addr constant [40 x i8] c"__omp_offloading_802_14c18b2_main_l1613\00"
@.omp_offloading.entry.__omp_offloading_802_14c18b2_main_l1613 = weak local_unnamed_addr constant %struct.__tgt_offload_entry { i8* @.__omp_offloading_802_14c18b2_main_l1613.region_id, i8* getelementptr inbounds ([40 x i8], [40 x i8]* @.omp_offloading.entry_name.331, i32 0, i32 0), i64 0, i32 0, i32 0 }, section ".omp_offloading.entries", align 1
@.omp_offloading.entries_begin = external constant %struct.__tgt_offload_entry
@.omp_offloading.entries_end = external constant %struct.__tgt_offload_entry
@.omp_offloading.img_start.nvptx64-nvidia-cuda = external constant i8
@.omp_offloading.img_end.nvptx64-nvidia-cuda = external constant i8
@.omp_offloading.device_images = internal unnamed_addr constant [1 x %struct.__tgt_device_image] [%struct.__tgt_device_image { i8* @.omp_offloading.img_start.nvptx64-nvidia-cuda, i8* @.omp_offloading.img_end.nvptx64-nvidia-cuda, %struct.__tgt_offload_entry* @.omp_offloading.entries_begin, %struct.__tgt_offload_entry* @.omp_offloading.entries_end }], comdat($.omp_offloading.descriptor_reg), align 8
@.omp_offloading.descriptor = internal constant %struct.__tgt_bin_desc { i32 1, %struct.__tgt_device_image* getelementptr inbounds ([1 x %struct.__tgt_device_image], [1 x %struct.__tgt_device_image]* @.omp_offloading.device_images, i32 0, i32 0), %struct.__tgt_offload_entry* @.omp_offloading.entries_begin, %struct.__tgt_offload_entry* @.omp_offloading.entries_end }, comdat($.omp_offloading.descriptor_reg), align 8
@__dso_handle = external hidden global i8
@llvm.global_ctors = appending global [1 x { i32, void ()*, i8* }] [{ i32, void ()*, i8* } { i32 0, void ()* @.omp_offloading.descriptor_reg, i8* bitcast (void ()* @.omp_offloading.descriptor_reg to i8*) }]
@str = private unnamed_addr constant [24 x i8] c"Able to use offloading!\00"
@str.332 = private unnamed_addr constant [39 x i8] c"### Unable to use offloading!  8^( ###\00"
@str.333 = private unnamed_addr constant [20 x i8] c"no schedule clauses\00"
@str.335 = private unnamed_addr constant [25 x i8] c"schedule static no chunk\00"
@str.337 = private unnamed_addr constant [22 x i8] c"schedule static chunk\00"
@str.339 = private unnamed_addr constant [26 x i8] c"schedule dynamic no chunk\00"
@str.341 = private unnamed_addr constant [23 x i8] c"schedule dynamic chunk\00"
@str.343 = private unnamed_addr constant [30 x i8] c"dist_schedule static no chunk\00"
@str.345 = private unnamed_addr constant [27 x i8] c"dist_schedule static chunk\00"
@str.347 = private unnamed_addr constant [56 x i8] c"dist_schedule static no chunk, schedule static no chunk\00"
@str.349 = private unnamed_addr constant [53 x i8] c"dist_schedule static no chunk, schedule static chunk\00"
@str.351 = private unnamed_addr constant [53 x i8] c"dist_schedule static chunk, schedule static no chunk\00"
@str.353 = private unnamed_addr constant [50 x i8] c"dist_schedule static chunk, schedule static chunk\00"
@str.355 = private unnamed_addr constant [54 x i8] c"dist_schedule static chunk, schedule dynamic no chunk\00"
@str.357 = private unnamed_addr constant [51 x i8] c"dist_schedule static chunk, schedule dynamic chunk\00"
@str.359 = private unnamed_addr constant [53 x i8] c"dist_schedule static chunk, schedule guided no chunk\00"
@str.361 = private unnamed_addr constant [50 x i8] c"dist_schedule static chunk, schedule guided chunk\00"
@str.363 = private unnamed_addr constant [42 x i8] c"dist_schedule static chunk, schedule auto\00"
@str.365 = private unnamed_addr constant [45 x i8] c"dist_schedule static chunk, schedule runtime\00"
@str.438 = private unnamed_addr constant [10 x i8] c"Succeeded\00"
@str.444 = private unnamed_addr constant [7 x i8] c"Failed\00"

; Function Attrs: nounwind readonly
define signext i32 @offloading_disabled() local_unnamed_addr #0 {
entry:
  %call = tail call i8* @getenv(i8* getelementptr inbounds ([19 x i8], [19 x i8]* @.str, i64 0, i64 0)) #6
  %tobool = icmp eq i8* %call, null
  br i1 %tobool, label %cleanup, label %cond.end

cond.end:                                         ; preds = %entry
  %call37 = tail call signext i32 @strcmp(i8* nonnull %call, i8* getelementptr inbounds ([9 x i8], [9 x i8]* @.str.1, i64 0, i64 0)) #6
  %cmp38 = icmp eq i32 %call37, 0
  %. = zext i1 %cmp38 to i32
  br label %cleanup

cleanup:                                          ; preds = %cond.end, %entry
  %retval.0 = phi i32 [ 0, %entry ], [ %., %cond.end ]
  ret i32 %retval.0
}

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.start.p0i8(i64, i8* nocapture) #1

; Function Attrs: nounwind readonly
declare i8* @getenv(i8* nocapture) local_unnamed_addr #2

; Function Attrs: argmemonly nounwind
declare void @llvm.lifetime.end.p0i8(i64, i8* nocapture) #1

; Function Attrs: nounwind readonly
declare signext i32 @strcmp(i8* nocapture, i8* nocapture) local_unnamed_addr #2

; Function Attrs: nounwind
define signext i32 @check_offloading() local_unnamed_addr #3 {
entry:
  %A = alloca [1 x i32], align 4
  %.offload_baseptrs = alloca [1 x i8*], align 8
  %.offload_ptrs = alloca [1 x i8*], align 8
  %0 = bitcast [1 x i32]* %A to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = getelementptr inbounds [1 x i32], [1 x i32]* %A, i64 0, i64 0
  store i32 -1, i32* %1, align 4
  %call.i = tail call i8* @getenv(i8* getelementptr inbounds ([19 x i8], [19 x i8]* @.str, i64 0, i64 0)) #6
  %tobool.i = icmp eq i8* %call.i, null
  br i1 %tobool.i, label %if.else, label %offloading_disabled.exit

offloading_disabled.exit:                         ; preds = %entry
  %call37.i = tail call signext i32 @strcmp(i8* nonnull %call.i, i8* getelementptr inbounds ([9 x i8], [9 x i8]* @.str.1, i64 0, i64 0)) #6
  %cmp38.i = icmp eq i32 %call37.i, 0
  br i1 %cmp38.i, label %if.end.thread, label %if.else

if.end.thread:                                    ; preds = %offloading_disabled.exit
  store i32 0, i32* %1, align 4, !tbaa !58
  br label %if.then3

if.else:                                          ; preds = %offloading_disabled.exit, %entry
  %2 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs, i64 0, i64 0
  %3 = bitcast [1 x i8*]* %.offload_baseptrs to [1 x i32]**
  store [1 x i32]* %A, [1 x i32]** %3, align 8
  %4 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs, i64 0, i64 0
  %5 = bitcast [1 x i8*]* %.offload_ptrs to [1 x i32]**
  store [1 x i32]* %A, [1 x i32]** %5, align 8
  %6 = call i32 @__tgt_target(i64 -1, i8* nonnull @.__omp_offloading_802_14c1929_check_offloading_l22.region_id, i32 1, i8** nonnull %2, i8** nonnull %4, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.4, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes, i64 0, i64 0)) #6
  %7 = icmp eq i32 %6, 0
  br i1 %7, label %if.end, label %if.end.thread11

if.end.thread11:                                  ; preds = %if.else
  store i32 1, i32* %1, align 4, !tbaa !58
  br label %if.else5

if.end:                                           ; preds = %if.else
  %.pr = load i32, i32* %1, align 4, !tbaa !58
  %tobool2 = icmp eq i32 %.pr, 0
  br i1 %tobool2, label %if.then3, label %if.else5

if.then3:                                         ; preds = %if.end.thread, %if.end
  %puts = call i32 @puts(i8* getelementptr inbounds ([24 x i8], [24 x i8]* @str, i64 0, i64 0))
  br label %cleanup

if.else5:                                         ; preds = %if.end.thread11, %if.end
  %puts8 = call i32 @puts(i8* getelementptr inbounds ([39 x i8], [39 x i8]* @str.332, i64 0, i64 0))
  br label %cleanup

cleanup:                                          ; preds = %if.else5, %if.then3
  %retval.0 = phi i32 [ 1, %if.else5 ], [ 0, %if.then3 ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret i32 %retval.0
}

declare i32 @__tgt_target(i64, i8*, i32, i8**, i8**, i64*, i64*) local_unnamed_addr

; Function Attrs: nounwind
declare signext i32 @printf(i8* nocapture readonly, ...) local_unnamed_addr #4

; Function Attrs: norecurse nounwind
define void @reset_input(double* nocapture %a, double* nocapture %a_h, double* nocapture %b, double* nocapture %c) local_unnamed_addr #5 {
entry:
  %scevgep = getelementptr double, double* %a_h, i64 25000
  %scevgep24 = getelementptr double, double* %a, i64 25000
  %scevgep27 = getelementptr double, double* %b, i64 25000
  %scevgep30 = getelementptr double, double* %c, i64 25000
  %bound0 = icmp ugt double* %scevgep24, %a_h
  %bound1 = icmp ugt double* %scevgep, %a
  %found.conflict = and i1 %bound0, %bound1
  %bound032 = icmp ugt double* %scevgep27, %a_h
  %bound133 = icmp ugt double* %scevgep, %b
  %found.conflict34 = and i1 %bound133, %bound032
  %conflict.rdx = or i1 %found.conflict, %found.conflict34
  %bound035 = icmp ugt double* %scevgep30, %a_h
  %bound136 = icmp ugt double* %scevgep, %c
  %found.conflict37 = and i1 %bound136, %bound035
  %conflict.rdx38 = or i1 %conflict.rdx, %found.conflict37
  %bound039 = icmp ugt double* %scevgep27, %a
  %bound140 = icmp ugt double* %scevgep24, %b
  %found.conflict41 = and i1 %bound140, %bound039
  %conflict.rdx42 = or i1 %found.conflict41, %conflict.rdx38
  %bound043 = icmp ugt double* %scevgep30, %a
  %bound144 = icmp ugt double* %scevgep24, %c
  %found.conflict45 = and i1 %bound144, %bound043
  %conflict.rdx46 = or i1 %found.conflict45, %conflict.rdx42
  %bound047 = icmp ugt double* %scevgep30, %b
  %bound148 = icmp ugt double* %scevgep27, %c
  %found.conflict49 = and i1 %bound148, %bound047
  %conflict.rdx50 = or i1 %found.conflict49, %conflict.rdx46
  br i1 %conflict.rdx50, label %for.body.preheader, label %vector.body

for.body.preheader:                               ; preds = %vector.body, %entry
  %indvars.iv.ph = phi i64 [ 0, %entry ], [ 24984, %vector.body ]
  br label %for.body

vector.body:                                      ; preds = %entry, %vector.body
  %index = phi i64 [ %index.next, %vector.body ], [ 0, %entry ]
  %vec.ind62 = phi <2 x i32> [ %vec.ind.next75, %vector.body ], [ <i32 0, i32 1>, %entry ]
  %vec.ind76 = phi <2 x i32> [ %vec.ind.next89, %vector.body ], [ <i32 0, i32 1>, %entry ]
  %vec.ind90 = phi <2 x i32> [ %vec.ind.next103, %vector.body ], [ <i32 0, i32 1>, %entry ]
  %step.add63 = add <2 x i32> %vec.ind62, <i32 2, i32 2>
  %step.add64 = add <2 x i32> %vec.ind62, <i32 4, i32 4>
  %step.add65 = add <2 x i32> %vec.ind62, <i32 6, i32 6>
  %step.add66 = add <2 x i32> %vec.ind62, <i32 8, i32 8>
  %step.add67 = add <2 x i32> %vec.ind62, <i32 10, i32 10>
  %step.add68 = add <2 x i32> %vec.ind62, <i32 12, i32 12>
  %step.add69 = add <2 x i32> %vec.ind62, <i32 14, i32 14>
  %step.add70 = add <2 x i32> %vec.ind62, <i32 16, i32 16>
  %step.add71 = add <2 x i32> %vec.ind62, <i32 18, i32 18>
  %step.add72 = add <2 x i32> %vec.ind62, <i32 20, i32 20>
  %step.add73 = add <2 x i32> %vec.ind62, <i32 22, i32 22>
  %0 = sitofp <2 x i32> %vec.ind62 to <2 x double>
  %1 = sitofp <2 x i32> %step.add63 to <2 x double>
  %2 = sitofp <2 x i32> %step.add64 to <2 x double>
  %3 = sitofp <2 x i32> %step.add65 to <2 x double>
  %4 = sitofp <2 x i32> %step.add66 to <2 x double>
  %5 = sitofp <2 x i32> %step.add67 to <2 x double>
  %6 = sitofp <2 x i32> %step.add68 to <2 x double>
  %7 = sitofp <2 x i32> %step.add69 to <2 x double>
  %8 = sitofp <2 x i32> %step.add70 to <2 x double>
  %9 = sitofp <2 x i32> %step.add71 to <2 x double>
  %10 = sitofp <2 x i32> %step.add72 to <2 x double>
  %11 = sitofp <2 x i32> %step.add73 to <2 x double>
  %12 = getelementptr inbounds double, double* %a_h, i64 %index
  %13 = bitcast double* %12 to <2 x double>*
  store <2 x double> %0, <2 x double>* %13, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %14 = getelementptr inbounds double, double* %12, i64 2
  %15 = bitcast double* %14 to <2 x double>*
  store <2 x double> %1, <2 x double>* %15, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %16 = getelementptr inbounds double, double* %12, i64 4
  %17 = bitcast double* %16 to <2 x double>*
  store <2 x double> %2, <2 x double>* %17, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %18 = getelementptr inbounds double, double* %12, i64 6
  %19 = bitcast double* %18 to <2 x double>*
  store <2 x double> %3, <2 x double>* %19, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %20 = getelementptr inbounds double, double* %12, i64 8
  %21 = bitcast double* %20 to <2 x double>*
  store <2 x double> %4, <2 x double>* %21, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %22 = getelementptr inbounds double, double* %12, i64 10
  %23 = bitcast double* %22 to <2 x double>*
  store <2 x double> %5, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %24 = getelementptr inbounds double, double* %12, i64 12
  %25 = bitcast double* %24 to <2 x double>*
  store <2 x double> %6, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %26 = getelementptr inbounds double, double* %12, i64 14
  %27 = bitcast double* %26 to <2 x double>*
  store <2 x double> %7, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %28 = getelementptr inbounds double, double* %12, i64 16
  %29 = bitcast double* %28 to <2 x double>*
  store <2 x double> %8, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %30 = getelementptr inbounds double, double* %12, i64 18
  %31 = bitcast double* %30 to <2 x double>*
  store <2 x double> %9, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %32 = getelementptr inbounds double, double* %12, i64 20
  %33 = bitcast double* %32 to <2 x double>*
  store <2 x double> %10, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %34 = getelementptr inbounds double, double* %12, i64 22
  %35 = bitcast double* %34 to <2 x double>*
  store <2 x double> %11, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !64, !noalias !67
  %36 = getelementptr inbounds double, double* %a, i64 %index
  %37 = bitcast double* %36 to <2 x double>*
  store <2 x double> %0, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %38 = getelementptr inbounds double, double* %36, i64 2
  %39 = bitcast double* %38 to <2 x double>*
  store <2 x double> %1, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %40 = getelementptr inbounds double, double* %36, i64 4
  %41 = bitcast double* %40 to <2 x double>*
  store <2 x double> %2, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %42 = getelementptr inbounds double, double* %36, i64 6
  %43 = bitcast double* %42 to <2 x double>*
  store <2 x double> %3, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %44 = getelementptr inbounds double, double* %36, i64 8
  %45 = bitcast double* %44 to <2 x double>*
  store <2 x double> %4, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %46 = getelementptr inbounds double, double* %36, i64 10
  %47 = bitcast double* %46 to <2 x double>*
  store <2 x double> %5, <2 x double>* %47, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %48 = getelementptr inbounds double, double* %36, i64 12
  %49 = bitcast double* %48 to <2 x double>*
  store <2 x double> %6, <2 x double>* %49, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %50 = getelementptr inbounds double, double* %36, i64 14
  %51 = bitcast double* %50 to <2 x double>*
  store <2 x double> %7, <2 x double>* %51, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %52 = getelementptr inbounds double, double* %36, i64 16
  %53 = bitcast double* %52 to <2 x double>*
  store <2 x double> %8, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %54 = getelementptr inbounds double, double* %36, i64 18
  %55 = bitcast double* %54 to <2 x double>*
  store <2 x double> %9, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %56 = getelementptr inbounds double, double* %36, i64 20
  %57 = bitcast double* %56 to <2 x double>*
  store <2 x double> %10, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %58 = getelementptr inbounds double, double* %36, i64 22
  %59 = bitcast double* %58 to <2 x double>*
  store <2 x double> %11, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !71, !noalias !72
  %60 = shl <2 x i32> %vec.ind76, <i32 1, i32 1>
  %61 = add <2 x i32> %60, <i32 4, i32 4>
  %62 = add <2 x i32> %60, <i32 8, i32 8>
  %63 = add <2 x i32> %60, <i32 12, i32 12>
  %64 = add <2 x i32> %60, <i32 16, i32 16>
  %65 = add <2 x i32> %60, <i32 20, i32 20>
  %66 = add <2 x i32> %60, <i32 24, i32 24>
  %67 = add <2 x i32> %60, <i32 28, i32 28>
  %68 = add <2 x i32> %60, <i32 32, i32 32>
  %69 = add <2 x i32> %60, <i32 36, i32 36>
  %70 = add <2 x i32> %60, <i32 40, i32 40>
  %71 = add <2 x i32> %60, <i32 44, i32 44>
  %72 = sitofp <2 x i32> %60 to <2 x double>
  %73 = sitofp <2 x i32> %61 to <2 x double>
  %74 = sitofp <2 x i32> %62 to <2 x double>
  %75 = sitofp <2 x i32> %63 to <2 x double>
  %76 = sitofp <2 x i32> %64 to <2 x double>
  %77 = sitofp <2 x i32> %65 to <2 x double>
  %78 = sitofp <2 x i32> %66 to <2 x double>
  %79 = sitofp <2 x i32> %67 to <2 x double>
  %80 = sitofp <2 x i32> %68 to <2 x double>
  %81 = sitofp <2 x i32> %69 to <2 x double>
  %82 = sitofp <2 x i32> %70 to <2 x double>
  %83 = sitofp <2 x i32> %71 to <2 x double>
  %84 = getelementptr inbounds double, double* %b, i64 %index
  %85 = bitcast double* %84 to <2 x double>*
  store <2 x double> %72, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %86 = getelementptr inbounds double, double* %84, i64 2
  %87 = bitcast double* %86 to <2 x double>*
  store <2 x double> %73, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %88 = getelementptr inbounds double, double* %84, i64 4
  %89 = bitcast double* %88 to <2 x double>*
  store <2 x double> %74, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %90 = getelementptr inbounds double, double* %84, i64 6
  %91 = bitcast double* %90 to <2 x double>*
  store <2 x double> %75, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %92 = getelementptr inbounds double, double* %84, i64 8
  %93 = bitcast double* %92 to <2 x double>*
  store <2 x double> %76, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %94 = getelementptr inbounds double, double* %84, i64 10
  %95 = bitcast double* %94 to <2 x double>*
  store <2 x double> %77, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %96 = getelementptr inbounds double, double* %84, i64 12
  %97 = bitcast double* %96 to <2 x double>*
  store <2 x double> %78, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %98 = getelementptr inbounds double, double* %84, i64 14
  %99 = bitcast double* %98 to <2 x double>*
  store <2 x double> %79, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %100 = getelementptr inbounds double, double* %84, i64 16
  %101 = bitcast double* %100 to <2 x double>*
  store <2 x double> %80, <2 x double>* %101, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %102 = getelementptr inbounds double, double* %84, i64 18
  %103 = bitcast double* %102 to <2 x double>*
  store <2 x double> %81, <2 x double>* %103, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %104 = getelementptr inbounds double, double* %84, i64 20
  %105 = bitcast double* %104 to <2 x double>*
  store <2 x double> %82, <2 x double>* %105, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %106 = getelementptr inbounds double, double* %84, i64 22
  %107 = bitcast double* %106 to <2 x double>*
  store <2 x double> %83, <2 x double>* %107, align 8, !tbaa !62, !alias.scope !73, !noalias !74
  %108 = add <2 x i32> %vec.ind90, <i32 -3, i32 -3>
  %109 = add <2 x i32> %vec.ind90, <i32 -1, i32 -1>
  %110 = add <2 x i32> %vec.ind90, <i32 1, i32 1>
  %111 = add <2 x i32> %vec.ind90, <i32 3, i32 3>
  %112 = add <2 x i32> %vec.ind90, <i32 5, i32 5>
  %113 = add <2 x i32> %vec.ind90, <i32 7, i32 7>
  %114 = add <2 x i32> %vec.ind90, <i32 9, i32 9>
  %115 = add <2 x i32> %vec.ind90, <i32 11, i32 11>
  %116 = add <2 x i32> %vec.ind90, <i32 13, i32 13>
  %117 = add <2 x i32> %vec.ind90, <i32 15, i32 15>
  %118 = add <2 x i32> %vec.ind90, <i32 17, i32 17>
  %119 = add <2 x i32> %vec.ind90, <i32 19, i32 19>
  %120 = sitofp <2 x i32> %108 to <2 x double>
  %121 = sitofp <2 x i32> %109 to <2 x double>
  %122 = sitofp <2 x i32> %110 to <2 x double>
  %123 = sitofp <2 x i32> %111 to <2 x double>
  %124 = sitofp <2 x i32> %112 to <2 x double>
  %125 = sitofp <2 x i32> %113 to <2 x double>
  %126 = sitofp <2 x i32> %114 to <2 x double>
  %127 = sitofp <2 x i32> %115 to <2 x double>
  %128 = sitofp <2 x i32> %116 to <2 x double>
  %129 = sitofp <2 x i32> %117 to <2 x double>
  %130 = sitofp <2 x i32> %118 to <2 x double>
  %131 = sitofp <2 x i32> %119 to <2 x double>
  %132 = getelementptr inbounds double, double* %c, i64 %index
  %133 = bitcast double* %132 to <2 x double>*
  store <2 x double> %120, <2 x double>* %133, align 8, !tbaa !62, !alias.scope !74
  %134 = getelementptr inbounds double, double* %132, i64 2
  %135 = bitcast double* %134 to <2 x double>*
  store <2 x double> %121, <2 x double>* %135, align 8, !tbaa !62, !alias.scope !74
  %136 = getelementptr inbounds double, double* %132, i64 4
  %137 = bitcast double* %136 to <2 x double>*
  store <2 x double> %122, <2 x double>* %137, align 8, !tbaa !62, !alias.scope !74
  %138 = getelementptr inbounds double, double* %132, i64 6
  %139 = bitcast double* %138 to <2 x double>*
  store <2 x double> %123, <2 x double>* %139, align 8, !tbaa !62, !alias.scope !74
  %140 = getelementptr inbounds double, double* %132, i64 8
  %141 = bitcast double* %140 to <2 x double>*
  store <2 x double> %124, <2 x double>* %141, align 8, !tbaa !62, !alias.scope !74
  %142 = getelementptr inbounds double, double* %132, i64 10
  %143 = bitcast double* %142 to <2 x double>*
  store <2 x double> %125, <2 x double>* %143, align 8, !tbaa !62, !alias.scope !74
  %144 = getelementptr inbounds double, double* %132, i64 12
  %145 = bitcast double* %144 to <2 x double>*
  store <2 x double> %126, <2 x double>* %145, align 8, !tbaa !62, !alias.scope !74
  %146 = getelementptr inbounds double, double* %132, i64 14
  %147 = bitcast double* %146 to <2 x double>*
  store <2 x double> %127, <2 x double>* %147, align 8, !tbaa !62, !alias.scope !74
  %148 = getelementptr inbounds double, double* %132, i64 16
  %149 = bitcast double* %148 to <2 x double>*
  store <2 x double> %128, <2 x double>* %149, align 8, !tbaa !62, !alias.scope !74
  %150 = getelementptr inbounds double, double* %132, i64 18
  %151 = bitcast double* %150 to <2 x double>*
  store <2 x double> %129, <2 x double>* %151, align 8, !tbaa !62, !alias.scope !74
  %152 = getelementptr inbounds double, double* %132, i64 20
  %153 = bitcast double* %152 to <2 x double>*
  store <2 x double> %130, <2 x double>* %153, align 8, !tbaa !62, !alias.scope !74
  %154 = getelementptr inbounds double, double* %132, i64 22
  %155 = bitcast double* %154 to <2 x double>*
  store <2 x double> %131, <2 x double>* %155, align 8, !tbaa !62, !alias.scope !74
  %index.next = add nuw nsw i64 %index, 24
  %vec.ind.next75 = add <2 x i32> %vec.ind62, <i32 24, i32 24>
  %vec.ind.next89 = add <2 x i32> %vec.ind76, <i32 24, i32 24>
  %vec.ind.next103 = add <2 x i32> %vec.ind90, <i32 24, i32 24>
  %156 = icmp eq i64 %index.next, 24984
  br i1 %156, label %for.body.preheader, label %vector.body, !llvm.loop !75

for.cond.cleanup:                                 ; preds = %for.body
  ret void

for.body:                                         ; preds = %for.body.preheader, %for.body
  %indvars.iv = phi i64 [ %indvars.iv.next.3, %for.body ], [ %indvars.iv.ph, %for.body.preheader ]
  %157 = trunc i64 %indvars.iv to i32
  %conv = sitofp i32 %157 to double
  %arrayidx = getelementptr inbounds double, double* %a_h, i64 %indvars.iv
  store double %conv, double* %arrayidx, align 8, !tbaa !62
  %arrayidx2 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  store double %conv, double* %arrayidx2, align 8, !tbaa !62
  %158 = shl i32 %157, 1
  %conv3 = sitofp i32 %158 to double
  %arrayidx5 = getelementptr inbounds double, double* %b, i64 %indvars.iv
  store double %conv3, double* %arrayidx5, align 8, !tbaa !62
  %159 = add i32 %157, -3
  %conv6 = sitofp i32 %159 to double
  %arrayidx8 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  store double %conv6, double* %arrayidx8, align 8, !tbaa !62
  %indvars.iv.next = or i64 %indvars.iv, 1
  %160 = trunc i64 %indvars.iv.next to i32
  %conv.1 = sitofp i32 %160 to double
  %arrayidx.1 = getelementptr inbounds double, double* %a_h, i64 %indvars.iv.next
  store double %conv.1, double* %arrayidx.1, align 8, !tbaa !62
  %arrayidx2.1 = getelementptr inbounds double, double* %a, i64 %indvars.iv.next
  store double %conv.1, double* %arrayidx2.1, align 8, !tbaa !62
  %161 = shl i32 %160, 1
  %conv3.1 = sitofp i32 %161 to double
  %arrayidx5.1 = getelementptr inbounds double, double* %b, i64 %indvars.iv.next
  store double %conv3.1, double* %arrayidx5.1, align 8, !tbaa !62
  %162 = add i32 %160, -3
  %conv6.1 = sitofp i32 %162 to double
  %arrayidx8.1 = getelementptr inbounds double, double* %c, i64 %indvars.iv.next
  store double %conv6.1, double* %arrayidx8.1, align 8, !tbaa !62
  %indvars.iv.next.1 = or i64 %indvars.iv, 2
  %163 = trunc i64 %indvars.iv.next.1 to i32
  %conv.2 = sitofp i32 %163 to double
  %arrayidx.2 = getelementptr inbounds double, double* %a_h, i64 %indvars.iv.next.1
  store double %conv.2, double* %arrayidx.2, align 8, !tbaa !62
  %arrayidx2.2 = getelementptr inbounds double, double* %a, i64 %indvars.iv.next.1
  store double %conv.2, double* %arrayidx2.2, align 8, !tbaa !62
  %164 = shl i32 %163, 1
  %conv3.2 = sitofp i32 %164 to double
  %arrayidx5.2 = getelementptr inbounds double, double* %b, i64 %indvars.iv.next.1
  store double %conv3.2, double* %arrayidx5.2, align 8, !tbaa !62
  %165 = add i32 %163, -3
  %conv6.2 = sitofp i32 %165 to double
  %arrayidx8.2 = getelementptr inbounds double, double* %c, i64 %indvars.iv.next.1
  store double %conv6.2, double* %arrayidx8.2, align 8, !tbaa !62
  %indvars.iv.next.2 = or i64 %indvars.iv, 3
  %166 = trunc i64 %indvars.iv.next.2 to i32
  %conv.3 = sitofp i32 %166 to double
  %arrayidx.3 = getelementptr inbounds double, double* %a_h, i64 %indvars.iv.next.2
  store double %conv.3, double* %arrayidx.3, align 8, !tbaa !62
  %arrayidx2.3 = getelementptr inbounds double, double* %a, i64 %indvars.iv.next.2
  store double %conv.3, double* %arrayidx2.3, align 8, !tbaa !62
  %167 = shl i32 %166, 1
  %conv3.3 = sitofp i32 %167 to double
  %arrayidx5.3 = getelementptr inbounds double, double* %b, i64 %indvars.iv.next.2
  store double %conv3.3, double* %arrayidx5.3, align 8, !tbaa !62
  %168 = add nsw i32 %166, -3
  %conv6.3 = sitofp i32 %168 to double
  %arrayidx8.3 = getelementptr inbounds double, double* %c, i64 %indvars.iv.next.2
  store double %conv6.3, double* %arrayidx8.3, align 8, !tbaa !62
  %indvars.iv.next.3 = add nuw nsw i64 %indvars.iv, 4
  %exitcond.3 = icmp eq i64 %indvars.iv.next.3, 25000
  br i1 %exitcond.3, label %for.cond.cleanup, label %for.body, !llvm.loop !77
}

; Function Attrs: nounwind
define signext i32 @main() local_unnamed_addr #3 {
entry:
  %A = alloca [3072 x double], align 8
  %0 = bitcast [3072 x double]* %A to i8*
  %B = alloca [3072 x double], align 8
  %1 = bitcast [3072 x double]* %B to i8*
  %C = alloca [3072 x double], align 8
  %D = alloca [3072 x double], align 8
  %E = alloca [3072 x double], align 8
  %cpuExec = alloca i32, align 4
  %.offload_baseptrs = alloca [1 x i8*], align 8
  %.offload_ptrs = alloca [1 x i8*], align 8
  %.offload_baseptrs55 = alloca [8 x i8*], align 8
  %.offload_ptrs56 = alloca [8 x i8*], align 8
  %.offload_baseptrs144 = alloca [8 x i8*], align 8
  %.offload_ptrs145 = alloca [8 x i8*], align 8
  %.offload_baseptrs240 = alloca [8 x i8*], align 8
  %.offload_ptrs241 = alloca [8 x i8*], align 8
  %.offload_baseptrs336 = alloca [8 x i8*], align 8
  %.offload_ptrs337 = alloca [8 x i8*], align 8
  %.offload_baseptrs432 = alloca [8 x i8*], align 8
  %.offload_ptrs433 = alloca [8 x i8*], align 8
  %.offload_baseptrs528 = alloca [8 x i8*], align 8
  %.offload_ptrs529 = alloca [8 x i8*], align 8
  %.offload_baseptrs662 = alloca [8 x i8*], align 8
  %.offload_ptrs663 = alloca [8 x i8*], align 8
  %.offload_baseptrs758 = alloca [8 x i8*], align 8
  %.offload_ptrs759 = alloca [8 x i8*], align 8
  %.offload_baseptrs854 = alloca [8 x i8*], align 8
  %.offload_ptrs855 = alloca [8 x i8*], align 8
  %.offload_baseptrs950 = alloca [8 x i8*], align 8
  %.offload_ptrs951 = alloca [8 x i8*], align 8
  %.offload_baseptrs1046 = alloca [8 x i8*], align 8
  %.offload_ptrs1047 = alloca [8 x i8*], align 8
  %.offload_baseptrs1142 = alloca [8 x i8*], align 8
  %.offload_ptrs1143 = alloca [8 x i8*], align 8
  %.offload_baseptrs1277 = alloca [8 x i8*], align 8
  %.offload_ptrs1278 = alloca [8 x i8*], align 8
  %.offload_baseptrs1373 = alloca [8 x i8*], align 8
  %.offload_ptrs1374 = alloca [8 x i8*], align 8
  %.offload_baseptrs1469 = alloca [8 x i8*], align 8
  %.offload_ptrs1470 = alloca [8 x i8*], align 8
  %.offload_baseptrs1565 = alloca [8 x i8*], align 8
  %.offload_ptrs1566 = alloca [8 x i8*], align 8
  %.offload_baseptrs1661 = alloca [8 x i8*], align 8
  %.offload_ptrs1662 = alloca [8 x i8*], align 8
  %.offload_baseptrs1757 = alloca [8 x i8*], align 8
  %.offload_ptrs1758 = alloca [8 x i8*], align 8
  %.offload_baseptrs1893 = alloca [8 x i8*], align 8
  %.offload_ptrs1894 = alloca [8 x i8*], align 8
  %.offload_baseptrs1992 = alloca [8 x i8*], align 8
  %.offload_ptrs1993 = alloca [8 x i8*], align 8
  %.offload_baseptrs2091 = alloca [8 x i8*], align 8
  %.offload_ptrs2092 = alloca [8 x i8*], align 8
  %.offload_baseptrs2190 = alloca [8 x i8*], align 8
  %.offload_ptrs2191 = alloca [8 x i8*], align 8
  %.offload_baseptrs2289 = alloca [8 x i8*], align 8
  %.offload_ptrs2290 = alloca [8 x i8*], align 8
  %.offload_baseptrs2388 = alloca [8 x i8*], align 8
  %.offload_ptrs2389 = alloca [8 x i8*], align 8
  %.offload_baseptrs2528 = alloca [10 x i8*], align 8
  %.offload_ptrs2529 = alloca [10 x i8*], align 8
  %.offload_baseptrs2631 = alloca [10 x i8*], align 8
  %.offload_ptrs2632 = alloca [10 x i8*], align 8
  %.offload_baseptrs2734 = alloca [10 x i8*], align 8
  %.offload_ptrs2735 = alloca [10 x i8*], align 8
  %.offload_baseptrs2837 = alloca [10 x i8*], align 8
  %.offload_ptrs2838 = alloca [10 x i8*], align 8
  %.offload_baseptrs2940 = alloca [10 x i8*], align 8
  %.offload_ptrs2941 = alloca [10 x i8*], align 8
  %.offload_baseptrs3043 = alloca [10 x i8*], align 8
  %.offload_ptrs3044 = alloca [10 x i8*], align 8
  %.offload_baseptrs3179 = alloca [8 x i8*], align 8
  %.offload_ptrs3180 = alloca [8 x i8*], align 8
  %.offload_baseptrs3275 = alloca [8 x i8*], align 8
  %.offload_ptrs3276 = alloca [8 x i8*], align 8
  %.offload_baseptrs3371 = alloca [8 x i8*], align 8
  %.offload_ptrs3372 = alloca [8 x i8*], align 8
  %.offload_baseptrs3467 = alloca [8 x i8*], align 8
  %.offload_ptrs3468 = alloca [8 x i8*], align 8
  %.offload_baseptrs3563 = alloca [8 x i8*], align 8
  %.offload_ptrs3564 = alloca [8 x i8*], align 8
  %.offload_baseptrs3659 = alloca [8 x i8*], align 8
  %.offload_ptrs3660 = alloca [8 x i8*], align 8
  %.offload_baseptrs3740 = alloca [3 x i8*], align 8
  %.offload_ptrs3741 = alloca [3 x i8*], align 8
  %.offload_baseptrs3751 = alloca [3 x i8*], align 8
  %.offload_ptrs3752 = alloca [3 x i8*], align 8
  %.offload_sizes = alloca [3 x i64], align 8
  %.offload_baseptrs3773 = alloca [6 x i8*], align 8
  %.offload_ptrs3774 = alloca [6 x i8*], align 8
  %.offload_baseptrs3809 = alloca [1 x i8*], align 8
  %.offload_ptrs3810 = alloca [1 x i8*], align 8
  %.offload_sizes3811 = alloca [1 x i64], align 8
  %.offload_baseptrs3852 = alloca [3 x i8*], align 8
  %.offload_ptrs3853 = alloca [3 x i8*], align 8
  %.offload_sizes3854 = alloca [3 x i64], align 8
  %.offload_baseptrs3877 = alloca [6 x i8*], align 8
  %.offload_ptrs3878 = alloca [6 x i8*], align 8
  %.offload_baseptrs3918 = alloca [1 x i8*], align 8
  %.offload_ptrs3919 = alloca [1 x i8*], align 8
  %.offload_sizes3920 = alloca [1 x i64], align 8
  %.offload_baseptrs3965 = alloca [3 x i8*], align 8
  %.offload_ptrs3966 = alloca [3 x i8*], align 8
  %.offload_sizes3967 = alloca [3 x i64], align 8
  %.offload_baseptrs3997 = alloca [7 x i8*], align 8
  %.offload_ptrs3998 = alloca [7 x i8*], align 8
  %.offload_baseptrs4042 = alloca [1 x i8*], align 8
  %.offload_ptrs4043 = alloca [1 x i8*], align 8
  %.offload_sizes4044 = alloca [1 x i64], align 8
  %.offload_baseptrs4089 = alloca [3 x i8*], align 8
  %.offload_ptrs4090 = alloca [3 x i8*], align 8
  %.offload_sizes4091 = alloca [3 x i64], align 8
  %.offload_baseptrs4113 = alloca [6 x i8*], align 8
  %.offload_ptrs4114 = alloca [6 x i8*], align 8
  %.offload_baseptrs4154 = alloca [1 x i8*], align 8
  %.offload_ptrs4155 = alloca [1 x i8*], align 8
  %.offload_sizes4156 = alloca [1 x i64], align 8
  %.offload_baseptrs4201 = alloca [3 x i8*], align 8
  %.offload_ptrs4202 = alloca [3 x i8*], align 8
  %.offload_sizes4203 = alloca [3 x i64], align 8
  %.offload_baseptrs4234 = alloca [7 x i8*], align 8
  %.offload_ptrs4235 = alloca [7 x i8*], align 8
  %.offload_baseptrs4279 = alloca [1 x i8*], align 8
  %.offload_ptrs4280 = alloca [1 x i8*], align 8
  %.offload_sizes4281 = alloca [1 x i64], align 8
  %.offload_baseptrs4325 = alloca [3 x i8*], align 8
  %.offload_ptrs4326 = alloca [3 x i8*], align 8
  %.offload_sizes4327 = alloca [3 x i64], align 8
  %.offload_baseptrs4350 = alloca [6 x i8*], align 8
  %.offload_ptrs4351 = alloca [6 x i8*], align 8
  %.offload_baseptrs4391 = alloca [1 x i8*], align 8
  %.offload_ptrs4392 = alloca [1 x i8*], align 8
  %.offload_sizes4393 = alloca [1 x i64], align 8
  %.offload_baseptrs4438 = alloca [3 x i8*], align 8
  %.offload_ptrs4439 = alloca [3 x i8*], align 8
  %.offload_sizes4440 = alloca [3 x i64], align 8
  %.offload_baseptrs4471 = alloca [7 x i8*], align 8
  %.offload_ptrs4472 = alloca [7 x i8*], align 8
  %.offload_baseptrs4516 = alloca [1 x i8*], align 8
  %.offload_ptrs4517 = alloca [1 x i8*], align 8
  %.offload_sizes4518 = alloca [1 x i64], align 8
  %.offload_baseptrs4562 = alloca [3 x i8*], align 8
  %.offload_ptrs4563 = alloca [3 x i8*], align 8
  %.offload_sizes4564 = alloca [3 x i64], align 8
  %.offload_baseptrs4587 = alloca [6 x i8*], align 8
  %.offload_ptrs4588 = alloca [6 x i8*], align 8
  %.offload_baseptrs4628 = alloca [1 x i8*], align 8
  %.offload_ptrs4629 = alloca [1 x i8*], align 8
  %.offload_sizes4630 = alloca [1 x i64], align 8
  %.offload_baseptrs4675 = alloca [3 x i8*], align 8
  %.offload_ptrs4676 = alloca [3 x i8*], align 8
  %.offload_sizes4677 = alloca [3 x i64], align 8
  %.offload_baseptrs4708 = alloca [7 x i8*], align 8
  %.offload_ptrs4709 = alloca [7 x i8*], align 8
  %.offload_baseptrs4753 = alloca [1 x i8*], align 8
  %.offload_ptrs4754 = alloca [1 x i8*], align 8
  %.offload_sizes4755 = alloca [1 x i64], align 8
  %.offload_baseptrs4800 = alloca [3 x i8*], align 8
  %.offload_ptrs4801 = alloca [3 x i8*], align 8
  %.offload_sizes4802 = alloca [3 x i64], align 8
  %.offload_baseptrs4833 = alloca [7 x i8*], align 8
  %.offload_ptrs4834 = alloca [7 x i8*], align 8
  %.offload_baseptrs4878 = alloca [1 x i8*], align 8
  %.offload_ptrs4879 = alloca [1 x i8*], align 8
  %.offload_sizes4880 = alloca [1 x i64], align 8
  %.offload_baseptrs4925 = alloca [3 x i8*], align 8
  %.offload_ptrs4926 = alloca [3 x i8*], align 8
  %.offload_sizes4927 = alloca [3 x i64], align 8
  %.offload_baseptrs4966 = alloca [8 x i8*], align 8
  %.offload_ptrs4967 = alloca [8 x i8*], align 8
  %.offload_baseptrs5015 = alloca [1 x i8*], align 8
  %.offload_ptrs5016 = alloca [1 x i8*], align 8
  %.offload_sizes5017 = alloca [1 x i64], align 8
  %.offload_baseptrs5062 = alloca [3 x i8*], align 8
  %.offload_ptrs5063 = alloca [3 x i8*], align 8
  %.offload_sizes5064 = alloca [3 x i64], align 8
  %.offload_baseptrs5095 = alloca [7 x i8*], align 8
  %.offload_ptrs5096 = alloca [7 x i8*], align 8
  %.offload_baseptrs5140 = alloca [1 x i8*], align 8
  %.offload_ptrs5141 = alloca [1 x i8*], align 8
  %.offload_sizes5142 = alloca [1 x i64], align 8
  %.offload_baseptrs5187 = alloca [3 x i8*], align 8
  %.offload_ptrs5188 = alloca [3 x i8*], align 8
  %.offload_sizes5189 = alloca [3 x i64], align 8
  %.offload_baseptrs5229 = alloca [8 x i8*], align 8
  %.offload_ptrs5230 = alloca [8 x i8*], align 8
  %.offload_baseptrs5278 = alloca [1 x i8*], align 8
  %.offload_ptrs5279 = alloca [1 x i8*], align 8
  %.offload_sizes5280 = alloca [1 x i64], align 8
  %.offload_baseptrs5325 = alloca [3 x i8*], align 8
  %.offload_ptrs5326 = alloca [3 x i8*], align 8
  %.offload_sizes5327 = alloca [3 x i64], align 8
  %.offload_baseptrs5358 = alloca [7 x i8*], align 8
  %.offload_ptrs5359 = alloca [7 x i8*], align 8
  %.offload_baseptrs5403 = alloca [1 x i8*], align 8
  %.offload_ptrs5404 = alloca [1 x i8*], align 8
  %.offload_sizes5405 = alloca [1 x i64], align 8
  %.offload_baseptrs5450 = alloca [3 x i8*], align 8
  %.offload_ptrs5451 = alloca [3 x i8*], align 8
  %.offload_sizes5452 = alloca [3 x i64], align 8
  %.offload_baseptrs5492 = alloca [8 x i8*], align 8
  %.offload_ptrs5493 = alloca [8 x i8*], align 8
  %.offload_baseptrs5541 = alloca [1 x i8*], align 8
  %.offload_ptrs5542 = alloca [1 x i8*], align 8
  %.offload_sizes5543 = alloca [1 x i64], align 8
  %.offload_baseptrs5588 = alloca [3 x i8*], align 8
  %.offload_ptrs5589 = alloca [3 x i8*], align 8
  %.offload_sizes5590 = alloca [3 x i64], align 8
  %.offload_baseptrs5621 = alloca [7 x i8*], align 8
  %.offload_ptrs5622 = alloca [7 x i8*], align 8
  %.offload_baseptrs5666 = alloca [1 x i8*], align 8
  %.offload_ptrs5667 = alloca [1 x i8*], align 8
  %.offload_sizes5668 = alloca [1 x i64], align 8
  %.offload_baseptrs5713 = alloca [3 x i8*], align 8
  %.offload_ptrs5714 = alloca [3 x i8*], align 8
  %.offload_sizes5715 = alloca [3 x i64], align 8
  %.offload_baseptrs5746 = alloca [7 x i8*], align 8
  %.offload_ptrs5747 = alloca [7 x i8*], align 8
  %.offload_baseptrs5791 = alloca [1 x i8*], align 8
  %.offload_ptrs5792 = alloca [1 x i8*], align 8
  %.offload_sizes5793 = alloca [1 x i64], align 8
  %.offload_baseptrs5830 = alloca [3 x i8*], align 8
  %.offload_ptrs5831 = alloca [3 x i8*], align 8
  %call = tail call signext i32 @check_offloading()
  call void @llvm.lifetime.start.p0i8(i64 24576, i8* nonnull %0) #6
  call void @llvm.lifetime.start.p0i8(i64 24576, i8* nonnull %1) #6
  %2 = bitcast [3072 x double]* %C to i8*
  call void @llvm.lifetime.start.p0i8(i64 24576, i8* nonnull %2) #6
  %3 = bitcast [3072 x double]* %D to i8*
  call void @llvm.lifetime.start.p0i8(i64 24576, i8* nonnull %3) #6
  %4 = bitcast [3072 x double]* %E to i8*
  call void @llvm.lifetime.start.p0i8(i64 24576, i8* nonnull %4) #6
  %5 = bitcast i32* %cpuExec to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %5) #6
  store i32 0, i32* %cpuExec, align 4, !tbaa !58
  %6 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs, i64 0, i64 0
  %7 = bitcast [1 x i8*]* %.offload_baseptrs to i32**
  store i32* %cpuExec, i32** %7, align 8
  %8 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs, i64 0, i64 0
  %9 = bitcast [1 x i8*]* %.offload_ptrs to i32**
  store i32* %cpuExec, i32** %9, align 8
  %10 = call i32 @__tgt_target(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l45.region_id, i32 1, i8** nonnull %6, i8** nonnull %8, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_sizes.4, i64 0, i64 0), i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.5, i64 0, i64 0)) #6
  %11 = icmp eq i32 %10, 0
  br i1 %11, label %entry.omp_offload.cont_crit_edge, label %omp_offload.failed

entry.omp_offload.cont_crit_edge:                 ; preds = %entry
  %.pre = load i32, i32* %cpuExec, align 4, !tbaa !58
  br label %omp_offload.cont

omp_offload.failed:                               ; preds = %entry
  store i32 1, i32* %cpuExec, align 4, !tbaa !58
  br label %omp_offload.cont

omp_offload.cont:                                 ; preds = %entry.omp_offload.cont_crit_edge, %omp_offload.failed
  %12 = phi i32 [ %.pre, %entry.omp_offload.cont_crit_edge ], [ 1, %omp_offload.failed ]
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %omp_offload.cont
  %index = phi i64 [ 0, %omp_offload.cont ], [ %index.next, %vector.body ]
  %vec.ind10047 = phi <2 x i32> [ <i32 0, i32 1>, %omp_offload.cont ], [ %vec.ind.next10060, %vector.body ]
  %vec.ind10061 = phi <2 x i32> [ <i32 0, i32 1>, %omp_offload.cont ], [ %vec.ind.next10074, %vector.body ]
  %13 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %index
  %14 = bitcast double* %13 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %14, align 8, !tbaa !62
  %15 = getelementptr inbounds double, double* %13, i64 2
  %16 = bitcast double* %15 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %16, align 8, !tbaa !62
  %17 = getelementptr inbounds double, double* %13, i64 4
  %18 = bitcast double* %17 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %18, align 8, !tbaa !62
  %19 = getelementptr inbounds double, double* %13, i64 6
  %20 = bitcast double* %19 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %20, align 8, !tbaa !62
  %21 = getelementptr inbounds double, double* %13, i64 8
  %22 = bitcast double* %21 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %22, align 8, !tbaa !62
  %23 = getelementptr inbounds double, double* %13, i64 10
  %24 = bitcast double* %23 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %24, align 8, !tbaa !62
  %25 = getelementptr inbounds double, double* %13, i64 12
  %26 = bitcast double* %25 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %26, align 8, !tbaa !62
  %27 = getelementptr inbounds double, double* %13, i64 14
  %28 = bitcast double* %27 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %28, align 8, !tbaa !62
  %29 = getelementptr inbounds double, double* %13, i64 16
  %30 = bitcast double* %29 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %30, align 8, !tbaa !62
  %31 = getelementptr inbounds double, double* %13, i64 18
  %32 = bitcast double* %31 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %32, align 8, !tbaa !62
  %33 = getelementptr inbounds double, double* %13, i64 20
  %34 = bitcast double* %33 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %34, align 8, !tbaa !62
  %35 = getelementptr inbounds double, double* %13, i64 22
  %36 = bitcast double* %35 to <2 x double>*
  store <2 x double> <double 1.000000e+00, double 1.000000e+00>, <2 x double>* %36, align 8, !tbaa !62
  %step.add10048 = add <2 x i32> %vec.ind10047, <i32 2, i32 2>
  %step.add10049 = add <2 x i32> %vec.ind10047, <i32 4, i32 4>
  %step.add10050 = add <2 x i32> %vec.ind10047, <i32 6, i32 6>
  %step.add10051 = add <2 x i32> %vec.ind10047, <i32 8, i32 8>
  %step.add10052 = add <2 x i32> %vec.ind10047, <i32 10, i32 10>
  %step.add10053 = add <2 x i32> %vec.ind10047, <i32 12, i32 12>
  %step.add10054 = add <2 x i32> %vec.ind10047, <i32 14, i32 14>
  %step.add10055 = add <2 x i32> %vec.ind10047, <i32 16, i32 16>
  %step.add10056 = add <2 x i32> %vec.ind10047, <i32 18, i32 18>
  %step.add10057 = add <2 x i32> %vec.ind10047, <i32 20, i32 20>
  %step.add10058 = add <2 x i32> %vec.ind10047, <i32 22, i32 22>
  %37 = sitofp <2 x i32> %vec.ind10047 to <2 x double>
  %38 = sitofp <2 x i32> %step.add10048 to <2 x double>
  %39 = sitofp <2 x i32> %step.add10049 to <2 x double>
  %40 = sitofp <2 x i32> %step.add10050 to <2 x double>
  %41 = sitofp <2 x i32> %step.add10051 to <2 x double>
  %42 = sitofp <2 x i32> %step.add10052 to <2 x double>
  %43 = sitofp <2 x i32> %step.add10053 to <2 x double>
  %44 = sitofp <2 x i32> %step.add10054 to <2 x double>
  %45 = sitofp <2 x i32> %step.add10055 to <2 x double>
  %46 = sitofp <2 x i32> %step.add10056 to <2 x double>
  %47 = sitofp <2 x i32> %step.add10057 to <2 x double>
  %48 = sitofp <2 x i32> %step.add10058 to <2 x double>
  %49 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %index
  %50 = bitcast double* %49 to <2 x double>*
  store <2 x double> %37, <2 x double>* %50, align 8, !tbaa !62
  %51 = getelementptr inbounds double, double* %49, i64 2
  %52 = bitcast double* %51 to <2 x double>*
  store <2 x double> %38, <2 x double>* %52, align 8, !tbaa !62
  %53 = getelementptr inbounds double, double* %49, i64 4
  %54 = bitcast double* %53 to <2 x double>*
  store <2 x double> %39, <2 x double>* %54, align 8, !tbaa !62
  %55 = getelementptr inbounds double, double* %49, i64 6
  %56 = bitcast double* %55 to <2 x double>*
  store <2 x double> %40, <2 x double>* %56, align 8, !tbaa !62
  %57 = getelementptr inbounds double, double* %49, i64 8
  %58 = bitcast double* %57 to <2 x double>*
  store <2 x double> %41, <2 x double>* %58, align 8, !tbaa !62
  %59 = getelementptr inbounds double, double* %49, i64 10
  %60 = bitcast double* %59 to <2 x double>*
  store <2 x double> %42, <2 x double>* %60, align 8, !tbaa !62
  %61 = getelementptr inbounds double, double* %49, i64 12
  %62 = bitcast double* %61 to <2 x double>*
  store <2 x double> %43, <2 x double>* %62, align 8, !tbaa !62
  %63 = getelementptr inbounds double, double* %49, i64 14
  %64 = bitcast double* %63 to <2 x double>*
  store <2 x double> %44, <2 x double>* %64, align 8, !tbaa !62
  %65 = getelementptr inbounds double, double* %49, i64 16
  %66 = bitcast double* %65 to <2 x double>*
  store <2 x double> %45, <2 x double>* %66, align 8, !tbaa !62
  %67 = getelementptr inbounds double, double* %49, i64 18
  %68 = bitcast double* %67 to <2 x double>*
  store <2 x double> %46, <2 x double>* %68, align 8, !tbaa !62
  %69 = getelementptr inbounds double, double* %49, i64 20
  %70 = bitcast double* %69 to <2 x double>*
  store <2 x double> %47, <2 x double>* %70, align 8, !tbaa !62
  %71 = getelementptr inbounds double, double* %49, i64 22
  %72 = bitcast double* %71 to <2 x double>*
  store <2 x double> %48, <2 x double>* %72, align 8, !tbaa !62
  %73 = sub <2 x i32> <i32 1, i32 1>, %vec.ind10061
  %74 = xor <2 x i32> %vec.ind10061, <i32 -1, i32 -1>
  %75 = sub <2 x i32> <i32 -3, i32 -3>, %vec.ind10061
  %76 = sub <2 x i32> <i32 -5, i32 -5>, %vec.ind10061
  %77 = sub <2 x i32> <i32 -7, i32 -7>, %vec.ind10061
  %78 = sub <2 x i32> <i32 -9, i32 -9>, %vec.ind10061
  %79 = sub <2 x i32> <i32 -11, i32 -11>, %vec.ind10061
  %80 = sub <2 x i32> <i32 -13, i32 -13>, %vec.ind10061
  %81 = sub <2 x i32> <i32 -15, i32 -15>, %vec.ind10061
  %82 = sub <2 x i32> <i32 -17, i32 -17>, %vec.ind10061
  %83 = sub <2 x i32> <i32 -19, i32 -19>, %vec.ind10061
  %84 = sub <2 x i32> <i32 -21, i32 -21>, %vec.ind10061
  %85 = sitofp <2 x i32> %73 to <2 x double>
  %86 = sitofp <2 x i32> %74 to <2 x double>
  %87 = sitofp <2 x i32> %75 to <2 x double>
  %88 = sitofp <2 x i32> %76 to <2 x double>
  %89 = sitofp <2 x i32> %77 to <2 x double>
  %90 = sitofp <2 x i32> %78 to <2 x double>
  %91 = sitofp <2 x i32> %79 to <2 x double>
  %92 = sitofp <2 x i32> %80 to <2 x double>
  %93 = sitofp <2 x i32> %81 to <2 x double>
  %94 = sitofp <2 x i32> %82 to <2 x double>
  %95 = sitofp <2 x i32> %83 to <2 x double>
  %96 = sitofp <2 x i32> %84 to <2 x double>
  %97 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %index
  %98 = bitcast double* %97 to <2 x double>*
  store <2 x double> %85, <2 x double>* %98, align 8, !tbaa !62
  %99 = getelementptr inbounds double, double* %97, i64 2
  %100 = bitcast double* %99 to <2 x double>*
  store <2 x double> %86, <2 x double>* %100, align 8, !tbaa !62
  %101 = getelementptr inbounds double, double* %97, i64 4
  %102 = bitcast double* %101 to <2 x double>*
  store <2 x double> %87, <2 x double>* %102, align 8, !tbaa !62
  %103 = getelementptr inbounds double, double* %97, i64 6
  %104 = bitcast double* %103 to <2 x double>*
  store <2 x double> %88, <2 x double>* %104, align 8, !tbaa !62
  %105 = getelementptr inbounds double, double* %97, i64 8
  %106 = bitcast double* %105 to <2 x double>*
  store <2 x double> %89, <2 x double>* %106, align 8, !tbaa !62
  %107 = getelementptr inbounds double, double* %97, i64 10
  %108 = bitcast double* %107 to <2 x double>*
  store <2 x double> %90, <2 x double>* %108, align 8, !tbaa !62
  %109 = getelementptr inbounds double, double* %97, i64 12
  %110 = bitcast double* %109 to <2 x double>*
  store <2 x double> %91, <2 x double>* %110, align 8, !tbaa !62
  %111 = getelementptr inbounds double, double* %97, i64 14
  %112 = bitcast double* %111 to <2 x double>*
  store <2 x double> %92, <2 x double>* %112, align 8, !tbaa !62
  %113 = getelementptr inbounds double, double* %97, i64 16
  %114 = bitcast double* %113 to <2 x double>*
  store <2 x double> %93, <2 x double>* %114, align 8, !tbaa !62
  %115 = getelementptr inbounds double, double* %97, i64 18
  %116 = bitcast double* %115 to <2 x double>*
  store <2 x double> %94, <2 x double>* %116, align 8, !tbaa !62
  %117 = getelementptr inbounds double, double* %97, i64 20
  %118 = bitcast double* %117 to <2 x double>*
  store <2 x double> %95, <2 x double>* %118, align 8, !tbaa !62
  %119 = getelementptr inbounds double, double* %97, i64 22
  %120 = bitcast double* %119 to <2 x double>*
  store <2 x double> %96, <2 x double>* %120, align 8, !tbaa !62
  %index.next = add nuw nsw i64 %index, 24
  %vec.ind.next10060 = add <2 x i32> %vec.ind10047, <i32 24, i32 24>
  %vec.ind.next10074 = add <2 x i32> %vec.ind10061, <i32 24, i32 24>
  %121 = icmp eq i64 %index.next, 3072
  br i1 %121, label %for.cond6.preheader, label %vector.body, !llvm.loop !78

for.cond6.preheader:                              ; preds = %vector.body
  %tobool = icmp eq i32 %12, 0
  %cond = select i1 %tobool, i64 256, i64 32
  %122 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs55, i64 0, i64 0
  %123 = bitcast [8 x i8*]* %.offload_baseptrs55 to [3072 x double]**
  %124 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs56, i64 0, i64 0
  %125 = bitcast [8 x i8*]* %.offload_ptrs56 to [3072 x double]**
  %126 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs55, i64 0, i64 1
  %127 = bitcast i8** %126 to [3072 x double]**
  %128 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs56, i64 0, i64 1
  %129 = bitcast i8** %128 to [3072 x double]**
  %130 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs55, i64 0, i64 2
  %131 = bitcast i8** %130 to [3072 x double]**
  %132 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs56, i64 0, i64 2
  %133 = bitcast i8** %132 to [3072 x double]**
  %134 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs55, i64 0, i64 3
  %135 = bitcast i8** %134 to [3072 x double]**
  %136 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs56, i64 0, i64 3
  %137 = bitcast i8** %136 to [3072 x double]**
  %138 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs55, i64 0, i64 4
  %139 = bitcast i8** %138 to [3072 x double]**
  %140 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs56, i64 0, i64 4
  %141 = bitcast i8** %140 to [3072 x double]**
  %142 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs55, i64 0, i64 5
  %143 = bitcast i8** %142 to i64*
  %144 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs56, i64 0, i64 5
  %145 = bitcast i8** %144 to i64*
  %146 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs55, i64 0, i64 6
  %147 = bitcast i8** %146 to i64*
  %148 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs56, i64 0, i64 6
  %149 = bitcast i8** %148 to i64*
  %150 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs55, i64 0, i64 7
  %151 = bitcast i8** %150 to i64*
  %152 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs56, i64 0, i64 7
  %153 = bitcast i8** %152 to i64*
  %154 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs144, i64 0, i64 0
  %155 = bitcast [8 x i8*]* %.offload_baseptrs144 to [3072 x double]**
  %156 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs145, i64 0, i64 0
  %157 = bitcast [8 x i8*]* %.offload_ptrs145 to [3072 x double]**
  %158 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs144, i64 0, i64 1
  %159 = bitcast i8** %158 to [3072 x double]**
  %160 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs145, i64 0, i64 1
  %161 = bitcast i8** %160 to [3072 x double]**
  %162 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs144, i64 0, i64 2
  %163 = bitcast i8** %162 to [3072 x double]**
  %164 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs145, i64 0, i64 2
  %165 = bitcast i8** %164 to [3072 x double]**
  %166 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs144, i64 0, i64 3
  %167 = bitcast i8** %166 to [3072 x double]**
  %168 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs145, i64 0, i64 3
  %169 = bitcast i8** %168 to [3072 x double]**
  %170 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs144, i64 0, i64 4
  %171 = bitcast i8** %170 to [3072 x double]**
  %172 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs145, i64 0, i64 4
  %173 = bitcast i8** %172 to [3072 x double]**
  %174 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs144, i64 0, i64 5
  %175 = bitcast i8** %174 to i64*
  %176 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs145, i64 0, i64 5
  %177 = bitcast i8** %176 to i64*
  %178 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs144, i64 0, i64 6
  %179 = bitcast i8** %178 to i64*
  %180 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs145, i64 0, i64 6
  %181 = bitcast i8** %180 to i64*
  %182 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs144, i64 0, i64 7
  %183 = bitcast i8** %182 to i64*
  %184 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs145, i64 0, i64 7
  %185 = bitcast i8** %184 to i64*
  %186 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs240, i64 0, i64 0
  %187 = bitcast [8 x i8*]* %.offload_baseptrs240 to [3072 x double]**
  %188 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs241, i64 0, i64 0
  %189 = bitcast [8 x i8*]* %.offload_ptrs241 to [3072 x double]**
  %190 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs240, i64 0, i64 1
  %191 = bitcast i8** %190 to [3072 x double]**
  %192 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs241, i64 0, i64 1
  %193 = bitcast i8** %192 to [3072 x double]**
  %194 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs240, i64 0, i64 2
  %195 = bitcast i8** %194 to [3072 x double]**
  %196 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs241, i64 0, i64 2
  %197 = bitcast i8** %196 to [3072 x double]**
  %198 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs240, i64 0, i64 3
  %199 = bitcast i8** %198 to [3072 x double]**
  %200 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs241, i64 0, i64 3
  %201 = bitcast i8** %200 to [3072 x double]**
  %202 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs240, i64 0, i64 4
  %203 = bitcast i8** %202 to [3072 x double]**
  %204 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs241, i64 0, i64 4
  %205 = bitcast i8** %204 to [3072 x double]**
  %206 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs240, i64 0, i64 5
  %207 = bitcast i8** %206 to i64*
  %208 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs241, i64 0, i64 5
  %209 = bitcast i8** %208 to i64*
  %210 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs240, i64 0, i64 6
  %211 = bitcast i8** %210 to i64*
  %212 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs241, i64 0, i64 6
  %213 = bitcast i8** %212 to i64*
  %214 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs240, i64 0, i64 7
  %215 = bitcast i8** %214 to i64*
  %216 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs241, i64 0, i64 7
  %217 = bitcast i8** %216 to i64*
  %218 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs336, i64 0, i64 0
  %219 = bitcast [8 x i8*]* %.offload_baseptrs336 to [3072 x double]**
  %220 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs337, i64 0, i64 0
  %221 = bitcast [8 x i8*]* %.offload_ptrs337 to [3072 x double]**
  %222 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs336, i64 0, i64 1
  %223 = bitcast i8** %222 to [3072 x double]**
  %224 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs337, i64 0, i64 1
  %225 = bitcast i8** %224 to [3072 x double]**
  %226 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs336, i64 0, i64 2
  %227 = bitcast i8** %226 to [3072 x double]**
  %228 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs337, i64 0, i64 2
  %229 = bitcast i8** %228 to [3072 x double]**
  %230 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs336, i64 0, i64 3
  %231 = bitcast i8** %230 to [3072 x double]**
  %232 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs337, i64 0, i64 3
  %233 = bitcast i8** %232 to [3072 x double]**
  %234 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs336, i64 0, i64 4
  %235 = bitcast i8** %234 to [3072 x double]**
  %236 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs337, i64 0, i64 4
  %237 = bitcast i8** %236 to [3072 x double]**
  %238 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs336, i64 0, i64 5
  %239 = bitcast i8** %238 to i64*
  %240 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs337, i64 0, i64 5
  %241 = bitcast i8** %240 to i64*
  %242 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs336, i64 0, i64 6
  %243 = bitcast i8** %242 to i64*
  %244 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs337, i64 0, i64 6
  %245 = bitcast i8** %244 to i64*
  %246 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs336, i64 0, i64 7
  %247 = bitcast i8** %246 to i64*
  %248 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs337, i64 0, i64 7
  %249 = bitcast i8** %248 to i64*
  %250 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs432, i64 0, i64 0
  %251 = bitcast [8 x i8*]* %.offload_baseptrs432 to [3072 x double]**
  %252 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs433, i64 0, i64 0
  %253 = bitcast [8 x i8*]* %.offload_ptrs433 to [3072 x double]**
  %254 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs432, i64 0, i64 1
  %255 = bitcast i8** %254 to [3072 x double]**
  %256 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs433, i64 0, i64 1
  %257 = bitcast i8** %256 to [3072 x double]**
  %258 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs432, i64 0, i64 2
  %259 = bitcast i8** %258 to [3072 x double]**
  %260 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs433, i64 0, i64 2
  %261 = bitcast i8** %260 to [3072 x double]**
  %262 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs432, i64 0, i64 3
  %263 = bitcast i8** %262 to [3072 x double]**
  %264 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs433, i64 0, i64 3
  %265 = bitcast i8** %264 to [3072 x double]**
  %266 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs432, i64 0, i64 4
  %267 = bitcast i8** %266 to [3072 x double]**
  %268 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs433, i64 0, i64 4
  %269 = bitcast i8** %268 to [3072 x double]**
  %270 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs432, i64 0, i64 5
  %271 = bitcast i8** %270 to i64*
  %272 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs433, i64 0, i64 5
  %273 = bitcast i8** %272 to i64*
  %274 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs432, i64 0, i64 6
  %275 = bitcast i8** %274 to i64*
  %276 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs433, i64 0, i64 6
  %277 = bitcast i8** %276 to i64*
  %278 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs432, i64 0, i64 7
  %279 = bitcast i8** %278 to i64*
  %280 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs433, i64 0, i64 7
  %281 = bitcast i8** %280 to i64*
  %282 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs528, i64 0, i64 0
  %283 = bitcast [8 x i8*]* %.offload_baseptrs528 to [3072 x double]**
  %284 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs529, i64 0, i64 0
  %285 = bitcast [8 x i8*]* %.offload_ptrs529 to [3072 x double]**
  %286 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs528, i64 0, i64 1
  %287 = bitcast i8** %286 to [3072 x double]**
  %288 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs529, i64 0, i64 1
  %289 = bitcast i8** %288 to [3072 x double]**
  %290 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs528, i64 0, i64 2
  %291 = bitcast i8** %290 to [3072 x double]**
  %292 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs529, i64 0, i64 2
  %293 = bitcast i8** %292 to [3072 x double]**
  %294 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs528, i64 0, i64 3
  %295 = bitcast i8** %294 to [3072 x double]**
  %296 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs529, i64 0, i64 3
  %297 = bitcast i8** %296 to [3072 x double]**
  %298 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs528, i64 0, i64 4
  %299 = bitcast i8** %298 to [3072 x double]**
  %300 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs529, i64 0, i64 4
  %301 = bitcast i8** %300 to [3072 x double]**
  %302 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs528, i64 0, i64 5
  %303 = bitcast i8** %302 to i64*
  %304 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs529, i64 0, i64 5
  %305 = bitcast i8** %304 to i64*
  %306 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs528, i64 0, i64 6
  %307 = bitcast i8** %306 to i64*
  %308 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs529, i64 0, i64 6
  %309 = bitcast i8** %308 to i64*
  %310 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs528, i64 0, i64 7
  %311 = bitcast i8** %310 to i64*
  %312 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs529, i64 0, i64 7
  %313 = bitcast i8** %312 to i64*
  br label %for.cond10.preheader

for.cond10.preheader:                             ; preds = %for.cond6.preheader, %for.inc599
  %tms.09335 = phi i32 [ 1, %for.cond6.preheader ], [ %mul, %for.inc599 ]
  %314 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool5869331 = icmp eq i32 %314, 0
  br label %for.body14

for.cond602.preheader:                            ; preds = %for.inc599
  %315 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs662, i64 0, i64 0
  %316 = bitcast [8 x i8*]* %.offload_baseptrs662 to [3072 x double]**
  %317 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs663, i64 0, i64 0
  %318 = bitcast [8 x i8*]* %.offload_ptrs663 to [3072 x double]**
  %319 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs662, i64 0, i64 1
  %320 = bitcast i8** %319 to [3072 x double]**
  %321 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs663, i64 0, i64 1
  %322 = bitcast i8** %321 to [3072 x double]**
  %323 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs662, i64 0, i64 2
  %324 = bitcast i8** %323 to [3072 x double]**
  %325 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs663, i64 0, i64 2
  %326 = bitcast i8** %325 to [3072 x double]**
  %327 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs662, i64 0, i64 3
  %328 = bitcast i8** %327 to [3072 x double]**
  %329 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs663, i64 0, i64 3
  %330 = bitcast i8** %329 to [3072 x double]**
  %331 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs662, i64 0, i64 4
  %332 = bitcast i8** %331 to [3072 x double]**
  %333 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs663, i64 0, i64 4
  %334 = bitcast i8** %333 to [3072 x double]**
  %335 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs662, i64 0, i64 5
  %336 = bitcast i8** %335 to i64*
  %337 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs663, i64 0, i64 5
  %338 = bitcast i8** %337 to i64*
  %339 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs662, i64 0, i64 6
  %340 = bitcast i8** %339 to i64*
  %341 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs663, i64 0, i64 6
  %342 = bitcast i8** %341 to i64*
  %343 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs662, i64 0, i64 7
  %344 = bitcast i8** %343 to i64*
  %345 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs663, i64 0, i64 7
  %346 = bitcast i8** %345 to i64*
  %347 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs758, i64 0, i64 0
  %348 = bitcast [8 x i8*]* %.offload_baseptrs758 to [3072 x double]**
  %349 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs759, i64 0, i64 0
  %350 = bitcast [8 x i8*]* %.offload_ptrs759 to [3072 x double]**
  %351 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs758, i64 0, i64 1
  %352 = bitcast i8** %351 to [3072 x double]**
  %353 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs759, i64 0, i64 1
  %354 = bitcast i8** %353 to [3072 x double]**
  %355 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs758, i64 0, i64 2
  %356 = bitcast i8** %355 to [3072 x double]**
  %357 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs759, i64 0, i64 2
  %358 = bitcast i8** %357 to [3072 x double]**
  %359 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs758, i64 0, i64 3
  %360 = bitcast i8** %359 to [3072 x double]**
  %361 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs759, i64 0, i64 3
  %362 = bitcast i8** %361 to [3072 x double]**
  %363 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs758, i64 0, i64 4
  %364 = bitcast i8** %363 to [3072 x double]**
  %365 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs759, i64 0, i64 4
  %366 = bitcast i8** %365 to [3072 x double]**
  %367 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs758, i64 0, i64 5
  %368 = bitcast i8** %367 to i64*
  %369 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs759, i64 0, i64 5
  %370 = bitcast i8** %369 to i64*
  %371 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs758, i64 0, i64 6
  %372 = bitcast i8** %371 to i64*
  %373 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs759, i64 0, i64 6
  %374 = bitcast i8** %373 to i64*
  %375 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs758, i64 0, i64 7
  %376 = bitcast i8** %375 to i64*
  %377 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs759, i64 0, i64 7
  %378 = bitcast i8** %377 to i64*
  %379 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs854, i64 0, i64 0
  %380 = bitcast [8 x i8*]* %.offload_baseptrs854 to [3072 x double]**
  %381 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs855, i64 0, i64 0
  %382 = bitcast [8 x i8*]* %.offload_ptrs855 to [3072 x double]**
  %383 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs854, i64 0, i64 1
  %384 = bitcast i8** %383 to [3072 x double]**
  %385 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs855, i64 0, i64 1
  %386 = bitcast i8** %385 to [3072 x double]**
  %387 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs854, i64 0, i64 2
  %388 = bitcast i8** %387 to [3072 x double]**
  %389 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs855, i64 0, i64 2
  %390 = bitcast i8** %389 to [3072 x double]**
  %391 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs854, i64 0, i64 3
  %392 = bitcast i8** %391 to [3072 x double]**
  %393 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs855, i64 0, i64 3
  %394 = bitcast i8** %393 to [3072 x double]**
  %395 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs854, i64 0, i64 4
  %396 = bitcast i8** %395 to [3072 x double]**
  %397 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs855, i64 0, i64 4
  %398 = bitcast i8** %397 to [3072 x double]**
  %399 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs854, i64 0, i64 5
  %400 = bitcast i8** %399 to i64*
  %401 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs855, i64 0, i64 5
  %402 = bitcast i8** %401 to i64*
  %403 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs854, i64 0, i64 6
  %404 = bitcast i8** %403 to i64*
  %405 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs855, i64 0, i64 6
  %406 = bitcast i8** %405 to i64*
  %407 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs854, i64 0, i64 7
  %408 = bitcast i8** %407 to i64*
  %409 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs855, i64 0, i64 7
  %410 = bitcast i8** %409 to i64*
  %411 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs950, i64 0, i64 0
  %412 = bitcast [8 x i8*]* %.offload_baseptrs950 to [3072 x double]**
  %413 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs951, i64 0, i64 0
  %414 = bitcast [8 x i8*]* %.offload_ptrs951 to [3072 x double]**
  %415 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs950, i64 0, i64 1
  %416 = bitcast i8** %415 to [3072 x double]**
  %417 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs951, i64 0, i64 1
  %418 = bitcast i8** %417 to [3072 x double]**
  %419 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs950, i64 0, i64 2
  %420 = bitcast i8** %419 to [3072 x double]**
  %421 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs951, i64 0, i64 2
  %422 = bitcast i8** %421 to [3072 x double]**
  %423 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs950, i64 0, i64 3
  %424 = bitcast i8** %423 to [3072 x double]**
  %425 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs951, i64 0, i64 3
  %426 = bitcast i8** %425 to [3072 x double]**
  %427 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs950, i64 0, i64 4
  %428 = bitcast i8** %427 to [3072 x double]**
  %429 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs951, i64 0, i64 4
  %430 = bitcast i8** %429 to [3072 x double]**
  %431 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs950, i64 0, i64 5
  %432 = bitcast i8** %431 to i64*
  %433 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs951, i64 0, i64 5
  %434 = bitcast i8** %433 to i64*
  %435 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs950, i64 0, i64 6
  %436 = bitcast i8** %435 to i64*
  %437 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs951, i64 0, i64 6
  %438 = bitcast i8** %437 to i64*
  %439 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs950, i64 0, i64 7
  %440 = bitcast i8** %439 to i64*
  %441 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs951, i64 0, i64 7
  %442 = bitcast i8** %441 to i64*
  %443 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1046, i64 0, i64 0
  %444 = bitcast [8 x i8*]* %.offload_baseptrs1046 to [3072 x double]**
  %445 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1047, i64 0, i64 0
  %446 = bitcast [8 x i8*]* %.offload_ptrs1047 to [3072 x double]**
  %447 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1046, i64 0, i64 1
  %448 = bitcast i8** %447 to [3072 x double]**
  %449 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1047, i64 0, i64 1
  %450 = bitcast i8** %449 to [3072 x double]**
  %451 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1046, i64 0, i64 2
  %452 = bitcast i8** %451 to [3072 x double]**
  %453 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1047, i64 0, i64 2
  %454 = bitcast i8** %453 to [3072 x double]**
  %455 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1046, i64 0, i64 3
  %456 = bitcast i8** %455 to [3072 x double]**
  %457 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1047, i64 0, i64 3
  %458 = bitcast i8** %457 to [3072 x double]**
  %459 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1046, i64 0, i64 4
  %460 = bitcast i8** %459 to [3072 x double]**
  %461 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1047, i64 0, i64 4
  %462 = bitcast i8** %461 to [3072 x double]**
  %463 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1046, i64 0, i64 5
  %464 = bitcast i8** %463 to i64*
  %465 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1047, i64 0, i64 5
  %466 = bitcast i8** %465 to i64*
  %467 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1046, i64 0, i64 6
  %468 = bitcast i8** %467 to i64*
  %469 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1047, i64 0, i64 6
  %470 = bitcast i8** %469 to i64*
  %471 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1046, i64 0, i64 7
  %472 = bitcast i8** %471 to i64*
  %473 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1047, i64 0, i64 7
  %474 = bitcast i8** %473 to i64*
  %475 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1142, i64 0, i64 0
  %476 = bitcast [8 x i8*]* %.offload_baseptrs1142 to [3072 x double]**
  %477 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1143, i64 0, i64 0
  %478 = bitcast [8 x i8*]* %.offload_ptrs1143 to [3072 x double]**
  %479 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1142, i64 0, i64 1
  %480 = bitcast i8** %479 to [3072 x double]**
  %481 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1143, i64 0, i64 1
  %482 = bitcast i8** %481 to [3072 x double]**
  %483 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1142, i64 0, i64 2
  %484 = bitcast i8** %483 to [3072 x double]**
  %485 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1143, i64 0, i64 2
  %486 = bitcast i8** %485 to [3072 x double]**
  %487 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1142, i64 0, i64 3
  %488 = bitcast i8** %487 to [3072 x double]**
  %489 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1143, i64 0, i64 3
  %490 = bitcast i8** %489 to [3072 x double]**
  %491 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1142, i64 0, i64 4
  %492 = bitcast i8** %491 to [3072 x double]**
  %493 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1143, i64 0, i64 4
  %494 = bitcast i8** %493 to [3072 x double]**
  %495 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1142, i64 0, i64 5
  %496 = bitcast i8** %495 to i64*
  %497 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1143, i64 0, i64 5
  %498 = bitcast i8** %497 to i64*
  %499 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1142, i64 0, i64 6
  %500 = bitcast i8** %499 to i64*
  %501 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1143, i64 0, i64 6
  %502 = bitcast i8** %501 to i64*
  %503 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1142, i64 0, i64 7
  %504 = bitcast i8** %503 to i64*
  %505 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1143, i64 0, i64 7
  %506 = bitcast i8** %505 to i64*
  br label %for.cond608.preheader

for.cond.cleanup13:                               ; preds = %if.end582
  br i1 %tobool586, label %for.inc599, label %for.body593.preheader

for.body593.preheader:                            ; preds = %for.cond.cleanup13
  %puts8343 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.1 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.2 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.3 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.4 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.5 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.6 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.7 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.8 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.9 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.10 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.11 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.12 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.13 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.14 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.15 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.16 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8343.17 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %for.inc599

for.body14:                                       ; preds = %for.cond10.preheader, %if.end582
  %indvars.iv9825 = phi i64 [ 1, %for.cond10.preheader ], [ %indvars.iv.next9826, %if.end582 ]
  %tobool5869333 = phi i1 [ %tobool5869331, %for.cond10.preheader ], [ %tobool586, %if.end582 ]
  %cond20 = select i1 %tobool5869333, i32 %tms.09335, i32 1
  %.capture_expr..casted52.sroa.0.0.insert.ext = zext i32 %cond20 to i64
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %123, align 8
  store [3072 x double]* %A, [3072 x double]** %125, align 8
  store [3072 x double]* %C, [3072 x double]** %127, align 8
  store [3072 x double]* %C, [3072 x double]** %129, align 8
  store [3072 x double]* %D, [3072 x double]** %131, align 8
  store [3072 x double]* %D, [3072 x double]** %133, align 8
  store [3072 x double]* %B, [3072 x double]** %135, align 8
  store [3072 x double]* %B, [3072 x double]** %137, align 8
  store [3072 x double]* %E, [3072 x double]** %139, align 8
  store [3072 x double]* %E, [3072 x double]** %141, align 8
  store i64 1, i64* %143, align 8
  store i64 1, i64* %145, align 8
  store i64 %indvars.iv9825, i64* %147, align 8
  store i64 %indvars.iv9825, i64* %149, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %151, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %153, align 8
  %507 = trunc i64 %indvars.iv9825 to i32
  %508 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l66.region_id, i32 8, i8** nonnull %122, i8** nonnull %124, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond20, i32 %507) #6
  %509 = icmp eq i32 %508, 0
  br i1 %509, label %for.body64.preheader, label %omp_offload.failed57

omp_offload.failed57:                             ; preds = %for.body14
  %510 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %511 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %510, i32 %cond20, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined. to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9825) #6
  br label %for.body64.preheader

for.body64.preheader:                             ; preds = %for.body14, %omp_offload.failed57
  br label %for.body64

for.cond.cleanup63:                               ; preds = %for.body64
  %add76 = fadd double %add71.5, 0.000000e+00
  %cmp83 = fcmp une double %add76, 0x4152018000000000
  br i1 %cmp83, label %if.then99, label %if.else

for.body64:                                       ; preds = %for.body64.preheader, %for.body64
  %indvars.iv9792 = phi i64 [ %indvars.iv.next9793.5, %for.body64 ], [ 0, %for.body64.preheader ]
  %tmp.09285 = phi double [ %add71.5, %for.body64 ], [ 0.000000e+00, %for.body64.preheader ]
  %arrayidx66 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9792
  %512 = load double, double* %arrayidx66, align 8, !tbaa !62
  %arrayidx68 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9792
  %513 = load double, double* %arrayidx68, align 8, !tbaa !62
  %add69 = fadd double %512, %513
  %sub70 = fadd double %add69, -1.000000e+00
  %add71 = fadd double %tmp.09285, %sub70
  %indvars.iv.next9793 = or i64 %indvars.iv9792, 1
  %arrayidx66.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9793
  %514 = load double, double* %arrayidx66.1, align 8, !tbaa !62
  %arrayidx68.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9793
  %515 = load double, double* %arrayidx68.1, align 8, !tbaa !62
  %add69.1 = fadd double %514, %515
  %sub70.1 = fadd double %add69.1, -1.000000e+00
  %add71.1 = fadd double %add71, %sub70.1
  %indvars.iv.next9793.1 = add nuw nsw i64 %indvars.iv9792, 2
  %arrayidx66.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9793.1
  %516 = load double, double* %arrayidx66.2, align 8, !tbaa !62
  %arrayidx68.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9793.1
  %517 = load double, double* %arrayidx68.2, align 8, !tbaa !62
  %add69.2 = fadd double %516, %517
  %sub70.2 = fadd double %add69.2, -1.000000e+00
  %add71.2 = fadd double %add71.1, %sub70.2
  %indvars.iv.next9793.2 = add nuw nsw i64 %indvars.iv9792, 3
  %arrayidx66.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9793.2
  %518 = load double, double* %arrayidx66.3, align 8, !tbaa !62
  %arrayidx68.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9793.2
  %519 = load double, double* %arrayidx68.3, align 8, !tbaa !62
  %add69.3 = fadd double %518, %519
  %sub70.3 = fadd double %add69.3, -1.000000e+00
  %add71.3 = fadd double %add71.2, %sub70.3
  %indvars.iv.next9793.3 = add nuw nsw i64 %indvars.iv9792, 4
  %arrayidx66.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9793.3
  %520 = load double, double* %arrayidx66.4, align 8, !tbaa !62
  %arrayidx68.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9793.3
  %521 = load double, double* %arrayidx68.4, align 8, !tbaa !62
  %add69.4 = fadd double %520, %521
  %sub70.4 = fadd double %add69.4, -1.000000e+00
  %add71.4 = fadd double %add71.3, %sub70.4
  %indvars.iv.next9793.4 = add nuw nsw i64 %indvars.iv9792, 5
  %arrayidx66.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9793.4
  %522 = load double, double* %arrayidx66.5, align 8, !tbaa !62
  %arrayidx68.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9793.4
  %523 = load double, double* %arrayidx68.5, align 8, !tbaa !62
  %add69.5 = fadd double %522, %523
  %sub70.5 = fadd double %add69.5, -1.000000e+00
  %add71.5 = fadd double %add71.4, %sub70.5
  %indvars.iv.next9793.5 = add nuw nsw i64 %indvars.iv9792, 6
  %exitcond9794.5 = icmp eq i64 %indvars.iv.next9793.5, 3072
  br i1 %exitcond9794.5, label %for.cond.cleanup63, label %for.body64

if.then99:                                        ; preds = %for.cond.cleanup63
  %call86.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add76)
  %puts8355 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end102

if.else:                                          ; preds = %for.cond.cleanup63
  %puts8344 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end102

if.end102:                                        ; preds = %if.else, %if.then99
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %155, align 8
  store [3072 x double]* %A, [3072 x double]** %157, align 8
  store [3072 x double]* %C, [3072 x double]** %159, align 8
  store [3072 x double]* %C, [3072 x double]** %161, align 8
  store [3072 x double]* %D, [3072 x double]** %163, align 8
  store [3072 x double]* %D, [3072 x double]** %165, align 8
  store [3072 x double]* %B, [3072 x double]** %167, align 8
  store [3072 x double]* %B, [3072 x double]** %169, align 8
  store [3072 x double]* %E, [3072 x double]** %171, align 8
  store [3072 x double]* %E, [3072 x double]** %173, align 8
  store i64 1, i64* %175, align 8
  store i64 1, i64* %177, align 8
  store i64 %indvars.iv9825, i64* %179, align 8
  store i64 %indvars.iv9825, i64* %181, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %183, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %185, align 8
  %524 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l86.region_id, i32 8, i8** nonnull %154, i8** nonnull %156, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond20, i32 %507) #6
  %525 = icmp eq i32 %524, 0
  br i1 %525, label %for.body156.preheader, label %omp_offload.failed146

omp_offload.failed146:                            ; preds = %if.end102
  %526 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %527 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %526, i32 %cond20, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..14 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9825) #6
  br label %for.body156.preheader

for.body156.preheader:                            ; preds = %if.end102, %omp_offload.failed146
  br label %for.body156

for.cond.cleanup155:                              ; preds = %for.body156
  %add168 = fadd double %add163.5, 0.000000e+00
  %cmp176 = fcmp une double %add168, 0x4152018000000000
  br i1 %cmp176, label %if.then194, label %if.else196

for.body156:                                      ; preds = %for.body156.preheader, %for.body156
  %indvars.iv9798 = phi i64 [ %indvars.iv.next9799.5, %for.body156 ], [ 0, %for.body156.preheader ]
  %tmp150.09293 = phi double [ %add163.5, %for.body156 ], [ 0.000000e+00, %for.body156.preheader ]
  %arrayidx158 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9798
  %528 = load double, double* %arrayidx158, align 8, !tbaa !62
  %arrayidx160 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9798
  %529 = load double, double* %arrayidx160, align 8, !tbaa !62
  %add161 = fadd double %528, %529
  %sub162 = fadd double %add161, -1.000000e+00
  %add163 = fadd double %tmp150.09293, %sub162
  %indvars.iv.next9799 = or i64 %indvars.iv9798, 1
  %arrayidx158.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9799
  %530 = load double, double* %arrayidx158.1, align 8, !tbaa !62
  %arrayidx160.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9799
  %531 = load double, double* %arrayidx160.1, align 8, !tbaa !62
  %add161.1 = fadd double %530, %531
  %sub162.1 = fadd double %add161.1, -1.000000e+00
  %add163.1 = fadd double %add163, %sub162.1
  %indvars.iv.next9799.1 = add nuw nsw i64 %indvars.iv9798, 2
  %arrayidx158.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9799.1
  %532 = load double, double* %arrayidx158.2, align 8, !tbaa !62
  %arrayidx160.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9799.1
  %533 = load double, double* %arrayidx160.2, align 8, !tbaa !62
  %add161.2 = fadd double %532, %533
  %sub162.2 = fadd double %add161.2, -1.000000e+00
  %add163.2 = fadd double %add163.1, %sub162.2
  %indvars.iv.next9799.2 = add nuw nsw i64 %indvars.iv9798, 3
  %arrayidx158.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9799.2
  %534 = load double, double* %arrayidx158.3, align 8, !tbaa !62
  %arrayidx160.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9799.2
  %535 = load double, double* %arrayidx160.3, align 8, !tbaa !62
  %add161.3 = fadd double %534, %535
  %sub162.3 = fadd double %add161.3, -1.000000e+00
  %add163.3 = fadd double %add163.2, %sub162.3
  %indvars.iv.next9799.3 = add nuw nsw i64 %indvars.iv9798, 4
  %arrayidx158.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9799.3
  %536 = load double, double* %arrayidx158.4, align 8, !tbaa !62
  %arrayidx160.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9799.3
  %537 = load double, double* %arrayidx160.4, align 8, !tbaa !62
  %add161.4 = fadd double %536, %537
  %sub162.4 = fadd double %add161.4, -1.000000e+00
  %add163.4 = fadd double %add163.3, %sub162.4
  %indvars.iv.next9799.4 = add nuw nsw i64 %indvars.iv9798, 5
  %arrayidx158.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9799.4
  %538 = load double, double* %arrayidx158.5, align 8, !tbaa !62
  %arrayidx160.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9799.4
  %539 = load double, double* %arrayidx160.5, align 8, !tbaa !62
  %add161.5 = fadd double %538, %539
  %sub162.5 = fadd double %add161.5, -1.000000e+00
  %add163.5 = fadd double %add163.4, %sub162.5
  %indvars.iv.next9799.5 = add nuw nsw i64 %indvars.iv9798, 6
  %exitcond9800.5 = icmp eq i64 %indvars.iv.next9799.5, 3072
  br i1 %exitcond9800.5, label %for.cond.cleanup155, label %for.body156

if.then194:                                       ; preds = %for.cond.cleanup155
  %call180.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add168)
  %puts8354 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end198

if.else196:                                       ; preds = %for.cond.cleanup155
  %puts8345 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end198

if.end198:                                        ; preds = %if.else196, %if.then194
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %187, align 8
  store [3072 x double]* %A, [3072 x double]** %189, align 8
  store [3072 x double]* %C, [3072 x double]** %191, align 8
  store [3072 x double]* %C, [3072 x double]** %193, align 8
  store [3072 x double]* %D, [3072 x double]** %195, align 8
  store [3072 x double]* %D, [3072 x double]** %197, align 8
  store [3072 x double]* %B, [3072 x double]** %199, align 8
  store [3072 x double]* %B, [3072 x double]** %201, align 8
  store [3072 x double]* %E, [3072 x double]** %203, align 8
  store [3072 x double]* %E, [3072 x double]** %205, align 8
  store i64 1, i64* %207, align 8
  store i64 1, i64* %209, align 8
  store i64 %indvars.iv9825, i64* %211, align 8
  store i64 %indvars.iv9825, i64* %213, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %215, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %217, align 8
  %540 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l106.region_id, i32 8, i8** nonnull %186, i8** nonnull %188, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond20, i32 %507) #6
  %541 = icmp eq i32 %540, 0
  br i1 %541, label %for.body252.preheader, label %omp_offload.failed242

omp_offload.failed242:                            ; preds = %if.end198
  %542 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %543 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %542, i32 %cond20, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..18 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9825) #6
  br label %for.body252.preheader

for.body252.preheader:                            ; preds = %if.end198, %omp_offload.failed242
  br label %for.body252

for.cond.cleanup251:                              ; preds = %for.body252
  %add264 = fadd double %add259.5, 0.000000e+00
  %cmp272 = fcmp une double %add264, 0x4152018000000000
  br i1 %cmp272, label %if.then290, label %if.else292

for.body252:                                      ; preds = %for.body252.preheader, %for.body252
  %indvars.iv9804 = phi i64 [ %indvars.iv.next9805.5, %for.body252 ], [ 0, %for.body252.preheader ]
  %tmp246.09301 = phi double [ %add259.5, %for.body252 ], [ 0.000000e+00, %for.body252.preheader ]
  %arrayidx254 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9804
  %544 = load double, double* %arrayidx254, align 8, !tbaa !62
  %arrayidx256 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9804
  %545 = load double, double* %arrayidx256, align 8, !tbaa !62
  %add257 = fadd double %544, %545
  %sub258 = fadd double %add257, -1.000000e+00
  %add259 = fadd double %tmp246.09301, %sub258
  %indvars.iv.next9805 = or i64 %indvars.iv9804, 1
  %arrayidx254.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9805
  %546 = load double, double* %arrayidx254.1, align 8, !tbaa !62
  %arrayidx256.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9805
  %547 = load double, double* %arrayidx256.1, align 8, !tbaa !62
  %add257.1 = fadd double %546, %547
  %sub258.1 = fadd double %add257.1, -1.000000e+00
  %add259.1 = fadd double %add259, %sub258.1
  %indvars.iv.next9805.1 = add nuw nsw i64 %indvars.iv9804, 2
  %arrayidx254.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9805.1
  %548 = load double, double* %arrayidx254.2, align 8, !tbaa !62
  %arrayidx256.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9805.1
  %549 = load double, double* %arrayidx256.2, align 8, !tbaa !62
  %add257.2 = fadd double %548, %549
  %sub258.2 = fadd double %add257.2, -1.000000e+00
  %add259.2 = fadd double %add259.1, %sub258.2
  %indvars.iv.next9805.2 = add nuw nsw i64 %indvars.iv9804, 3
  %arrayidx254.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9805.2
  %550 = load double, double* %arrayidx254.3, align 8, !tbaa !62
  %arrayidx256.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9805.2
  %551 = load double, double* %arrayidx256.3, align 8, !tbaa !62
  %add257.3 = fadd double %550, %551
  %sub258.3 = fadd double %add257.3, -1.000000e+00
  %add259.3 = fadd double %add259.2, %sub258.3
  %indvars.iv.next9805.3 = add nuw nsw i64 %indvars.iv9804, 4
  %arrayidx254.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9805.3
  %552 = load double, double* %arrayidx254.4, align 8, !tbaa !62
  %arrayidx256.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9805.3
  %553 = load double, double* %arrayidx256.4, align 8, !tbaa !62
  %add257.4 = fadd double %552, %553
  %sub258.4 = fadd double %add257.4, -1.000000e+00
  %add259.4 = fadd double %add259.3, %sub258.4
  %indvars.iv.next9805.4 = add nuw nsw i64 %indvars.iv9804, 5
  %arrayidx254.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9805.4
  %554 = load double, double* %arrayidx254.5, align 8, !tbaa !62
  %arrayidx256.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9805.4
  %555 = load double, double* %arrayidx256.5, align 8, !tbaa !62
  %add257.5 = fadd double %554, %555
  %sub258.5 = fadd double %add257.5, -1.000000e+00
  %add259.5 = fadd double %add259.4, %sub258.5
  %indvars.iv.next9805.5 = add nuw nsw i64 %indvars.iv9804, 6
  %exitcond9806.5 = icmp eq i64 %indvars.iv.next9805.5, 3072
  br i1 %exitcond9806.5, label %for.cond.cleanup251, label %for.body252

if.then290:                                       ; preds = %for.cond.cleanup251
  %call276.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add264)
  %puts8353 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end294

if.else292:                                       ; preds = %for.cond.cleanup251
  %puts8346 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end294

if.end294:                                        ; preds = %if.else292, %if.then290
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %219, align 8
  store [3072 x double]* %A, [3072 x double]** %221, align 8
  store [3072 x double]* %C, [3072 x double]** %223, align 8
  store [3072 x double]* %C, [3072 x double]** %225, align 8
  store [3072 x double]* %D, [3072 x double]** %227, align 8
  store [3072 x double]* %D, [3072 x double]** %229, align 8
  store [3072 x double]* %B, [3072 x double]** %231, align 8
  store [3072 x double]* %B, [3072 x double]** %233, align 8
  store [3072 x double]* %E, [3072 x double]** %235, align 8
  store [3072 x double]* %E, [3072 x double]** %237, align 8
  store i64 1, i64* %239, align 8
  store i64 1, i64* %241, align 8
  store i64 %indvars.iv9825, i64* %243, align 8
  store i64 %indvars.iv9825, i64* %245, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %247, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %249, align 8
  %556 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l126.region_id, i32 8, i8** nonnull %218, i8** nonnull %220, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond20, i32 %507) #6
  %557 = icmp eq i32 %556, 0
  br i1 %557, label %for.body348.preheader, label %omp_offload.failed338

omp_offload.failed338:                            ; preds = %if.end294
  %558 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %559 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %558, i32 %cond20, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..22 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9825) #6
  br label %for.body348.preheader

for.body348.preheader:                            ; preds = %if.end294, %omp_offload.failed338
  br label %for.body348

for.cond.cleanup347:                              ; preds = %for.body348
  %add360 = fadd double %add355.5, 0.000000e+00
  %cmp368 = fcmp une double %add360, 0x4152018000000000
  br i1 %cmp368, label %if.then386, label %if.else388

for.body348:                                      ; preds = %for.body348.preheader, %for.body348
  %indvars.iv9810 = phi i64 [ %indvars.iv.next9811.5, %for.body348 ], [ 0, %for.body348.preheader ]
  %tmp342.09309 = phi double [ %add355.5, %for.body348 ], [ 0.000000e+00, %for.body348.preheader ]
  %arrayidx350 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9810
  %560 = load double, double* %arrayidx350, align 8, !tbaa !62
  %arrayidx352 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9810
  %561 = load double, double* %arrayidx352, align 8, !tbaa !62
  %add353 = fadd double %560, %561
  %sub354 = fadd double %add353, -1.000000e+00
  %add355 = fadd double %tmp342.09309, %sub354
  %indvars.iv.next9811 = or i64 %indvars.iv9810, 1
  %arrayidx350.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9811
  %562 = load double, double* %arrayidx350.1, align 8, !tbaa !62
  %arrayidx352.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9811
  %563 = load double, double* %arrayidx352.1, align 8, !tbaa !62
  %add353.1 = fadd double %562, %563
  %sub354.1 = fadd double %add353.1, -1.000000e+00
  %add355.1 = fadd double %add355, %sub354.1
  %indvars.iv.next9811.1 = add nuw nsw i64 %indvars.iv9810, 2
  %arrayidx350.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9811.1
  %564 = load double, double* %arrayidx350.2, align 8, !tbaa !62
  %arrayidx352.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9811.1
  %565 = load double, double* %arrayidx352.2, align 8, !tbaa !62
  %add353.2 = fadd double %564, %565
  %sub354.2 = fadd double %add353.2, -1.000000e+00
  %add355.2 = fadd double %add355.1, %sub354.2
  %indvars.iv.next9811.2 = add nuw nsw i64 %indvars.iv9810, 3
  %arrayidx350.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9811.2
  %566 = load double, double* %arrayidx350.3, align 8, !tbaa !62
  %arrayidx352.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9811.2
  %567 = load double, double* %arrayidx352.3, align 8, !tbaa !62
  %add353.3 = fadd double %566, %567
  %sub354.3 = fadd double %add353.3, -1.000000e+00
  %add355.3 = fadd double %add355.2, %sub354.3
  %indvars.iv.next9811.3 = add nuw nsw i64 %indvars.iv9810, 4
  %arrayidx350.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9811.3
  %568 = load double, double* %arrayidx350.4, align 8, !tbaa !62
  %arrayidx352.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9811.3
  %569 = load double, double* %arrayidx352.4, align 8, !tbaa !62
  %add353.4 = fadd double %568, %569
  %sub354.4 = fadd double %add353.4, -1.000000e+00
  %add355.4 = fadd double %add355.3, %sub354.4
  %indvars.iv.next9811.4 = add nuw nsw i64 %indvars.iv9810, 5
  %arrayidx350.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9811.4
  %570 = load double, double* %arrayidx350.5, align 8, !tbaa !62
  %arrayidx352.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9811.4
  %571 = load double, double* %arrayidx352.5, align 8, !tbaa !62
  %add353.5 = fadd double %570, %571
  %sub354.5 = fadd double %add353.5, -1.000000e+00
  %add355.5 = fadd double %add355.4, %sub354.5
  %indvars.iv.next9811.5 = add nuw nsw i64 %indvars.iv9810, 6
  %exitcond9812.5 = icmp eq i64 %indvars.iv.next9811.5, 3072
  br i1 %exitcond9812.5, label %for.cond.cleanup347, label %for.body348

if.then386:                                       ; preds = %for.cond.cleanup347
  %call372.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add360)
  %puts8352 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end390

if.else388:                                       ; preds = %for.cond.cleanup347
  %puts8347 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end390

if.end390:                                        ; preds = %if.else388, %if.then386
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %251, align 8
  store [3072 x double]* %A, [3072 x double]** %253, align 8
  store [3072 x double]* %C, [3072 x double]** %255, align 8
  store [3072 x double]* %C, [3072 x double]** %257, align 8
  store [3072 x double]* %D, [3072 x double]** %259, align 8
  store [3072 x double]* %D, [3072 x double]** %261, align 8
  store [3072 x double]* %B, [3072 x double]** %263, align 8
  store [3072 x double]* %B, [3072 x double]** %265, align 8
  store [3072 x double]* %E, [3072 x double]** %267, align 8
  store [3072 x double]* %E, [3072 x double]** %269, align 8
  store i64 1, i64* %271, align 8
  store i64 1, i64* %273, align 8
  store i64 %indvars.iv9825, i64* %275, align 8
  store i64 %indvars.iv9825, i64* %277, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %279, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %281, align 8
  %572 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l146.region_id, i32 8, i8** nonnull %250, i8** nonnull %252, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond20, i32 %507) #6
  %573 = icmp eq i32 %572, 0
  br i1 %573, label %for.body444.preheader, label %omp_offload.failed434

omp_offload.failed434:                            ; preds = %if.end390
  %574 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %575 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %574, i32 %cond20, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..26 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9825) #6
  br label %for.body444.preheader

for.body444.preheader:                            ; preds = %if.end390, %omp_offload.failed434
  br label %for.body444

for.cond.cleanup443:                              ; preds = %for.body444
  %add456 = fadd double %add451.5, 0.000000e+00
  %cmp464 = fcmp une double %add456, 0x4152018000000000
  br i1 %cmp464, label %if.then482, label %if.else484

for.body444:                                      ; preds = %for.body444.preheader, %for.body444
  %indvars.iv9816 = phi i64 [ %indvars.iv.next9817.5, %for.body444 ], [ 0, %for.body444.preheader ]
  %tmp438.09317 = phi double [ %add451.5, %for.body444 ], [ 0.000000e+00, %for.body444.preheader ]
  %arrayidx446 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9816
  %576 = load double, double* %arrayidx446, align 8, !tbaa !62
  %arrayidx448 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9816
  %577 = load double, double* %arrayidx448, align 8, !tbaa !62
  %add449 = fadd double %576, %577
  %sub450 = fadd double %add449, -1.000000e+00
  %add451 = fadd double %tmp438.09317, %sub450
  %indvars.iv.next9817 = or i64 %indvars.iv9816, 1
  %arrayidx446.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9817
  %578 = load double, double* %arrayidx446.1, align 8, !tbaa !62
  %arrayidx448.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9817
  %579 = load double, double* %arrayidx448.1, align 8, !tbaa !62
  %add449.1 = fadd double %578, %579
  %sub450.1 = fadd double %add449.1, -1.000000e+00
  %add451.1 = fadd double %add451, %sub450.1
  %indvars.iv.next9817.1 = add nuw nsw i64 %indvars.iv9816, 2
  %arrayidx446.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9817.1
  %580 = load double, double* %arrayidx446.2, align 8, !tbaa !62
  %arrayidx448.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9817.1
  %581 = load double, double* %arrayidx448.2, align 8, !tbaa !62
  %add449.2 = fadd double %580, %581
  %sub450.2 = fadd double %add449.2, -1.000000e+00
  %add451.2 = fadd double %add451.1, %sub450.2
  %indvars.iv.next9817.2 = add nuw nsw i64 %indvars.iv9816, 3
  %arrayidx446.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9817.2
  %582 = load double, double* %arrayidx446.3, align 8, !tbaa !62
  %arrayidx448.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9817.2
  %583 = load double, double* %arrayidx448.3, align 8, !tbaa !62
  %add449.3 = fadd double %582, %583
  %sub450.3 = fadd double %add449.3, -1.000000e+00
  %add451.3 = fadd double %add451.2, %sub450.3
  %indvars.iv.next9817.3 = add nuw nsw i64 %indvars.iv9816, 4
  %arrayidx446.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9817.3
  %584 = load double, double* %arrayidx446.4, align 8, !tbaa !62
  %arrayidx448.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9817.3
  %585 = load double, double* %arrayidx448.4, align 8, !tbaa !62
  %add449.4 = fadd double %584, %585
  %sub450.4 = fadd double %add449.4, -1.000000e+00
  %add451.4 = fadd double %add451.3, %sub450.4
  %indvars.iv.next9817.4 = add nuw nsw i64 %indvars.iv9816, 5
  %arrayidx446.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9817.4
  %586 = load double, double* %arrayidx446.5, align 8, !tbaa !62
  %arrayidx448.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9817.4
  %587 = load double, double* %arrayidx448.5, align 8, !tbaa !62
  %add449.5 = fadd double %586, %587
  %sub450.5 = fadd double %add449.5, -1.000000e+00
  %add451.5 = fadd double %add451.4, %sub450.5
  %indvars.iv.next9817.5 = add nuw nsw i64 %indvars.iv9816, 6
  %exitcond9818.5 = icmp eq i64 %indvars.iv.next9817.5, 3072
  br i1 %exitcond9818.5, label %for.cond.cleanup443, label %for.body444

if.then482:                                       ; preds = %for.cond.cleanup443
  %call468.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add456)
  %puts8351 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end486

if.else484:                                       ; preds = %for.cond.cleanup443
  %puts8348 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end486

if.end486:                                        ; preds = %if.else484, %if.then482
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %283, align 8
  store [3072 x double]* %A, [3072 x double]** %285, align 8
  store [3072 x double]* %C, [3072 x double]** %287, align 8
  store [3072 x double]* %C, [3072 x double]** %289, align 8
  store [3072 x double]* %D, [3072 x double]** %291, align 8
  store [3072 x double]* %D, [3072 x double]** %293, align 8
  store [3072 x double]* %B, [3072 x double]** %295, align 8
  store [3072 x double]* %B, [3072 x double]** %297, align 8
  store [3072 x double]* %E, [3072 x double]** %299, align 8
  store [3072 x double]* %E, [3072 x double]** %301, align 8
  store i64 1, i64* %303, align 8
  store i64 1, i64* %305, align 8
  store i64 %indvars.iv9825, i64* %307, align 8
  store i64 %indvars.iv9825, i64* %309, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %311, align 8
  store i64 %.capture_expr..casted52.sroa.0.0.insert.ext, i64* %313, align 8
  %588 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l166.region_id, i32 8, i8** nonnull %282, i8** nonnull %284, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond20, i32 %507) #6
  %589 = icmp eq i32 %588, 0
  br i1 %589, label %for.body540.preheader, label %omp_offload.failed530

omp_offload.failed530:                            ; preds = %if.end486
  %590 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %591 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %590, i32 %cond20, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..30 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9825) #6
  br label %for.body540.preheader

for.body540.preheader:                            ; preds = %if.end486, %omp_offload.failed530
  br label %for.body540

for.cond.cleanup539:                              ; preds = %for.body540
  %add552 = fadd double %add547.5, 0.000000e+00
  %cmp560 = fcmp une double %add552, 0x4152018000000000
  br i1 %cmp560, label %if.then578, label %if.else580

for.body540:                                      ; preds = %for.body540.preheader, %for.body540
  %indvars.iv9822 = phi i64 [ %indvars.iv.next9823.5, %for.body540 ], [ 0, %for.body540.preheader ]
  %tmp534.09325 = phi double [ %add547.5, %for.body540 ], [ 0.000000e+00, %for.body540.preheader ]
  %arrayidx542 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9822
  %592 = load double, double* %arrayidx542, align 8, !tbaa !62
  %arrayidx544 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9822
  %593 = load double, double* %arrayidx544, align 8, !tbaa !62
  %add545 = fadd double %592, %593
  %sub546 = fadd double %add545, -1.000000e+00
  %add547 = fadd double %tmp534.09325, %sub546
  %indvars.iv.next9823 = or i64 %indvars.iv9822, 1
  %arrayidx542.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9823
  %594 = load double, double* %arrayidx542.1, align 8, !tbaa !62
  %arrayidx544.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9823
  %595 = load double, double* %arrayidx544.1, align 8, !tbaa !62
  %add545.1 = fadd double %594, %595
  %sub546.1 = fadd double %add545.1, -1.000000e+00
  %add547.1 = fadd double %add547, %sub546.1
  %indvars.iv.next9823.1 = add nuw nsw i64 %indvars.iv9822, 2
  %arrayidx542.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9823.1
  %596 = load double, double* %arrayidx542.2, align 8, !tbaa !62
  %arrayidx544.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9823.1
  %597 = load double, double* %arrayidx544.2, align 8, !tbaa !62
  %add545.2 = fadd double %596, %597
  %sub546.2 = fadd double %add545.2, -1.000000e+00
  %add547.2 = fadd double %add547.1, %sub546.2
  %indvars.iv.next9823.2 = add nuw nsw i64 %indvars.iv9822, 3
  %arrayidx542.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9823.2
  %598 = load double, double* %arrayidx542.3, align 8, !tbaa !62
  %arrayidx544.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9823.2
  %599 = load double, double* %arrayidx544.3, align 8, !tbaa !62
  %add545.3 = fadd double %598, %599
  %sub546.3 = fadd double %add545.3, -1.000000e+00
  %add547.3 = fadd double %add547.2, %sub546.3
  %indvars.iv.next9823.3 = add nuw nsw i64 %indvars.iv9822, 4
  %arrayidx542.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9823.3
  %600 = load double, double* %arrayidx542.4, align 8, !tbaa !62
  %arrayidx544.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9823.3
  %601 = load double, double* %arrayidx544.4, align 8, !tbaa !62
  %add545.4 = fadd double %600, %601
  %sub546.4 = fadd double %add545.4, -1.000000e+00
  %add547.4 = fadd double %add547.3, %sub546.4
  %indvars.iv.next9823.4 = add nuw nsw i64 %indvars.iv9822, 5
  %arrayidx542.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9823.4
  %602 = load double, double* %arrayidx542.5, align 8, !tbaa !62
  %arrayidx544.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9823.4
  %603 = load double, double* %arrayidx544.5, align 8, !tbaa !62
  %add545.5 = fadd double %602, %603
  %sub546.5 = fadd double %add545.5, -1.000000e+00
  %add547.5 = fadd double %add547.4, %sub546.5
  %indvars.iv.next9823.5 = add nuw nsw i64 %indvars.iv9822, 6
  %exitcond9824.5 = icmp eq i64 %indvars.iv.next9823.5, 3072
  br i1 %exitcond9824.5, label %for.cond.cleanup539, label %for.body540

if.then578:                                       ; preds = %for.cond.cleanup539
  %call564.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add552)
  %puts8350 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end582

if.else580:                                       ; preds = %for.cond.cleanup539
  %puts8349 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end582

if.end582:                                        ; preds = %if.else580, %if.then578
  %indvars.iv.next9826 = add nuw nsw i64 %indvars.iv9825, 78
  %cmp11 = icmp ugt i64 %indvars.iv.next9826, %cond
  %604 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool586 = icmp eq i32 %604, 0
  br i1 %cmp11, label %for.cond.cleanup13, label %for.body14

for.inc599:                                       ; preds = %for.body593.preheader, %for.cond.cleanup13
  %mul = mul nsw i32 %tms.09335, 3
  %cmp7 = icmp ult i32 %mul, 257
  br i1 %cmp7, label %for.cond10.preheader, label %for.cond602.preheader

for.cond608.preheader:                            ; preds = %for.cond602.preheader, %for.inc1213
  %tms601.09282 = phi i32 [ 1, %for.cond602.preheader ], [ %mul1214, %for.inc1213 ]
  %605 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool12009278 = icmp eq i32 %605, 0
  br label %for.body612

for.cond1217.preheader:                           ; preds = %for.inc1213
  %606 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1277, i64 0, i64 0
  %607 = bitcast [8 x i8*]* %.offload_baseptrs1277 to [3072 x double]**
  %608 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1278, i64 0, i64 0
  %609 = bitcast [8 x i8*]* %.offload_ptrs1278 to [3072 x double]**
  %610 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1277, i64 0, i64 1
  %611 = bitcast i8** %610 to [3072 x double]**
  %612 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1278, i64 0, i64 1
  %613 = bitcast i8** %612 to [3072 x double]**
  %614 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1277, i64 0, i64 2
  %615 = bitcast i8** %614 to [3072 x double]**
  %616 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1278, i64 0, i64 2
  %617 = bitcast i8** %616 to [3072 x double]**
  %618 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1277, i64 0, i64 3
  %619 = bitcast i8** %618 to [3072 x double]**
  %620 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1278, i64 0, i64 3
  %621 = bitcast i8** %620 to [3072 x double]**
  %622 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1277, i64 0, i64 4
  %623 = bitcast i8** %622 to [3072 x double]**
  %624 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1278, i64 0, i64 4
  %625 = bitcast i8** %624 to [3072 x double]**
  %626 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1277, i64 0, i64 5
  %627 = bitcast i8** %626 to i64*
  %628 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1278, i64 0, i64 5
  %629 = bitcast i8** %628 to i64*
  %630 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1277, i64 0, i64 6
  %631 = bitcast i8** %630 to i64*
  %632 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1278, i64 0, i64 6
  %633 = bitcast i8** %632 to i64*
  %634 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1277, i64 0, i64 7
  %635 = bitcast i8** %634 to i64*
  %636 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1278, i64 0, i64 7
  %637 = bitcast i8** %636 to i64*
  %638 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1373, i64 0, i64 0
  %639 = bitcast [8 x i8*]* %.offload_baseptrs1373 to [3072 x double]**
  %640 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1374, i64 0, i64 0
  %641 = bitcast [8 x i8*]* %.offload_ptrs1374 to [3072 x double]**
  %642 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1373, i64 0, i64 1
  %643 = bitcast i8** %642 to [3072 x double]**
  %644 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1374, i64 0, i64 1
  %645 = bitcast i8** %644 to [3072 x double]**
  %646 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1373, i64 0, i64 2
  %647 = bitcast i8** %646 to [3072 x double]**
  %648 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1374, i64 0, i64 2
  %649 = bitcast i8** %648 to [3072 x double]**
  %650 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1373, i64 0, i64 3
  %651 = bitcast i8** %650 to [3072 x double]**
  %652 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1374, i64 0, i64 3
  %653 = bitcast i8** %652 to [3072 x double]**
  %654 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1373, i64 0, i64 4
  %655 = bitcast i8** %654 to [3072 x double]**
  %656 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1374, i64 0, i64 4
  %657 = bitcast i8** %656 to [3072 x double]**
  %658 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1373, i64 0, i64 5
  %659 = bitcast i8** %658 to i64*
  %660 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1374, i64 0, i64 5
  %661 = bitcast i8** %660 to i64*
  %662 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1373, i64 0, i64 6
  %663 = bitcast i8** %662 to i64*
  %664 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1374, i64 0, i64 6
  %665 = bitcast i8** %664 to i64*
  %666 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1373, i64 0, i64 7
  %667 = bitcast i8** %666 to i64*
  %668 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1374, i64 0, i64 7
  %669 = bitcast i8** %668 to i64*
  %670 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1469, i64 0, i64 0
  %671 = bitcast [8 x i8*]* %.offload_baseptrs1469 to [3072 x double]**
  %672 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1470, i64 0, i64 0
  %673 = bitcast [8 x i8*]* %.offload_ptrs1470 to [3072 x double]**
  %674 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1469, i64 0, i64 1
  %675 = bitcast i8** %674 to [3072 x double]**
  %676 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1470, i64 0, i64 1
  %677 = bitcast i8** %676 to [3072 x double]**
  %678 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1469, i64 0, i64 2
  %679 = bitcast i8** %678 to [3072 x double]**
  %680 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1470, i64 0, i64 2
  %681 = bitcast i8** %680 to [3072 x double]**
  %682 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1469, i64 0, i64 3
  %683 = bitcast i8** %682 to [3072 x double]**
  %684 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1470, i64 0, i64 3
  %685 = bitcast i8** %684 to [3072 x double]**
  %686 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1469, i64 0, i64 4
  %687 = bitcast i8** %686 to [3072 x double]**
  %688 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1470, i64 0, i64 4
  %689 = bitcast i8** %688 to [3072 x double]**
  %690 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1469, i64 0, i64 5
  %691 = bitcast i8** %690 to i64*
  %692 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1470, i64 0, i64 5
  %693 = bitcast i8** %692 to i64*
  %694 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1469, i64 0, i64 6
  %695 = bitcast i8** %694 to i64*
  %696 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1470, i64 0, i64 6
  %697 = bitcast i8** %696 to i64*
  %698 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1469, i64 0, i64 7
  %699 = bitcast i8** %698 to i64*
  %700 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1470, i64 0, i64 7
  %701 = bitcast i8** %700 to i64*
  %702 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1565, i64 0, i64 0
  %703 = bitcast [8 x i8*]* %.offload_baseptrs1565 to [3072 x double]**
  %704 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1566, i64 0, i64 0
  %705 = bitcast [8 x i8*]* %.offload_ptrs1566 to [3072 x double]**
  %706 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1565, i64 0, i64 1
  %707 = bitcast i8** %706 to [3072 x double]**
  %708 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1566, i64 0, i64 1
  %709 = bitcast i8** %708 to [3072 x double]**
  %710 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1565, i64 0, i64 2
  %711 = bitcast i8** %710 to [3072 x double]**
  %712 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1566, i64 0, i64 2
  %713 = bitcast i8** %712 to [3072 x double]**
  %714 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1565, i64 0, i64 3
  %715 = bitcast i8** %714 to [3072 x double]**
  %716 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1566, i64 0, i64 3
  %717 = bitcast i8** %716 to [3072 x double]**
  %718 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1565, i64 0, i64 4
  %719 = bitcast i8** %718 to [3072 x double]**
  %720 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1566, i64 0, i64 4
  %721 = bitcast i8** %720 to [3072 x double]**
  %722 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1565, i64 0, i64 5
  %723 = bitcast i8** %722 to i64*
  %724 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1566, i64 0, i64 5
  %725 = bitcast i8** %724 to i64*
  %726 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1565, i64 0, i64 6
  %727 = bitcast i8** %726 to i64*
  %728 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1566, i64 0, i64 6
  %729 = bitcast i8** %728 to i64*
  %730 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1565, i64 0, i64 7
  %731 = bitcast i8** %730 to i64*
  %732 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1566, i64 0, i64 7
  %733 = bitcast i8** %732 to i64*
  %734 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1661, i64 0, i64 0
  %735 = bitcast [8 x i8*]* %.offload_baseptrs1661 to [3072 x double]**
  %736 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1662, i64 0, i64 0
  %737 = bitcast [8 x i8*]* %.offload_ptrs1662 to [3072 x double]**
  %738 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1661, i64 0, i64 1
  %739 = bitcast i8** %738 to [3072 x double]**
  %740 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1662, i64 0, i64 1
  %741 = bitcast i8** %740 to [3072 x double]**
  %742 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1661, i64 0, i64 2
  %743 = bitcast i8** %742 to [3072 x double]**
  %744 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1662, i64 0, i64 2
  %745 = bitcast i8** %744 to [3072 x double]**
  %746 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1661, i64 0, i64 3
  %747 = bitcast i8** %746 to [3072 x double]**
  %748 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1662, i64 0, i64 3
  %749 = bitcast i8** %748 to [3072 x double]**
  %750 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1661, i64 0, i64 4
  %751 = bitcast i8** %750 to [3072 x double]**
  %752 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1662, i64 0, i64 4
  %753 = bitcast i8** %752 to [3072 x double]**
  %754 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1661, i64 0, i64 5
  %755 = bitcast i8** %754 to i64*
  %756 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1662, i64 0, i64 5
  %757 = bitcast i8** %756 to i64*
  %758 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1661, i64 0, i64 6
  %759 = bitcast i8** %758 to i64*
  %760 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1662, i64 0, i64 6
  %761 = bitcast i8** %760 to i64*
  %762 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1661, i64 0, i64 7
  %763 = bitcast i8** %762 to i64*
  %764 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1662, i64 0, i64 7
  %765 = bitcast i8** %764 to i64*
  %766 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1757, i64 0, i64 0
  %767 = bitcast [8 x i8*]* %.offload_baseptrs1757 to [3072 x double]**
  %768 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1758, i64 0, i64 0
  %769 = bitcast [8 x i8*]* %.offload_ptrs1758 to [3072 x double]**
  %770 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1757, i64 0, i64 1
  %771 = bitcast i8** %770 to [3072 x double]**
  %772 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1758, i64 0, i64 1
  %773 = bitcast i8** %772 to [3072 x double]**
  %774 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1757, i64 0, i64 2
  %775 = bitcast i8** %774 to [3072 x double]**
  %776 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1758, i64 0, i64 2
  %777 = bitcast i8** %776 to [3072 x double]**
  %778 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1757, i64 0, i64 3
  %779 = bitcast i8** %778 to [3072 x double]**
  %780 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1758, i64 0, i64 3
  %781 = bitcast i8** %780 to [3072 x double]**
  %782 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1757, i64 0, i64 4
  %783 = bitcast i8** %782 to [3072 x double]**
  %784 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1758, i64 0, i64 4
  %785 = bitcast i8** %784 to [3072 x double]**
  %786 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1757, i64 0, i64 5
  %787 = bitcast i8** %786 to i64*
  %788 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1758, i64 0, i64 5
  %789 = bitcast i8** %788 to i64*
  %790 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1757, i64 0, i64 6
  %791 = bitcast i8** %790 to i64*
  %792 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1758, i64 0, i64 6
  %793 = bitcast i8** %792 to i64*
  %794 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1757, i64 0, i64 7
  %795 = bitcast i8** %794 to i64*
  %796 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1758, i64 0, i64 7
  %797 = bitcast i8** %796 to i64*
  br label %for.cond1223.preheader

for.cond.cleanup611:                              ; preds = %if.end1196
  br i1 %tobool1200, label %for.inc1213, label %for.body1207.preheader

for.body1207.preheader:                           ; preds = %for.cond.cleanup611
  %puts8330 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.1 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.2 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.3 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.4 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.5 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.6 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.7 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.8 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.9 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.10 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.11 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.12 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.13 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.14 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.15 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.16 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8330.17 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %for.inc1213

for.body612:                                      ; preds = %for.cond608.preheader, %if.end1196
  %indvars.iv9786 = phi i64 [ 1, %for.cond608.preheader ], [ %indvars.iv.next9787, %if.end1196 ]
  %tobool12009280 = phi i1 [ %tobool12009278, %for.cond608.preheader ], [ %tobool1200, %if.end1196 ]
  %cond620 = select i1 %tobool12009280, i32 %tms601.09282, i32 1
  %.capture_expr..casted658.sroa.0.0.insert.ext = zext i32 %cond620 to i64
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %316, align 8
  store [3072 x double]* %A, [3072 x double]** %318, align 8
  store [3072 x double]* %C, [3072 x double]** %320, align 8
  store [3072 x double]* %C, [3072 x double]** %322, align 8
  store [3072 x double]* %D, [3072 x double]** %324, align 8
  store [3072 x double]* %D, [3072 x double]** %326, align 8
  store [3072 x double]* %B, [3072 x double]** %328, align 8
  store [3072 x double]* %B, [3072 x double]** %330, align 8
  store [3072 x double]* %E, [3072 x double]** %332, align 8
  store [3072 x double]* %E, [3072 x double]** %334, align 8
  store i64 1, i64* %336, align 8
  store i64 1, i64* %338, align 8
  store i64 %indvars.iv9786, i64* %340, align 8
  store i64 %indvars.iv9786, i64* %342, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %344, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %346, align 8
  %798 = trunc i64 %indvars.iv9786 to i32
  %799 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l196.region_id, i32 8, i8** nonnull %315, i8** nonnull %317, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond620, i32 %798) #6
  %800 = icmp eq i32 %799, 0
  br i1 %800, label %for.body674.preheader, label %omp_offload.failed664

omp_offload.failed664:                            ; preds = %for.body612
  %801 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %802 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %801, i32 %cond620, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..34 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9786) #6
  br label %for.body674.preheader

for.body674.preheader:                            ; preds = %for.body612, %omp_offload.failed664
  br label %for.body674

for.cond.cleanup673:                              ; preds = %for.body674
  %add686 = fadd double %add681.5, 0.000000e+00
  %cmp694 = fcmp une double %add686, 0x4152018000000000
  br i1 %cmp694, label %if.then712, label %if.else714

for.body674:                                      ; preds = %for.body674.preheader, %for.body674
  %indvars.iv9753 = phi i64 [ %indvars.iv.next9754.5, %for.body674 ], [ 0, %for.body674.preheader ]
  %tmp668.09232 = phi double [ %add681.5, %for.body674 ], [ 0.000000e+00, %for.body674.preheader ]
  %arrayidx676 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9753
  %803 = load double, double* %arrayidx676, align 8, !tbaa !62
  %arrayidx678 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9753
  %804 = load double, double* %arrayidx678, align 8, !tbaa !62
  %add679 = fadd double %803, %804
  %sub680 = fadd double %add679, -1.000000e+00
  %add681 = fadd double %tmp668.09232, %sub680
  %indvars.iv.next9754 = or i64 %indvars.iv9753, 1
  %arrayidx676.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9754
  %805 = load double, double* %arrayidx676.1, align 8, !tbaa !62
  %arrayidx678.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9754
  %806 = load double, double* %arrayidx678.1, align 8, !tbaa !62
  %add679.1 = fadd double %805, %806
  %sub680.1 = fadd double %add679.1, -1.000000e+00
  %add681.1 = fadd double %add681, %sub680.1
  %indvars.iv.next9754.1 = add nuw nsw i64 %indvars.iv9753, 2
  %arrayidx676.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9754.1
  %807 = load double, double* %arrayidx676.2, align 8, !tbaa !62
  %arrayidx678.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9754.1
  %808 = load double, double* %arrayidx678.2, align 8, !tbaa !62
  %add679.2 = fadd double %807, %808
  %sub680.2 = fadd double %add679.2, -1.000000e+00
  %add681.2 = fadd double %add681.1, %sub680.2
  %indvars.iv.next9754.2 = add nuw nsw i64 %indvars.iv9753, 3
  %arrayidx676.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9754.2
  %809 = load double, double* %arrayidx676.3, align 8, !tbaa !62
  %arrayidx678.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9754.2
  %810 = load double, double* %arrayidx678.3, align 8, !tbaa !62
  %add679.3 = fadd double %809, %810
  %sub680.3 = fadd double %add679.3, -1.000000e+00
  %add681.3 = fadd double %add681.2, %sub680.3
  %indvars.iv.next9754.3 = add nuw nsw i64 %indvars.iv9753, 4
  %arrayidx676.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9754.3
  %811 = load double, double* %arrayidx676.4, align 8, !tbaa !62
  %arrayidx678.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9754.3
  %812 = load double, double* %arrayidx678.4, align 8, !tbaa !62
  %add679.4 = fadd double %811, %812
  %sub680.4 = fadd double %add679.4, -1.000000e+00
  %add681.4 = fadd double %add681.3, %sub680.4
  %indvars.iv.next9754.4 = add nuw nsw i64 %indvars.iv9753, 5
  %arrayidx676.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9754.4
  %813 = load double, double* %arrayidx676.5, align 8, !tbaa !62
  %arrayidx678.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9754.4
  %814 = load double, double* %arrayidx678.5, align 8, !tbaa !62
  %add679.5 = fadd double %813, %814
  %sub680.5 = fadd double %add679.5, -1.000000e+00
  %add681.5 = fadd double %add681.4, %sub680.5
  %indvars.iv.next9754.5 = add nuw nsw i64 %indvars.iv9753, 6
  %exitcond9755.5 = icmp eq i64 %indvars.iv.next9754.5, 3072
  br i1 %exitcond9755.5, label %for.cond.cleanup673, label %for.body674

if.then712:                                       ; preds = %for.cond.cleanup673
  %call698.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add686)
  %puts8342 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end716

if.else714:                                       ; preds = %for.cond.cleanup673
  %puts8331 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end716

if.end716:                                        ; preds = %if.else714, %if.then712
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %348, align 8
  store [3072 x double]* %A, [3072 x double]** %350, align 8
  store [3072 x double]* %C, [3072 x double]** %352, align 8
  store [3072 x double]* %C, [3072 x double]** %354, align 8
  store [3072 x double]* %D, [3072 x double]** %356, align 8
  store [3072 x double]* %D, [3072 x double]** %358, align 8
  store [3072 x double]* %B, [3072 x double]** %360, align 8
  store [3072 x double]* %B, [3072 x double]** %362, align 8
  store [3072 x double]* %E, [3072 x double]** %364, align 8
  store [3072 x double]* %E, [3072 x double]** %366, align 8
  store i64 1, i64* %368, align 8
  store i64 1, i64* %370, align 8
  store i64 %indvars.iv9786, i64* %372, align 8
  store i64 %indvars.iv9786, i64* %374, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %376, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %378, align 8
  %815 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l216.region_id, i32 8, i8** nonnull %347, i8** nonnull %349, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond620, i32 %798) #6
  %816 = icmp eq i32 %815, 0
  br i1 %816, label %for.body770.preheader, label %omp_offload.failed760

omp_offload.failed760:                            ; preds = %if.end716
  %817 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %818 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %817, i32 %cond620, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..38 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9786) #6
  br label %for.body770.preheader

for.body770.preheader:                            ; preds = %if.end716, %omp_offload.failed760
  br label %for.body770

for.cond.cleanup769:                              ; preds = %for.body770
  %add782 = fadd double %add777.5, 0.000000e+00
  %cmp790 = fcmp une double %add782, 0x4152018000000000
  br i1 %cmp790, label %if.then808, label %if.else810

for.body770:                                      ; preds = %for.body770.preheader, %for.body770
  %indvars.iv9759 = phi i64 [ %indvars.iv.next9760.5, %for.body770 ], [ 0, %for.body770.preheader ]
  %tmp764.09240 = phi double [ %add777.5, %for.body770 ], [ 0.000000e+00, %for.body770.preheader ]
  %arrayidx772 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9759
  %819 = load double, double* %arrayidx772, align 8, !tbaa !62
  %arrayidx774 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9759
  %820 = load double, double* %arrayidx774, align 8, !tbaa !62
  %add775 = fadd double %819, %820
  %sub776 = fadd double %add775, -1.000000e+00
  %add777 = fadd double %tmp764.09240, %sub776
  %indvars.iv.next9760 = or i64 %indvars.iv9759, 1
  %arrayidx772.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9760
  %821 = load double, double* %arrayidx772.1, align 8, !tbaa !62
  %arrayidx774.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9760
  %822 = load double, double* %arrayidx774.1, align 8, !tbaa !62
  %add775.1 = fadd double %821, %822
  %sub776.1 = fadd double %add775.1, -1.000000e+00
  %add777.1 = fadd double %add777, %sub776.1
  %indvars.iv.next9760.1 = add nuw nsw i64 %indvars.iv9759, 2
  %arrayidx772.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9760.1
  %823 = load double, double* %arrayidx772.2, align 8, !tbaa !62
  %arrayidx774.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9760.1
  %824 = load double, double* %arrayidx774.2, align 8, !tbaa !62
  %add775.2 = fadd double %823, %824
  %sub776.2 = fadd double %add775.2, -1.000000e+00
  %add777.2 = fadd double %add777.1, %sub776.2
  %indvars.iv.next9760.2 = add nuw nsw i64 %indvars.iv9759, 3
  %arrayidx772.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9760.2
  %825 = load double, double* %arrayidx772.3, align 8, !tbaa !62
  %arrayidx774.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9760.2
  %826 = load double, double* %arrayidx774.3, align 8, !tbaa !62
  %add775.3 = fadd double %825, %826
  %sub776.3 = fadd double %add775.3, -1.000000e+00
  %add777.3 = fadd double %add777.2, %sub776.3
  %indvars.iv.next9760.3 = add nuw nsw i64 %indvars.iv9759, 4
  %arrayidx772.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9760.3
  %827 = load double, double* %arrayidx772.4, align 8, !tbaa !62
  %arrayidx774.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9760.3
  %828 = load double, double* %arrayidx774.4, align 8, !tbaa !62
  %add775.4 = fadd double %827, %828
  %sub776.4 = fadd double %add775.4, -1.000000e+00
  %add777.4 = fadd double %add777.3, %sub776.4
  %indvars.iv.next9760.4 = add nuw nsw i64 %indvars.iv9759, 5
  %arrayidx772.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9760.4
  %829 = load double, double* %arrayidx772.5, align 8, !tbaa !62
  %arrayidx774.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9760.4
  %830 = load double, double* %arrayidx774.5, align 8, !tbaa !62
  %add775.5 = fadd double %829, %830
  %sub776.5 = fadd double %add775.5, -1.000000e+00
  %add777.5 = fadd double %add777.4, %sub776.5
  %indvars.iv.next9760.5 = add nuw nsw i64 %indvars.iv9759, 6
  %exitcond9761.5 = icmp eq i64 %indvars.iv.next9760.5, 3072
  br i1 %exitcond9761.5, label %for.cond.cleanup769, label %for.body770

if.then808:                                       ; preds = %for.cond.cleanup769
  %call794.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add782)
  %puts8341 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end812

if.else810:                                       ; preds = %for.cond.cleanup769
  %puts8332 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end812

if.end812:                                        ; preds = %if.else810, %if.then808
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %380, align 8
  store [3072 x double]* %A, [3072 x double]** %382, align 8
  store [3072 x double]* %C, [3072 x double]** %384, align 8
  store [3072 x double]* %C, [3072 x double]** %386, align 8
  store [3072 x double]* %D, [3072 x double]** %388, align 8
  store [3072 x double]* %D, [3072 x double]** %390, align 8
  store [3072 x double]* %B, [3072 x double]** %392, align 8
  store [3072 x double]* %B, [3072 x double]** %394, align 8
  store [3072 x double]* %E, [3072 x double]** %396, align 8
  store [3072 x double]* %E, [3072 x double]** %398, align 8
  store i64 1, i64* %400, align 8
  store i64 1, i64* %402, align 8
  store i64 %indvars.iv9786, i64* %404, align 8
  store i64 %indvars.iv9786, i64* %406, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %408, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %410, align 8
  %831 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l236.region_id, i32 8, i8** nonnull %379, i8** nonnull %381, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond620, i32 %798) #6
  %832 = icmp eq i32 %831, 0
  br i1 %832, label %for.body866.preheader, label %omp_offload.failed856

omp_offload.failed856:                            ; preds = %if.end812
  %833 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %834 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %833, i32 %cond620, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..42 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9786) #6
  br label %for.body866.preheader

for.body866.preheader:                            ; preds = %if.end812, %omp_offload.failed856
  br label %for.body866

for.cond.cleanup865:                              ; preds = %for.body866
  %add878 = fadd double %add873.5, 0.000000e+00
  %cmp886 = fcmp une double %add878, 0x4152018000000000
  br i1 %cmp886, label %if.then904, label %if.else906

for.body866:                                      ; preds = %for.body866.preheader, %for.body866
  %indvars.iv9765 = phi i64 [ %indvars.iv.next9766.5, %for.body866 ], [ 0, %for.body866.preheader ]
  %tmp860.09248 = phi double [ %add873.5, %for.body866 ], [ 0.000000e+00, %for.body866.preheader ]
  %arrayidx868 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9765
  %835 = load double, double* %arrayidx868, align 8, !tbaa !62
  %arrayidx870 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9765
  %836 = load double, double* %arrayidx870, align 8, !tbaa !62
  %add871 = fadd double %835, %836
  %sub872 = fadd double %add871, -1.000000e+00
  %add873 = fadd double %tmp860.09248, %sub872
  %indvars.iv.next9766 = or i64 %indvars.iv9765, 1
  %arrayidx868.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9766
  %837 = load double, double* %arrayidx868.1, align 8, !tbaa !62
  %arrayidx870.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9766
  %838 = load double, double* %arrayidx870.1, align 8, !tbaa !62
  %add871.1 = fadd double %837, %838
  %sub872.1 = fadd double %add871.1, -1.000000e+00
  %add873.1 = fadd double %add873, %sub872.1
  %indvars.iv.next9766.1 = add nuw nsw i64 %indvars.iv9765, 2
  %arrayidx868.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9766.1
  %839 = load double, double* %arrayidx868.2, align 8, !tbaa !62
  %arrayidx870.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9766.1
  %840 = load double, double* %arrayidx870.2, align 8, !tbaa !62
  %add871.2 = fadd double %839, %840
  %sub872.2 = fadd double %add871.2, -1.000000e+00
  %add873.2 = fadd double %add873.1, %sub872.2
  %indvars.iv.next9766.2 = add nuw nsw i64 %indvars.iv9765, 3
  %arrayidx868.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9766.2
  %841 = load double, double* %arrayidx868.3, align 8, !tbaa !62
  %arrayidx870.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9766.2
  %842 = load double, double* %arrayidx870.3, align 8, !tbaa !62
  %add871.3 = fadd double %841, %842
  %sub872.3 = fadd double %add871.3, -1.000000e+00
  %add873.3 = fadd double %add873.2, %sub872.3
  %indvars.iv.next9766.3 = add nuw nsw i64 %indvars.iv9765, 4
  %arrayidx868.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9766.3
  %843 = load double, double* %arrayidx868.4, align 8, !tbaa !62
  %arrayidx870.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9766.3
  %844 = load double, double* %arrayidx870.4, align 8, !tbaa !62
  %add871.4 = fadd double %843, %844
  %sub872.4 = fadd double %add871.4, -1.000000e+00
  %add873.4 = fadd double %add873.3, %sub872.4
  %indvars.iv.next9766.4 = add nuw nsw i64 %indvars.iv9765, 5
  %arrayidx868.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9766.4
  %845 = load double, double* %arrayidx868.5, align 8, !tbaa !62
  %arrayidx870.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9766.4
  %846 = load double, double* %arrayidx870.5, align 8, !tbaa !62
  %add871.5 = fadd double %845, %846
  %sub872.5 = fadd double %add871.5, -1.000000e+00
  %add873.5 = fadd double %add873.4, %sub872.5
  %indvars.iv.next9766.5 = add nuw nsw i64 %indvars.iv9765, 6
  %exitcond9767.5 = icmp eq i64 %indvars.iv.next9766.5, 3072
  br i1 %exitcond9767.5, label %for.cond.cleanup865, label %for.body866

if.then904:                                       ; preds = %for.cond.cleanup865
  %call890.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add878)
  %puts8340 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end908

if.else906:                                       ; preds = %for.cond.cleanup865
  %puts8333 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end908

if.end908:                                        ; preds = %if.else906, %if.then904
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %412, align 8
  store [3072 x double]* %A, [3072 x double]** %414, align 8
  store [3072 x double]* %C, [3072 x double]** %416, align 8
  store [3072 x double]* %C, [3072 x double]** %418, align 8
  store [3072 x double]* %D, [3072 x double]** %420, align 8
  store [3072 x double]* %D, [3072 x double]** %422, align 8
  store [3072 x double]* %B, [3072 x double]** %424, align 8
  store [3072 x double]* %B, [3072 x double]** %426, align 8
  store [3072 x double]* %E, [3072 x double]** %428, align 8
  store [3072 x double]* %E, [3072 x double]** %430, align 8
  store i64 1, i64* %432, align 8
  store i64 1, i64* %434, align 8
  store i64 %indvars.iv9786, i64* %436, align 8
  store i64 %indvars.iv9786, i64* %438, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %440, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %442, align 8
  %847 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l256.region_id, i32 8, i8** nonnull %411, i8** nonnull %413, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond620, i32 %798) #6
  %848 = icmp eq i32 %847, 0
  br i1 %848, label %for.body962.preheader, label %omp_offload.failed952

omp_offload.failed952:                            ; preds = %if.end908
  %849 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %850 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %849, i32 %cond620, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..46 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9786) #6
  br label %for.body962.preheader

for.body962.preheader:                            ; preds = %if.end908, %omp_offload.failed952
  br label %for.body962

for.cond.cleanup961:                              ; preds = %for.body962
  %add974 = fadd double %add969.5, 0.000000e+00
  %cmp982 = fcmp une double %add974, 0x4152018000000000
  br i1 %cmp982, label %if.then1000, label %if.else1002

for.body962:                                      ; preds = %for.body962.preheader, %for.body962
  %indvars.iv9771 = phi i64 [ %indvars.iv.next9772.5, %for.body962 ], [ 0, %for.body962.preheader ]
  %tmp956.09256 = phi double [ %add969.5, %for.body962 ], [ 0.000000e+00, %for.body962.preheader ]
  %arrayidx964 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9771
  %851 = load double, double* %arrayidx964, align 8, !tbaa !62
  %arrayidx966 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9771
  %852 = load double, double* %arrayidx966, align 8, !tbaa !62
  %add967 = fadd double %851, %852
  %sub968 = fadd double %add967, -1.000000e+00
  %add969 = fadd double %tmp956.09256, %sub968
  %indvars.iv.next9772 = or i64 %indvars.iv9771, 1
  %arrayidx964.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9772
  %853 = load double, double* %arrayidx964.1, align 8, !tbaa !62
  %arrayidx966.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9772
  %854 = load double, double* %arrayidx966.1, align 8, !tbaa !62
  %add967.1 = fadd double %853, %854
  %sub968.1 = fadd double %add967.1, -1.000000e+00
  %add969.1 = fadd double %add969, %sub968.1
  %indvars.iv.next9772.1 = add nuw nsw i64 %indvars.iv9771, 2
  %arrayidx964.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9772.1
  %855 = load double, double* %arrayidx964.2, align 8, !tbaa !62
  %arrayidx966.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9772.1
  %856 = load double, double* %arrayidx966.2, align 8, !tbaa !62
  %add967.2 = fadd double %855, %856
  %sub968.2 = fadd double %add967.2, -1.000000e+00
  %add969.2 = fadd double %add969.1, %sub968.2
  %indvars.iv.next9772.2 = add nuw nsw i64 %indvars.iv9771, 3
  %arrayidx964.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9772.2
  %857 = load double, double* %arrayidx964.3, align 8, !tbaa !62
  %arrayidx966.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9772.2
  %858 = load double, double* %arrayidx966.3, align 8, !tbaa !62
  %add967.3 = fadd double %857, %858
  %sub968.3 = fadd double %add967.3, -1.000000e+00
  %add969.3 = fadd double %add969.2, %sub968.3
  %indvars.iv.next9772.3 = add nuw nsw i64 %indvars.iv9771, 4
  %arrayidx964.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9772.3
  %859 = load double, double* %arrayidx964.4, align 8, !tbaa !62
  %arrayidx966.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9772.3
  %860 = load double, double* %arrayidx966.4, align 8, !tbaa !62
  %add967.4 = fadd double %859, %860
  %sub968.4 = fadd double %add967.4, -1.000000e+00
  %add969.4 = fadd double %add969.3, %sub968.4
  %indvars.iv.next9772.4 = add nuw nsw i64 %indvars.iv9771, 5
  %arrayidx964.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9772.4
  %861 = load double, double* %arrayidx964.5, align 8, !tbaa !62
  %arrayidx966.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9772.4
  %862 = load double, double* %arrayidx966.5, align 8, !tbaa !62
  %add967.5 = fadd double %861, %862
  %sub968.5 = fadd double %add967.5, -1.000000e+00
  %add969.5 = fadd double %add969.4, %sub968.5
  %indvars.iv.next9772.5 = add nuw nsw i64 %indvars.iv9771, 6
  %exitcond9773.5 = icmp eq i64 %indvars.iv.next9772.5, 3072
  br i1 %exitcond9773.5, label %for.cond.cleanup961, label %for.body962

if.then1000:                                      ; preds = %for.cond.cleanup961
  %call986.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add974)
  %puts8339 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end1004

if.else1002:                                      ; preds = %for.cond.cleanup961
  %puts8334 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end1004

if.end1004:                                       ; preds = %if.else1002, %if.then1000
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %444, align 8
  store [3072 x double]* %A, [3072 x double]** %446, align 8
  store [3072 x double]* %C, [3072 x double]** %448, align 8
  store [3072 x double]* %C, [3072 x double]** %450, align 8
  store [3072 x double]* %D, [3072 x double]** %452, align 8
  store [3072 x double]* %D, [3072 x double]** %454, align 8
  store [3072 x double]* %B, [3072 x double]** %456, align 8
  store [3072 x double]* %B, [3072 x double]** %458, align 8
  store [3072 x double]* %E, [3072 x double]** %460, align 8
  store [3072 x double]* %E, [3072 x double]** %462, align 8
  store i64 1, i64* %464, align 8
  store i64 1, i64* %466, align 8
  store i64 %indvars.iv9786, i64* %468, align 8
  store i64 %indvars.iv9786, i64* %470, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %472, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %474, align 8
  %863 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l276.region_id, i32 8, i8** nonnull %443, i8** nonnull %445, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond620, i32 %798) #6
  %864 = icmp eq i32 %863, 0
  br i1 %864, label %for.body1058.preheader, label %omp_offload.failed1048

omp_offload.failed1048:                           ; preds = %if.end1004
  %865 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %866 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %865, i32 %cond620, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..50 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9786) #6
  br label %for.body1058.preheader

for.body1058.preheader:                           ; preds = %if.end1004, %omp_offload.failed1048
  br label %for.body1058

for.cond.cleanup1057:                             ; preds = %for.body1058
  %add1070 = fadd double %add1065.5, 0.000000e+00
  %cmp1078 = fcmp une double %add1070, 0x4152018000000000
  br i1 %cmp1078, label %if.then1096, label %if.else1098

for.body1058:                                     ; preds = %for.body1058.preheader, %for.body1058
  %indvars.iv9777 = phi i64 [ %indvars.iv.next9778.5, %for.body1058 ], [ 0, %for.body1058.preheader ]
  %tmp1052.09264 = phi double [ %add1065.5, %for.body1058 ], [ 0.000000e+00, %for.body1058.preheader ]
  %arrayidx1060 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9777
  %867 = load double, double* %arrayidx1060, align 8, !tbaa !62
  %arrayidx1062 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9777
  %868 = load double, double* %arrayidx1062, align 8, !tbaa !62
  %add1063 = fadd double %867, %868
  %sub1064 = fadd double %add1063, -1.000000e+00
  %add1065 = fadd double %tmp1052.09264, %sub1064
  %indvars.iv.next9778 = or i64 %indvars.iv9777, 1
  %arrayidx1060.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9778
  %869 = load double, double* %arrayidx1060.1, align 8, !tbaa !62
  %arrayidx1062.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9778
  %870 = load double, double* %arrayidx1062.1, align 8, !tbaa !62
  %add1063.1 = fadd double %869, %870
  %sub1064.1 = fadd double %add1063.1, -1.000000e+00
  %add1065.1 = fadd double %add1065, %sub1064.1
  %indvars.iv.next9778.1 = add nuw nsw i64 %indvars.iv9777, 2
  %arrayidx1060.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9778.1
  %871 = load double, double* %arrayidx1060.2, align 8, !tbaa !62
  %arrayidx1062.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9778.1
  %872 = load double, double* %arrayidx1062.2, align 8, !tbaa !62
  %add1063.2 = fadd double %871, %872
  %sub1064.2 = fadd double %add1063.2, -1.000000e+00
  %add1065.2 = fadd double %add1065.1, %sub1064.2
  %indvars.iv.next9778.2 = add nuw nsw i64 %indvars.iv9777, 3
  %arrayidx1060.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9778.2
  %873 = load double, double* %arrayidx1060.3, align 8, !tbaa !62
  %arrayidx1062.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9778.2
  %874 = load double, double* %arrayidx1062.3, align 8, !tbaa !62
  %add1063.3 = fadd double %873, %874
  %sub1064.3 = fadd double %add1063.3, -1.000000e+00
  %add1065.3 = fadd double %add1065.2, %sub1064.3
  %indvars.iv.next9778.3 = add nuw nsw i64 %indvars.iv9777, 4
  %arrayidx1060.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9778.3
  %875 = load double, double* %arrayidx1060.4, align 8, !tbaa !62
  %arrayidx1062.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9778.3
  %876 = load double, double* %arrayidx1062.4, align 8, !tbaa !62
  %add1063.4 = fadd double %875, %876
  %sub1064.4 = fadd double %add1063.4, -1.000000e+00
  %add1065.4 = fadd double %add1065.3, %sub1064.4
  %indvars.iv.next9778.4 = add nuw nsw i64 %indvars.iv9777, 5
  %arrayidx1060.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9778.4
  %877 = load double, double* %arrayidx1060.5, align 8, !tbaa !62
  %arrayidx1062.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9778.4
  %878 = load double, double* %arrayidx1062.5, align 8, !tbaa !62
  %add1063.5 = fadd double %877, %878
  %sub1064.5 = fadd double %add1063.5, -1.000000e+00
  %add1065.5 = fadd double %add1065.4, %sub1064.5
  %indvars.iv.next9778.5 = add nuw nsw i64 %indvars.iv9777, 6
  %exitcond9779.5 = icmp eq i64 %indvars.iv.next9778.5, 3072
  br i1 %exitcond9779.5, label %for.cond.cleanup1057, label %for.body1058

if.then1096:                                      ; preds = %for.cond.cleanup1057
  %call1082.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add1070)
  %puts8338 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end1100

if.else1098:                                      ; preds = %for.cond.cleanup1057
  %puts8335 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end1100

if.end1100:                                       ; preds = %if.else1098, %if.then1096
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %476, align 8
  store [3072 x double]* %A, [3072 x double]** %478, align 8
  store [3072 x double]* %C, [3072 x double]** %480, align 8
  store [3072 x double]* %C, [3072 x double]** %482, align 8
  store [3072 x double]* %D, [3072 x double]** %484, align 8
  store [3072 x double]* %D, [3072 x double]** %486, align 8
  store [3072 x double]* %B, [3072 x double]** %488, align 8
  store [3072 x double]* %B, [3072 x double]** %490, align 8
  store [3072 x double]* %E, [3072 x double]** %492, align 8
  store [3072 x double]* %E, [3072 x double]** %494, align 8
  store i64 1, i64* %496, align 8
  store i64 1, i64* %498, align 8
  store i64 %indvars.iv9786, i64* %500, align 8
  store i64 %indvars.iv9786, i64* %502, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %504, align 8
  store i64 %.capture_expr..casted658.sroa.0.0.insert.ext, i64* %506, align 8
  %879 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l296.region_id, i32 8, i8** nonnull %475, i8** nonnull %477, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond620, i32 %798) #6
  %880 = icmp eq i32 %879, 0
  br i1 %880, label %for.body1154.preheader, label %omp_offload.failed1144

omp_offload.failed1144:                           ; preds = %if.end1100
  %881 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %882 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %881, i32 %cond620, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..54 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9786) #6
  br label %for.body1154.preheader

for.body1154.preheader:                           ; preds = %if.end1100, %omp_offload.failed1144
  br label %for.body1154

for.cond.cleanup1153:                             ; preds = %for.body1154
  %add1166 = fadd double %add1161.5, 0.000000e+00
  %cmp1174 = fcmp une double %add1166, 0x4152018000000000
  br i1 %cmp1174, label %if.then1192, label %if.else1194

for.body1154:                                     ; preds = %for.body1154.preheader, %for.body1154
  %indvars.iv9783 = phi i64 [ %indvars.iv.next9784.5, %for.body1154 ], [ 0, %for.body1154.preheader ]
  %tmp1148.09272 = phi double [ %add1161.5, %for.body1154 ], [ 0.000000e+00, %for.body1154.preheader ]
  %arrayidx1156 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9783
  %883 = load double, double* %arrayidx1156, align 8, !tbaa !62
  %arrayidx1158 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9783
  %884 = load double, double* %arrayidx1158, align 8, !tbaa !62
  %add1159 = fadd double %883, %884
  %sub1160 = fadd double %add1159, -1.000000e+00
  %add1161 = fadd double %tmp1148.09272, %sub1160
  %indvars.iv.next9784 = or i64 %indvars.iv9783, 1
  %arrayidx1156.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9784
  %885 = load double, double* %arrayidx1156.1, align 8, !tbaa !62
  %arrayidx1158.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9784
  %886 = load double, double* %arrayidx1158.1, align 8, !tbaa !62
  %add1159.1 = fadd double %885, %886
  %sub1160.1 = fadd double %add1159.1, -1.000000e+00
  %add1161.1 = fadd double %add1161, %sub1160.1
  %indvars.iv.next9784.1 = add nuw nsw i64 %indvars.iv9783, 2
  %arrayidx1156.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9784.1
  %887 = load double, double* %arrayidx1156.2, align 8, !tbaa !62
  %arrayidx1158.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9784.1
  %888 = load double, double* %arrayidx1158.2, align 8, !tbaa !62
  %add1159.2 = fadd double %887, %888
  %sub1160.2 = fadd double %add1159.2, -1.000000e+00
  %add1161.2 = fadd double %add1161.1, %sub1160.2
  %indvars.iv.next9784.2 = add nuw nsw i64 %indvars.iv9783, 3
  %arrayidx1156.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9784.2
  %889 = load double, double* %arrayidx1156.3, align 8, !tbaa !62
  %arrayidx1158.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9784.2
  %890 = load double, double* %arrayidx1158.3, align 8, !tbaa !62
  %add1159.3 = fadd double %889, %890
  %sub1160.3 = fadd double %add1159.3, -1.000000e+00
  %add1161.3 = fadd double %add1161.2, %sub1160.3
  %indvars.iv.next9784.3 = add nuw nsw i64 %indvars.iv9783, 4
  %arrayidx1156.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9784.3
  %891 = load double, double* %arrayidx1156.4, align 8, !tbaa !62
  %arrayidx1158.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9784.3
  %892 = load double, double* %arrayidx1158.4, align 8, !tbaa !62
  %add1159.4 = fadd double %891, %892
  %sub1160.4 = fadd double %add1159.4, -1.000000e+00
  %add1161.4 = fadd double %add1161.3, %sub1160.4
  %indvars.iv.next9784.4 = add nuw nsw i64 %indvars.iv9783, 5
  %arrayidx1156.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9784.4
  %893 = load double, double* %arrayidx1156.5, align 8, !tbaa !62
  %arrayidx1158.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9784.4
  %894 = load double, double* %arrayidx1158.5, align 8, !tbaa !62
  %add1159.5 = fadd double %893, %894
  %sub1160.5 = fadd double %add1159.5, -1.000000e+00
  %add1161.5 = fadd double %add1161.4, %sub1160.5
  %indvars.iv.next9784.5 = add nuw nsw i64 %indvars.iv9783, 6
  %exitcond9785.5 = icmp eq i64 %indvars.iv.next9784.5, 3072
  br i1 %exitcond9785.5, label %for.cond.cleanup1153, label %for.body1154

if.then1192:                                      ; preds = %for.cond.cleanup1153
  %call1178.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add1166)
  %puts8337 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end1196

if.else1194:                                      ; preds = %for.cond.cleanup1153
  %puts8336 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end1196

if.end1196:                                       ; preds = %if.else1194, %if.then1192
  %indvars.iv.next9787 = add nuw nsw i64 %indvars.iv9786, 78
  %cmp609 = icmp ugt i64 %indvars.iv.next9787, %cond
  %895 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool1200 = icmp eq i32 %895, 0
  br i1 %cmp609, label %for.cond.cleanup611, label %for.body612

for.inc1213:                                      ; preds = %for.body1207.preheader, %for.cond.cleanup611
  %mul1214 = mul nsw i32 %tms601.09282, 3
  %cmp603 = icmp ult i32 %mul1214, 257
  br i1 %cmp603, label %for.cond608.preheader, label %for.cond1217.preheader

for.cond1223.preheader:                           ; preds = %for.cond1217.preheader, %for.inc1828
  %tms1216.09229 = phi i32 [ 1, %for.cond1217.preheader ], [ %mul1829, %for.inc1828 ]
  %896 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool18159225 = icmp eq i32 %896, 0
  br label %for.body1227

for.cond1832.preheader:                           ; preds = %for.inc1828
  %897 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1893, i64 0, i64 0
  %898 = bitcast [8 x i8*]* %.offload_baseptrs1893 to [3072 x double]**
  %899 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1894, i64 0, i64 0
  %900 = bitcast [8 x i8*]* %.offload_ptrs1894 to [3072 x double]**
  %901 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1893, i64 0, i64 1
  %902 = bitcast i8** %901 to [3072 x double]**
  %903 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1894, i64 0, i64 1
  %904 = bitcast i8** %903 to [3072 x double]**
  %905 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1893, i64 0, i64 2
  %906 = bitcast i8** %905 to [3072 x double]**
  %907 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1894, i64 0, i64 2
  %908 = bitcast i8** %907 to [3072 x double]**
  %909 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1893, i64 0, i64 3
  %910 = bitcast i8** %909 to [3072 x double]**
  %911 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1894, i64 0, i64 3
  %912 = bitcast i8** %911 to [3072 x double]**
  %913 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1893, i64 0, i64 4
  %914 = bitcast i8** %913 to [3072 x double]**
  %915 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1894, i64 0, i64 4
  %916 = bitcast i8** %915 to [3072 x double]**
  %917 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1893, i64 0, i64 5
  %918 = bitcast i8** %917 to i64*
  %919 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1894, i64 0, i64 5
  %920 = bitcast i8** %919 to i64*
  %921 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1893, i64 0, i64 6
  %922 = bitcast i8** %921 to i64*
  %923 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1894, i64 0, i64 6
  %924 = bitcast i8** %923 to i64*
  %925 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1893, i64 0, i64 7
  %926 = bitcast i8** %925 to i64*
  %927 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1894, i64 0, i64 7
  %928 = bitcast i8** %927 to i64*
  %929 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1992, i64 0, i64 0
  %930 = bitcast [8 x i8*]* %.offload_baseptrs1992 to [3072 x double]**
  %931 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1993, i64 0, i64 0
  %932 = bitcast [8 x i8*]* %.offload_ptrs1993 to [3072 x double]**
  %933 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1992, i64 0, i64 1
  %934 = bitcast i8** %933 to [3072 x double]**
  %935 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1993, i64 0, i64 1
  %936 = bitcast i8** %935 to [3072 x double]**
  %937 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1992, i64 0, i64 2
  %938 = bitcast i8** %937 to [3072 x double]**
  %939 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1993, i64 0, i64 2
  %940 = bitcast i8** %939 to [3072 x double]**
  %941 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1992, i64 0, i64 3
  %942 = bitcast i8** %941 to [3072 x double]**
  %943 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1993, i64 0, i64 3
  %944 = bitcast i8** %943 to [3072 x double]**
  %945 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1992, i64 0, i64 4
  %946 = bitcast i8** %945 to [3072 x double]**
  %947 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1993, i64 0, i64 4
  %948 = bitcast i8** %947 to [3072 x double]**
  %949 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1992, i64 0, i64 5
  %950 = bitcast i8** %949 to i64*
  %951 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1993, i64 0, i64 5
  %952 = bitcast i8** %951 to i64*
  %953 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1992, i64 0, i64 6
  %954 = bitcast i8** %953 to i64*
  %955 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1993, i64 0, i64 6
  %956 = bitcast i8** %955 to i64*
  %957 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs1992, i64 0, i64 7
  %958 = bitcast i8** %957 to i64*
  %959 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs1993, i64 0, i64 7
  %960 = bitcast i8** %959 to i64*
  %961 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2091, i64 0, i64 0
  %962 = bitcast [8 x i8*]* %.offload_baseptrs2091 to [3072 x double]**
  %963 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2092, i64 0, i64 0
  %964 = bitcast [8 x i8*]* %.offload_ptrs2092 to [3072 x double]**
  %965 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2091, i64 0, i64 1
  %966 = bitcast i8** %965 to [3072 x double]**
  %967 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2092, i64 0, i64 1
  %968 = bitcast i8** %967 to [3072 x double]**
  %969 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2091, i64 0, i64 2
  %970 = bitcast i8** %969 to [3072 x double]**
  %971 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2092, i64 0, i64 2
  %972 = bitcast i8** %971 to [3072 x double]**
  %973 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2091, i64 0, i64 3
  %974 = bitcast i8** %973 to [3072 x double]**
  %975 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2092, i64 0, i64 3
  %976 = bitcast i8** %975 to [3072 x double]**
  %977 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2091, i64 0, i64 4
  %978 = bitcast i8** %977 to [3072 x double]**
  %979 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2092, i64 0, i64 4
  %980 = bitcast i8** %979 to [3072 x double]**
  %981 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2091, i64 0, i64 5
  %982 = bitcast i8** %981 to i64*
  %983 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2092, i64 0, i64 5
  %984 = bitcast i8** %983 to i64*
  %985 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2091, i64 0, i64 6
  %986 = bitcast i8** %985 to i64*
  %987 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2092, i64 0, i64 6
  %988 = bitcast i8** %987 to i64*
  %989 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2091, i64 0, i64 7
  %990 = bitcast i8** %989 to i64*
  %991 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2092, i64 0, i64 7
  %992 = bitcast i8** %991 to i64*
  %993 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2190, i64 0, i64 0
  %994 = bitcast [8 x i8*]* %.offload_baseptrs2190 to [3072 x double]**
  %995 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2191, i64 0, i64 0
  %996 = bitcast [8 x i8*]* %.offload_ptrs2191 to [3072 x double]**
  %997 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2190, i64 0, i64 1
  %998 = bitcast i8** %997 to [3072 x double]**
  %999 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2191, i64 0, i64 1
  %1000 = bitcast i8** %999 to [3072 x double]**
  %1001 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2190, i64 0, i64 2
  %1002 = bitcast i8** %1001 to [3072 x double]**
  %1003 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2191, i64 0, i64 2
  %1004 = bitcast i8** %1003 to [3072 x double]**
  %1005 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2190, i64 0, i64 3
  %1006 = bitcast i8** %1005 to [3072 x double]**
  %1007 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2191, i64 0, i64 3
  %1008 = bitcast i8** %1007 to [3072 x double]**
  %1009 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2190, i64 0, i64 4
  %1010 = bitcast i8** %1009 to [3072 x double]**
  %1011 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2191, i64 0, i64 4
  %1012 = bitcast i8** %1011 to [3072 x double]**
  %1013 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2190, i64 0, i64 5
  %1014 = bitcast i8** %1013 to i64*
  %1015 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2191, i64 0, i64 5
  %1016 = bitcast i8** %1015 to i64*
  %1017 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2190, i64 0, i64 6
  %1018 = bitcast i8** %1017 to i64*
  %1019 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2191, i64 0, i64 6
  %1020 = bitcast i8** %1019 to i64*
  %1021 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2190, i64 0, i64 7
  %1022 = bitcast i8** %1021 to i64*
  %1023 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2191, i64 0, i64 7
  %1024 = bitcast i8** %1023 to i64*
  %1025 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2289, i64 0, i64 0
  %1026 = bitcast [8 x i8*]* %.offload_baseptrs2289 to [3072 x double]**
  %1027 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2290, i64 0, i64 0
  %1028 = bitcast [8 x i8*]* %.offload_ptrs2290 to [3072 x double]**
  %1029 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2289, i64 0, i64 1
  %1030 = bitcast i8** %1029 to [3072 x double]**
  %1031 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2290, i64 0, i64 1
  %1032 = bitcast i8** %1031 to [3072 x double]**
  %1033 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2289, i64 0, i64 2
  %1034 = bitcast i8** %1033 to [3072 x double]**
  %1035 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2290, i64 0, i64 2
  %1036 = bitcast i8** %1035 to [3072 x double]**
  %1037 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2289, i64 0, i64 3
  %1038 = bitcast i8** %1037 to [3072 x double]**
  %1039 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2290, i64 0, i64 3
  %1040 = bitcast i8** %1039 to [3072 x double]**
  %1041 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2289, i64 0, i64 4
  %1042 = bitcast i8** %1041 to [3072 x double]**
  %1043 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2290, i64 0, i64 4
  %1044 = bitcast i8** %1043 to [3072 x double]**
  %1045 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2289, i64 0, i64 5
  %1046 = bitcast i8** %1045 to i64*
  %1047 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2290, i64 0, i64 5
  %1048 = bitcast i8** %1047 to i64*
  %1049 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2289, i64 0, i64 6
  %1050 = bitcast i8** %1049 to i64*
  %1051 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2290, i64 0, i64 6
  %1052 = bitcast i8** %1051 to i64*
  %1053 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2289, i64 0, i64 7
  %1054 = bitcast i8** %1053 to i64*
  %1055 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2290, i64 0, i64 7
  %1056 = bitcast i8** %1055 to i64*
  %1057 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2388, i64 0, i64 0
  %1058 = bitcast [8 x i8*]* %.offload_baseptrs2388 to [3072 x double]**
  %1059 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2389, i64 0, i64 0
  %1060 = bitcast [8 x i8*]* %.offload_ptrs2389 to [3072 x double]**
  %1061 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2388, i64 0, i64 1
  %1062 = bitcast i8** %1061 to [3072 x double]**
  %1063 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2389, i64 0, i64 1
  %1064 = bitcast i8** %1063 to [3072 x double]**
  %1065 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2388, i64 0, i64 2
  %1066 = bitcast i8** %1065 to [3072 x double]**
  %1067 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2389, i64 0, i64 2
  %1068 = bitcast i8** %1067 to [3072 x double]**
  %1069 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2388, i64 0, i64 3
  %1070 = bitcast i8** %1069 to [3072 x double]**
  %1071 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2389, i64 0, i64 3
  %1072 = bitcast i8** %1071 to [3072 x double]**
  %1073 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2388, i64 0, i64 4
  %1074 = bitcast i8** %1073 to [3072 x double]**
  %1075 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2389, i64 0, i64 4
  %1076 = bitcast i8** %1075 to [3072 x double]**
  %1077 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2388, i64 0, i64 5
  %1078 = bitcast i8** %1077 to i64*
  %1079 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2389, i64 0, i64 5
  %1080 = bitcast i8** %1079 to i64*
  %1081 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2388, i64 0, i64 6
  %1082 = bitcast i8** %1081 to i64*
  %1083 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2389, i64 0, i64 6
  %1084 = bitcast i8** %1083 to i64*
  %1085 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs2388, i64 0, i64 7
  %1086 = bitcast i8** %1085 to i64*
  %1087 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs2389, i64 0, i64 7
  %1088 = bitcast i8** %1087 to i64*
  br label %for.cond1838.preheader

for.cond.cleanup1226:                             ; preds = %if.end1811
  br i1 %tobool1815, label %for.inc1828, label %for.body1822.preheader

for.body1822.preheader:                           ; preds = %for.cond.cleanup1226
  %puts8317 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.1 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.2 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.3 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.4 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.5 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.6 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.7 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.8 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.9 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.10 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.11 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.12 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.13 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.14 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.15 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.16 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8317.17 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %for.inc1828

for.body1227:                                     ; preds = %for.cond1223.preheader, %if.end1811
  %indvars.iv9747 = phi i64 [ 1, %for.cond1223.preheader ], [ %indvars.iv.next9748, %if.end1811 ]
  %tobool18159227 = phi i1 [ %tobool18159225, %for.cond1223.preheader ], [ %tobool1815, %if.end1811 ]
  %cond1235 = select i1 %tobool18159227, i32 %tms1216.09229, i32 1
  %.capture_expr..casted1273.sroa.0.0.insert.ext = zext i32 %cond1235 to i64
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %607, align 8
  store [3072 x double]* %A, [3072 x double]** %609, align 8
  store [3072 x double]* %C, [3072 x double]** %611, align 8
  store [3072 x double]* %C, [3072 x double]** %613, align 8
  store [3072 x double]* %D, [3072 x double]** %615, align 8
  store [3072 x double]* %D, [3072 x double]** %617, align 8
  store [3072 x double]* %B, [3072 x double]** %619, align 8
  store [3072 x double]* %B, [3072 x double]** %621, align 8
  store [3072 x double]* %E, [3072 x double]** %623, align 8
  store [3072 x double]* %E, [3072 x double]** %625, align 8
  store i64 1, i64* %627, align 8
  store i64 1, i64* %629, align 8
  store i64 %indvars.iv9747, i64* %631, align 8
  store i64 %indvars.iv9747, i64* %633, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %635, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %637, align 8
  %1089 = trunc i64 %indvars.iv9747 to i32
  %1090 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l326.region_id, i32 8, i8** nonnull %606, i8** nonnull %608, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond1235, i32 %1089) #6
  %1091 = icmp eq i32 %1090, 0
  br i1 %1091, label %for.body1289.preheader, label %omp_offload.failed1279

omp_offload.failed1279:                           ; preds = %for.body1227
  %1092 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1093 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1092, i32 %cond1235, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..58 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9747) #6
  br label %for.body1289.preheader

for.body1289.preheader:                           ; preds = %for.body1227, %omp_offload.failed1279
  br label %for.body1289

for.cond.cleanup1288:                             ; preds = %for.body1289
  %add1301 = fadd double %add1296.5, 0.000000e+00
  %cmp1309 = fcmp une double %add1301, 0x4152018000000000
  br i1 %cmp1309, label %if.then1327, label %if.else1329

for.body1289:                                     ; preds = %for.body1289.preheader, %for.body1289
  %indvars.iv9714 = phi i64 [ %indvars.iv.next9715.5, %for.body1289 ], [ 0, %for.body1289.preheader ]
  %tmp1283.09179 = phi double [ %add1296.5, %for.body1289 ], [ 0.000000e+00, %for.body1289.preheader ]
  %arrayidx1291 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9714
  %1094 = load double, double* %arrayidx1291, align 8, !tbaa !62
  %arrayidx1293 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9714
  %1095 = load double, double* %arrayidx1293, align 8, !tbaa !62
  %add1294 = fadd double %1094, %1095
  %sub1295 = fadd double %add1294, -1.000000e+00
  %add1296 = fadd double %tmp1283.09179, %sub1295
  %indvars.iv.next9715 = or i64 %indvars.iv9714, 1
  %arrayidx1291.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9715
  %1096 = load double, double* %arrayidx1291.1, align 8, !tbaa !62
  %arrayidx1293.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9715
  %1097 = load double, double* %arrayidx1293.1, align 8, !tbaa !62
  %add1294.1 = fadd double %1096, %1097
  %sub1295.1 = fadd double %add1294.1, -1.000000e+00
  %add1296.1 = fadd double %add1296, %sub1295.1
  %indvars.iv.next9715.1 = add nuw nsw i64 %indvars.iv9714, 2
  %arrayidx1291.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9715.1
  %1098 = load double, double* %arrayidx1291.2, align 8, !tbaa !62
  %arrayidx1293.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9715.1
  %1099 = load double, double* %arrayidx1293.2, align 8, !tbaa !62
  %add1294.2 = fadd double %1098, %1099
  %sub1295.2 = fadd double %add1294.2, -1.000000e+00
  %add1296.2 = fadd double %add1296.1, %sub1295.2
  %indvars.iv.next9715.2 = add nuw nsw i64 %indvars.iv9714, 3
  %arrayidx1291.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9715.2
  %1100 = load double, double* %arrayidx1291.3, align 8, !tbaa !62
  %arrayidx1293.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9715.2
  %1101 = load double, double* %arrayidx1293.3, align 8, !tbaa !62
  %add1294.3 = fadd double %1100, %1101
  %sub1295.3 = fadd double %add1294.3, -1.000000e+00
  %add1296.3 = fadd double %add1296.2, %sub1295.3
  %indvars.iv.next9715.3 = add nuw nsw i64 %indvars.iv9714, 4
  %arrayidx1291.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9715.3
  %1102 = load double, double* %arrayidx1291.4, align 8, !tbaa !62
  %arrayidx1293.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9715.3
  %1103 = load double, double* %arrayidx1293.4, align 8, !tbaa !62
  %add1294.4 = fadd double %1102, %1103
  %sub1295.4 = fadd double %add1294.4, -1.000000e+00
  %add1296.4 = fadd double %add1296.3, %sub1295.4
  %indvars.iv.next9715.4 = add nuw nsw i64 %indvars.iv9714, 5
  %arrayidx1291.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9715.4
  %1104 = load double, double* %arrayidx1291.5, align 8, !tbaa !62
  %arrayidx1293.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9715.4
  %1105 = load double, double* %arrayidx1293.5, align 8, !tbaa !62
  %add1294.5 = fadd double %1104, %1105
  %sub1295.5 = fadd double %add1294.5, -1.000000e+00
  %add1296.5 = fadd double %add1296.4, %sub1295.5
  %indvars.iv.next9715.5 = add nuw nsw i64 %indvars.iv9714, 6
  %exitcond9716.5 = icmp eq i64 %indvars.iv.next9715.5, 3072
  br i1 %exitcond9716.5, label %for.cond.cleanup1288, label %for.body1289

if.then1327:                                      ; preds = %for.cond.cleanup1288
  %call1313.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add1301)
  %puts8329 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end1331

if.else1329:                                      ; preds = %for.cond.cleanup1288
  %puts8318 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end1331

if.end1331:                                       ; preds = %if.else1329, %if.then1327
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %639, align 8
  store [3072 x double]* %A, [3072 x double]** %641, align 8
  store [3072 x double]* %C, [3072 x double]** %643, align 8
  store [3072 x double]* %C, [3072 x double]** %645, align 8
  store [3072 x double]* %D, [3072 x double]** %647, align 8
  store [3072 x double]* %D, [3072 x double]** %649, align 8
  store [3072 x double]* %B, [3072 x double]** %651, align 8
  store [3072 x double]* %B, [3072 x double]** %653, align 8
  store [3072 x double]* %E, [3072 x double]** %655, align 8
  store [3072 x double]* %E, [3072 x double]** %657, align 8
  store i64 1, i64* %659, align 8
  store i64 1, i64* %661, align 8
  store i64 %indvars.iv9747, i64* %663, align 8
  store i64 %indvars.iv9747, i64* %665, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %667, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %669, align 8
  %1106 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l346.region_id, i32 8, i8** nonnull %638, i8** nonnull %640, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond1235, i32 %1089) #6
  %1107 = icmp eq i32 %1106, 0
  br i1 %1107, label %for.body1385.preheader, label %omp_offload.failed1375

omp_offload.failed1375:                           ; preds = %if.end1331
  %1108 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1109 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1108, i32 %cond1235, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..62 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9747) #6
  br label %for.body1385.preheader

for.body1385.preheader:                           ; preds = %if.end1331, %omp_offload.failed1375
  br label %for.body1385

for.cond.cleanup1384:                             ; preds = %for.body1385
  %add1397 = fadd double %add1392.5, 0.000000e+00
  %cmp1405 = fcmp une double %add1397, 0x4152018000000000
  br i1 %cmp1405, label %if.then1423, label %if.else1425

for.body1385:                                     ; preds = %for.body1385.preheader, %for.body1385
  %indvars.iv9720 = phi i64 [ %indvars.iv.next9721.5, %for.body1385 ], [ 0, %for.body1385.preheader ]
  %tmp1379.09187 = phi double [ %add1392.5, %for.body1385 ], [ 0.000000e+00, %for.body1385.preheader ]
  %arrayidx1387 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9720
  %1110 = load double, double* %arrayidx1387, align 8, !tbaa !62
  %arrayidx1389 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9720
  %1111 = load double, double* %arrayidx1389, align 8, !tbaa !62
  %add1390 = fadd double %1110, %1111
  %sub1391 = fadd double %add1390, -1.000000e+00
  %add1392 = fadd double %tmp1379.09187, %sub1391
  %indvars.iv.next9721 = or i64 %indvars.iv9720, 1
  %arrayidx1387.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9721
  %1112 = load double, double* %arrayidx1387.1, align 8, !tbaa !62
  %arrayidx1389.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9721
  %1113 = load double, double* %arrayidx1389.1, align 8, !tbaa !62
  %add1390.1 = fadd double %1112, %1113
  %sub1391.1 = fadd double %add1390.1, -1.000000e+00
  %add1392.1 = fadd double %add1392, %sub1391.1
  %indvars.iv.next9721.1 = add nuw nsw i64 %indvars.iv9720, 2
  %arrayidx1387.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9721.1
  %1114 = load double, double* %arrayidx1387.2, align 8, !tbaa !62
  %arrayidx1389.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9721.1
  %1115 = load double, double* %arrayidx1389.2, align 8, !tbaa !62
  %add1390.2 = fadd double %1114, %1115
  %sub1391.2 = fadd double %add1390.2, -1.000000e+00
  %add1392.2 = fadd double %add1392.1, %sub1391.2
  %indvars.iv.next9721.2 = add nuw nsw i64 %indvars.iv9720, 3
  %arrayidx1387.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9721.2
  %1116 = load double, double* %arrayidx1387.3, align 8, !tbaa !62
  %arrayidx1389.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9721.2
  %1117 = load double, double* %arrayidx1389.3, align 8, !tbaa !62
  %add1390.3 = fadd double %1116, %1117
  %sub1391.3 = fadd double %add1390.3, -1.000000e+00
  %add1392.3 = fadd double %add1392.2, %sub1391.3
  %indvars.iv.next9721.3 = add nuw nsw i64 %indvars.iv9720, 4
  %arrayidx1387.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9721.3
  %1118 = load double, double* %arrayidx1387.4, align 8, !tbaa !62
  %arrayidx1389.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9721.3
  %1119 = load double, double* %arrayidx1389.4, align 8, !tbaa !62
  %add1390.4 = fadd double %1118, %1119
  %sub1391.4 = fadd double %add1390.4, -1.000000e+00
  %add1392.4 = fadd double %add1392.3, %sub1391.4
  %indvars.iv.next9721.4 = add nuw nsw i64 %indvars.iv9720, 5
  %arrayidx1387.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9721.4
  %1120 = load double, double* %arrayidx1387.5, align 8, !tbaa !62
  %arrayidx1389.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9721.4
  %1121 = load double, double* %arrayidx1389.5, align 8, !tbaa !62
  %add1390.5 = fadd double %1120, %1121
  %sub1391.5 = fadd double %add1390.5, -1.000000e+00
  %add1392.5 = fadd double %add1392.4, %sub1391.5
  %indvars.iv.next9721.5 = add nuw nsw i64 %indvars.iv9720, 6
  %exitcond9722.5 = icmp eq i64 %indvars.iv.next9721.5, 3072
  br i1 %exitcond9722.5, label %for.cond.cleanup1384, label %for.body1385

if.then1423:                                      ; preds = %for.cond.cleanup1384
  %call1409.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add1397)
  %puts8328 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end1427

if.else1425:                                      ; preds = %for.cond.cleanup1384
  %puts8319 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end1427

if.end1427:                                       ; preds = %if.else1425, %if.then1423
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %671, align 8
  store [3072 x double]* %A, [3072 x double]** %673, align 8
  store [3072 x double]* %C, [3072 x double]** %675, align 8
  store [3072 x double]* %C, [3072 x double]** %677, align 8
  store [3072 x double]* %D, [3072 x double]** %679, align 8
  store [3072 x double]* %D, [3072 x double]** %681, align 8
  store [3072 x double]* %B, [3072 x double]** %683, align 8
  store [3072 x double]* %B, [3072 x double]** %685, align 8
  store [3072 x double]* %E, [3072 x double]** %687, align 8
  store [3072 x double]* %E, [3072 x double]** %689, align 8
  store i64 1, i64* %691, align 8
  store i64 1, i64* %693, align 8
  store i64 %indvars.iv9747, i64* %695, align 8
  store i64 %indvars.iv9747, i64* %697, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %699, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %701, align 8
  %1122 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l366.region_id, i32 8, i8** nonnull %670, i8** nonnull %672, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond1235, i32 %1089) #6
  %1123 = icmp eq i32 %1122, 0
  br i1 %1123, label %for.body1481.preheader, label %omp_offload.failed1471

omp_offload.failed1471:                           ; preds = %if.end1427
  %1124 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1125 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1124, i32 %cond1235, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..66 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9747) #6
  br label %for.body1481.preheader

for.body1481.preheader:                           ; preds = %if.end1427, %omp_offload.failed1471
  br label %for.body1481

for.cond.cleanup1480:                             ; preds = %for.body1481
  %add1493 = fadd double %add1488.5, 0.000000e+00
  %cmp1501 = fcmp une double %add1493, 0x4152018000000000
  br i1 %cmp1501, label %if.then1519, label %if.else1521

for.body1481:                                     ; preds = %for.body1481.preheader, %for.body1481
  %indvars.iv9726 = phi i64 [ %indvars.iv.next9727.5, %for.body1481 ], [ 0, %for.body1481.preheader ]
  %tmp1475.09195 = phi double [ %add1488.5, %for.body1481 ], [ 0.000000e+00, %for.body1481.preheader ]
  %arrayidx1483 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9726
  %1126 = load double, double* %arrayidx1483, align 8, !tbaa !62
  %arrayidx1485 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9726
  %1127 = load double, double* %arrayidx1485, align 8, !tbaa !62
  %add1486 = fadd double %1126, %1127
  %sub1487 = fadd double %add1486, -1.000000e+00
  %add1488 = fadd double %tmp1475.09195, %sub1487
  %indvars.iv.next9727 = or i64 %indvars.iv9726, 1
  %arrayidx1483.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9727
  %1128 = load double, double* %arrayidx1483.1, align 8, !tbaa !62
  %arrayidx1485.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9727
  %1129 = load double, double* %arrayidx1485.1, align 8, !tbaa !62
  %add1486.1 = fadd double %1128, %1129
  %sub1487.1 = fadd double %add1486.1, -1.000000e+00
  %add1488.1 = fadd double %add1488, %sub1487.1
  %indvars.iv.next9727.1 = add nuw nsw i64 %indvars.iv9726, 2
  %arrayidx1483.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9727.1
  %1130 = load double, double* %arrayidx1483.2, align 8, !tbaa !62
  %arrayidx1485.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9727.1
  %1131 = load double, double* %arrayidx1485.2, align 8, !tbaa !62
  %add1486.2 = fadd double %1130, %1131
  %sub1487.2 = fadd double %add1486.2, -1.000000e+00
  %add1488.2 = fadd double %add1488.1, %sub1487.2
  %indvars.iv.next9727.2 = add nuw nsw i64 %indvars.iv9726, 3
  %arrayidx1483.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9727.2
  %1132 = load double, double* %arrayidx1483.3, align 8, !tbaa !62
  %arrayidx1485.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9727.2
  %1133 = load double, double* %arrayidx1485.3, align 8, !tbaa !62
  %add1486.3 = fadd double %1132, %1133
  %sub1487.3 = fadd double %add1486.3, -1.000000e+00
  %add1488.3 = fadd double %add1488.2, %sub1487.3
  %indvars.iv.next9727.3 = add nuw nsw i64 %indvars.iv9726, 4
  %arrayidx1483.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9727.3
  %1134 = load double, double* %arrayidx1483.4, align 8, !tbaa !62
  %arrayidx1485.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9727.3
  %1135 = load double, double* %arrayidx1485.4, align 8, !tbaa !62
  %add1486.4 = fadd double %1134, %1135
  %sub1487.4 = fadd double %add1486.4, -1.000000e+00
  %add1488.4 = fadd double %add1488.3, %sub1487.4
  %indvars.iv.next9727.4 = add nuw nsw i64 %indvars.iv9726, 5
  %arrayidx1483.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9727.4
  %1136 = load double, double* %arrayidx1483.5, align 8, !tbaa !62
  %arrayidx1485.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9727.4
  %1137 = load double, double* %arrayidx1485.5, align 8, !tbaa !62
  %add1486.5 = fadd double %1136, %1137
  %sub1487.5 = fadd double %add1486.5, -1.000000e+00
  %add1488.5 = fadd double %add1488.4, %sub1487.5
  %indvars.iv.next9727.5 = add nuw nsw i64 %indvars.iv9726, 6
  %exitcond9728.5 = icmp eq i64 %indvars.iv.next9727.5, 3072
  br i1 %exitcond9728.5, label %for.cond.cleanup1480, label %for.body1481

if.then1519:                                      ; preds = %for.cond.cleanup1480
  %call1505.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add1493)
  %puts8327 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end1523

if.else1521:                                      ; preds = %for.cond.cleanup1480
  %puts8320 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end1523

if.end1523:                                       ; preds = %if.else1521, %if.then1519
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %703, align 8
  store [3072 x double]* %A, [3072 x double]** %705, align 8
  store [3072 x double]* %C, [3072 x double]** %707, align 8
  store [3072 x double]* %C, [3072 x double]** %709, align 8
  store [3072 x double]* %D, [3072 x double]** %711, align 8
  store [3072 x double]* %D, [3072 x double]** %713, align 8
  store [3072 x double]* %B, [3072 x double]** %715, align 8
  store [3072 x double]* %B, [3072 x double]** %717, align 8
  store [3072 x double]* %E, [3072 x double]** %719, align 8
  store [3072 x double]* %E, [3072 x double]** %721, align 8
  store i64 1, i64* %723, align 8
  store i64 1, i64* %725, align 8
  store i64 %indvars.iv9747, i64* %727, align 8
  store i64 %indvars.iv9747, i64* %729, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %731, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %733, align 8
  %1138 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l386.region_id, i32 8, i8** nonnull %702, i8** nonnull %704, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond1235, i32 %1089) #6
  %1139 = icmp eq i32 %1138, 0
  br i1 %1139, label %for.body1577.preheader, label %omp_offload.failed1567

omp_offload.failed1567:                           ; preds = %if.end1523
  %1140 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1141 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1140, i32 %cond1235, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..70 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9747) #6
  br label %for.body1577.preheader

for.body1577.preheader:                           ; preds = %if.end1523, %omp_offload.failed1567
  br label %for.body1577

for.cond.cleanup1576:                             ; preds = %for.body1577
  %add1589 = fadd double %add1584.5, 0.000000e+00
  %cmp1597 = fcmp une double %add1589, 0x4152018000000000
  br i1 %cmp1597, label %if.then1615, label %if.else1617

for.body1577:                                     ; preds = %for.body1577.preheader, %for.body1577
  %indvars.iv9732 = phi i64 [ %indvars.iv.next9733.5, %for.body1577 ], [ 0, %for.body1577.preheader ]
  %tmp1571.09203 = phi double [ %add1584.5, %for.body1577 ], [ 0.000000e+00, %for.body1577.preheader ]
  %arrayidx1579 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9732
  %1142 = load double, double* %arrayidx1579, align 8, !tbaa !62
  %arrayidx1581 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9732
  %1143 = load double, double* %arrayidx1581, align 8, !tbaa !62
  %add1582 = fadd double %1142, %1143
  %sub1583 = fadd double %add1582, -1.000000e+00
  %add1584 = fadd double %tmp1571.09203, %sub1583
  %indvars.iv.next9733 = or i64 %indvars.iv9732, 1
  %arrayidx1579.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9733
  %1144 = load double, double* %arrayidx1579.1, align 8, !tbaa !62
  %arrayidx1581.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9733
  %1145 = load double, double* %arrayidx1581.1, align 8, !tbaa !62
  %add1582.1 = fadd double %1144, %1145
  %sub1583.1 = fadd double %add1582.1, -1.000000e+00
  %add1584.1 = fadd double %add1584, %sub1583.1
  %indvars.iv.next9733.1 = add nuw nsw i64 %indvars.iv9732, 2
  %arrayidx1579.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9733.1
  %1146 = load double, double* %arrayidx1579.2, align 8, !tbaa !62
  %arrayidx1581.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9733.1
  %1147 = load double, double* %arrayidx1581.2, align 8, !tbaa !62
  %add1582.2 = fadd double %1146, %1147
  %sub1583.2 = fadd double %add1582.2, -1.000000e+00
  %add1584.2 = fadd double %add1584.1, %sub1583.2
  %indvars.iv.next9733.2 = add nuw nsw i64 %indvars.iv9732, 3
  %arrayidx1579.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9733.2
  %1148 = load double, double* %arrayidx1579.3, align 8, !tbaa !62
  %arrayidx1581.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9733.2
  %1149 = load double, double* %arrayidx1581.3, align 8, !tbaa !62
  %add1582.3 = fadd double %1148, %1149
  %sub1583.3 = fadd double %add1582.3, -1.000000e+00
  %add1584.3 = fadd double %add1584.2, %sub1583.3
  %indvars.iv.next9733.3 = add nuw nsw i64 %indvars.iv9732, 4
  %arrayidx1579.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9733.3
  %1150 = load double, double* %arrayidx1579.4, align 8, !tbaa !62
  %arrayidx1581.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9733.3
  %1151 = load double, double* %arrayidx1581.4, align 8, !tbaa !62
  %add1582.4 = fadd double %1150, %1151
  %sub1583.4 = fadd double %add1582.4, -1.000000e+00
  %add1584.4 = fadd double %add1584.3, %sub1583.4
  %indvars.iv.next9733.4 = add nuw nsw i64 %indvars.iv9732, 5
  %arrayidx1579.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9733.4
  %1152 = load double, double* %arrayidx1579.5, align 8, !tbaa !62
  %arrayidx1581.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9733.4
  %1153 = load double, double* %arrayidx1581.5, align 8, !tbaa !62
  %add1582.5 = fadd double %1152, %1153
  %sub1583.5 = fadd double %add1582.5, -1.000000e+00
  %add1584.5 = fadd double %add1584.4, %sub1583.5
  %indvars.iv.next9733.5 = add nuw nsw i64 %indvars.iv9732, 6
  %exitcond9734.5 = icmp eq i64 %indvars.iv.next9733.5, 3072
  br i1 %exitcond9734.5, label %for.cond.cleanup1576, label %for.body1577

if.then1615:                                      ; preds = %for.cond.cleanup1576
  %call1601.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add1589)
  %puts8326 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end1619

if.else1617:                                      ; preds = %for.cond.cleanup1576
  %puts8321 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end1619

if.end1619:                                       ; preds = %if.else1617, %if.then1615
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %735, align 8
  store [3072 x double]* %A, [3072 x double]** %737, align 8
  store [3072 x double]* %C, [3072 x double]** %739, align 8
  store [3072 x double]* %C, [3072 x double]** %741, align 8
  store [3072 x double]* %D, [3072 x double]** %743, align 8
  store [3072 x double]* %D, [3072 x double]** %745, align 8
  store [3072 x double]* %B, [3072 x double]** %747, align 8
  store [3072 x double]* %B, [3072 x double]** %749, align 8
  store [3072 x double]* %E, [3072 x double]** %751, align 8
  store [3072 x double]* %E, [3072 x double]** %753, align 8
  store i64 1, i64* %755, align 8
  store i64 1, i64* %757, align 8
  store i64 %indvars.iv9747, i64* %759, align 8
  store i64 %indvars.iv9747, i64* %761, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %763, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %765, align 8
  %1154 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l406.region_id, i32 8, i8** nonnull %734, i8** nonnull %736, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond1235, i32 %1089) #6
  %1155 = icmp eq i32 %1154, 0
  br i1 %1155, label %for.body1673.preheader, label %omp_offload.failed1663

omp_offload.failed1663:                           ; preds = %if.end1619
  %1156 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1157 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1156, i32 %cond1235, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..74 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9747) #6
  br label %for.body1673.preheader

for.body1673.preheader:                           ; preds = %if.end1619, %omp_offload.failed1663
  br label %for.body1673

for.cond.cleanup1672:                             ; preds = %for.body1673
  %add1685 = fadd double %add1680.5, 0.000000e+00
  %cmp1693 = fcmp une double %add1685, 0x4152018000000000
  br i1 %cmp1693, label %if.then1711, label %if.else1713

for.body1673:                                     ; preds = %for.body1673.preheader, %for.body1673
  %indvars.iv9738 = phi i64 [ %indvars.iv.next9739.5, %for.body1673 ], [ 0, %for.body1673.preheader ]
  %tmp1667.09211 = phi double [ %add1680.5, %for.body1673 ], [ 0.000000e+00, %for.body1673.preheader ]
  %arrayidx1675 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9738
  %1158 = load double, double* %arrayidx1675, align 8, !tbaa !62
  %arrayidx1677 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9738
  %1159 = load double, double* %arrayidx1677, align 8, !tbaa !62
  %add1678 = fadd double %1158, %1159
  %sub1679 = fadd double %add1678, -1.000000e+00
  %add1680 = fadd double %tmp1667.09211, %sub1679
  %indvars.iv.next9739 = or i64 %indvars.iv9738, 1
  %arrayidx1675.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9739
  %1160 = load double, double* %arrayidx1675.1, align 8, !tbaa !62
  %arrayidx1677.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9739
  %1161 = load double, double* %arrayidx1677.1, align 8, !tbaa !62
  %add1678.1 = fadd double %1160, %1161
  %sub1679.1 = fadd double %add1678.1, -1.000000e+00
  %add1680.1 = fadd double %add1680, %sub1679.1
  %indvars.iv.next9739.1 = add nuw nsw i64 %indvars.iv9738, 2
  %arrayidx1675.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9739.1
  %1162 = load double, double* %arrayidx1675.2, align 8, !tbaa !62
  %arrayidx1677.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9739.1
  %1163 = load double, double* %arrayidx1677.2, align 8, !tbaa !62
  %add1678.2 = fadd double %1162, %1163
  %sub1679.2 = fadd double %add1678.2, -1.000000e+00
  %add1680.2 = fadd double %add1680.1, %sub1679.2
  %indvars.iv.next9739.2 = add nuw nsw i64 %indvars.iv9738, 3
  %arrayidx1675.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9739.2
  %1164 = load double, double* %arrayidx1675.3, align 8, !tbaa !62
  %arrayidx1677.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9739.2
  %1165 = load double, double* %arrayidx1677.3, align 8, !tbaa !62
  %add1678.3 = fadd double %1164, %1165
  %sub1679.3 = fadd double %add1678.3, -1.000000e+00
  %add1680.3 = fadd double %add1680.2, %sub1679.3
  %indvars.iv.next9739.3 = add nuw nsw i64 %indvars.iv9738, 4
  %arrayidx1675.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9739.3
  %1166 = load double, double* %arrayidx1675.4, align 8, !tbaa !62
  %arrayidx1677.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9739.3
  %1167 = load double, double* %arrayidx1677.4, align 8, !tbaa !62
  %add1678.4 = fadd double %1166, %1167
  %sub1679.4 = fadd double %add1678.4, -1.000000e+00
  %add1680.4 = fadd double %add1680.3, %sub1679.4
  %indvars.iv.next9739.4 = add nuw nsw i64 %indvars.iv9738, 5
  %arrayidx1675.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9739.4
  %1168 = load double, double* %arrayidx1675.5, align 8, !tbaa !62
  %arrayidx1677.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9739.4
  %1169 = load double, double* %arrayidx1677.5, align 8, !tbaa !62
  %add1678.5 = fadd double %1168, %1169
  %sub1679.5 = fadd double %add1678.5, -1.000000e+00
  %add1680.5 = fadd double %add1680.4, %sub1679.5
  %indvars.iv.next9739.5 = add nuw nsw i64 %indvars.iv9738, 6
  %exitcond9740.5 = icmp eq i64 %indvars.iv.next9739.5, 3072
  br i1 %exitcond9740.5, label %for.cond.cleanup1672, label %for.body1673

if.then1711:                                      ; preds = %for.cond.cleanup1672
  %call1697.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add1685)
  %puts8325 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end1715

if.else1713:                                      ; preds = %for.cond.cleanup1672
  %puts8322 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end1715

if.end1715:                                       ; preds = %if.else1713, %if.then1711
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %767, align 8
  store [3072 x double]* %A, [3072 x double]** %769, align 8
  store [3072 x double]* %C, [3072 x double]** %771, align 8
  store [3072 x double]* %C, [3072 x double]** %773, align 8
  store [3072 x double]* %D, [3072 x double]** %775, align 8
  store [3072 x double]* %D, [3072 x double]** %777, align 8
  store [3072 x double]* %B, [3072 x double]** %779, align 8
  store [3072 x double]* %B, [3072 x double]** %781, align 8
  store [3072 x double]* %E, [3072 x double]** %783, align 8
  store [3072 x double]* %E, [3072 x double]** %785, align 8
  store i64 1, i64* %787, align 8
  store i64 1, i64* %789, align 8
  store i64 %indvars.iv9747, i64* %791, align 8
  store i64 %indvars.iv9747, i64* %793, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %795, align 8
  store i64 %.capture_expr..casted1273.sroa.0.0.insert.ext, i64* %797, align 8
  %1170 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l426.region_id, i32 8, i8** nonnull %766, i8** nonnull %768, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond1235, i32 %1089) #6
  %1171 = icmp eq i32 %1170, 0
  br i1 %1171, label %for.body1769.preheader, label %omp_offload.failed1759

omp_offload.failed1759:                           ; preds = %if.end1715
  %1172 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1173 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1172, i32 %cond1235, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..78 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9747) #6
  br label %for.body1769.preheader

for.body1769.preheader:                           ; preds = %if.end1715, %omp_offload.failed1759
  br label %for.body1769

for.cond.cleanup1768:                             ; preds = %for.body1769
  %add1781 = fadd double %add1776.5, 0.000000e+00
  %cmp1789 = fcmp une double %add1781, 0x4152018000000000
  br i1 %cmp1789, label %if.then1807, label %if.else1809

for.body1769:                                     ; preds = %for.body1769.preheader, %for.body1769
  %indvars.iv9744 = phi i64 [ %indvars.iv.next9745.5, %for.body1769 ], [ 0, %for.body1769.preheader ]
  %tmp1763.09219 = phi double [ %add1776.5, %for.body1769 ], [ 0.000000e+00, %for.body1769.preheader ]
  %arrayidx1771 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9744
  %1174 = load double, double* %arrayidx1771, align 8, !tbaa !62
  %arrayidx1773 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9744
  %1175 = load double, double* %arrayidx1773, align 8, !tbaa !62
  %add1774 = fadd double %1174, %1175
  %sub1775 = fadd double %add1774, -1.000000e+00
  %add1776 = fadd double %tmp1763.09219, %sub1775
  %indvars.iv.next9745 = or i64 %indvars.iv9744, 1
  %arrayidx1771.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9745
  %1176 = load double, double* %arrayidx1771.1, align 8, !tbaa !62
  %arrayidx1773.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9745
  %1177 = load double, double* %arrayidx1773.1, align 8, !tbaa !62
  %add1774.1 = fadd double %1176, %1177
  %sub1775.1 = fadd double %add1774.1, -1.000000e+00
  %add1776.1 = fadd double %add1776, %sub1775.1
  %indvars.iv.next9745.1 = add nuw nsw i64 %indvars.iv9744, 2
  %arrayidx1771.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9745.1
  %1178 = load double, double* %arrayidx1771.2, align 8, !tbaa !62
  %arrayidx1773.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9745.1
  %1179 = load double, double* %arrayidx1773.2, align 8, !tbaa !62
  %add1774.2 = fadd double %1178, %1179
  %sub1775.2 = fadd double %add1774.2, -1.000000e+00
  %add1776.2 = fadd double %add1776.1, %sub1775.2
  %indvars.iv.next9745.2 = add nuw nsw i64 %indvars.iv9744, 3
  %arrayidx1771.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9745.2
  %1180 = load double, double* %arrayidx1771.3, align 8, !tbaa !62
  %arrayidx1773.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9745.2
  %1181 = load double, double* %arrayidx1773.3, align 8, !tbaa !62
  %add1774.3 = fadd double %1180, %1181
  %sub1775.3 = fadd double %add1774.3, -1.000000e+00
  %add1776.3 = fadd double %add1776.2, %sub1775.3
  %indvars.iv.next9745.3 = add nuw nsw i64 %indvars.iv9744, 4
  %arrayidx1771.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9745.3
  %1182 = load double, double* %arrayidx1771.4, align 8, !tbaa !62
  %arrayidx1773.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9745.3
  %1183 = load double, double* %arrayidx1773.4, align 8, !tbaa !62
  %add1774.4 = fadd double %1182, %1183
  %sub1775.4 = fadd double %add1774.4, -1.000000e+00
  %add1776.4 = fadd double %add1776.3, %sub1775.4
  %indvars.iv.next9745.4 = add nuw nsw i64 %indvars.iv9744, 5
  %arrayidx1771.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9745.4
  %1184 = load double, double* %arrayidx1771.5, align 8, !tbaa !62
  %arrayidx1773.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9745.4
  %1185 = load double, double* %arrayidx1773.5, align 8, !tbaa !62
  %add1774.5 = fadd double %1184, %1185
  %sub1775.5 = fadd double %add1774.5, -1.000000e+00
  %add1776.5 = fadd double %add1776.4, %sub1775.5
  %indvars.iv.next9745.5 = add nuw nsw i64 %indvars.iv9744, 6
  %exitcond9746.5 = icmp eq i64 %indvars.iv.next9745.5, 3072
  br i1 %exitcond9746.5, label %for.cond.cleanup1768, label %for.body1769

if.then1807:                                      ; preds = %for.cond.cleanup1768
  %call1793.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add1781)
  %puts8324 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end1811

if.else1809:                                      ; preds = %for.cond.cleanup1768
  %puts8323 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end1811

if.end1811:                                       ; preds = %if.else1809, %if.then1807
  %indvars.iv.next9748 = add nuw nsw i64 %indvars.iv9747, 78
  %cmp1224 = icmp ugt i64 %indvars.iv.next9748, %cond
  %1186 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool1815 = icmp eq i32 %1186, 0
  br i1 %cmp1224, label %for.cond.cleanup1226, label %for.body1227

for.inc1828:                                      ; preds = %for.body1822.preheader, %for.cond.cleanup1226
  %mul1829 = mul nsw i32 %tms1216.09229, 3
  %cmp1218 = icmp ult i32 %mul1829, 257
  br i1 %cmp1218, label %for.cond1223.preheader, label %for.cond1832.preheader

for.cond1838.preheader:                           ; preds = %for.cond1832.preheader, %for.inc2460
  %tms1831.09176 = phi i32 [ 1, %for.cond1832.preheader ], [ %mul2461, %for.inc2460 ]
  %1187 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool24479172 = icmp eq i32 %1187, 0
  br label %for.body1842

for.cond2464.preheader:                           ; preds = %for.inc2460
  %1188 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2528, i64 0, i64 0
  %1189 = bitcast [10 x i8*]* %.offload_baseptrs2528 to [3072 x double]**
  %1190 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2529, i64 0, i64 0
  %1191 = bitcast [10 x i8*]* %.offload_ptrs2529 to [3072 x double]**
  %1192 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2528, i64 0, i64 1
  %1193 = bitcast i8** %1192 to [3072 x double]**
  %1194 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2529, i64 0, i64 1
  %1195 = bitcast i8** %1194 to [3072 x double]**
  %1196 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2528, i64 0, i64 2
  %1197 = bitcast i8** %1196 to [3072 x double]**
  %1198 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2529, i64 0, i64 2
  %1199 = bitcast i8** %1198 to [3072 x double]**
  %1200 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2528, i64 0, i64 3
  %1201 = bitcast i8** %1200 to i64*
  %1202 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2529, i64 0, i64 3
  %1203 = bitcast i8** %1202 to i64*
  %1204 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2528, i64 0, i64 4
  %1205 = bitcast i8** %1204 to [3072 x double]**
  %1206 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2529, i64 0, i64 4
  %1207 = bitcast i8** %1206 to [3072 x double]**
  %1208 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2528, i64 0, i64 5
  %1209 = bitcast i8** %1208 to [3072 x double]**
  %1210 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2529, i64 0, i64 5
  %1211 = bitcast i8** %1210 to [3072 x double]**
  %1212 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2528, i64 0, i64 6
  %1213 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2529, i64 0, i64 6
  %1214 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2528, i64 0, i64 8
  %1215 = bitcast i8** %1214 to i64*
  %1216 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2529, i64 0, i64 8
  %1217 = bitcast i8** %1216 to i64*
  %1218 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2528, i64 0, i64 9
  %1219 = bitcast i8** %1218 to i64*
  %1220 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2529, i64 0, i64 9
  %1221 = bitcast i8** %1220 to i64*
  %1222 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2631, i64 0, i64 0
  %1223 = bitcast [10 x i8*]* %.offload_baseptrs2631 to [3072 x double]**
  %1224 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2632, i64 0, i64 0
  %1225 = bitcast [10 x i8*]* %.offload_ptrs2632 to [3072 x double]**
  %1226 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2631, i64 0, i64 1
  %1227 = bitcast i8** %1226 to [3072 x double]**
  %1228 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2632, i64 0, i64 1
  %1229 = bitcast i8** %1228 to [3072 x double]**
  %1230 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2631, i64 0, i64 2
  %1231 = bitcast i8** %1230 to [3072 x double]**
  %1232 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2632, i64 0, i64 2
  %1233 = bitcast i8** %1232 to [3072 x double]**
  %1234 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2631, i64 0, i64 3
  %1235 = bitcast i8** %1234 to i64*
  %1236 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2632, i64 0, i64 3
  %1237 = bitcast i8** %1236 to i64*
  %1238 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2631, i64 0, i64 4
  %1239 = bitcast i8** %1238 to [3072 x double]**
  %1240 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2632, i64 0, i64 4
  %1241 = bitcast i8** %1240 to [3072 x double]**
  %1242 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2631, i64 0, i64 5
  %1243 = bitcast i8** %1242 to [3072 x double]**
  %1244 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2632, i64 0, i64 5
  %1245 = bitcast i8** %1244 to [3072 x double]**
  %1246 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2631, i64 0, i64 6
  %1247 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2632, i64 0, i64 6
  %1248 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2631, i64 0, i64 8
  %1249 = bitcast i8** %1248 to i64*
  %1250 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2632, i64 0, i64 8
  %1251 = bitcast i8** %1250 to i64*
  %1252 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2631, i64 0, i64 9
  %1253 = bitcast i8** %1252 to i64*
  %1254 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2632, i64 0, i64 9
  %1255 = bitcast i8** %1254 to i64*
  %1256 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2734, i64 0, i64 0
  %1257 = bitcast [10 x i8*]* %.offload_baseptrs2734 to [3072 x double]**
  %1258 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2735, i64 0, i64 0
  %1259 = bitcast [10 x i8*]* %.offload_ptrs2735 to [3072 x double]**
  %1260 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2734, i64 0, i64 1
  %1261 = bitcast i8** %1260 to [3072 x double]**
  %1262 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2735, i64 0, i64 1
  %1263 = bitcast i8** %1262 to [3072 x double]**
  %1264 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2734, i64 0, i64 2
  %1265 = bitcast i8** %1264 to [3072 x double]**
  %1266 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2735, i64 0, i64 2
  %1267 = bitcast i8** %1266 to [3072 x double]**
  %1268 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2734, i64 0, i64 3
  %1269 = bitcast i8** %1268 to i64*
  %1270 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2735, i64 0, i64 3
  %1271 = bitcast i8** %1270 to i64*
  %1272 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2734, i64 0, i64 4
  %1273 = bitcast i8** %1272 to [3072 x double]**
  %1274 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2735, i64 0, i64 4
  %1275 = bitcast i8** %1274 to [3072 x double]**
  %1276 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2734, i64 0, i64 5
  %1277 = bitcast i8** %1276 to [3072 x double]**
  %1278 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2735, i64 0, i64 5
  %1279 = bitcast i8** %1278 to [3072 x double]**
  %1280 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2734, i64 0, i64 6
  %1281 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2735, i64 0, i64 6
  %1282 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2734, i64 0, i64 8
  %1283 = bitcast i8** %1282 to i64*
  %1284 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2735, i64 0, i64 8
  %1285 = bitcast i8** %1284 to i64*
  %1286 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2734, i64 0, i64 9
  %1287 = bitcast i8** %1286 to i64*
  %1288 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2735, i64 0, i64 9
  %1289 = bitcast i8** %1288 to i64*
  %1290 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2837, i64 0, i64 0
  %1291 = bitcast [10 x i8*]* %.offload_baseptrs2837 to [3072 x double]**
  %1292 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2838, i64 0, i64 0
  %1293 = bitcast [10 x i8*]* %.offload_ptrs2838 to [3072 x double]**
  %1294 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2837, i64 0, i64 1
  %1295 = bitcast i8** %1294 to [3072 x double]**
  %1296 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2838, i64 0, i64 1
  %1297 = bitcast i8** %1296 to [3072 x double]**
  %1298 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2837, i64 0, i64 2
  %1299 = bitcast i8** %1298 to [3072 x double]**
  %1300 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2838, i64 0, i64 2
  %1301 = bitcast i8** %1300 to [3072 x double]**
  %1302 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2837, i64 0, i64 3
  %1303 = bitcast i8** %1302 to i64*
  %1304 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2838, i64 0, i64 3
  %1305 = bitcast i8** %1304 to i64*
  %1306 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2837, i64 0, i64 4
  %1307 = bitcast i8** %1306 to [3072 x double]**
  %1308 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2838, i64 0, i64 4
  %1309 = bitcast i8** %1308 to [3072 x double]**
  %1310 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2837, i64 0, i64 5
  %1311 = bitcast i8** %1310 to [3072 x double]**
  %1312 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2838, i64 0, i64 5
  %1313 = bitcast i8** %1312 to [3072 x double]**
  %1314 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2837, i64 0, i64 6
  %1315 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2838, i64 0, i64 6
  %1316 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2837, i64 0, i64 8
  %1317 = bitcast i8** %1316 to i64*
  %1318 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2838, i64 0, i64 8
  %1319 = bitcast i8** %1318 to i64*
  %1320 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2837, i64 0, i64 9
  %1321 = bitcast i8** %1320 to i64*
  %1322 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2838, i64 0, i64 9
  %1323 = bitcast i8** %1322 to i64*
  %1324 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2940, i64 0, i64 0
  %1325 = bitcast [10 x i8*]* %.offload_baseptrs2940 to [3072 x double]**
  %1326 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2941, i64 0, i64 0
  %1327 = bitcast [10 x i8*]* %.offload_ptrs2941 to [3072 x double]**
  %1328 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2940, i64 0, i64 1
  %1329 = bitcast i8** %1328 to [3072 x double]**
  %1330 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2941, i64 0, i64 1
  %1331 = bitcast i8** %1330 to [3072 x double]**
  %1332 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2940, i64 0, i64 2
  %1333 = bitcast i8** %1332 to [3072 x double]**
  %1334 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2941, i64 0, i64 2
  %1335 = bitcast i8** %1334 to [3072 x double]**
  %1336 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2940, i64 0, i64 3
  %1337 = bitcast i8** %1336 to i64*
  %1338 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2941, i64 0, i64 3
  %1339 = bitcast i8** %1338 to i64*
  %1340 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2940, i64 0, i64 4
  %1341 = bitcast i8** %1340 to [3072 x double]**
  %1342 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2941, i64 0, i64 4
  %1343 = bitcast i8** %1342 to [3072 x double]**
  %1344 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2940, i64 0, i64 5
  %1345 = bitcast i8** %1344 to [3072 x double]**
  %1346 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2941, i64 0, i64 5
  %1347 = bitcast i8** %1346 to [3072 x double]**
  %1348 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2940, i64 0, i64 6
  %1349 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2941, i64 0, i64 6
  %1350 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2940, i64 0, i64 8
  %1351 = bitcast i8** %1350 to i64*
  %1352 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2941, i64 0, i64 8
  %1353 = bitcast i8** %1352 to i64*
  %1354 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs2940, i64 0, i64 9
  %1355 = bitcast i8** %1354 to i64*
  %1356 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs2941, i64 0, i64 9
  %1357 = bitcast i8** %1356 to i64*
  %1358 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs3043, i64 0, i64 0
  %1359 = bitcast [10 x i8*]* %.offload_baseptrs3043 to [3072 x double]**
  %1360 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs3044, i64 0, i64 0
  %1361 = bitcast [10 x i8*]* %.offload_ptrs3044 to [3072 x double]**
  %1362 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs3043, i64 0, i64 1
  %1363 = bitcast i8** %1362 to [3072 x double]**
  %1364 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs3044, i64 0, i64 1
  %1365 = bitcast i8** %1364 to [3072 x double]**
  %1366 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs3043, i64 0, i64 2
  %1367 = bitcast i8** %1366 to [3072 x double]**
  %1368 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs3044, i64 0, i64 2
  %1369 = bitcast i8** %1368 to [3072 x double]**
  %1370 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs3043, i64 0, i64 3
  %1371 = bitcast i8** %1370 to i64*
  %1372 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs3044, i64 0, i64 3
  %1373 = bitcast i8** %1372 to i64*
  %1374 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs3043, i64 0, i64 4
  %1375 = bitcast i8** %1374 to [3072 x double]**
  %1376 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs3044, i64 0, i64 4
  %1377 = bitcast i8** %1376 to [3072 x double]**
  %1378 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs3043, i64 0, i64 5
  %1379 = bitcast i8** %1378 to [3072 x double]**
  %1380 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs3044, i64 0, i64 5
  %1381 = bitcast i8** %1380 to [3072 x double]**
  %1382 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs3043, i64 0, i64 6
  %1383 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs3044, i64 0, i64 6
  %1384 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs3043, i64 0, i64 8
  %1385 = bitcast i8** %1384 to i64*
  %1386 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs3044, i64 0, i64 8
  %1387 = bitcast i8** %1386 to i64*
  %1388 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_baseptrs3043, i64 0, i64 9
  %1389 = bitcast i8** %1388 to i64*
  %1390 = getelementptr inbounds [10 x i8*], [10 x i8*]* %.offload_ptrs3044, i64 0, i64 9
  %1391 = bitcast i8** %1390 to i64*
  %1392 = bitcast i8** %1212 to <2 x i64>*
  %1393 = bitcast i8** %1213 to <2 x i64>*
  %1394 = bitcast i8** %1246 to <2 x i64>*
  %1395 = bitcast i8** %1247 to <2 x i64>*
  %1396 = bitcast i8** %1280 to <2 x i64>*
  %1397 = bitcast i8** %1281 to <2 x i64>*
  %1398 = bitcast i8** %1314 to <2 x i64>*
  %1399 = bitcast i8** %1315 to <2 x i64>*
  %1400 = bitcast i8** %1348 to <2 x i64>*
  %1401 = bitcast i8** %1349 to <2 x i64>*
  %1402 = bitcast i8** %1382 to <2 x i64>*
  %1403 = bitcast i8** %1383 to <2 x i64>*
  br label %for.cond2470.preheader

for.cond.cleanup1841:                             ; preds = %if.end2443
  br i1 %tobool2447, label %for.inc2460, label %for.body2454.preheader

for.body2454.preheader:                           ; preds = %for.cond.cleanup1841
  %puts8304 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.1 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.2 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.3 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.4 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.5 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.6 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.7 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.8 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.9 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.10 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.11 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.12 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.13 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.14 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.15 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.16 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8304.17 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %for.inc2460

for.body1842:                                     ; preds = %for.cond1838.preheader, %if.end2443
  %indvars.iv9708 = phi i64 [ 1, %for.cond1838.preheader ], [ %indvars.iv.next9709, %if.end2443 ]
  %tobool24479174 = phi i1 [ %tobool24479172, %for.cond1838.preheader ], [ %tobool2447, %if.end2443 ]
  %cond1850 = select i1 %tobool24479174, i32 %tms1831.09176, i32 1
  %.capture_expr..casted1889.sroa.0.0.insert.ext = zext i32 %cond1850 to i64
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %C, [3072 x double]** %898, align 8
  store [3072 x double]* %C, [3072 x double]** %900, align 8
  store [3072 x double]* %D, [3072 x double]** %902, align 8
  store [3072 x double]* %D, [3072 x double]** %904, align 8
  store [3072 x double]* %E, [3072 x double]** %906, align 8
  store [3072 x double]* %E, [3072 x double]** %908, align 8
  store [3072 x double]* %A, [3072 x double]** %910, align 8
  store [3072 x double]* %A, [3072 x double]** %912, align 8
  store [3072 x double]* %B, [3072 x double]** %914, align 8
  store [3072 x double]* %B, [3072 x double]** %916, align 8
  store i64 1, i64* %918, align 8
  store i64 1, i64* %920, align 8
  store i64 %indvars.iv9708, i64* %922, align 8
  store i64 %indvars.iv9708, i64* %924, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %926, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %928, align 8
  %1404 = trunc i64 %indvars.iv9708 to i32
  %1405 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l461.region_id, i32 8, i8** nonnull %897, i8** nonnull %899, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.105, i64 0, i64 0), i32 %cond1850, i32 %1404) #6
  %1406 = icmp eq i32 %1405, 0
  br i1 %1406, label %for.body1906.preheader, label %omp_offload.failed1895

omp_offload.failed1895:                           ; preds = %for.body1842
  %1407 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1408 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1407, i32 %cond1850, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..82 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B, i64 1, i64 %indvars.iv9708) #6
  br label %for.body1906.preheader

for.body1906.preheader:                           ; preds = %for.body1842, %omp_offload.failed1895
  br label %for.body1906

for.cond.cleanup1905:                             ; preds = %for.body1906
  %add1918 = fadd double %add1913.5, 0.000000e+00
  %cmp1926 = fcmp une double %add1918, 0x4152018180000000
  br i1 %cmp1926, label %if.then1944, label %if.else1946

for.body1906:                                     ; preds = %for.body1906.preheader, %for.body1906
  %indvars.iv9675 = phi i64 [ %indvars.iv.next9676.5, %for.body1906 ], [ 0, %for.body1906.preheader ]
  %tmp1899.09126 = phi double [ %add1913.5, %for.body1906 ], [ 6.000000e+00, %for.body1906.preheader ]
  %arrayidx1908 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9675
  %1409 = load double, double* %arrayidx1908, align 8, !tbaa !62
  %arrayidx1910 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9675
  %1410 = load double, double* %arrayidx1910, align 8, !tbaa !62
  %add1911 = fadd double %1409, %1410
  %sub1912 = fadd double %add1911, -1.000000e+00
  %add1913 = fadd double %tmp1899.09126, %sub1912
  %indvars.iv.next9676 = or i64 %indvars.iv9675, 1
  %arrayidx1908.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9676
  %1411 = load double, double* %arrayidx1908.1, align 8, !tbaa !62
  %arrayidx1910.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9676
  %1412 = load double, double* %arrayidx1910.1, align 8, !tbaa !62
  %add1911.1 = fadd double %1411, %1412
  %sub1912.1 = fadd double %add1911.1, -1.000000e+00
  %add1913.1 = fadd double %add1913, %sub1912.1
  %indvars.iv.next9676.1 = add nuw nsw i64 %indvars.iv9675, 2
  %arrayidx1908.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9676.1
  %1413 = load double, double* %arrayidx1908.2, align 8, !tbaa !62
  %arrayidx1910.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9676.1
  %1414 = load double, double* %arrayidx1910.2, align 8, !tbaa !62
  %add1911.2 = fadd double %1413, %1414
  %sub1912.2 = fadd double %add1911.2, -1.000000e+00
  %add1913.2 = fadd double %add1913.1, %sub1912.2
  %indvars.iv.next9676.2 = add nuw nsw i64 %indvars.iv9675, 3
  %arrayidx1908.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9676.2
  %1415 = load double, double* %arrayidx1908.3, align 8, !tbaa !62
  %arrayidx1910.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9676.2
  %1416 = load double, double* %arrayidx1910.3, align 8, !tbaa !62
  %add1911.3 = fadd double %1415, %1416
  %sub1912.3 = fadd double %add1911.3, -1.000000e+00
  %add1913.3 = fadd double %add1913.2, %sub1912.3
  %indvars.iv.next9676.3 = add nuw nsw i64 %indvars.iv9675, 4
  %arrayidx1908.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9676.3
  %1417 = load double, double* %arrayidx1908.4, align 8, !tbaa !62
  %arrayidx1910.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9676.3
  %1418 = load double, double* %arrayidx1910.4, align 8, !tbaa !62
  %add1911.4 = fadd double %1417, %1418
  %sub1912.4 = fadd double %add1911.4, -1.000000e+00
  %add1913.4 = fadd double %add1913.3, %sub1912.4
  %indvars.iv.next9676.4 = add nuw nsw i64 %indvars.iv9675, 5
  %arrayidx1908.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9676.4
  %1419 = load double, double* %arrayidx1908.5, align 8, !tbaa !62
  %arrayidx1910.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9676.4
  %1420 = load double, double* %arrayidx1910.5, align 8, !tbaa !62
  %add1911.5 = fadd double %1419, %1420
  %sub1912.5 = fadd double %add1911.5, -1.000000e+00
  %add1913.5 = fadd double %add1913.4, %sub1912.5
  %indvars.iv.next9676.5 = add nuw nsw i64 %indvars.iv9675, 6
  %exitcond9677.5 = icmp eq i64 %indvars.iv.next9676.5, 3072
  br i1 %exitcond9677.5, label %for.cond.cleanup1905, label %for.body1906

if.then1944:                                      ; preds = %for.cond.cleanup1905
  %call1930.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018180000000, double %add1918)
  %puts8316 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end1948

if.else1946:                                      ; preds = %for.cond.cleanup1905
  %puts8305 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end1948

if.end1948:                                       ; preds = %if.else1946, %if.then1944
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %C, [3072 x double]** %930, align 8
  store [3072 x double]* %C, [3072 x double]** %932, align 8
  store [3072 x double]* %D, [3072 x double]** %934, align 8
  store [3072 x double]* %D, [3072 x double]** %936, align 8
  store [3072 x double]* %E, [3072 x double]** %938, align 8
  store [3072 x double]* %E, [3072 x double]** %940, align 8
  store [3072 x double]* %A, [3072 x double]** %942, align 8
  store [3072 x double]* %A, [3072 x double]** %944, align 8
  store [3072 x double]* %B, [3072 x double]** %946, align 8
  store [3072 x double]* %B, [3072 x double]** %948, align 8
  store i64 1, i64* %950, align 8
  store i64 1, i64* %952, align 8
  store i64 %indvars.iv9708, i64* %954, align 8
  store i64 %indvars.iv9708, i64* %956, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %958, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %960, align 8
  %1421 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l484.region_id, i32 8, i8** nonnull %929, i8** nonnull %931, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.105, i64 0, i64 0), i32 %cond1850, i32 %1404) #6
  %1422 = icmp eq i32 %1421, 0
  br i1 %1422, label %for.body2005.preheader, label %omp_offload.failed1994

omp_offload.failed1994:                           ; preds = %if.end1948
  %1423 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1424 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1423, i32 %cond1850, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..86 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B, i64 1, i64 %indvars.iv9708) #6
  br label %for.body2005.preheader

for.body2005.preheader:                           ; preds = %if.end1948, %omp_offload.failed1994
  br label %for.body2005

for.cond.cleanup2004:                             ; preds = %for.body2005
  %add2017 = fadd double %add2012.5, 0.000000e+00
  %cmp2025 = fcmp une double %add2017, 0x4152018180000000
  br i1 %cmp2025, label %if.then2043, label %if.else2045

for.body2005:                                     ; preds = %for.body2005.preheader, %for.body2005
  %indvars.iv9681 = phi i64 [ %indvars.iv.next9682.5, %for.body2005 ], [ 0, %for.body2005.preheader ]
  %tmp1998.09134 = phi double [ %add2012.5, %for.body2005 ], [ 6.000000e+00, %for.body2005.preheader ]
  %arrayidx2007 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9681
  %1425 = load double, double* %arrayidx2007, align 8, !tbaa !62
  %arrayidx2009 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9681
  %1426 = load double, double* %arrayidx2009, align 8, !tbaa !62
  %add2010 = fadd double %1425, %1426
  %sub2011 = fadd double %add2010, -1.000000e+00
  %add2012 = fadd double %tmp1998.09134, %sub2011
  %indvars.iv.next9682 = or i64 %indvars.iv9681, 1
  %arrayidx2007.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9682
  %1427 = load double, double* %arrayidx2007.1, align 8, !tbaa !62
  %arrayidx2009.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9682
  %1428 = load double, double* %arrayidx2009.1, align 8, !tbaa !62
  %add2010.1 = fadd double %1427, %1428
  %sub2011.1 = fadd double %add2010.1, -1.000000e+00
  %add2012.1 = fadd double %add2012, %sub2011.1
  %indvars.iv.next9682.1 = add nuw nsw i64 %indvars.iv9681, 2
  %arrayidx2007.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9682.1
  %1429 = load double, double* %arrayidx2007.2, align 8, !tbaa !62
  %arrayidx2009.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9682.1
  %1430 = load double, double* %arrayidx2009.2, align 8, !tbaa !62
  %add2010.2 = fadd double %1429, %1430
  %sub2011.2 = fadd double %add2010.2, -1.000000e+00
  %add2012.2 = fadd double %add2012.1, %sub2011.2
  %indvars.iv.next9682.2 = add nuw nsw i64 %indvars.iv9681, 3
  %arrayidx2007.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9682.2
  %1431 = load double, double* %arrayidx2007.3, align 8, !tbaa !62
  %arrayidx2009.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9682.2
  %1432 = load double, double* %arrayidx2009.3, align 8, !tbaa !62
  %add2010.3 = fadd double %1431, %1432
  %sub2011.3 = fadd double %add2010.3, -1.000000e+00
  %add2012.3 = fadd double %add2012.2, %sub2011.3
  %indvars.iv.next9682.3 = add nuw nsw i64 %indvars.iv9681, 4
  %arrayidx2007.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9682.3
  %1433 = load double, double* %arrayidx2007.4, align 8, !tbaa !62
  %arrayidx2009.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9682.3
  %1434 = load double, double* %arrayidx2009.4, align 8, !tbaa !62
  %add2010.4 = fadd double %1433, %1434
  %sub2011.4 = fadd double %add2010.4, -1.000000e+00
  %add2012.4 = fadd double %add2012.3, %sub2011.4
  %indvars.iv.next9682.4 = add nuw nsw i64 %indvars.iv9681, 5
  %arrayidx2007.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9682.4
  %1435 = load double, double* %arrayidx2007.5, align 8, !tbaa !62
  %arrayidx2009.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9682.4
  %1436 = load double, double* %arrayidx2009.5, align 8, !tbaa !62
  %add2010.5 = fadd double %1435, %1436
  %sub2011.5 = fadd double %add2010.5, -1.000000e+00
  %add2012.5 = fadd double %add2012.4, %sub2011.5
  %indvars.iv.next9682.5 = add nuw nsw i64 %indvars.iv9681, 6
  %exitcond9683.5 = icmp eq i64 %indvars.iv.next9682.5, 3072
  br i1 %exitcond9683.5, label %for.cond.cleanup2004, label %for.body2005

if.then2043:                                      ; preds = %for.cond.cleanup2004
  %call2029.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018180000000, double %add2017)
  %puts8315 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end2047

if.else2045:                                      ; preds = %for.cond.cleanup2004
  %puts8306 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end2047

if.end2047:                                       ; preds = %if.else2045, %if.then2043
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %C, [3072 x double]** %962, align 8
  store [3072 x double]* %C, [3072 x double]** %964, align 8
  store [3072 x double]* %D, [3072 x double]** %966, align 8
  store [3072 x double]* %D, [3072 x double]** %968, align 8
  store [3072 x double]* %E, [3072 x double]** %970, align 8
  store [3072 x double]* %E, [3072 x double]** %972, align 8
  store [3072 x double]* %A, [3072 x double]** %974, align 8
  store [3072 x double]* %A, [3072 x double]** %976, align 8
  store [3072 x double]* %B, [3072 x double]** %978, align 8
  store [3072 x double]* %B, [3072 x double]** %980, align 8
  store i64 1, i64* %982, align 8
  store i64 1, i64* %984, align 8
  store i64 %indvars.iv9708, i64* %986, align 8
  store i64 %indvars.iv9708, i64* %988, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %990, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %992, align 8
  %1437 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l507.region_id, i32 8, i8** nonnull %961, i8** nonnull %963, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.105, i64 0, i64 0), i32 %cond1850, i32 %1404) #6
  %1438 = icmp eq i32 %1437, 0
  br i1 %1438, label %for.body2104.preheader, label %omp_offload.failed2093

omp_offload.failed2093:                           ; preds = %if.end2047
  %1439 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1440 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1439, i32 %cond1850, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..90 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B, i64 1, i64 %indvars.iv9708) #6
  br label %for.body2104.preheader

for.body2104.preheader:                           ; preds = %if.end2047, %omp_offload.failed2093
  br label %for.body2104

for.cond.cleanup2103:                             ; preds = %for.body2104
  %add2116 = fadd double %add2111.5, 0.000000e+00
  %cmp2124 = fcmp une double %add2116, 0x4152018180000000
  br i1 %cmp2124, label %if.then2142, label %if.else2144

for.body2104:                                     ; preds = %for.body2104.preheader, %for.body2104
  %indvars.iv9687 = phi i64 [ %indvars.iv.next9688.5, %for.body2104 ], [ 0, %for.body2104.preheader ]
  %tmp2097.09142 = phi double [ %add2111.5, %for.body2104 ], [ 6.000000e+00, %for.body2104.preheader ]
  %arrayidx2106 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9687
  %1441 = load double, double* %arrayidx2106, align 8, !tbaa !62
  %arrayidx2108 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9687
  %1442 = load double, double* %arrayidx2108, align 8, !tbaa !62
  %add2109 = fadd double %1441, %1442
  %sub2110 = fadd double %add2109, -1.000000e+00
  %add2111 = fadd double %tmp2097.09142, %sub2110
  %indvars.iv.next9688 = or i64 %indvars.iv9687, 1
  %arrayidx2106.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9688
  %1443 = load double, double* %arrayidx2106.1, align 8, !tbaa !62
  %arrayidx2108.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9688
  %1444 = load double, double* %arrayidx2108.1, align 8, !tbaa !62
  %add2109.1 = fadd double %1443, %1444
  %sub2110.1 = fadd double %add2109.1, -1.000000e+00
  %add2111.1 = fadd double %add2111, %sub2110.1
  %indvars.iv.next9688.1 = add nuw nsw i64 %indvars.iv9687, 2
  %arrayidx2106.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9688.1
  %1445 = load double, double* %arrayidx2106.2, align 8, !tbaa !62
  %arrayidx2108.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9688.1
  %1446 = load double, double* %arrayidx2108.2, align 8, !tbaa !62
  %add2109.2 = fadd double %1445, %1446
  %sub2110.2 = fadd double %add2109.2, -1.000000e+00
  %add2111.2 = fadd double %add2111.1, %sub2110.2
  %indvars.iv.next9688.2 = add nuw nsw i64 %indvars.iv9687, 3
  %arrayidx2106.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9688.2
  %1447 = load double, double* %arrayidx2106.3, align 8, !tbaa !62
  %arrayidx2108.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9688.2
  %1448 = load double, double* %arrayidx2108.3, align 8, !tbaa !62
  %add2109.3 = fadd double %1447, %1448
  %sub2110.3 = fadd double %add2109.3, -1.000000e+00
  %add2111.3 = fadd double %add2111.2, %sub2110.3
  %indvars.iv.next9688.3 = add nuw nsw i64 %indvars.iv9687, 4
  %arrayidx2106.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9688.3
  %1449 = load double, double* %arrayidx2106.4, align 8, !tbaa !62
  %arrayidx2108.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9688.3
  %1450 = load double, double* %arrayidx2108.4, align 8, !tbaa !62
  %add2109.4 = fadd double %1449, %1450
  %sub2110.4 = fadd double %add2109.4, -1.000000e+00
  %add2111.4 = fadd double %add2111.3, %sub2110.4
  %indvars.iv.next9688.4 = add nuw nsw i64 %indvars.iv9687, 5
  %arrayidx2106.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9688.4
  %1451 = load double, double* %arrayidx2106.5, align 8, !tbaa !62
  %arrayidx2108.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9688.4
  %1452 = load double, double* %arrayidx2108.5, align 8, !tbaa !62
  %add2109.5 = fadd double %1451, %1452
  %sub2110.5 = fadd double %add2109.5, -1.000000e+00
  %add2111.5 = fadd double %add2111.4, %sub2110.5
  %indvars.iv.next9688.5 = add nuw nsw i64 %indvars.iv9687, 6
  %exitcond9689.5 = icmp eq i64 %indvars.iv.next9688.5, 3072
  br i1 %exitcond9689.5, label %for.cond.cleanup2103, label %for.body2104

if.then2142:                                      ; preds = %for.cond.cleanup2103
  %call2128.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018180000000, double %add2116)
  %puts8314 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end2146

if.else2144:                                      ; preds = %for.cond.cleanup2103
  %puts8307 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end2146

if.end2146:                                       ; preds = %if.else2144, %if.then2142
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %C, [3072 x double]** %994, align 8
  store [3072 x double]* %C, [3072 x double]** %996, align 8
  store [3072 x double]* %D, [3072 x double]** %998, align 8
  store [3072 x double]* %D, [3072 x double]** %1000, align 8
  store [3072 x double]* %E, [3072 x double]** %1002, align 8
  store [3072 x double]* %E, [3072 x double]** %1004, align 8
  store [3072 x double]* %A, [3072 x double]** %1006, align 8
  store [3072 x double]* %A, [3072 x double]** %1008, align 8
  store [3072 x double]* %B, [3072 x double]** %1010, align 8
  store [3072 x double]* %B, [3072 x double]** %1012, align 8
  store i64 1, i64* %1014, align 8
  store i64 1, i64* %1016, align 8
  store i64 %indvars.iv9708, i64* %1018, align 8
  store i64 %indvars.iv9708, i64* %1020, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %1022, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %1024, align 8
  %1453 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l530.region_id, i32 8, i8** nonnull %993, i8** nonnull %995, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.105, i64 0, i64 0), i32 %cond1850, i32 %1404) #6
  %1454 = icmp eq i32 %1453, 0
  br i1 %1454, label %for.body2203.preheader, label %omp_offload.failed2192

omp_offload.failed2192:                           ; preds = %if.end2146
  %1455 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1456 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1455, i32 %cond1850, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..94 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B, i64 1, i64 %indvars.iv9708) #6
  br label %for.body2203.preheader

for.body2203.preheader:                           ; preds = %if.end2146, %omp_offload.failed2192
  br label %for.body2203

for.cond.cleanup2202:                             ; preds = %for.body2203
  %add2215 = fadd double %add2210.5, 0.000000e+00
  %cmp2223 = fcmp une double %add2215, 0x4152018180000000
  br i1 %cmp2223, label %if.then2241, label %if.else2243

for.body2203:                                     ; preds = %for.body2203.preheader, %for.body2203
  %indvars.iv9693 = phi i64 [ %indvars.iv.next9694.5, %for.body2203 ], [ 0, %for.body2203.preheader ]
  %tmp2196.09150 = phi double [ %add2210.5, %for.body2203 ], [ 6.000000e+00, %for.body2203.preheader ]
  %arrayidx2205 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9693
  %1457 = load double, double* %arrayidx2205, align 8, !tbaa !62
  %arrayidx2207 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9693
  %1458 = load double, double* %arrayidx2207, align 8, !tbaa !62
  %add2208 = fadd double %1457, %1458
  %sub2209 = fadd double %add2208, -1.000000e+00
  %add2210 = fadd double %tmp2196.09150, %sub2209
  %indvars.iv.next9694 = or i64 %indvars.iv9693, 1
  %arrayidx2205.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9694
  %1459 = load double, double* %arrayidx2205.1, align 8, !tbaa !62
  %arrayidx2207.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9694
  %1460 = load double, double* %arrayidx2207.1, align 8, !tbaa !62
  %add2208.1 = fadd double %1459, %1460
  %sub2209.1 = fadd double %add2208.1, -1.000000e+00
  %add2210.1 = fadd double %add2210, %sub2209.1
  %indvars.iv.next9694.1 = add nuw nsw i64 %indvars.iv9693, 2
  %arrayidx2205.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9694.1
  %1461 = load double, double* %arrayidx2205.2, align 8, !tbaa !62
  %arrayidx2207.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9694.1
  %1462 = load double, double* %arrayidx2207.2, align 8, !tbaa !62
  %add2208.2 = fadd double %1461, %1462
  %sub2209.2 = fadd double %add2208.2, -1.000000e+00
  %add2210.2 = fadd double %add2210.1, %sub2209.2
  %indvars.iv.next9694.2 = add nuw nsw i64 %indvars.iv9693, 3
  %arrayidx2205.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9694.2
  %1463 = load double, double* %arrayidx2205.3, align 8, !tbaa !62
  %arrayidx2207.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9694.2
  %1464 = load double, double* %arrayidx2207.3, align 8, !tbaa !62
  %add2208.3 = fadd double %1463, %1464
  %sub2209.3 = fadd double %add2208.3, -1.000000e+00
  %add2210.3 = fadd double %add2210.2, %sub2209.3
  %indvars.iv.next9694.3 = add nuw nsw i64 %indvars.iv9693, 4
  %arrayidx2205.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9694.3
  %1465 = load double, double* %arrayidx2205.4, align 8, !tbaa !62
  %arrayidx2207.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9694.3
  %1466 = load double, double* %arrayidx2207.4, align 8, !tbaa !62
  %add2208.4 = fadd double %1465, %1466
  %sub2209.4 = fadd double %add2208.4, -1.000000e+00
  %add2210.4 = fadd double %add2210.3, %sub2209.4
  %indvars.iv.next9694.4 = add nuw nsw i64 %indvars.iv9693, 5
  %arrayidx2205.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9694.4
  %1467 = load double, double* %arrayidx2205.5, align 8, !tbaa !62
  %arrayidx2207.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9694.4
  %1468 = load double, double* %arrayidx2207.5, align 8, !tbaa !62
  %add2208.5 = fadd double %1467, %1468
  %sub2209.5 = fadd double %add2208.5, -1.000000e+00
  %add2210.5 = fadd double %add2210.4, %sub2209.5
  %indvars.iv.next9694.5 = add nuw nsw i64 %indvars.iv9693, 6
  %exitcond9695.5 = icmp eq i64 %indvars.iv.next9694.5, 3072
  br i1 %exitcond9695.5, label %for.cond.cleanup2202, label %for.body2203

if.then2241:                                      ; preds = %for.cond.cleanup2202
  %call2227.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018180000000, double %add2215)
  %puts8313 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end2245

if.else2243:                                      ; preds = %for.cond.cleanup2202
  %puts8308 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end2245

if.end2245:                                       ; preds = %if.else2243, %if.then2241
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %C, [3072 x double]** %1026, align 8
  store [3072 x double]* %C, [3072 x double]** %1028, align 8
  store [3072 x double]* %D, [3072 x double]** %1030, align 8
  store [3072 x double]* %D, [3072 x double]** %1032, align 8
  store [3072 x double]* %E, [3072 x double]** %1034, align 8
  store [3072 x double]* %E, [3072 x double]** %1036, align 8
  store [3072 x double]* %A, [3072 x double]** %1038, align 8
  store [3072 x double]* %A, [3072 x double]** %1040, align 8
  store [3072 x double]* %B, [3072 x double]** %1042, align 8
  store [3072 x double]* %B, [3072 x double]** %1044, align 8
  store i64 1, i64* %1046, align 8
  store i64 1, i64* %1048, align 8
  store i64 %indvars.iv9708, i64* %1050, align 8
  store i64 %indvars.iv9708, i64* %1052, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %1054, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %1056, align 8
  %1469 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l553.region_id, i32 8, i8** nonnull %1025, i8** nonnull %1027, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.105, i64 0, i64 0), i32 %cond1850, i32 %1404) #6
  %1470 = icmp eq i32 %1469, 0
  br i1 %1470, label %for.body2302.preheader, label %omp_offload.failed2291

omp_offload.failed2291:                           ; preds = %if.end2245
  %1471 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1472 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1471, i32 %cond1850, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..98 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B, i64 1, i64 %indvars.iv9708) #6
  br label %for.body2302.preheader

for.body2302.preheader:                           ; preds = %if.end2245, %omp_offload.failed2291
  br label %for.body2302

for.cond.cleanup2301:                             ; preds = %for.body2302
  %add2314 = fadd double %add2309.5, 0.000000e+00
  %cmp2322 = fcmp une double %add2314, 0x4152018180000000
  br i1 %cmp2322, label %if.then2340, label %if.else2342

for.body2302:                                     ; preds = %for.body2302.preheader, %for.body2302
  %indvars.iv9699 = phi i64 [ %indvars.iv.next9700.5, %for.body2302 ], [ 0, %for.body2302.preheader ]
  %tmp2295.09158 = phi double [ %add2309.5, %for.body2302 ], [ 6.000000e+00, %for.body2302.preheader ]
  %arrayidx2304 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9699
  %1473 = load double, double* %arrayidx2304, align 8, !tbaa !62
  %arrayidx2306 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9699
  %1474 = load double, double* %arrayidx2306, align 8, !tbaa !62
  %add2307 = fadd double %1473, %1474
  %sub2308 = fadd double %add2307, -1.000000e+00
  %add2309 = fadd double %tmp2295.09158, %sub2308
  %indvars.iv.next9700 = or i64 %indvars.iv9699, 1
  %arrayidx2304.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9700
  %1475 = load double, double* %arrayidx2304.1, align 8, !tbaa !62
  %arrayidx2306.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9700
  %1476 = load double, double* %arrayidx2306.1, align 8, !tbaa !62
  %add2307.1 = fadd double %1475, %1476
  %sub2308.1 = fadd double %add2307.1, -1.000000e+00
  %add2309.1 = fadd double %add2309, %sub2308.1
  %indvars.iv.next9700.1 = add nuw nsw i64 %indvars.iv9699, 2
  %arrayidx2304.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9700.1
  %1477 = load double, double* %arrayidx2304.2, align 8, !tbaa !62
  %arrayidx2306.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9700.1
  %1478 = load double, double* %arrayidx2306.2, align 8, !tbaa !62
  %add2307.2 = fadd double %1477, %1478
  %sub2308.2 = fadd double %add2307.2, -1.000000e+00
  %add2309.2 = fadd double %add2309.1, %sub2308.2
  %indvars.iv.next9700.2 = add nuw nsw i64 %indvars.iv9699, 3
  %arrayidx2304.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9700.2
  %1479 = load double, double* %arrayidx2304.3, align 8, !tbaa !62
  %arrayidx2306.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9700.2
  %1480 = load double, double* %arrayidx2306.3, align 8, !tbaa !62
  %add2307.3 = fadd double %1479, %1480
  %sub2308.3 = fadd double %add2307.3, -1.000000e+00
  %add2309.3 = fadd double %add2309.2, %sub2308.3
  %indvars.iv.next9700.3 = add nuw nsw i64 %indvars.iv9699, 4
  %arrayidx2304.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9700.3
  %1481 = load double, double* %arrayidx2304.4, align 8, !tbaa !62
  %arrayidx2306.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9700.3
  %1482 = load double, double* %arrayidx2306.4, align 8, !tbaa !62
  %add2307.4 = fadd double %1481, %1482
  %sub2308.4 = fadd double %add2307.4, -1.000000e+00
  %add2309.4 = fadd double %add2309.3, %sub2308.4
  %indvars.iv.next9700.4 = add nuw nsw i64 %indvars.iv9699, 5
  %arrayidx2304.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9700.4
  %1483 = load double, double* %arrayidx2304.5, align 8, !tbaa !62
  %arrayidx2306.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9700.4
  %1484 = load double, double* %arrayidx2306.5, align 8, !tbaa !62
  %add2307.5 = fadd double %1483, %1484
  %sub2308.5 = fadd double %add2307.5, -1.000000e+00
  %add2309.5 = fadd double %add2309.4, %sub2308.5
  %indvars.iv.next9700.5 = add nuw nsw i64 %indvars.iv9699, 6
  %exitcond9701.5 = icmp eq i64 %indvars.iv.next9700.5, 3072
  br i1 %exitcond9701.5, label %for.cond.cleanup2301, label %for.body2302

if.then2340:                                      ; preds = %for.cond.cleanup2301
  %call2326.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018180000000, double %add2314)
  %puts8312 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end2344

if.else2342:                                      ; preds = %for.cond.cleanup2301
  %puts8309 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end2344

if.end2344:                                       ; preds = %if.else2342, %if.then2340
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %C, [3072 x double]** %1058, align 8
  store [3072 x double]* %C, [3072 x double]** %1060, align 8
  store [3072 x double]* %D, [3072 x double]** %1062, align 8
  store [3072 x double]* %D, [3072 x double]** %1064, align 8
  store [3072 x double]* %E, [3072 x double]** %1066, align 8
  store [3072 x double]* %E, [3072 x double]** %1068, align 8
  store [3072 x double]* %A, [3072 x double]** %1070, align 8
  store [3072 x double]* %A, [3072 x double]** %1072, align 8
  store [3072 x double]* %B, [3072 x double]** %1074, align 8
  store [3072 x double]* %B, [3072 x double]** %1076, align 8
  store i64 1, i64* %1078, align 8
  store i64 1, i64* %1080, align 8
  store i64 %indvars.iv9708, i64* %1082, align 8
  store i64 %indvars.iv9708, i64* %1084, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %1086, align 8
  store i64 %.capture_expr..casted1889.sroa.0.0.insert.ext, i64* %1088, align 8
  %1485 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l576.region_id, i32 8, i8** nonnull %1057, i8** nonnull %1059, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.105, i64 0, i64 0), i32 %cond1850, i32 %1404) #6
  %1486 = icmp eq i32 %1485, 0
  br i1 %1486, label %for.body2401.preheader, label %omp_offload.failed2390

omp_offload.failed2390:                           ; preds = %if.end2344
  %1487 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1488 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1487, i32 %cond1850, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..102 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B, i64 1, i64 %indvars.iv9708) #6
  br label %for.body2401.preheader

for.body2401.preheader:                           ; preds = %if.end2344, %omp_offload.failed2390
  br label %for.body2401

for.cond.cleanup2400:                             ; preds = %for.body2401
  %add2413 = fadd double %add2408.5, 0.000000e+00
  %cmp2421 = fcmp une double %add2413, 0x4152018180000000
  br i1 %cmp2421, label %if.then2439, label %if.else2441

for.body2401:                                     ; preds = %for.body2401.preheader, %for.body2401
  %indvars.iv9705 = phi i64 [ %indvars.iv.next9706.5, %for.body2401 ], [ 0, %for.body2401.preheader ]
  %tmp2394.09166 = phi double [ %add2408.5, %for.body2401 ], [ 6.000000e+00, %for.body2401.preheader ]
  %arrayidx2403 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9705
  %1489 = load double, double* %arrayidx2403, align 8, !tbaa !62
  %arrayidx2405 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9705
  %1490 = load double, double* %arrayidx2405, align 8, !tbaa !62
  %add2406 = fadd double %1489, %1490
  %sub2407 = fadd double %add2406, -1.000000e+00
  %add2408 = fadd double %tmp2394.09166, %sub2407
  %indvars.iv.next9706 = or i64 %indvars.iv9705, 1
  %arrayidx2403.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9706
  %1491 = load double, double* %arrayidx2403.1, align 8, !tbaa !62
  %arrayidx2405.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9706
  %1492 = load double, double* %arrayidx2405.1, align 8, !tbaa !62
  %add2406.1 = fadd double %1491, %1492
  %sub2407.1 = fadd double %add2406.1, -1.000000e+00
  %add2408.1 = fadd double %add2408, %sub2407.1
  %indvars.iv.next9706.1 = add nuw nsw i64 %indvars.iv9705, 2
  %arrayidx2403.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9706.1
  %1493 = load double, double* %arrayidx2403.2, align 8, !tbaa !62
  %arrayidx2405.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9706.1
  %1494 = load double, double* %arrayidx2405.2, align 8, !tbaa !62
  %add2406.2 = fadd double %1493, %1494
  %sub2407.2 = fadd double %add2406.2, -1.000000e+00
  %add2408.2 = fadd double %add2408.1, %sub2407.2
  %indvars.iv.next9706.2 = add nuw nsw i64 %indvars.iv9705, 3
  %arrayidx2403.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9706.2
  %1495 = load double, double* %arrayidx2403.3, align 8, !tbaa !62
  %arrayidx2405.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9706.2
  %1496 = load double, double* %arrayidx2405.3, align 8, !tbaa !62
  %add2406.3 = fadd double %1495, %1496
  %sub2407.3 = fadd double %add2406.3, -1.000000e+00
  %add2408.3 = fadd double %add2408.2, %sub2407.3
  %indvars.iv.next9706.3 = add nuw nsw i64 %indvars.iv9705, 4
  %arrayidx2403.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9706.3
  %1497 = load double, double* %arrayidx2403.4, align 8, !tbaa !62
  %arrayidx2405.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9706.3
  %1498 = load double, double* %arrayidx2405.4, align 8, !tbaa !62
  %add2406.4 = fadd double %1497, %1498
  %sub2407.4 = fadd double %add2406.4, -1.000000e+00
  %add2408.4 = fadd double %add2408.3, %sub2407.4
  %indvars.iv.next9706.4 = add nuw nsw i64 %indvars.iv9705, 5
  %arrayidx2403.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9706.4
  %1499 = load double, double* %arrayidx2403.5, align 8, !tbaa !62
  %arrayidx2405.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9706.4
  %1500 = load double, double* %arrayidx2405.5, align 8, !tbaa !62
  %add2406.5 = fadd double %1499, %1500
  %sub2407.5 = fadd double %add2406.5, -1.000000e+00
  %add2408.5 = fadd double %add2408.4, %sub2407.5
  %indvars.iv.next9706.5 = add nuw nsw i64 %indvars.iv9705, 6
  %exitcond9707.5 = icmp eq i64 %indvars.iv.next9706.5, 3072
  br i1 %exitcond9707.5, label %for.cond.cleanup2400, label %for.body2401

if.then2439:                                      ; preds = %for.cond.cleanup2400
  %call2425.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018180000000, double %add2413)
  %puts8311 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end2443

if.else2441:                                      ; preds = %for.cond.cleanup2400
  %puts8310 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end2443

if.end2443:                                       ; preds = %if.else2441, %if.then2439
  %indvars.iv.next9709 = add nuw nsw i64 %indvars.iv9708, 78
  %cmp1839 = icmp ugt i64 %indvars.iv.next9709, %cond
  %1501 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool2447 = icmp eq i32 %1501, 0
  br i1 %cmp1839, label %for.cond.cleanup1841, label %for.body1842

for.inc2460:                                      ; preds = %for.body2454.preheader, %for.cond.cleanup1841
  %mul2461 = mul nsw i32 %tms1831.09176, 3
  %cmp1833 = icmp ult i32 %mul2461, 257
  br i1 %cmp1833, label %for.cond1838.preheader, label %for.cond2464.preheader

for.cond2470.preheader:                           ; preds = %for.cond2464.preheader, %for.inc3115
  %tms2463.09123 = phi i32 [ 1, %for.cond2464.preheader ], [ %mul3116, %for.inc3115 ]
  %1502 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool31029119 = icmp eq i32 %1502, 0
  br label %for.body2474

for.cond3119.preheader:                           ; preds = %for.inc3115
  %1503 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3179, i64 0, i64 0
  %1504 = bitcast [8 x i8*]* %.offload_baseptrs3179 to [3072 x double]**
  %1505 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3180, i64 0, i64 0
  %1506 = bitcast [8 x i8*]* %.offload_ptrs3180 to [3072 x double]**
  %1507 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3179, i64 0, i64 1
  %1508 = bitcast i8** %1507 to [3072 x double]**
  %1509 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3180, i64 0, i64 1
  %1510 = bitcast i8** %1509 to [3072 x double]**
  %1511 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3179, i64 0, i64 2
  %1512 = bitcast i8** %1511 to [3072 x double]**
  %1513 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3180, i64 0, i64 2
  %1514 = bitcast i8** %1513 to [3072 x double]**
  %1515 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3179, i64 0, i64 3
  %1516 = bitcast i8** %1515 to [3072 x double]**
  %1517 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3180, i64 0, i64 3
  %1518 = bitcast i8** %1517 to [3072 x double]**
  %1519 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3179, i64 0, i64 4
  %1520 = bitcast i8** %1519 to [3072 x double]**
  %1521 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3180, i64 0, i64 4
  %1522 = bitcast i8** %1521 to [3072 x double]**
  %1523 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3179, i64 0, i64 5
  %1524 = bitcast i8** %1523 to i64*
  %1525 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3180, i64 0, i64 5
  %1526 = bitcast i8** %1525 to i64*
  %1527 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3179, i64 0, i64 6
  %1528 = bitcast i8** %1527 to i64*
  %1529 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3180, i64 0, i64 6
  %1530 = bitcast i8** %1529 to i64*
  %1531 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3179, i64 0, i64 7
  %1532 = bitcast i8** %1531 to i64*
  %1533 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3180, i64 0, i64 7
  %1534 = bitcast i8** %1533 to i64*
  %1535 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3275, i64 0, i64 0
  %1536 = bitcast [8 x i8*]* %.offload_baseptrs3275 to [3072 x double]**
  %1537 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3276, i64 0, i64 0
  %1538 = bitcast [8 x i8*]* %.offload_ptrs3276 to [3072 x double]**
  %1539 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3275, i64 0, i64 1
  %1540 = bitcast i8** %1539 to [3072 x double]**
  %1541 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3276, i64 0, i64 1
  %1542 = bitcast i8** %1541 to [3072 x double]**
  %1543 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3275, i64 0, i64 2
  %1544 = bitcast i8** %1543 to [3072 x double]**
  %1545 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3276, i64 0, i64 2
  %1546 = bitcast i8** %1545 to [3072 x double]**
  %1547 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3275, i64 0, i64 3
  %1548 = bitcast i8** %1547 to [3072 x double]**
  %1549 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3276, i64 0, i64 3
  %1550 = bitcast i8** %1549 to [3072 x double]**
  %1551 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3275, i64 0, i64 4
  %1552 = bitcast i8** %1551 to [3072 x double]**
  %1553 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3276, i64 0, i64 4
  %1554 = bitcast i8** %1553 to [3072 x double]**
  %1555 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3275, i64 0, i64 5
  %1556 = bitcast i8** %1555 to i64*
  %1557 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3276, i64 0, i64 5
  %1558 = bitcast i8** %1557 to i64*
  %1559 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3275, i64 0, i64 6
  %1560 = bitcast i8** %1559 to i64*
  %1561 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3276, i64 0, i64 6
  %1562 = bitcast i8** %1561 to i64*
  %1563 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3275, i64 0, i64 7
  %1564 = bitcast i8** %1563 to i64*
  %1565 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3276, i64 0, i64 7
  %1566 = bitcast i8** %1565 to i64*
  %1567 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3371, i64 0, i64 0
  %1568 = bitcast [8 x i8*]* %.offload_baseptrs3371 to [3072 x double]**
  %1569 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3372, i64 0, i64 0
  %1570 = bitcast [8 x i8*]* %.offload_ptrs3372 to [3072 x double]**
  %1571 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3371, i64 0, i64 1
  %1572 = bitcast i8** %1571 to [3072 x double]**
  %1573 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3372, i64 0, i64 1
  %1574 = bitcast i8** %1573 to [3072 x double]**
  %1575 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3371, i64 0, i64 2
  %1576 = bitcast i8** %1575 to [3072 x double]**
  %1577 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3372, i64 0, i64 2
  %1578 = bitcast i8** %1577 to [3072 x double]**
  %1579 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3371, i64 0, i64 3
  %1580 = bitcast i8** %1579 to [3072 x double]**
  %1581 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3372, i64 0, i64 3
  %1582 = bitcast i8** %1581 to [3072 x double]**
  %1583 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3371, i64 0, i64 4
  %1584 = bitcast i8** %1583 to [3072 x double]**
  %1585 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3372, i64 0, i64 4
  %1586 = bitcast i8** %1585 to [3072 x double]**
  %1587 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3371, i64 0, i64 5
  %1588 = bitcast i8** %1587 to i64*
  %1589 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3372, i64 0, i64 5
  %1590 = bitcast i8** %1589 to i64*
  %1591 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3371, i64 0, i64 6
  %1592 = bitcast i8** %1591 to i64*
  %1593 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3372, i64 0, i64 6
  %1594 = bitcast i8** %1593 to i64*
  %1595 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3371, i64 0, i64 7
  %1596 = bitcast i8** %1595 to i64*
  %1597 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3372, i64 0, i64 7
  %1598 = bitcast i8** %1597 to i64*
  %1599 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3467, i64 0, i64 0
  %1600 = bitcast [8 x i8*]* %.offload_baseptrs3467 to [3072 x double]**
  %1601 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3468, i64 0, i64 0
  %1602 = bitcast [8 x i8*]* %.offload_ptrs3468 to [3072 x double]**
  %1603 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3467, i64 0, i64 1
  %1604 = bitcast i8** %1603 to [3072 x double]**
  %1605 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3468, i64 0, i64 1
  %1606 = bitcast i8** %1605 to [3072 x double]**
  %1607 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3467, i64 0, i64 2
  %1608 = bitcast i8** %1607 to [3072 x double]**
  %1609 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3468, i64 0, i64 2
  %1610 = bitcast i8** %1609 to [3072 x double]**
  %1611 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3467, i64 0, i64 3
  %1612 = bitcast i8** %1611 to [3072 x double]**
  %1613 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3468, i64 0, i64 3
  %1614 = bitcast i8** %1613 to [3072 x double]**
  %1615 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3467, i64 0, i64 4
  %1616 = bitcast i8** %1615 to [3072 x double]**
  %1617 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3468, i64 0, i64 4
  %1618 = bitcast i8** %1617 to [3072 x double]**
  %1619 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3467, i64 0, i64 5
  %1620 = bitcast i8** %1619 to i64*
  %1621 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3468, i64 0, i64 5
  %1622 = bitcast i8** %1621 to i64*
  %1623 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3467, i64 0, i64 6
  %1624 = bitcast i8** %1623 to i64*
  %1625 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3468, i64 0, i64 6
  %1626 = bitcast i8** %1625 to i64*
  %1627 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3467, i64 0, i64 7
  %1628 = bitcast i8** %1627 to i64*
  %1629 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3468, i64 0, i64 7
  %1630 = bitcast i8** %1629 to i64*
  %1631 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3563, i64 0, i64 0
  %1632 = bitcast [8 x i8*]* %.offload_baseptrs3563 to [3072 x double]**
  %1633 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3564, i64 0, i64 0
  %1634 = bitcast [8 x i8*]* %.offload_ptrs3564 to [3072 x double]**
  %1635 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3563, i64 0, i64 1
  %1636 = bitcast i8** %1635 to [3072 x double]**
  %1637 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3564, i64 0, i64 1
  %1638 = bitcast i8** %1637 to [3072 x double]**
  %1639 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3563, i64 0, i64 2
  %1640 = bitcast i8** %1639 to [3072 x double]**
  %1641 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3564, i64 0, i64 2
  %1642 = bitcast i8** %1641 to [3072 x double]**
  %1643 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3563, i64 0, i64 3
  %1644 = bitcast i8** %1643 to [3072 x double]**
  %1645 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3564, i64 0, i64 3
  %1646 = bitcast i8** %1645 to [3072 x double]**
  %1647 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3563, i64 0, i64 4
  %1648 = bitcast i8** %1647 to [3072 x double]**
  %1649 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3564, i64 0, i64 4
  %1650 = bitcast i8** %1649 to [3072 x double]**
  %1651 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3563, i64 0, i64 5
  %1652 = bitcast i8** %1651 to i64*
  %1653 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3564, i64 0, i64 5
  %1654 = bitcast i8** %1653 to i64*
  %1655 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3563, i64 0, i64 6
  %1656 = bitcast i8** %1655 to i64*
  %1657 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3564, i64 0, i64 6
  %1658 = bitcast i8** %1657 to i64*
  %1659 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3563, i64 0, i64 7
  %1660 = bitcast i8** %1659 to i64*
  %1661 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3564, i64 0, i64 7
  %1662 = bitcast i8** %1661 to i64*
  %1663 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3659, i64 0, i64 0
  %1664 = bitcast [8 x i8*]* %.offload_baseptrs3659 to [3072 x double]**
  %1665 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3660, i64 0, i64 0
  %1666 = bitcast [8 x i8*]* %.offload_ptrs3660 to [3072 x double]**
  %1667 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3659, i64 0, i64 1
  %1668 = bitcast i8** %1667 to [3072 x double]**
  %1669 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3660, i64 0, i64 1
  %1670 = bitcast i8** %1669 to [3072 x double]**
  %1671 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3659, i64 0, i64 2
  %1672 = bitcast i8** %1671 to [3072 x double]**
  %1673 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3660, i64 0, i64 2
  %1674 = bitcast i8** %1673 to [3072 x double]**
  %1675 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3659, i64 0, i64 3
  %1676 = bitcast i8** %1675 to [3072 x double]**
  %1677 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3660, i64 0, i64 3
  %1678 = bitcast i8** %1677 to [3072 x double]**
  %1679 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3659, i64 0, i64 4
  %1680 = bitcast i8** %1679 to [3072 x double]**
  %1681 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3660, i64 0, i64 4
  %1682 = bitcast i8** %1681 to [3072 x double]**
  %1683 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3659, i64 0, i64 5
  %1684 = bitcast i8** %1683 to i64*
  %1685 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3660, i64 0, i64 5
  %1686 = bitcast i8** %1685 to i64*
  %1687 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3659, i64 0, i64 6
  %1688 = bitcast i8** %1687 to i64*
  %1689 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3660, i64 0, i64 6
  %1690 = bitcast i8** %1689 to i64*
  %1691 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs3659, i64 0, i64 7
  %1692 = bitcast i8** %1691 to i64*
  %1693 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs3660, i64 0, i64 7
  %1694 = bitcast i8** %1693 to i64*
  br label %for.cond3125.preheader

for.cond.cleanup2473:                             ; preds = %if.end3098
  br i1 %tobool3102, label %for.inc3115, label %for.body3109.preheader

for.body3109.preheader:                           ; preds = %for.cond.cleanup2473
  %puts8291 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.1 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.2 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.3 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.4 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.5 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.6 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.7 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.8 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.9 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.10 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.11 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.12 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.13 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.14 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.15 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.16 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8291.17 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %for.inc3115

for.body2474:                                     ; preds = %for.cond2470.preheader, %if.end3098
  %indvars.iv9669 = phi i64 [ 1, %for.cond2470.preheader ], [ %indvars.iv.next9670, %if.end3098 ]
  %tobool31029121 = phi i1 [ %tobool31029119, %for.cond2470.preheader ], [ %tobool3102, %if.end3098 ]
  %cond2482 = select i1 %tobool31029121, i32 %tms2463.09123, i32 1
  %.capture_expr..casted2524.sroa.0.0.insert.ext = zext i32 %cond2482 to i64
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1189, align 8
  store [3072 x double]* %A, [3072 x double]** %1191, align 8
  store [3072 x double]* %C, [3072 x double]** %1193, align 8
  store [3072 x double]* %C, [3072 x double]** %1195, align 8
  store [3072 x double]* %D, [3072 x double]** %1197, align 8
  store [3072 x double]* %D, [3072 x double]** %1199, align 8
  store i64 -4607182418800017408, i64* %1201, align 8
  store i64 -4607182418800017408, i64* %1203, align 8
  store [3072 x double]* %B, [3072 x double]** %1205, align 8
  store [3072 x double]* %B, [3072 x double]** %1207, align 8
  store [3072 x double]* %E, [3072 x double]** %1209, align 8
  store [3072 x double]* %E, [3072 x double]** %1211, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1392, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1393, align 8
  store i64 %indvars.iv9669, i64* %1215, align 8
  store i64 %indvars.iv9669, i64* %1217, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1219, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1221, align 8
  %1695 = trunc i64 %indvars.iv9669 to i32
  %1696 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l612.region_id, i32 10, i8** nonnull %1188, i8** nonnull %1190, i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_sizes.128, i64 0, i64 0), i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_maptypes.129, i64 0, i64 0), i32 %cond2482, i32 %1695) #6
  %1697 = icmp eq i32 %1696, 0
  br i1 %1697, label %for.body2541.preheader, label %omp_offload.failed2530

omp_offload.failed2530:                           ; preds = %for.body2474
  %1698 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1699 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1698, i32 %cond2482, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @.omp_outlined..106 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 -4607182418800017408, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 4616189618054758400, i64 1, i64 %indvars.iv9669) #6
  br label %for.body2541.preheader

for.body2541.preheader:                           ; preds = %for.body2474, %omp_offload.failed2530
  br label %for.body2541

for.cond.cleanup2540:                             ; preds = %for.body2541
  %add2553 = fadd double %add2548.5, 0.000000e+00
  %cmp2561 = fcmp une double %add2553, 0x4152018000000000
  br i1 %cmp2561, label %if.then2579, label %if.else2581

for.body2541:                                     ; preds = %for.body2541.preheader, %for.body2541
  %indvars.iv9636 = phi i64 [ %indvars.iv.next9637.5, %for.body2541 ], [ 0, %for.body2541.preheader ]
  %tmp2534.09073 = phi double [ %add2548.5, %for.body2541 ], [ 0.000000e+00, %for.body2541.preheader ]
  %arrayidx2543 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9636
  %1700 = load double, double* %arrayidx2543, align 8, !tbaa !62
  %arrayidx2545 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9636
  %1701 = load double, double* %arrayidx2545, align 8, !tbaa !62
  %add2546 = fadd double %1700, %1701
  %sub2547 = fadd double %add2546, -1.000000e+00
  %add2548 = fadd double %tmp2534.09073, %sub2547
  %indvars.iv.next9637 = or i64 %indvars.iv9636, 1
  %arrayidx2543.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9637
  %1702 = load double, double* %arrayidx2543.1, align 8, !tbaa !62
  %arrayidx2545.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9637
  %1703 = load double, double* %arrayidx2545.1, align 8, !tbaa !62
  %add2546.1 = fadd double %1702, %1703
  %sub2547.1 = fadd double %add2546.1, -1.000000e+00
  %add2548.1 = fadd double %add2548, %sub2547.1
  %indvars.iv.next9637.1 = add nuw nsw i64 %indvars.iv9636, 2
  %arrayidx2543.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9637.1
  %1704 = load double, double* %arrayidx2543.2, align 8, !tbaa !62
  %arrayidx2545.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9637.1
  %1705 = load double, double* %arrayidx2545.2, align 8, !tbaa !62
  %add2546.2 = fadd double %1704, %1705
  %sub2547.2 = fadd double %add2546.2, -1.000000e+00
  %add2548.2 = fadd double %add2548.1, %sub2547.2
  %indvars.iv.next9637.2 = add nuw nsw i64 %indvars.iv9636, 3
  %arrayidx2543.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9637.2
  %1706 = load double, double* %arrayidx2543.3, align 8, !tbaa !62
  %arrayidx2545.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9637.2
  %1707 = load double, double* %arrayidx2545.3, align 8, !tbaa !62
  %add2546.3 = fadd double %1706, %1707
  %sub2547.3 = fadd double %add2546.3, -1.000000e+00
  %add2548.3 = fadd double %add2548.2, %sub2547.3
  %indvars.iv.next9637.3 = add nuw nsw i64 %indvars.iv9636, 4
  %arrayidx2543.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9637.3
  %1708 = load double, double* %arrayidx2543.4, align 8, !tbaa !62
  %arrayidx2545.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9637.3
  %1709 = load double, double* %arrayidx2545.4, align 8, !tbaa !62
  %add2546.4 = fadd double %1708, %1709
  %sub2547.4 = fadd double %add2546.4, -1.000000e+00
  %add2548.4 = fadd double %add2548.3, %sub2547.4
  %indvars.iv.next9637.4 = add nuw nsw i64 %indvars.iv9636, 5
  %arrayidx2543.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9637.4
  %1710 = load double, double* %arrayidx2543.5, align 8, !tbaa !62
  %arrayidx2545.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9637.4
  %1711 = load double, double* %arrayidx2545.5, align 8, !tbaa !62
  %add2546.5 = fadd double %1710, %1711
  %sub2547.5 = fadd double %add2546.5, -1.000000e+00
  %add2548.5 = fadd double %add2548.4, %sub2547.5
  %indvars.iv.next9637.5 = add nuw nsw i64 %indvars.iv9636, 6
  %exitcond9638.5 = icmp eq i64 %indvars.iv.next9637.5, 3072
  br i1 %exitcond9638.5, label %for.cond.cleanup2540, label %for.body2541

if.then2579:                                      ; preds = %for.cond.cleanup2540
  %call2565.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add2553)
  %puts8303 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end2583

if.else2581:                                      ; preds = %for.cond.cleanup2540
  %puts8292 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end2583

if.end2583:                                       ; preds = %if.else2581, %if.then2579
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1223, align 8
  store [3072 x double]* %A, [3072 x double]** %1225, align 8
  store [3072 x double]* %C, [3072 x double]** %1227, align 8
  store [3072 x double]* %C, [3072 x double]** %1229, align 8
  store [3072 x double]* %D, [3072 x double]** %1231, align 8
  store [3072 x double]* %D, [3072 x double]** %1233, align 8
  store i64 -4607182418800017408, i64* %1235, align 8
  store i64 -4607182418800017408, i64* %1237, align 8
  store [3072 x double]* %B, [3072 x double]** %1239, align 8
  store [3072 x double]* %B, [3072 x double]** %1241, align 8
  store [3072 x double]* %E, [3072 x double]** %1243, align 8
  store [3072 x double]* %E, [3072 x double]** %1245, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1394, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1395, align 8
  store i64 %indvars.iv9669, i64* %1249, align 8
  store i64 %indvars.iv9669, i64* %1251, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1253, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1255, align 8
  %1712 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l637.region_id, i32 10, i8** nonnull %1222, i8** nonnull %1224, i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_sizes.128, i64 0, i64 0), i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_maptypes.129, i64 0, i64 0), i32 %cond2482, i32 %1695) #6
  %1713 = icmp eq i32 %1712, 0
  br i1 %1713, label %for.body2644.preheader, label %omp_offload.failed2633

omp_offload.failed2633:                           ; preds = %if.end2583
  %1714 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1715 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1714, i32 %cond2482, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @.omp_outlined..110 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 -4607182418800017408, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 4616189618054758400, i64 1, i64 %indvars.iv9669) #6
  br label %for.body2644.preheader

for.body2644.preheader:                           ; preds = %if.end2583, %omp_offload.failed2633
  br label %for.body2644

for.cond.cleanup2643:                             ; preds = %for.body2644
  %add2656 = fadd double %add2651.5, 0.000000e+00
  %cmp2664 = fcmp une double %add2656, 0x4152018000000000
  br i1 %cmp2664, label %if.then2682, label %if.else2684

for.body2644:                                     ; preds = %for.body2644.preheader, %for.body2644
  %indvars.iv9642 = phi i64 [ %indvars.iv.next9643.5, %for.body2644 ], [ 0, %for.body2644.preheader ]
  %tmp2637.09081 = phi double [ %add2651.5, %for.body2644 ], [ 0.000000e+00, %for.body2644.preheader ]
  %arrayidx2646 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9642
  %1716 = load double, double* %arrayidx2646, align 8, !tbaa !62
  %arrayidx2648 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9642
  %1717 = load double, double* %arrayidx2648, align 8, !tbaa !62
  %add2649 = fadd double %1716, %1717
  %sub2650 = fadd double %add2649, -1.000000e+00
  %add2651 = fadd double %tmp2637.09081, %sub2650
  %indvars.iv.next9643 = or i64 %indvars.iv9642, 1
  %arrayidx2646.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9643
  %1718 = load double, double* %arrayidx2646.1, align 8, !tbaa !62
  %arrayidx2648.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9643
  %1719 = load double, double* %arrayidx2648.1, align 8, !tbaa !62
  %add2649.1 = fadd double %1718, %1719
  %sub2650.1 = fadd double %add2649.1, -1.000000e+00
  %add2651.1 = fadd double %add2651, %sub2650.1
  %indvars.iv.next9643.1 = add nuw nsw i64 %indvars.iv9642, 2
  %arrayidx2646.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9643.1
  %1720 = load double, double* %arrayidx2646.2, align 8, !tbaa !62
  %arrayidx2648.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9643.1
  %1721 = load double, double* %arrayidx2648.2, align 8, !tbaa !62
  %add2649.2 = fadd double %1720, %1721
  %sub2650.2 = fadd double %add2649.2, -1.000000e+00
  %add2651.2 = fadd double %add2651.1, %sub2650.2
  %indvars.iv.next9643.2 = add nuw nsw i64 %indvars.iv9642, 3
  %arrayidx2646.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9643.2
  %1722 = load double, double* %arrayidx2646.3, align 8, !tbaa !62
  %arrayidx2648.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9643.2
  %1723 = load double, double* %arrayidx2648.3, align 8, !tbaa !62
  %add2649.3 = fadd double %1722, %1723
  %sub2650.3 = fadd double %add2649.3, -1.000000e+00
  %add2651.3 = fadd double %add2651.2, %sub2650.3
  %indvars.iv.next9643.3 = add nuw nsw i64 %indvars.iv9642, 4
  %arrayidx2646.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9643.3
  %1724 = load double, double* %arrayidx2646.4, align 8, !tbaa !62
  %arrayidx2648.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9643.3
  %1725 = load double, double* %arrayidx2648.4, align 8, !tbaa !62
  %add2649.4 = fadd double %1724, %1725
  %sub2650.4 = fadd double %add2649.4, -1.000000e+00
  %add2651.4 = fadd double %add2651.3, %sub2650.4
  %indvars.iv.next9643.4 = add nuw nsw i64 %indvars.iv9642, 5
  %arrayidx2646.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9643.4
  %1726 = load double, double* %arrayidx2646.5, align 8, !tbaa !62
  %arrayidx2648.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9643.4
  %1727 = load double, double* %arrayidx2648.5, align 8, !tbaa !62
  %add2649.5 = fadd double %1726, %1727
  %sub2650.5 = fadd double %add2649.5, -1.000000e+00
  %add2651.5 = fadd double %add2651.4, %sub2650.5
  %indvars.iv.next9643.5 = add nuw nsw i64 %indvars.iv9642, 6
  %exitcond9644.5 = icmp eq i64 %indvars.iv.next9643.5, 3072
  br i1 %exitcond9644.5, label %for.cond.cleanup2643, label %for.body2644

if.then2682:                                      ; preds = %for.cond.cleanup2643
  %call2668.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add2656)
  %puts8302 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end2686

if.else2684:                                      ; preds = %for.cond.cleanup2643
  %puts8293 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end2686

if.end2686:                                       ; preds = %if.else2684, %if.then2682
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1257, align 8
  store [3072 x double]* %A, [3072 x double]** %1259, align 8
  store [3072 x double]* %C, [3072 x double]** %1261, align 8
  store [3072 x double]* %C, [3072 x double]** %1263, align 8
  store [3072 x double]* %D, [3072 x double]** %1265, align 8
  store [3072 x double]* %D, [3072 x double]** %1267, align 8
  store i64 -4607182418800017408, i64* %1269, align 8
  store i64 -4607182418800017408, i64* %1271, align 8
  store [3072 x double]* %B, [3072 x double]** %1273, align 8
  store [3072 x double]* %B, [3072 x double]** %1275, align 8
  store [3072 x double]* %E, [3072 x double]** %1277, align 8
  store [3072 x double]* %E, [3072 x double]** %1279, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1396, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1397, align 8
  store i64 %indvars.iv9669, i64* %1283, align 8
  store i64 %indvars.iv9669, i64* %1285, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1287, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1289, align 8
  %1728 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l662.region_id, i32 10, i8** nonnull %1256, i8** nonnull %1258, i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_sizes.128, i64 0, i64 0), i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_maptypes.129, i64 0, i64 0), i32 %cond2482, i32 %1695) #6
  %1729 = icmp eq i32 %1728, 0
  br i1 %1729, label %for.body2747.preheader, label %omp_offload.failed2736

omp_offload.failed2736:                           ; preds = %if.end2686
  %1730 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1731 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1730, i32 %cond2482, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @.omp_outlined..114 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 -4607182418800017408, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 4616189618054758400, i64 1, i64 %indvars.iv9669) #6
  br label %for.body2747.preheader

for.body2747.preheader:                           ; preds = %if.end2686, %omp_offload.failed2736
  br label %for.body2747

for.cond.cleanup2746:                             ; preds = %for.body2747
  %add2759 = fadd double %add2754.5, 0.000000e+00
  %cmp2767 = fcmp une double %add2759, 0x4152018000000000
  br i1 %cmp2767, label %if.then2785, label %if.else2787

for.body2747:                                     ; preds = %for.body2747.preheader, %for.body2747
  %indvars.iv9648 = phi i64 [ %indvars.iv.next9649.5, %for.body2747 ], [ 0, %for.body2747.preheader ]
  %tmp2740.09089 = phi double [ %add2754.5, %for.body2747 ], [ 0.000000e+00, %for.body2747.preheader ]
  %arrayidx2749 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9648
  %1732 = load double, double* %arrayidx2749, align 8, !tbaa !62
  %arrayidx2751 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9648
  %1733 = load double, double* %arrayidx2751, align 8, !tbaa !62
  %add2752 = fadd double %1732, %1733
  %sub2753 = fadd double %add2752, -1.000000e+00
  %add2754 = fadd double %tmp2740.09089, %sub2753
  %indvars.iv.next9649 = or i64 %indvars.iv9648, 1
  %arrayidx2749.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9649
  %1734 = load double, double* %arrayidx2749.1, align 8, !tbaa !62
  %arrayidx2751.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9649
  %1735 = load double, double* %arrayidx2751.1, align 8, !tbaa !62
  %add2752.1 = fadd double %1734, %1735
  %sub2753.1 = fadd double %add2752.1, -1.000000e+00
  %add2754.1 = fadd double %add2754, %sub2753.1
  %indvars.iv.next9649.1 = add nuw nsw i64 %indvars.iv9648, 2
  %arrayidx2749.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9649.1
  %1736 = load double, double* %arrayidx2749.2, align 8, !tbaa !62
  %arrayidx2751.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9649.1
  %1737 = load double, double* %arrayidx2751.2, align 8, !tbaa !62
  %add2752.2 = fadd double %1736, %1737
  %sub2753.2 = fadd double %add2752.2, -1.000000e+00
  %add2754.2 = fadd double %add2754.1, %sub2753.2
  %indvars.iv.next9649.2 = add nuw nsw i64 %indvars.iv9648, 3
  %arrayidx2749.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9649.2
  %1738 = load double, double* %arrayidx2749.3, align 8, !tbaa !62
  %arrayidx2751.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9649.2
  %1739 = load double, double* %arrayidx2751.3, align 8, !tbaa !62
  %add2752.3 = fadd double %1738, %1739
  %sub2753.3 = fadd double %add2752.3, -1.000000e+00
  %add2754.3 = fadd double %add2754.2, %sub2753.3
  %indvars.iv.next9649.3 = add nuw nsw i64 %indvars.iv9648, 4
  %arrayidx2749.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9649.3
  %1740 = load double, double* %arrayidx2749.4, align 8, !tbaa !62
  %arrayidx2751.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9649.3
  %1741 = load double, double* %arrayidx2751.4, align 8, !tbaa !62
  %add2752.4 = fadd double %1740, %1741
  %sub2753.4 = fadd double %add2752.4, -1.000000e+00
  %add2754.4 = fadd double %add2754.3, %sub2753.4
  %indvars.iv.next9649.4 = add nuw nsw i64 %indvars.iv9648, 5
  %arrayidx2749.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9649.4
  %1742 = load double, double* %arrayidx2749.5, align 8, !tbaa !62
  %arrayidx2751.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9649.4
  %1743 = load double, double* %arrayidx2751.5, align 8, !tbaa !62
  %add2752.5 = fadd double %1742, %1743
  %sub2753.5 = fadd double %add2752.5, -1.000000e+00
  %add2754.5 = fadd double %add2754.4, %sub2753.5
  %indvars.iv.next9649.5 = add nuw nsw i64 %indvars.iv9648, 6
  %exitcond9650.5 = icmp eq i64 %indvars.iv.next9649.5, 3072
  br i1 %exitcond9650.5, label %for.cond.cleanup2746, label %for.body2747

if.then2785:                                      ; preds = %for.cond.cleanup2746
  %call2771.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add2759)
  %puts8301 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end2789

if.else2787:                                      ; preds = %for.cond.cleanup2746
  %puts8294 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end2789

if.end2789:                                       ; preds = %if.else2787, %if.then2785
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1291, align 8
  store [3072 x double]* %A, [3072 x double]** %1293, align 8
  store [3072 x double]* %C, [3072 x double]** %1295, align 8
  store [3072 x double]* %C, [3072 x double]** %1297, align 8
  store [3072 x double]* %D, [3072 x double]** %1299, align 8
  store [3072 x double]* %D, [3072 x double]** %1301, align 8
  store i64 -4607182418800017408, i64* %1303, align 8
  store i64 -4607182418800017408, i64* %1305, align 8
  store [3072 x double]* %B, [3072 x double]** %1307, align 8
  store [3072 x double]* %B, [3072 x double]** %1309, align 8
  store [3072 x double]* %E, [3072 x double]** %1311, align 8
  store [3072 x double]* %E, [3072 x double]** %1313, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1398, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1399, align 8
  store i64 %indvars.iv9669, i64* %1317, align 8
  store i64 %indvars.iv9669, i64* %1319, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1321, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1323, align 8
  %1744 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l687.region_id, i32 10, i8** nonnull %1290, i8** nonnull %1292, i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_sizes.128, i64 0, i64 0), i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_maptypes.129, i64 0, i64 0), i32 %cond2482, i32 %1695) #6
  %1745 = icmp eq i32 %1744, 0
  br i1 %1745, label %for.body2850.preheader, label %omp_offload.failed2839

omp_offload.failed2839:                           ; preds = %if.end2789
  %1746 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1747 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1746, i32 %cond2482, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @.omp_outlined..118 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 -4607182418800017408, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 4616189618054758400, i64 1, i64 %indvars.iv9669) #6
  br label %for.body2850.preheader

for.body2850.preheader:                           ; preds = %if.end2789, %omp_offload.failed2839
  br label %for.body2850

for.cond.cleanup2849:                             ; preds = %for.body2850
  %add2862 = fadd double %add2857.5, 0.000000e+00
  %cmp2870 = fcmp une double %add2862, 0x4152018000000000
  br i1 %cmp2870, label %if.then2888, label %if.else2890

for.body2850:                                     ; preds = %for.body2850.preheader, %for.body2850
  %indvars.iv9654 = phi i64 [ %indvars.iv.next9655.5, %for.body2850 ], [ 0, %for.body2850.preheader ]
  %tmp2843.09097 = phi double [ %add2857.5, %for.body2850 ], [ 0.000000e+00, %for.body2850.preheader ]
  %arrayidx2852 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9654
  %1748 = load double, double* %arrayidx2852, align 8, !tbaa !62
  %arrayidx2854 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9654
  %1749 = load double, double* %arrayidx2854, align 8, !tbaa !62
  %add2855 = fadd double %1748, %1749
  %sub2856 = fadd double %add2855, -1.000000e+00
  %add2857 = fadd double %tmp2843.09097, %sub2856
  %indvars.iv.next9655 = or i64 %indvars.iv9654, 1
  %arrayidx2852.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9655
  %1750 = load double, double* %arrayidx2852.1, align 8, !tbaa !62
  %arrayidx2854.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9655
  %1751 = load double, double* %arrayidx2854.1, align 8, !tbaa !62
  %add2855.1 = fadd double %1750, %1751
  %sub2856.1 = fadd double %add2855.1, -1.000000e+00
  %add2857.1 = fadd double %add2857, %sub2856.1
  %indvars.iv.next9655.1 = add nuw nsw i64 %indvars.iv9654, 2
  %arrayidx2852.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9655.1
  %1752 = load double, double* %arrayidx2852.2, align 8, !tbaa !62
  %arrayidx2854.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9655.1
  %1753 = load double, double* %arrayidx2854.2, align 8, !tbaa !62
  %add2855.2 = fadd double %1752, %1753
  %sub2856.2 = fadd double %add2855.2, -1.000000e+00
  %add2857.2 = fadd double %add2857.1, %sub2856.2
  %indvars.iv.next9655.2 = add nuw nsw i64 %indvars.iv9654, 3
  %arrayidx2852.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9655.2
  %1754 = load double, double* %arrayidx2852.3, align 8, !tbaa !62
  %arrayidx2854.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9655.2
  %1755 = load double, double* %arrayidx2854.3, align 8, !tbaa !62
  %add2855.3 = fadd double %1754, %1755
  %sub2856.3 = fadd double %add2855.3, -1.000000e+00
  %add2857.3 = fadd double %add2857.2, %sub2856.3
  %indvars.iv.next9655.3 = add nuw nsw i64 %indvars.iv9654, 4
  %arrayidx2852.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9655.3
  %1756 = load double, double* %arrayidx2852.4, align 8, !tbaa !62
  %arrayidx2854.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9655.3
  %1757 = load double, double* %arrayidx2854.4, align 8, !tbaa !62
  %add2855.4 = fadd double %1756, %1757
  %sub2856.4 = fadd double %add2855.4, -1.000000e+00
  %add2857.4 = fadd double %add2857.3, %sub2856.4
  %indvars.iv.next9655.4 = add nuw nsw i64 %indvars.iv9654, 5
  %arrayidx2852.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9655.4
  %1758 = load double, double* %arrayidx2852.5, align 8, !tbaa !62
  %arrayidx2854.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9655.4
  %1759 = load double, double* %arrayidx2854.5, align 8, !tbaa !62
  %add2855.5 = fadd double %1758, %1759
  %sub2856.5 = fadd double %add2855.5, -1.000000e+00
  %add2857.5 = fadd double %add2857.4, %sub2856.5
  %indvars.iv.next9655.5 = add nuw nsw i64 %indvars.iv9654, 6
  %exitcond9656.5 = icmp eq i64 %indvars.iv.next9655.5, 3072
  br i1 %exitcond9656.5, label %for.cond.cleanup2849, label %for.body2850

if.then2888:                                      ; preds = %for.cond.cleanup2849
  %call2874.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add2862)
  %puts8300 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end2892

if.else2890:                                      ; preds = %for.cond.cleanup2849
  %puts8295 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end2892

if.end2892:                                       ; preds = %if.else2890, %if.then2888
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1325, align 8
  store [3072 x double]* %A, [3072 x double]** %1327, align 8
  store [3072 x double]* %C, [3072 x double]** %1329, align 8
  store [3072 x double]* %C, [3072 x double]** %1331, align 8
  store [3072 x double]* %D, [3072 x double]** %1333, align 8
  store [3072 x double]* %D, [3072 x double]** %1335, align 8
  store i64 -4607182418800017408, i64* %1337, align 8
  store i64 -4607182418800017408, i64* %1339, align 8
  store [3072 x double]* %B, [3072 x double]** %1341, align 8
  store [3072 x double]* %B, [3072 x double]** %1343, align 8
  store [3072 x double]* %E, [3072 x double]** %1345, align 8
  store [3072 x double]* %E, [3072 x double]** %1347, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1400, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1401, align 8
  store i64 %indvars.iv9669, i64* %1351, align 8
  store i64 %indvars.iv9669, i64* %1353, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1355, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1357, align 8
  %1760 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l712.region_id, i32 10, i8** nonnull %1324, i8** nonnull %1326, i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_sizes.128, i64 0, i64 0), i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_maptypes.129, i64 0, i64 0), i32 %cond2482, i32 %1695) #6
  %1761 = icmp eq i32 %1760, 0
  br i1 %1761, label %for.body2953.preheader, label %omp_offload.failed2942

omp_offload.failed2942:                           ; preds = %if.end2892
  %1762 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1763 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1762, i32 %cond2482, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @.omp_outlined..122 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 -4607182418800017408, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 4616189618054758400, i64 1, i64 %indvars.iv9669) #6
  br label %for.body2953.preheader

for.body2953.preheader:                           ; preds = %if.end2892, %omp_offload.failed2942
  br label %for.body2953

for.cond.cleanup2952:                             ; preds = %for.body2953
  %add2965 = fadd double %add2960.5, 0.000000e+00
  %cmp2973 = fcmp une double %add2965, 0x4152018000000000
  br i1 %cmp2973, label %if.then2991, label %if.else2993

for.body2953:                                     ; preds = %for.body2953.preheader, %for.body2953
  %indvars.iv9660 = phi i64 [ %indvars.iv.next9661.5, %for.body2953 ], [ 0, %for.body2953.preheader ]
  %tmp2946.09105 = phi double [ %add2960.5, %for.body2953 ], [ 0.000000e+00, %for.body2953.preheader ]
  %arrayidx2955 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9660
  %1764 = load double, double* %arrayidx2955, align 8, !tbaa !62
  %arrayidx2957 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9660
  %1765 = load double, double* %arrayidx2957, align 8, !tbaa !62
  %add2958 = fadd double %1764, %1765
  %sub2959 = fadd double %add2958, -1.000000e+00
  %add2960 = fadd double %tmp2946.09105, %sub2959
  %indvars.iv.next9661 = or i64 %indvars.iv9660, 1
  %arrayidx2955.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9661
  %1766 = load double, double* %arrayidx2955.1, align 8, !tbaa !62
  %arrayidx2957.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9661
  %1767 = load double, double* %arrayidx2957.1, align 8, !tbaa !62
  %add2958.1 = fadd double %1766, %1767
  %sub2959.1 = fadd double %add2958.1, -1.000000e+00
  %add2960.1 = fadd double %add2960, %sub2959.1
  %indvars.iv.next9661.1 = add nuw nsw i64 %indvars.iv9660, 2
  %arrayidx2955.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9661.1
  %1768 = load double, double* %arrayidx2955.2, align 8, !tbaa !62
  %arrayidx2957.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9661.1
  %1769 = load double, double* %arrayidx2957.2, align 8, !tbaa !62
  %add2958.2 = fadd double %1768, %1769
  %sub2959.2 = fadd double %add2958.2, -1.000000e+00
  %add2960.2 = fadd double %add2960.1, %sub2959.2
  %indvars.iv.next9661.2 = add nuw nsw i64 %indvars.iv9660, 3
  %arrayidx2955.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9661.2
  %1770 = load double, double* %arrayidx2955.3, align 8, !tbaa !62
  %arrayidx2957.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9661.2
  %1771 = load double, double* %arrayidx2957.3, align 8, !tbaa !62
  %add2958.3 = fadd double %1770, %1771
  %sub2959.3 = fadd double %add2958.3, -1.000000e+00
  %add2960.3 = fadd double %add2960.2, %sub2959.3
  %indvars.iv.next9661.3 = add nuw nsw i64 %indvars.iv9660, 4
  %arrayidx2955.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9661.3
  %1772 = load double, double* %arrayidx2955.4, align 8, !tbaa !62
  %arrayidx2957.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9661.3
  %1773 = load double, double* %arrayidx2957.4, align 8, !tbaa !62
  %add2958.4 = fadd double %1772, %1773
  %sub2959.4 = fadd double %add2958.4, -1.000000e+00
  %add2960.4 = fadd double %add2960.3, %sub2959.4
  %indvars.iv.next9661.4 = add nuw nsw i64 %indvars.iv9660, 5
  %arrayidx2955.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9661.4
  %1774 = load double, double* %arrayidx2955.5, align 8, !tbaa !62
  %arrayidx2957.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9661.4
  %1775 = load double, double* %arrayidx2957.5, align 8, !tbaa !62
  %add2958.5 = fadd double %1774, %1775
  %sub2959.5 = fadd double %add2958.5, -1.000000e+00
  %add2960.5 = fadd double %add2960.4, %sub2959.5
  %indvars.iv.next9661.5 = add nuw nsw i64 %indvars.iv9660, 6
  %exitcond9662.5 = icmp eq i64 %indvars.iv.next9661.5, 3072
  br i1 %exitcond9662.5, label %for.cond.cleanup2952, label %for.body2953

if.then2991:                                      ; preds = %for.cond.cleanup2952
  %call2977.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add2965)
  %puts8299 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end2995

if.else2993:                                      ; preds = %for.cond.cleanup2952
  %puts8296 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end2995

if.end2995:                                       ; preds = %if.else2993, %if.then2991
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1359, align 8
  store [3072 x double]* %A, [3072 x double]** %1361, align 8
  store [3072 x double]* %C, [3072 x double]** %1363, align 8
  store [3072 x double]* %C, [3072 x double]** %1365, align 8
  store [3072 x double]* %D, [3072 x double]** %1367, align 8
  store [3072 x double]* %D, [3072 x double]** %1369, align 8
  store i64 -4607182418800017408, i64* %1371, align 8
  store i64 -4607182418800017408, i64* %1373, align 8
  store [3072 x double]* %B, [3072 x double]** %1375, align 8
  store [3072 x double]* %B, [3072 x double]** %1377, align 8
  store [3072 x double]* %E, [3072 x double]** %1379, align 8
  store [3072 x double]* %E, [3072 x double]** %1381, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1402, align 8
  store <2 x i64> <i64 4616189618054758400, i64 1>, <2 x i64>* %1403, align 8
  store i64 %indvars.iv9669, i64* %1385, align 8
  store i64 %indvars.iv9669, i64* %1387, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1389, align 8
  store i64 %.capture_expr..casted2524.sroa.0.0.insert.ext, i64* %1391, align 8
  %1776 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l737.region_id, i32 10, i8** nonnull %1358, i8** nonnull %1360, i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_sizes.128, i64 0, i64 0), i64* getelementptr inbounds ([10 x i64], [10 x i64]* @.offload_maptypes.129, i64 0, i64 0), i32 %cond2482, i32 %1695) #6
  %1777 = icmp eq i32 %1776, 0
  br i1 %1777, label %for.body3056.preheader, label %omp_offload.failed3045

omp_offload.failed3045:                           ; preds = %if.end2995
  %1778 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1779 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1778, i32 %cond2482, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64, i64, i64)* @.omp_outlined..126 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 -4607182418800017408, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 4616189618054758400, i64 1, i64 %indvars.iv9669) #6
  br label %for.body3056.preheader

for.body3056.preheader:                           ; preds = %if.end2995, %omp_offload.failed3045
  br label %for.body3056

for.cond.cleanup3055:                             ; preds = %for.body3056
  %add3068 = fadd double %add3063.5, 0.000000e+00
  %cmp3076 = fcmp une double %add3068, 0x4152018000000000
  br i1 %cmp3076, label %if.then3094, label %if.else3096

for.body3056:                                     ; preds = %for.body3056.preheader, %for.body3056
  %indvars.iv9666 = phi i64 [ %indvars.iv.next9667.5, %for.body3056 ], [ 0, %for.body3056.preheader ]
  %tmp3049.09113 = phi double [ %add3063.5, %for.body3056 ], [ 0.000000e+00, %for.body3056.preheader ]
  %arrayidx3058 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9666
  %1780 = load double, double* %arrayidx3058, align 8, !tbaa !62
  %arrayidx3060 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9666
  %1781 = load double, double* %arrayidx3060, align 8, !tbaa !62
  %add3061 = fadd double %1780, %1781
  %sub3062 = fadd double %add3061, -1.000000e+00
  %add3063 = fadd double %tmp3049.09113, %sub3062
  %indvars.iv.next9667 = or i64 %indvars.iv9666, 1
  %arrayidx3058.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9667
  %1782 = load double, double* %arrayidx3058.1, align 8, !tbaa !62
  %arrayidx3060.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9667
  %1783 = load double, double* %arrayidx3060.1, align 8, !tbaa !62
  %add3061.1 = fadd double %1782, %1783
  %sub3062.1 = fadd double %add3061.1, -1.000000e+00
  %add3063.1 = fadd double %add3063, %sub3062.1
  %indvars.iv.next9667.1 = add nuw nsw i64 %indvars.iv9666, 2
  %arrayidx3058.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9667.1
  %1784 = load double, double* %arrayidx3058.2, align 8, !tbaa !62
  %arrayidx3060.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9667.1
  %1785 = load double, double* %arrayidx3060.2, align 8, !tbaa !62
  %add3061.2 = fadd double %1784, %1785
  %sub3062.2 = fadd double %add3061.2, -1.000000e+00
  %add3063.2 = fadd double %add3063.1, %sub3062.2
  %indvars.iv.next9667.2 = add nuw nsw i64 %indvars.iv9666, 3
  %arrayidx3058.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9667.2
  %1786 = load double, double* %arrayidx3058.3, align 8, !tbaa !62
  %arrayidx3060.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9667.2
  %1787 = load double, double* %arrayidx3060.3, align 8, !tbaa !62
  %add3061.3 = fadd double %1786, %1787
  %sub3062.3 = fadd double %add3061.3, -1.000000e+00
  %add3063.3 = fadd double %add3063.2, %sub3062.3
  %indvars.iv.next9667.3 = add nuw nsw i64 %indvars.iv9666, 4
  %arrayidx3058.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9667.3
  %1788 = load double, double* %arrayidx3058.4, align 8, !tbaa !62
  %arrayidx3060.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9667.3
  %1789 = load double, double* %arrayidx3060.4, align 8, !tbaa !62
  %add3061.4 = fadd double %1788, %1789
  %sub3062.4 = fadd double %add3061.4, -1.000000e+00
  %add3063.4 = fadd double %add3063.3, %sub3062.4
  %indvars.iv.next9667.4 = add nuw nsw i64 %indvars.iv9666, 5
  %arrayidx3058.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9667.4
  %1790 = load double, double* %arrayidx3058.5, align 8, !tbaa !62
  %arrayidx3060.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9667.4
  %1791 = load double, double* %arrayidx3060.5, align 8, !tbaa !62
  %add3061.5 = fadd double %1790, %1791
  %sub3062.5 = fadd double %add3061.5, -1.000000e+00
  %add3063.5 = fadd double %add3063.4, %sub3062.5
  %indvars.iv.next9667.5 = add nuw nsw i64 %indvars.iv9666, 6
  %exitcond9668.5 = icmp eq i64 %indvars.iv.next9667.5, 3072
  br i1 %exitcond9668.5, label %for.cond.cleanup3055, label %for.body3056

if.then3094:                                      ; preds = %for.cond.cleanup3055
  %call3080.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add3068)
  %puts8298 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end3098

if.else3096:                                      ; preds = %for.cond.cleanup3055
  %puts8297 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end3098

if.end3098:                                       ; preds = %if.else3096, %if.then3094
  %indvars.iv.next9670 = add nuw nsw i64 %indvars.iv9669, 78
  %cmp2471 = icmp ugt i64 %indvars.iv.next9670, %cond
  %1792 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool3102 = icmp eq i32 %1792, 0
  br i1 %cmp2471, label %for.cond.cleanup2473, label %for.body2474

for.inc3115:                                      ; preds = %for.body3109.preheader, %for.cond.cleanup2473
  %mul3116 = mul nsw i32 %tms2463.09123, 3
  %cmp2465 = icmp ult i32 %mul3116, 257
  br i1 %cmp2465, label %for.cond2470.preheader, label %for.cond3119.preheader

for.cond3125.preheader:                           ; preds = %for.cond3119.preheader, %for.inc3730
  %tms3118.09070 = phi i32 [ 1, %for.cond3119.preheader ], [ %mul3731, %for.inc3730 ]
  %1793 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool37179066 = icmp eq i32 %1793, 0
  br label %for.body3129

for.cond.cleanup3122:                             ; preds = %for.inc3730
  %call3733 = call noalias i8* @malloc(i64 200000) #6
  %1794 = bitcast i8* %call3733 to double*
  %call3734 = call noalias i8* @malloc(i64 200000) #6
  %1795 = bitcast i8* %call3734 to double*
  %call3735 = call noalias i8* @malloc(i64 200000) #6
  %1796 = bitcast i8* %call3735 to double*
  %call3736 = call noalias i8* @malloc(i64 200000) #6
  %1797 = bitcast i8* %call3736 to double*
  %1798 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3740, i64 0, i64 0
  store i8* %call3733, i8** %1798, align 8
  %1799 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3741, i64 0, i64 0
  store i8* %call3733, i8** %1799, align 8
  %1800 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3740, i64 0, i64 1
  store i8* %call3735, i8** %1800, align 8
  %1801 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3741, i64 0, i64 1
  store i8* %call3735, i8** %1801, align 8
  %1802 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3740, i64 0, i64 2
  store i8* %call3736, i8** %1802, align 8
  %1803 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3741, i64 0, i64 2
  store i8* %call3736, i8** %1803, align 8
  call void @__tgt_target_data_begin(i64 -1, i32 3, i8** nonnull %1798, i8** nonnull %1799, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_sizes.276, i64 0, i64 0), i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  %puts = call i32 @puts(i8* getelementptr inbounds ([20 x i8], [20 x i8]* @str.333, i64 0, i64 0))
  %1804 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3751, i64 0, i64 0
  %1805 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3752, i64 0, i64 0
  %1806 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes, i64 0, i64 0
  %1807 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3751, i64 0, i64 1
  %1808 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3752, i64 0, i64 1
  %1809 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes, i64 0, i64 1
  %1810 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3751, i64 0, i64 2
  %1811 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3752, i64 0, i64 2
  %1812 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes, i64 0, i64 2
  %1813 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3773, i64 0, i64 0
  %1814 = bitcast [6 x i8*]* %.offload_baseptrs3773 to i64*
  %1815 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3774, i64 0, i64 0
  %1816 = bitcast [6 x i8*]* %.offload_ptrs3774 to i64*
  %1817 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3773, i64 0, i64 1
  %1818 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3774, i64 0, i64 1
  %1819 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3773, i64 0, i64 2
  %1820 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3774, i64 0, i64 2
  %1821 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3773, i64 0, i64 3
  %1822 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3774, i64 0, i64 3
  %1823 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3773, i64 0, i64 4
  %1824 = bitcast i8** %1823 to i64*
  %1825 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3774, i64 0, i64 4
  %1826 = bitcast i8** %1825 to i64*
  %1827 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3773, i64 0, i64 5
  %1828 = bitcast i8** %1827 to i64*
  %1829 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3774, i64 0, i64 5
  %1830 = bitcast i8** %1829 to i64*
  %1831 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs3809, i64 0, i64 0
  %1832 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs3810, i64 0, i64 0
  %1833 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes3811, i64 0, i64 0
  %arrayidx.i = getelementptr inbounds i8, i8* %call3734, i64 199872
  %arrayidx2.i = getelementptr inbounds i8, i8* %call3733, i64 199872
  %arrayidx5.i = getelementptr inbounds i8, i8* %call3735, i64 199872
  %arrayidx8.i = getelementptr inbounds i8, i8* %call3736, i64 199872
  %arrayidx.i.2 = getelementptr inbounds i8, i8* %call3734, i64 199888
  %arrayidx2.i.2 = getelementptr inbounds i8, i8* %call3733, i64 199888
  %arrayidx5.i.2 = getelementptr inbounds i8, i8* %call3735, i64 199888
  %arrayidx8.i.2 = getelementptr inbounds i8, i8* %call3736, i64 199888
  %arrayidx.i.4 = getelementptr inbounds i8, i8* %call3734, i64 199904
  %arrayidx2.i.4 = getelementptr inbounds i8, i8* %call3733, i64 199904
  %arrayidx5.i.4 = getelementptr inbounds i8, i8* %call3735, i64 199904
  %arrayidx8.i.4 = getelementptr inbounds i8, i8* %call3736, i64 199904
  %arrayidx.i.6 = getelementptr inbounds i8, i8* %call3734, i64 199920
  %arrayidx2.i.6 = getelementptr inbounds i8, i8* %call3733, i64 199920
  %arrayidx5.i.6 = getelementptr inbounds i8, i8* %call3735, i64 199920
  %arrayidx8.i.6 = getelementptr inbounds i8, i8* %call3736, i64 199920
  %arrayidx.i.8 = getelementptr inbounds i8, i8* %call3734, i64 199936
  %arrayidx2.i.8 = getelementptr inbounds i8, i8* %call3733, i64 199936
  %arrayidx5.i.8 = getelementptr inbounds i8, i8* %call3735, i64 199936
  %arrayidx8.i.8 = getelementptr inbounds i8, i8* %call3736, i64 199936
  %arrayidx.i.10 = getelementptr inbounds i8, i8* %call3734, i64 199952
  %arrayidx2.i.10 = getelementptr inbounds i8, i8* %call3733, i64 199952
  %arrayidx5.i.10 = getelementptr inbounds i8, i8* %call3735, i64 199952
  %arrayidx8.i.10 = getelementptr inbounds i8, i8* %call3736, i64 199952
  %arrayidx.i.12 = getelementptr inbounds i8, i8* %call3734, i64 199968
  %arrayidx2.i.12 = getelementptr inbounds i8, i8* %call3733, i64 199968
  %arrayidx5.i.12 = getelementptr inbounds i8, i8* %call3735, i64 199968
  %arrayidx8.i.12 = getelementptr inbounds i8, i8* %call3736, i64 199968
  %arrayidx.i.14 = getelementptr inbounds i8, i8* %call3734, i64 199984
  %arrayidx2.i.14 = getelementptr inbounds i8, i8* %call3733, i64 199984
  %arrayidx5.i.14 = getelementptr inbounds i8, i8* %call3735, i64 199984
  %arrayidx8.i.14 = getelementptr inbounds i8, i8* %call3736, i64 199984
  %1834 = bitcast i8* %arrayidx.i to <2 x double>*
  %1835 = bitcast i8* %arrayidx2.i to <2 x double>*
  %1836 = bitcast i8* %arrayidx5.i to <2 x double>*
  %1837 = bitcast i8* %arrayidx8.i to <2 x double>*
  %1838 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %1839 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %1840 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %1841 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %1842 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %1843 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %1844 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %1845 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %1846 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %1847 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %1848 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %1849 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %1850 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %1851 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %1852 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %1853 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %1854 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %1855 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %1856 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %1857 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %1858 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %1859 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %1860 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %1861 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %1862 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %1863 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %1864 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %1865 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i.preheader

for.cond.cleanup3128:                             ; preds = %if.end3713
  br i1 %tobool3717, label %for.inc3730, label %for.body3724.preheader

for.body3724.preheader:                           ; preds = %for.cond.cleanup3128
  %puts8278 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.1 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.2 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.3 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.4 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.5 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.6 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.7 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.8 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.9 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.10 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.11 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.12 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.13 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.14 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.15 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.16 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8278.17 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %for.inc3730

for.body3129:                                     ; preds = %for.cond3125.preheader, %if.end3713
  %indvars.iv9630 = phi i64 [ 1, %for.cond3125.preheader ], [ %indvars.iv.next9631, %if.end3713 ]
  %tobool37179068 = phi i1 [ %tobool37179066, %for.cond3125.preheader ], [ %tobool3717, %if.end3713 ]
  %cond3137 = select i1 %tobool37179068, i32 %tms3118.09070, i32 1
  %.capture_expr..casted3175.sroa.0.0.insert.ext = zext i32 %cond3137 to i64
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1504, align 8
  store [3072 x double]* %A, [3072 x double]** %1506, align 8
  store [3072 x double]* %C, [3072 x double]** %1508, align 8
  store [3072 x double]* %C, [3072 x double]** %1510, align 8
  store [3072 x double]* %D, [3072 x double]** %1512, align 8
  store [3072 x double]* %D, [3072 x double]** %1514, align 8
  store [3072 x double]* %B, [3072 x double]** %1516, align 8
  store [3072 x double]* %B, [3072 x double]** %1518, align 8
  store [3072 x double]* %E, [3072 x double]** %1520, align 8
  store [3072 x double]* %E, [3072 x double]** %1522, align 8
  store i64 1, i64* %1524, align 8
  store i64 1, i64* %1526, align 8
  store i64 %indvars.iv9630, i64* %1528, align 8
  store i64 %indvars.iv9630, i64* %1530, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1532, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1534, align 8
  %1866 = trunc i64 %indvars.iv9630 to i32
  %1867 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l883.region_id, i32 8, i8** nonnull %1503, i8** nonnull %1505, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond3137, i32 %1866) #6
  %1868 = icmp eq i32 %1867, 0
  br i1 %1868, label %for.body3191.preheader, label %omp_offload.failed3181

omp_offload.failed3181:                           ; preds = %for.body3129
  %1869 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1870 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1869, i32 %cond3137, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..130 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9630) #6
  br label %for.body3191.preheader

for.body3191.preheader:                           ; preds = %for.body3129, %omp_offload.failed3181
  br label %for.body3191

for.cond.cleanup3190:                             ; preds = %for.body3191
  %add3203 = fadd double %add3198.5, 0.000000e+00
  %cmp3211 = fcmp une double %add3203, 0x4152018000000000
  br i1 %cmp3211, label %if.then3229, label %if.else3231

for.body3191:                                     ; preds = %for.body3191.preheader, %for.body3191
  %indvars.iv9597 = phi i64 [ %indvars.iv.next9598.5, %for.body3191 ], [ 0, %for.body3191.preheader ]
  %tmp3185.09020 = phi double [ %add3198.5, %for.body3191 ], [ 0.000000e+00, %for.body3191.preheader ]
  %arrayidx3193 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9597
  %1871 = load double, double* %arrayidx3193, align 8, !tbaa !62
  %arrayidx3195 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9597
  %1872 = load double, double* %arrayidx3195, align 8, !tbaa !62
  %add3196 = fadd double %1871, %1872
  %sub3197 = fadd double %add3196, -1.000000e+00
  %add3198 = fadd double %tmp3185.09020, %sub3197
  %indvars.iv.next9598 = or i64 %indvars.iv9597, 1
  %arrayidx3193.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9598
  %1873 = load double, double* %arrayidx3193.1, align 8, !tbaa !62
  %arrayidx3195.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9598
  %1874 = load double, double* %arrayidx3195.1, align 8, !tbaa !62
  %add3196.1 = fadd double %1873, %1874
  %sub3197.1 = fadd double %add3196.1, -1.000000e+00
  %add3198.1 = fadd double %add3198, %sub3197.1
  %indvars.iv.next9598.1 = add nuw nsw i64 %indvars.iv9597, 2
  %arrayidx3193.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9598.1
  %1875 = load double, double* %arrayidx3193.2, align 8, !tbaa !62
  %arrayidx3195.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9598.1
  %1876 = load double, double* %arrayidx3195.2, align 8, !tbaa !62
  %add3196.2 = fadd double %1875, %1876
  %sub3197.2 = fadd double %add3196.2, -1.000000e+00
  %add3198.2 = fadd double %add3198.1, %sub3197.2
  %indvars.iv.next9598.2 = add nuw nsw i64 %indvars.iv9597, 3
  %arrayidx3193.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9598.2
  %1877 = load double, double* %arrayidx3193.3, align 8, !tbaa !62
  %arrayidx3195.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9598.2
  %1878 = load double, double* %arrayidx3195.3, align 8, !tbaa !62
  %add3196.3 = fadd double %1877, %1878
  %sub3197.3 = fadd double %add3196.3, -1.000000e+00
  %add3198.3 = fadd double %add3198.2, %sub3197.3
  %indvars.iv.next9598.3 = add nuw nsw i64 %indvars.iv9597, 4
  %arrayidx3193.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9598.3
  %1879 = load double, double* %arrayidx3193.4, align 8, !tbaa !62
  %arrayidx3195.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9598.3
  %1880 = load double, double* %arrayidx3195.4, align 8, !tbaa !62
  %add3196.4 = fadd double %1879, %1880
  %sub3197.4 = fadd double %add3196.4, -1.000000e+00
  %add3198.4 = fadd double %add3198.3, %sub3197.4
  %indvars.iv.next9598.4 = add nuw nsw i64 %indvars.iv9597, 5
  %arrayidx3193.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9598.4
  %1881 = load double, double* %arrayidx3193.5, align 8, !tbaa !62
  %arrayidx3195.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9598.4
  %1882 = load double, double* %arrayidx3195.5, align 8, !tbaa !62
  %add3196.5 = fadd double %1881, %1882
  %sub3197.5 = fadd double %add3196.5, -1.000000e+00
  %add3198.5 = fadd double %add3198.4, %sub3197.5
  %indvars.iv.next9598.5 = add nuw nsw i64 %indvars.iv9597, 6
  %exitcond9599.5 = icmp eq i64 %indvars.iv.next9598.5, 3072
  br i1 %exitcond9599.5, label %for.cond.cleanup3190, label %for.body3191

if.then3229:                                      ; preds = %for.cond.cleanup3190
  %call3215.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add3203)
  %puts8290 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end3233

if.else3231:                                      ; preds = %for.cond.cleanup3190
  %puts8279 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end3233

if.end3233:                                       ; preds = %if.else3231, %if.then3229
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1536, align 8
  store [3072 x double]* %A, [3072 x double]** %1538, align 8
  store [3072 x double]* %C, [3072 x double]** %1540, align 8
  store [3072 x double]* %C, [3072 x double]** %1542, align 8
  store [3072 x double]* %D, [3072 x double]** %1544, align 8
  store [3072 x double]* %D, [3072 x double]** %1546, align 8
  store [3072 x double]* %B, [3072 x double]** %1548, align 8
  store [3072 x double]* %B, [3072 x double]** %1550, align 8
  store [3072 x double]* %E, [3072 x double]** %1552, align 8
  store [3072 x double]* %E, [3072 x double]** %1554, align 8
  store i64 1, i64* %1556, align 8
  store i64 1, i64* %1558, align 8
  store i64 %indvars.iv9630, i64* %1560, align 8
  store i64 %indvars.iv9630, i64* %1562, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1564, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1566, align 8
  %1883 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l905.region_id, i32 8, i8** nonnull %1535, i8** nonnull %1537, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond3137, i32 %1866) #6
  %1884 = icmp eq i32 %1883, 0
  br i1 %1884, label %for.body3287.preheader, label %omp_offload.failed3277

omp_offload.failed3277:                           ; preds = %if.end3233
  %1885 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1886 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1885, i32 %cond3137, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..134 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9630) #6
  br label %for.body3287.preheader

for.body3287.preheader:                           ; preds = %if.end3233, %omp_offload.failed3277
  br label %for.body3287

for.cond.cleanup3286:                             ; preds = %for.body3287
  %add3299 = fadd double %add3294.5, 0.000000e+00
  %cmp3307 = fcmp une double %add3299, 0x4152018000000000
  br i1 %cmp3307, label %if.then3325, label %if.else3327

for.body3287:                                     ; preds = %for.body3287.preheader, %for.body3287
  %indvars.iv9603 = phi i64 [ %indvars.iv.next9604.5, %for.body3287 ], [ 0, %for.body3287.preheader ]
  %tmp3281.09027 = phi double [ %add3294.5, %for.body3287 ], [ 0.000000e+00, %for.body3287.preheader ]
  %arrayidx3289 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9603
  %1887 = load double, double* %arrayidx3289, align 8, !tbaa !62
  %arrayidx3291 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9603
  %1888 = load double, double* %arrayidx3291, align 8, !tbaa !62
  %add3292 = fadd double %1887, %1888
  %sub3293 = fadd double %add3292, -1.000000e+00
  %add3294 = fadd double %tmp3281.09027, %sub3293
  %indvars.iv.next9604 = or i64 %indvars.iv9603, 1
  %arrayidx3289.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9604
  %1889 = load double, double* %arrayidx3289.1, align 8, !tbaa !62
  %arrayidx3291.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9604
  %1890 = load double, double* %arrayidx3291.1, align 8, !tbaa !62
  %add3292.1 = fadd double %1889, %1890
  %sub3293.1 = fadd double %add3292.1, -1.000000e+00
  %add3294.1 = fadd double %add3294, %sub3293.1
  %indvars.iv.next9604.1 = add nuw nsw i64 %indvars.iv9603, 2
  %arrayidx3289.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9604.1
  %1891 = load double, double* %arrayidx3289.2, align 8, !tbaa !62
  %arrayidx3291.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9604.1
  %1892 = load double, double* %arrayidx3291.2, align 8, !tbaa !62
  %add3292.2 = fadd double %1891, %1892
  %sub3293.2 = fadd double %add3292.2, -1.000000e+00
  %add3294.2 = fadd double %add3294.1, %sub3293.2
  %indvars.iv.next9604.2 = add nuw nsw i64 %indvars.iv9603, 3
  %arrayidx3289.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9604.2
  %1893 = load double, double* %arrayidx3289.3, align 8, !tbaa !62
  %arrayidx3291.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9604.2
  %1894 = load double, double* %arrayidx3291.3, align 8, !tbaa !62
  %add3292.3 = fadd double %1893, %1894
  %sub3293.3 = fadd double %add3292.3, -1.000000e+00
  %add3294.3 = fadd double %add3294.2, %sub3293.3
  %indvars.iv.next9604.3 = add nuw nsw i64 %indvars.iv9603, 4
  %arrayidx3289.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9604.3
  %1895 = load double, double* %arrayidx3289.4, align 8, !tbaa !62
  %arrayidx3291.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9604.3
  %1896 = load double, double* %arrayidx3291.4, align 8, !tbaa !62
  %add3292.4 = fadd double %1895, %1896
  %sub3293.4 = fadd double %add3292.4, -1.000000e+00
  %add3294.4 = fadd double %add3294.3, %sub3293.4
  %indvars.iv.next9604.4 = add nuw nsw i64 %indvars.iv9603, 5
  %arrayidx3289.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9604.4
  %1897 = load double, double* %arrayidx3289.5, align 8, !tbaa !62
  %arrayidx3291.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9604.4
  %1898 = load double, double* %arrayidx3291.5, align 8, !tbaa !62
  %add3292.5 = fadd double %1897, %1898
  %sub3293.5 = fadd double %add3292.5, -1.000000e+00
  %add3294.5 = fadd double %add3294.4, %sub3293.5
  %indvars.iv.next9604.5 = add nuw nsw i64 %indvars.iv9603, 6
  %exitcond9605.5 = icmp eq i64 %indvars.iv.next9604.5, 3072
  br i1 %exitcond9605.5, label %for.cond.cleanup3286, label %for.body3287

if.then3325:                                      ; preds = %for.cond.cleanup3286
  %call3311.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add3299)
  %puts8289 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end3329

if.else3327:                                      ; preds = %for.cond.cleanup3286
  %puts8280 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end3329

if.end3329:                                       ; preds = %if.else3327, %if.then3325
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1568, align 8
  store [3072 x double]* %A, [3072 x double]** %1570, align 8
  store [3072 x double]* %C, [3072 x double]** %1572, align 8
  store [3072 x double]* %C, [3072 x double]** %1574, align 8
  store [3072 x double]* %D, [3072 x double]** %1576, align 8
  store [3072 x double]* %D, [3072 x double]** %1578, align 8
  store [3072 x double]* %B, [3072 x double]** %1580, align 8
  store [3072 x double]* %B, [3072 x double]** %1582, align 8
  store [3072 x double]* %E, [3072 x double]** %1584, align 8
  store [3072 x double]* %E, [3072 x double]** %1586, align 8
  store i64 1, i64* %1588, align 8
  store i64 1, i64* %1590, align 8
  store i64 %indvars.iv9630, i64* %1592, align 8
  store i64 %indvars.iv9630, i64* %1594, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1596, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1598, align 8
  %1899 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l927.region_id, i32 8, i8** nonnull %1567, i8** nonnull %1569, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond3137, i32 %1866) #6
  %1900 = icmp eq i32 %1899, 0
  br i1 %1900, label %for.body3383.preheader, label %omp_offload.failed3373

omp_offload.failed3373:                           ; preds = %if.end3329
  %1901 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1902 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1901, i32 %cond3137, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..138 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9630) #6
  br label %for.body3383.preheader

for.body3383.preheader:                           ; preds = %if.end3329, %omp_offload.failed3373
  br label %for.body3383

for.cond.cleanup3382:                             ; preds = %for.body3383
  %add3395 = fadd double %add3390.5, 0.000000e+00
  %cmp3403 = fcmp une double %add3395, 0x4152018000000000
  br i1 %cmp3403, label %if.then3421, label %if.else3423

for.body3383:                                     ; preds = %for.body3383.preheader, %for.body3383
  %indvars.iv9609 = phi i64 [ %indvars.iv.next9610.5, %for.body3383 ], [ 0, %for.body3383.preheader ]
  %tmp3377.09035 = phi double [ %add3390.5, %for.body3383 ], [ 0.000000e+00, %for.body3383.preheader ]
  %arrayidx3385 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9609
  %1903 = load double, double* %arrayidx3385, align 8, !tbaa !62
  %arrayidx3387 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9609
  %1904 = load double, double* %arrayidx3387, align 8, !tbaa !62
  %add3388 = fadd double %1903, %1904
  %sub3389 = fadd double %add3388, -1.000000e+00
  %add3390 = fadd double %tmp3377.09035, %sub3389
  %indvars.iv.next9610 = or i64 %indvars.iv9609, 1
  %arrayidx3385.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9610
  %1905 = load double, double* %arrayidx3385.1, align 8, !tbaa !62
  %arrayidx3387.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9610
  %1906 = load double, double* %arrayidx3387.1, align 8, !tbaa !62
  %add3388.1 = fadd double %1905, %1906
  %sub3389.1 = fadd double %add3388.1, -1.000000e+00
  %add3390.1 = fadd double %add3390, %sub3389.1
  %indvars.iv.next9610.1 = add nuw nsw i64 %indvars.iv9609, 2
  %arrayidx3385.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9610.1
  %1907 = load double, double* %arrayidx3385.2, align 8, !tbaa !62
  %arrayidx3387.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9610.1
  %1908 = load double, double* %arrayidx3387.2, align 8, !tbaa !62
  %add3388.2 = fadd double %1907, %1908
  %sub3389.2 = fadd double %add3388.2, -1.000000e+00
  %add3390.2 = fadd double %add3390.1, %sub3389.2
  %indvars.iv.next9610.2 = add nuw nsw i64 %indvars.iv9609, 3
  %arrayidx3385.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9610.2
  %1909 = load double, double* %arrayidx3385.3, align 8, !tbaa !62
  %arrayidx3387.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9610.2
  %1910 = load double, double* %arrayidx3387.3, align 8, !tbaa !62
  %add3388.3 = fadd double %1909, %1910
  %sub3389.3 = fadd double %add3388.3, -1.000000e+00
  %add3390.3 = fadd double %add3390.2, %sub3389.3
  %indvars.iv.next9610.3 = add nuw nsw i64 %indvars.iv9609, 4
  %arrayidx3385.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9610.3
  %1911 = load double, double* %arrayidx3385.4, align 8, !tbaa !62
  %arrayidx3387.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9610.3
  %1912 = load double, double* %arrayidx3387.4, align 8, !tbaa !62
  %add3388.4 = fadd double %1911, %1912
  %sub3389.4 = fadd double %add3388.4, -1.000000e+00
  %add3390.4 = fadd double %add3390.3, %sub3389.4
  %indvars.iv.next9610.4 = add nuw nsw i64 %indvars.iv9609, 5
  %arrayidx3385.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9610.4
  %1913 = load double, double* %arrayidx3385.5, align 8, !tbaa !62
  %arrayidx3387.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9610.4
  %1914 = load double, double* %arrayidx3387.5, align 8, !tbaa !62
  %add3388.5 = fadd double %1913, %1914
  %sub3389.5 = fadd double %add3388.5, -1.000000e+00
  %add3390.5 = fadd double %add3390.4, %sub3389.5
  %indvars.iv.next9610.5 = add nuw nsw i64 %indvars.iv9609, 6
  %exitcond9611.5 = icmp eq i64 %indvars.iv.next9610.5, 3072
  br i1 %exitcond9611.5, label %for.cond.cleanup3382, label %for.body3383

if.then3421:                                      ; preds = %for.cond.cleanup3382
  %call3407.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add3395)
  %puts8288 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end3425

if.else3423:                                      ; preds = %for.cond.cleanup3382
  %puts8281 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end3425

if.end3425:                                       ; preds = %if.else3423, %if.then3421
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1600, align 8
  store [3072 x double]* %A, [3072 x double]** %1602, align 8
  store [3072 x double]* %C, [3072 x double]** %1604, align 8
  store [3072 x double]* %C, [3072 x double]** %1606, align 8
  store [3072 x double]* %D, [3072 x double]** %1608, align 8
  store [3072 x double]* %D, [3072 x double]** %1610, align 8
  store [3072 x double]* %B, [3072 x double]** %1612, align 8
  store [3072 x double]* %B, [3072 x double]** %1614, align 8
  store [3072 x double]* %E, [3072 x double]** %1616, align 8
  store [3072 x double]* %E, [3072 x double]** %1618, align 8
  store i64 1, i64* %1620, align 8
  store i64 1, i64* %1622, align 8
  store i64 %indvars.iv9630, i64* %1624, align 8
  store i64 %indvars.iv9630, i64* %1626, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1628, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1630, align 8
  %1915 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l949.region_id, i32 8, i8** nonnull %1599, i8** nonnull %1601, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond3137, i32 %1866) #6
  %1916 = icmp eq i32 %1915, 0
  br i1 %1916, label %for.body3479.preheader, label %omp_offload.failed3469

omp_offload.failed3469:                           ; preds = %if.end3425
  %1917 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1918 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1917, i32 %cond3137, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..142 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9630) #6
  br label %for.body3479.preheader

for.body3479.preheader:                           ; preds = %if.end3425, %omp_offload.failed3469
  br label %for.body3479

for.cond.cleanup3478:                             ; preds = %for.body3479
  %add3491 = fadd double %add3486.5, 0.000000e+00
  %cmp3499 = fcmp une double %add3491, 0x4152018000000000
  br i1 %cmp3499, label %if.then3517, label %if.else3519

for.body3479:                                     ; preds = %for.body3479.preheader, %for.body3479
  %indvars.iv9615 = phi i64 [ %indvars.iv.next9616.5, %for.body3479 ], [ 0, %for.body3479.preheader ]
  %tmp3473.09043 = phi double [ %add3486.5, %for.body3479 ], [ 0.000000e+00, %for.body3479.preheader ]
  %arrayidx3481 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9615
  %1919 = load double, double* %arrayidx3481, align 8, !tbaa !62
  %arrayidx3483 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9615
  %1920 = load double, double* %arrayidx3483, align 8, !tbaa !62
  %add3484 = fadd double %1919, %1920
  %sub3485 = fadd double %add3484, -1.000000e+00
  %add3486 = fadd double %tmp3473.09043, %sub3485
  %indvars.iv.next9616 = or i64 %indvars.iv9615, 1
  %arrayidx3481.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9616
  %1921 = load double, double* %arrayidx3481.1, align 8, !tbaa !62
  %arrayidx3483.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9616
  %1922 = load double, double* %arrayidx3483.1, align 8, !tbaa !62
  %add3484.1 = fadd double %1921, %1922
  %sub3485.1 = fadd double %add3484.1, -1.000000e+00
  %add3486.1 = fadd double %add3486, %sub3485.1
  %indvars.iv.next9616.1 = add nuw nsw i64 %indvars.iv9615, 2
  %arrayidx3481.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9616.1
  %1923 = load double, double* %arrayidx3481.2, align 8, !tbaa !62
  %arrayidx3483.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9616.1
  %1924 = load double, double* %arrayidx3483.2, align 8, !tbaa !62
  %add3484.2 = fadd double %1923, %1924
  %sub3485.2 = fadd double %add3484.2, -1.000000e+00
  %add3486.2 = fadd double %add3486.1, %sub3485.2
  %indvars.iv.next9616.2 = add nuw nsw i64 %indvars.iv9615, 3
  %arrayidx3481.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9616.2
  %1925 = load double, double* %arrayidx3481.3, align 8, !tbaa !62
  %arrayidx3483.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9616.2
  %1926 = load double, double* %arrayidx3483.3, align 8, !tbaa !62
  %add3484.3 = fadd double %1925, %1926
  %sub3485.3 = fadd double %add3484.3, -1.000000e+00
  %add3486.3 = fadd double %add3486.2, %sub3485.3
  %indvars.iv.next9616.3 = add nuw nsw i64 %indvars.iv9615, 4
  %arrayidx3481.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9616.3
  %1927 = load double, double* %arrayidx3481.4, align 8, !tbaa !62
  %arrayidx3483.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9616.3
  %1928 = load double, double* %arrayidx3483.4, align 8, !tbaa !62
  %add3484.4 = fadd double %1927, %1928
  %sub3485.4 = fadd double %add3484.4, -1.000000e+00
  %add3486.4 = fadd double %add3486.3, %sub3485.4
  %indvars.iv.next9616.4 = add nuw nsw i64 %indvars.iv9615, 5
  %arrayidx3481.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9616.4
  %1929 = load double, double* %arrayidx3481.5, align 8, !tbaa !62
  %arrayidx3483.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9616.4
  %1930 = load double, double* %arrayidx3483.5, align 8, !tbaa !62
  %add3484.5 = fadd double %1929, %1930
  %sub3485.5 = fadd double %add3484.5, -1.000000e+00
  %add3486.5 = fadd double %add3486.4, %sub3485.5
  %indvars.iv.next9616.5 = add nuw nsw i64 %indvars.iv9615, 6
  %exitcond9617.5 = icmp eq i64 %indvars.iv.next9616.5, 3072
  br i1 %exitcond9617.5, label %for.cond.cleanup3478, label %for.body3479

if.then3517:                                      ; preds = %for.cond.cleanup3478
  %call3503.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add3491)
  %puts8287 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end3521

if.else3519:                                      ; preds = %for.cond.cleanup3478
  %puts8282 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end3521

if.end3521:                                       ; preds = %if.else3519, %if.then3517
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1632, align 8
  store [3072 x double]* %A, [3072 x double]** %1634, align 8
  store [3072 x double]* %C, [3072 x double]** %1636, align 8
  store [3072 x double]* %C, [3072 x double]** %1638, align 8
  store [3072 x double]* %D, [3072 x double]** %1640, align 8
  store [3072 x double]* %D, [3072 x double]** %1642, align 8
  store [3072 x double]* %B, [3072 x double]** %1644, align 8
  store [3072 x double]* %B, [3072 x double]** %1646, align 8
  store [3072 x double]* %E, [3072 x double]** %1648, align 8
  store [3072 x double]* %E, [3072 x double]** %1650, align 8
  store i64 1, i64* %1652, align 8
  store i64 1, i64* %1654, align 8
  store i64 %indvars.iv9630, i64* %1656, align 8
  store i64 %indvars.iv9630, i64* %1658, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1660, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1662, align 8
  %1931 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l971.region_id, i32 8, i8** nonnull %1631, i8** nonnull %1633, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond3137, i32 %1866) #6
  %1932 = icmp eq i32 %1931, 0
  br i1 %1932, label %for.body3575.preheader, label %omp_offload.failed3565

omp_offload.failed3565:                           ; preds = %if.end3521
  %1933 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1934 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1933, i32 %cond3137, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..146 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9630) #6
  br label %for.body3575.preheader

for.body3575.preheader:                           ; preds = %if.end3521, %omp_offload.failed3565
  br label %for.body3575

for.cond.cleanup3574:                             ; preds = %for.body3575
  %add3587 = fadd double %add3582.5, 0.000000e+00
  %cmp3595 = fcmp une double %add3587, 0x4152018000000000
  br i1 %cmp3595, label %if.then3613, label %if.else3615

for.body3575:                                     ; preds = %for.body3575.preheader, %for.body3575
  %indvars.iv9621 = phi i64 [ %indvars.iv.next9622.5, %for.body3575 ], [ 0, %for.body3575.preheader ]
  %tmp3569.09051 = phi double [ %add3582.5, %for.body3575 ], [ 0.000000e+00, %for.body3575.preheader ]
  %arrayidx3577 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9621
  %1935 = load double, double* %arrayidx3577, align 8, !tbaa !62
  %arrayidx3579 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9621
  %1936 = load double, double* %arrayidx3579, align 8, !tbaa !62
  %add3580 = fadd double %1935, %1936
  %sub3581 = fadd double %add3580, -1.000000e+00
  %add3582 = fadd double %tmp3569.09051, %sub3581
  %indvars.iv.next9622 = or i64 %indvars.iv9621, 1
  %arrayidx3577.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9622
  %1937 = load double, double* %arrayidx3577.1, align 8, !tbaa !62
  %arrayidx3579.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9622
  %1938 = load double, double* %arrayidx3579.1, align 8, !tbaa !62
  %add3580.1 = fadd double %1937, %1938
  %sub3581.1 = fadd double %add3580.1, -1.000000e+00
  %add3582.1 = fadd double %add3582, %sub3581.1
  %indvars.iv.next9622.1 = add nuw nsw i64 %indvars.iv9621, 2
  %arrayidx3577.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9622.1
  %1939 = load double, double* %arrayidx3577.2, align 8, !tbaa !62
  %arrayidx3579.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9622.1
  %1940 = load double, double* %arrayidx3579.2, align 8, !tbaa !62
  %add3580.2 = fadd double %1939, %1940
  %sub3581.2 = fadd double %add3580.2, -1.000000e+00
  %add3582.2 = fadd double %add3582.1, %sub3581.2
  %indvars.iv.next9622.2 = add nuw nsw i64 %indvars.iv9621, 3
  %arrayidx3577.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9622.2
  %1941 = load double, double* %arrayidx3577.3, align 8, !tbaa !62
  %arrayidx3579.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9622.2
  %1942 = load double, double* %arrayidx3579.3, align 8, !tbaa !62
  %add3580.3 = fadd double %1941, %1942
  %sub3581.3 = fadd double %add3580.3, -1.000000e+00
  %add3582.3 = fadd double %add3582.2, %sub3581.3
  %indvars.iv.next9622.3 = add nuw nsw i64 %indvars.iv9621, 4
  %arrayidx3577.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9622.3
  %1943 = load double, double* %arrayidx3577.4, align 8, !tbaa !62
  %arrayidx3579.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9622.3
  %1944 = load double, double* %arrayidx3579.4, align 8, !tbaa !62
  %add3580.4 = fadd double %1943, %1944
  %sub3581.4 = fadd double %add3580.4, -1.000000e+00
  %add3582.4 = fadd double %add3582.3, %sub3581.4
  %indvars.iv.next9622.4 = add nuw nsw i64 %indvars.iv9621, 5
  %arrayidx3577.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9622.4
  %1945 = load double, double* %arrayidx3577.5, align 8, !tbaa !62
  %arrayidx3579.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9622.4
  %1946 = load double, double* %arrayidx3579.5, align 8, !tbaa !62
  %add3580.5 = fadd double %1945, %1946
  %sub3581.5 = fadd double %add3580.5, -1.000000e+00
  %add3582.5 = fadd double %add3582.4, %sub3581.5
  %indvars.iv.next9622.5 = add nuw nsw i64 %indvars.iv9621, 6
  %exitcond9623.5 = icmp eq i64 %indvars.iv.next9622.5, 3072
  br i1 %exitcond9623.5, label %for.cond.cleanup3574, label %for.body3575

if.then3613:                                      ; preds = %for.cond.cleanup3574
  %call3599.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add3587)
  %puts8286 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end3617

if.else3615:                                      ; preds = %for.cond.cleanup3574
  %puts8283 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end3617

if.end3617:                                       ; preds = %if.else3615, %if.then3613
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %1, i8 0, i64 24576, i1 false)
  call void @llvm.memset.p0i8.i64(i8* nonnull align 8 %0, i8 0, i64 24576, i1 false)
  store [3072 x double]* %A, [3072 x double]** %1664, align 8
  store [3072 x double]* %A, [3072 x double]** %1666, align 8
  store [3072 x double]* %C, [3072 x double]** %1668, align 8
  store [3072 x double]* %C, [3072 x double]** %1670, align 8
  store [3072 x double]* %D, [3072 x double]** %1672, align 8
  store [3072 x double]* %D, [3072 x double]** %1674, align 8
  store [3072 x double]* %B, [3072 x double]** %1676, align 8
  store [3072 x double]* %B, [3072 x double]** %1678, align 8
  store [3072 x double]* %E, [3072 x double]** %1680, align 8
  store [3072 x double]* %E, [3072 x double]** %1682, align 8
  store i64 1, i64* %1684, align 8
  store i64 1, i64* %1686, align 8
  store i64 %indvars.iv9630, i64* %1688, align 8
  store i64 %indvars.iv9630, i64* %1690, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1692, align 8
  store i64 %.capture_expr..casted3175.sroa.0.0.insert.ext, i64* %1694, align 8
  %1947 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l993.region_id, i32 8, i8** nonnull %1663, i8** nonnull %1665, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.152, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.153, i64 0, i64 0), i32 %cond3137, i32 %1866) #6
  %1948 = icmp eq i32 %1947, 0
  br i1 %1948, label %for.body3671.preheader, label %omp_offload.failed3661

omp_offload.failed3661:                           ; preds = %if.end3617
  %1949 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %1950 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %1949, i32 %cond3137, i32 0) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, i64)* @.omp_outlined..150 to void (i32*, i32*, ...)*), [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 1, i64 %indvars.iv9630) #6
  br label %for.body3671.preheader

for.body3671.preheader:                           ; preds = %if.end3617, %omp_offload.failed3661
  br label %for.body3671

for.cond.cleanup3670:                             ; preds = %for.body3671
  %add3683 = fadd double %add3678.5, 0.000000e+00
  %cmp3691 = fcmp une double %add3683, 0x4152018000000000
  br i1 %cmp3691, label %if.then3709, label %if.else3711

for.body3671:                                     ; preds = %for.body3671.preheader, %for.body3671
  %indvars.iv9627 = phi i64 [ %indvars.iv.next9628.5, %for.body3671 ], [ 0, %for.body3671.preheader ]
  %tmp3665.09059 = phi double [ %add3678.5, %for.body3671 ], [ 0.000000e+00, %for.body3671.preheader ]
  %arrayidx3673 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv9627
  %1951 = load double, double* %arrayidx3673, align 8, !tbaa !62
  %arrayidx3675 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv9627
  %1952 = load double, double* %arrayidx3675, align 8, !tbaa !62
  %add3676 = fadd double %1951, %1952
  %sub3677 = fadd double %add3676, -1.000000e+00
  %add3678 = fadd double %tmp3665.09059, %sub3677
  %indvars.iv.next9628 = or i64 %indvars.iv9627, 1
  %arrayidx3673.1 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9628
  %1953 = load double, double* %arrayidx3673.1, align 8, !tbaa !62
  %arrayidx3675.1 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9628
  %1954 = load double, double* %arrayidx3675.1, align 8, !tbaa !62
  %add3676.1 = fadd double %1953, %1954
  %sub3677.1 = fadd double %add3676.1, -1.000000e+00
  %add3678.1 = fadd double %add3678, %sub3677.1
  %indvars.iv.next9628.1 = add nuw nsw i64 %indvars.iv9627, 2
  %arrayidx3673.2 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9628.1
  %1955 = load double, double* %arrayidx3673.2, align 8, !tbaa !62
  %arrayidx3675.2 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9628.1
  %1956 = load double, double* %arrayidx3675.2, align 8, !tbaa !62
  %add3676.2 = fadd double %1955, %1956
  %sub3677.2 = fadd double %add3676.2, -1.000000e+00
  %add3678.2 = fadd double %add3678.1, %sub3677.2
  %indvars.iv.next9628.2 = add nuw nsw i64 %indvars.iv9627, 3
  %arrayidx3673.3 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9628.2
  %1957 = load double, double* %arrayidx3673.3, align 8, !tbaa !62
  %arrayidx3675.3 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9628.2
  %1958 = load double, double* %arrayidx3675.3, align 8, !tbaa !62
  %add3676.3 = fadd double %1957, %1958
  %sub3677.3 = fadd double %add3676.3, -1.000000e+00
  %add3678.3 = fadd double %add3678.2, %sub3677.3
  %indvars.iv.next9628.3 = add nuw nsw i64 %indvars.iv9627, 4
  %arrayidx3673.4 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9628.3
  %1959 = load double, double* %arrayidx3673.4, align 8, !tbaa !62
  %arrayidx3675.4 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9628.3
  %1960 = load double, double* %arrayidx3675.4, align 8, !tbaa !62
  %add3676.4 = fadd double %1959, %1960
  %sub3677.4 = fadd double %add3676.4, -1.000000e+00
  %add3678.4 = fadd double %add3678.3, %sub3677.4
  %indvars.iv.next9628.4 = add nuw nsw i64 %indvars.iv9627, 5
  %arrayidx3673.5 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.next9628.4
  %1961 = load double, double* %arrayidx3673.5, align 8, !tbaa !62
  %arrayidx3675.5 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.next9628.4
  %1962 = load double, double* %arrayidx3675.5, align 8, !tbaa !62
  %add3676.5 = fadd double %1961, %1962
  %sub3677.5 = fadd double %add3676.5, -1.000000e+00
  %add3678.5 = fadd double %add3678.4, %sub3677.5
  %indvars.iv.next9628.5 = add nuw nsw i64 %indvars.iv9627, 6
  %exitcond9629.5 = icmp eq i64 %indvars.iv.next9628.5, 3072
  br i1 %exitcond9629.5, label %for.cond.cleanup3670, label %for.body3671

if.then3709:                                      ; preds = %for.cond.cleanup3670
  %call3695.us = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @.str.10, i64 0, i64 0), i32 signext 0, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.11, i64 0, i64 0), double 0x4152018000000000, double %add3683)
  %puts8285 = call i32 @puts(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @str.444, i64 0, i64 0))
  br label %if.end3713

if.else3711:                                      ; preds = %for.cond.cleanup3670
  %puts8284 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  br label %if.end3713

if.end3713:                                       ; preds = %if.else3711, %if.then3709
  %indvars.iv.next9631 = add nuw nsw i64 %indvars.iv9630, 78
  %cmp3126 = icmp ugt i64 %indvars.iv.next9631, %cond
  %1963 = load i32, i32* %cpuExec, align 4, !tbaa !58
  %tobool3717 = icmp eq i32 %1963, 0
  br i1 %cmp3126, label %for.cond.cleanup3128, label %for.body3129

for.inc3730:                                      ; preds = %for.body3724.preheader, %for.cond.cleanup3128
  %mul3731 = mul nsw i32 %tms3118.09070, 3
  %cmp3120 = icmp ult i32 %mul3731, 257
  br i1 %cmp3120, label %for.cond3125.preheader, label %for.cond.cleanup3122

for.body.i.preheader:                             ; preds = %for.cond.cleanup3122, %for.inc3836
  %indvar = phi i64 [ 0, %for.cond.cleanup3122 ], [ %indvar.next, %for.inc3836 ]
  %indvars.iv9592 = phi i64 [ 32, %for.cond.cleanup3122 ], [ %indvars.iv.next9593, %for.inc3836 ]
  %1964 = mul nuw nsw i64 %indvar, 5000
  br label %vector.body10129

vector.body10129:                                 ; preds = %vector.body10129, %for.body.i.preheader
  %index10133 = phi i64 [ 0, %for.body.i.preheader ], [ %index.next10134, %vector.body10129 ]
  %vec.ind10151 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i.preheader ], [ %vec.ind.next10164, %vector.body10129 ]
  %step.add10152 = add <2 x i32> %vec.ind10151, <i32 2, i32 2>
  %step.add10153 = add <2 x i32> %vec.ind10151, <i32 4, i32 4>
  %step.add10154 = add <2 x i32> %vec.ind10151, <i32 6, i32 6>
  %step.add10155 = add <2 x i32> %vec.ind10151, <i32 8, i32 8>
  %step.add10156 = add <2 x i32> %vec.ind10151, <i32 10, i32 10>
  %step.add10157 = add <2 x i32> %vec.ind10151, <i32 12, i32 12>
  %step.add10158 = add <2 x i32> %vec.ind10151, <i32 14, i32 14>
  %step.add10159 = add <2 x i32> %vec.ind10151, <i32 16, i32 16>
  %step.add10160 = add <2 x i32> %vec.ind10151, <i32 18, i32 18>
  %step.add10161 = add <2 x i32> %vec.ind10151, <i32 20, i32 20>
  %step.add10162 = add <2 x i32> %vec.ind10151, <i32 22, i32 22>
  %1965 = sitofp <2 x i32> %vec.ind10151 to <2 x double>
  %1966 = sitofp <2 x i32> %step.add10152 to <2 x double>
  %1967 = sitofp <2 x i32> %step.add10153 to <2 x double>
  %1968 = sitofp <2 x i32> %step.add10154 to <2 x double>
  %1969 = sitofp <2 x i32> %step.add10155 to <2 x double>
  %1970 = sitofp <2 x i32> %step.add10156 to <2 x double>
  %1971 = sitofp <2 x i32> %step.add10157 to <2 x double>
  %1972 = sitofp <2 x i32> %step.add10158 to <2 x double>
  %1973 = sitofp <2 x i32> %step.add10159 to <2 x double>
  %1974 = sitofp <2 x i32> %step.add10160 to <2 x double>
  %1975 = sitofp <2 x i32> %step.add10161 to <2 x double>
  %1976 = sitofp <2 x i32> %step.add10162 to <2 x double>
  %1977 = getelementptr inbounds double, double* %1795, i64 %index10133
  %1978 = bitcast double* %1977 to <2 x double>*
  store <2 x double> %1965, <2 x double>* %1978, align 8, !tbaa !62
  %1979 = getelementptr inbounds double, double* %1977, i64 2
  %1980 = bitcast double* %1979 to <2 x double>*
  store <2 x double> %1966, <2 x double>* %1980, align 8, !tbaa !62
  %1981 = getelementptr inbounds double, double* %1977, i64 4
  %1982 = bitcast double* %1981 to <2 x double>*
  store <2 x double> %1967, <2 x double>* %1982, align 8, !tbaa !62
  %1983 = getelementptr inbounds double, double* %1977, i64 6
  %1984 = bitcast double* %1983 to <2 x double>*
  store <2 x double> %1968, <2 x double>* %1984, align 8, !tbaa !62
  %1985 = getelementptr inbounds double, double* %1977, i64 8
  %1986 = bitcast double* %1985 to <2 x double>*
  store <2 x double> %1969, <2 x double>* %1986, align 8, !tbaa !62
  %1987 = getelementptr inbounds double, double* %1977, i64 10
  %1988 = bitcast double* %1987 to <2 x double>*
  store <2 x double> %1970, <2 x double>* %1988, align 8, !tbaa !62
  %1989 = getelementptr inbounds double, double* %1977, i64 12
  %1990 = bitcast double* %1989 to <2 x double>*
  store <2 x double> %1971, <2 x double>* %1990, align 8, !tbaa !62
  %1991 = getelementptr inbounds double, double* %1977, i64 14
  %1992 = bitcast double* %1991 to <2 x double>*
  store <2 x double> %1972, <2 x double>* %1992, align 8, !tbaa !62
  %1993 = getelementptr inbounds double, double* %1977, i64 16
  %1994 = bitcast double* %1993 to <2 x double>*
  store <2 x double> %1973, <2 x double>* %1994, align 8, !tbaa !62
  %1995 = getelementptr inbounds double, double* %1977, i64 18
  %1996 = bitcast double* %1995 to <2 x double>*
  store <2 x double> %1974, <2 x double>* %1996, align 8, !tbaa !62
  %1997 = getelementptr inbounds double, double* %1977, i64 20
  %1998 = bitcast double* %1997 to <2 x double>*
  store <2 x double> %1975, <2 x double>* %1998, align 8, !tbaa !62
  %1999 = getelementptr inbounds double, double* %1977, i64 22
  %2000 = bitcast double* %1999 to <2 x double>*
  store <2 x double> %1976, <2 x double>* %2000, align 8, !tbaa !62
  %2001 = getelementptr inbounds double, double* %1794, i64 %index10133
  %2002 = bitcast double* %2001 to <2 x double>*
  store <2 x double> %1965, <2 x double>* %2002, align 8, !tbaa !62
  %2003 = getelementptr inbounds double, double* %2001, i64 2
  %2004 = bitcast double* %2003 to <2 x double>*
  store <2 x double> %1966, <2 x double>* %2004, align 8, !tbaa !62
  %2005 = getelementptr inbounds double, double* %2001, i64 4
  %2006 = bitcast double* %2005 to <2 x double>*
  store <2 x double> %1967, <2 x double>* %2006, align 8, !tbaa !62
  %2007 = getelementptr inbounds double, double* %2001, i64 6
  %2008 = bitcast double* %2007 to <2 x double>*
  store <2 x double> %1968, <2 x double>* %2008, align 8, !tbaa !62
  %2009 = getelementptr inbounds double, double* %2001, i64 8
  %2010 = bitcast double* %2009 to <2 x double>*
  store <2 x double> %1969, <2 x double>* %2010, align 8, !tbaa !62
  %2011 = getelementptr inbounds double, double* %2001, i64 10
  %2012 = bitcast double* %2011 to <2 x double>*
  store <2 x double> %1970, <2 x double>* %2012, align 8, !tbaa !62
  %2013 = getelementptr inbounds double, double* %2001, i64 12
  %2014 = bitcast double* %2013 to <2 x double>*
  store <2 x double> %1971, <2 x double>* %2014, align 8, !tbaa !62
  %2015 = getelementptr inbounds double, double* %2001, i64 14
  %2016 = bitcast double* %2015 to <2 x double>*
  store <2 x double> %1972, <2 x double>* %2016, align 8, !tbaa !62
  %2017 = getelementptr inbounds double, double* %2001, i64 16
  %2018 = bitcast double* %2017 to <2 x double>*
  store <2 x double> %1973, <2 x double>* %2018, align 8, !tbaa !62
  %2019 = getelementptr inbounds double, double* %2001, i64 18
  %2020 = bitcast double* %2019 to <2 x double>*
  store <2 x double> %1974, <2 x double>* %2020, align 8, !tbaa !62
  %2021 = getelementptr inbounds double, double* %2001, i64 20
  %2022 = bitcast double* %2021 to <2 x double>*
  store <2 x double> %1975, <2 x double>* %2022, align 8, !tbaa !62
  %2023 = getelementptr inbounds double, double* %2001, i64 22
  %2024 = bitcast double* %2023 to <2 x double>*
  store <2 x double> %1976, <2 x double>* %2024, align 8, !tbaa !62
  %2025 = shl <2 x i32> %vec.ind10151, <i32 1, i32 1>
  %2026 = shl <2 x i32> %step.add10152, <i32 1, i32 1>
  %2027 = shl <2 x i32> %step.add10153, <i32 1, i32 1>
  %2028 = shl <2 x i32> %step.add10154, <i32 1, i32 1>
  %2029 = shl <2 x i32> %step.add10155, <i32 1, i32 1>
  %2030 = shl <2 x i32> %step.add10156, <i32 1, i32 1>
  %2031 = shl <2 x i32> %step.add10157, <i32 1, i32 1>
  %2032 = shl <2 x i32> %step.add10158, <i32 1, i32 1>
  %2033 = shl <2 x i32> %step.add10159, <i32 1, i32 1>
  %2034 = shl <2 x i32> %step.add10160, <i32 1, i32 1>
  %2035 = shl <2 x i32> %step.add10161, <i32 1, i32 1>
  %2036 = shl <2 x i32> %step.add10162, <i32 1, i32 1>
  %2037 = sitofp <2 x i32> %2025 to <2 x double>
  %2038 = sitofp <2 x i32> %2026 to <2 x double>
  %2039 = sitofp <2 x i32> %2027 to <2 x double>
  %2040 = sitofp <2 x i32> %2028 to <2 x double>
  %2041 = sitofp <2 x i32> %2029 to <2 x double>
  %2042 = sitofp <2 x i32> %2030 to <2 x double>
  %2043 = sitofp <2 x i32> %2031 to <2 x double>
  %2044 = sitofp <2 x i32> %2032 to <2 x double>
  %2045 = sitofp <2 x i32> %2033 to <2 x double>
  %2046 = sitofp <2 x i32> %2034 to <2 x double>
  %2047 = sitofp <2 x i32> %2035 to <2 x double>
  %2048 = sitofp <2 x i32> %2036 to <2 x double>
  %2049 = getelementptr inbounds double, double* %1796, i64 %index10133
  %2050 = bitcast double* %2049 to <2 x double>*
  store <2 x double> %2037, <2 x double>* %2050, align 8, !tbaa !62
  %2051 = getelementptr inbounds double, double* %2049, i64 2
  %2052 = bitcast double* %2051 to <2 x double>*
  store <2 x double> %2038, <2 x double>* %2052, align 8, !tbaa !62
  %2053 = getelementptr inbounds double, double* %2049, i64 4
  %2054 = bitcast double* %2053 to <2 x double>*
  store <2 x double> %2039, <2 x double>* %2054, align 8, !tbaa !62
  %2055 = getelementptr inbounds double, double* %2049, i64 6
  %2056 = bitcast double* %2055 to <2 x double>*
  store <2 x double> %2040, <2 x double>* %2056, align 8, !tbaa !62
  %2057 = getelementptr inbounds double, double* %2049, i64 8
  %2058 = bitcast double* %2057 to <2 x double>*
  store <2 x double> %2041, <2 x double>* %2058, align 8, !tbaa !62
  %2059 = getelementptr inbounds double, double* %2049, i64 10
  %2060 = bitcast double* %2059 to <2 x double>*
  store <2 x double> %2042, <2 x double>* %2060, align 8, !tbaa !62
  %2061 = getelementptr inbounds double, double* %2049, i64 12
  %2062 = bitcast double* %2061 to <2 x double>*
  store <2 x double> %2043, <2 x double>* %2062, align 8, !tbaa !62
  %2063 = getelementptr inbounds double, double* %2049, i64 14
  %2064 = bitcast double* %2063 to <2 x double>*
  store <2 x double> %2044, <2 x double>* %2064, align 8, !tbaa !62
  %2065 = getelementptr inbounds double, double* %2049, i64 16
  %2066 = bitcast double* %2065 to <2 x double>*
  store <2 x double> %2045, <2 x double>* %2066, align 8, !tbaa !62
  %2067 = getelementptr inbounds double, double* %2049, i64 18
  %2068 = bitcast double* %2067 to <2 x double>*
  store <2 x double> %2046, <2 x double>* %2068, align 8, !tbaa !62
  %2069 = getelementptr inbounds double, double* %2049, i64 20
  %2070 = bitcast double* %2069 to <2 x double>*
  store <2 x double> %2047, <2 x double>* %2070, align 8, !tbaa !62
  %2071 = getelementptr inbounds double, double* %2049, i64 22
  %2072 = bitcast double* %2071 to <2 x double>*
  store <2 x double> %2048, <2 x double>* %2072, align 8, !tbaa !62
  %2073 = add <2 x i32> %vec.ind10151, <i32 -3, i32 -3>
  %2074 = add <2 x i32> %vec.ind10151, <i32 -1, i32 -1>
  %2075 = add <2 x i32> %vec.ind10151, <i32 1, i32 1>
  %2076 = add <2 x i32> %vec.ind10151, <i32 3, i32 3>
  %2077 = add <2 x i32> %vec.ind10151, <i32 5, i32 5>
  %2078 = add <2 x i32> %vec.ind10151, <i32 7, i32 7>
  %2079 = add <2 x i32> %vec.ind10151, <i32 9, i32 9>
  %2080 = add <2 x i32> %vec.ind10151, <i32 11, i32 11>
  %2081 = add <2 x i32> %vec.ind10151, <i32 13, i32 13>
  %2082 = add <2 x i32> %vec.ind10151, <i32 15, i32 15>
  %2083 = add <2 x i32> %vec.ind10151, <i32 17, i32 17>
  %2084 = add <2 x i32> %vec.ind10151, <i32 19, i32 19>
  %2085 = sitofp <2 x i32> %2073 to <2 x double>
  %2086 = sitofp <2 x i32> %2074 to <2 x double>
  %2087 = sitofp <2 x i32> %2075 to <2 x double>
  %2088 = sitofp <2 x i32> %2076 to <2 x double>
  %2089 = sitofp <2 x i32> %2077 to <2 x double>
  %2090 = sitofp <2 x i32> %2078 to <2 x double>
  %2091 = sitofp <2 x i32> %2079 to <2 x double>
  %2092 = sitofp <2 x i32> %2080 to <2 x double>
  %2093 = sitofp <2 x i32> %2081 to <2 x double>
  %2094 = sitofp <2 x i32> %2082 to <2 x double>
  %2095 = sitofp <2 x i32> %2083 to <2 x double>
  %2096 = sitofp <2 x i32> %2084 to <2 x double>
  %2097 = getelementptr inbounds double, double* %1797, i64 %index10133
  %2098 = bitcast double* %2097 to <2 x double>*
  store <2 x double> %2085, <2 x double>* %2098, align 8, !tbaa !62
  %2099 = getelementptr inbounds double, double* %2097, i64 2
  %2100 = bitcast double* %2099 to <2 x double>*
  store <2 x double> %2086, <2 x double>* %2100, align 8, !tbaa !62
  %2101 = getelementptr inbounds double, double* %2097, i64 4
  %2102 = bitcast double* %2101 to <2 x double>*
  store <2 x double> %2087, <2 x double>* %2102, align 8, !tbaa !62
  %2103 = getelementptr inbounds double, double* %2097, i64 6
  %2104 = bitcast double* %2103 to <2 x double>*
  store <2 x double> %2088, <2 x double>* %2104, align 8, !tbaa !62
  %2105 = getelementptr inbounds double, double* %2097, i64 8
  %2106 = bitcast double* %2105 to <2 x double>*
  store <2 x double> %2089, <2 x double>* %2106, align 8, !tbaa !62
  %2107 = getelementptr inbounds double, double* %2097, i64 10
  %2108 = bitcast double* %2107 to <2 x double>*
  store <2 x double> %2090, <2 x double>* %2108, align 8, !tbaa !62
  %2109 = getelementptr inbounds double, double* %2097, i64 12
  %2110 = bitcast double* %2109 to <2 x double>*
  store <2 x double> %2091, <2 x double>* %2110, align 8, !tbaa !62
  %2111 = getelementptr inbounds double, double* %2097, i64 14
  %2112 = bitcast double* %2111 to <2 x double>*
  store <2 x double> %2092, <2 x double>* %2112, align 8, !tbaa !62
  %2113 = getelementptr inbounds double, double* %2097, i64 16
  %2114 = bitcast double* %2113 to <2 x double>*
  store <2 x double> %2093, <2 x double>* %2114, align 8, !tbaa !62
  %2115 = getelementptr inbounds double, double* %2097, i64 18
  %2116 = bitcast double* %2115 to <2 x double>*
  store <2 x double> %2094, <2 x double>* %2116, align 8, !tbaa !62
  %2117 = getelementptr inbounds double, double* %2097, i64 20
  %2118 = bitcast double* %2117 to <2 x double>*
  store <2 x double> %2095, <2 x double>* %2118, align 8, !tbaa !62
  %2119 = getelementptr inbounds double, double* %2097, i64 22
  %2120 = bitcast double* %2119 to <2 x double>*
  store <2 x double> %2096, <2 x double>* %2120, align 8, !tbaa !62
  %index.next10134 = add nuw nsw i64 %index10133, 24
  %vec.ind.next10164 = add <2 x i32> %vec.ind10151, <i32 24, i32 24>
  %2121 = icmp eq i64 %index.next10134, 24984
  br i1 %2121, label %for.body.i, label %vector.body10129, !llvm.loop !79

for.body.i:                                       ; preds = %vector.body10129
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %1834, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %1835, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %1836, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %1837, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %1838, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %1839, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %1840, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %1841, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %1842, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %1843, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %1844, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %1845, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %1846, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %1847, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %1848, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %1849, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %1850, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %1851, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %1852, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %1853, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %1854, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %1855, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %1856, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %1857, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %1858, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %1859, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %1860, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %1861, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %1862, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %1863, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %1864, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %1865, align 8, !tbaa !62
  %2122 = shl nuw nsw i64 %indvars.iv9592, 3
  store i8* %call3733, i8** %1804, align 8
  store i8* %call3733, i8** %1805, align 8
  store i64 %2122, i64* %1806, align 8
  store i8* %call3735, i8** %1807, align 8
  store i8* %call3735, i8** %1808, align 8
  store i64 %2122, i64* %1809, align 8
  store i8* %call3736, i8** %1810, align 8
  store i8* %call3736, i8** %1811, align 8
  store i64 %2122, i64* %1812, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %1804, i8** nonnull %1805, i64* nonnull %1806, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  br label %for.cond3760.preheader

for.cond3760.preheader:                           ; preds = %for.body.i, %omp_offload.cont3776.5
  %tms3754.09012 = phi i32 [ 1, %for.body.i ], [ %mul3781, %omp_offload.cont3776.5 ]
  %.capture_expr..casted3769.sroa.0.0.insert.ext = zext i32 %tms3754.09012 to i64
  store i64 %indvars.iv9592, i64* %1814, align 8
  store i64 %indvars.iv9592, i64* %1816, align 8
  store i8* %call3733, i8** %1817, align 8
  store i8* %call3733, i8** %1818, align 8
  store i8* %call3735, i8** %1819, align 8
  store i8* %call3735, i8** %1820, align 8
  store i8* %call3736, i8** %1821, align 8
  store i8* %call3736, i8** %1822, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1824, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1826, align 8
  store i64 32, i64* %1828, align 8
  store i64 32, i64* %1830, align 8
  %2123 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1036.region_id, i32 6, i8** nonnull %1813, i8** nonnull %1815, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3754.09012, i32 32) #6
  %2124 = icmp eq i32 %2123, 0
  br i1 %2124, label %omp_offload.cont3776, label %omp_offload.failed3775

omp_offload.failed3775:                           ; preds = %for.cond3760.preheader
  %2125 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %2126 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %2125, i32 %tms3754.09012, i32 32) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..158 to void (i32*, i32*, ...)*), i64 %indvars.iv9592, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3776

omp_offload.cont3776:                             ; preds = %for.cond3760.preheader, %omp_offload.failed3775
  store i64 %indvars.iv9592, i64* %1814, align 8
  store i64 %indvars.iv9592, i64* %1816, align 8
  store i8* %call3733, i8** %1817, align 8
  store i8* %call3733, i8** %1818, align 8
  store i8* %call3735, i8** %1819, align 8
  store i8* %call3735, i8** %1820, align 8
  store i8* %call3736, i8** %1821, align 8
  store i8* %call3736, i8** %1822, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1824, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1826, align 8
  store i64 64, i64* %1828, align 8
  store i64 64, i64* %1830, align 8
  %2127 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1036.region_id, i32 6, i8** nonnull %1813, i8** nonnull %1815, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3754.09012, i32 64) #6
  %2128 = icmp eq i32 %2127, 0
  br i1 %2128, label %omp_offload.cont3776.1, label %omp_offload.failed3775.1

for.cond3789.preheader:                           ; preds = %for.cond3789.preheader.preheader65, %for.cond.cleanup3792
  %times.09015 = phi i32 [ %inc3806, %for.cond.cleanup3792 ], [ 0, %for.cond3789.preheader.preheader65 ]
  br label %vector.body10075

vector.body10075:                                 ; preds = %for.cond3789.preheader, %vector.body10075
  %index10079 = phi i64 [ %index.next10080, %vector.body10075 ], [ 0, %for.cond3789.preheader ]
  %2129 = getelementptr inbounds double, double* %1796, i64 %index10079
  %2130 = bitcast double* %2129 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %2130, align 8, !tbaa !62
  %2131 = getelementptr inbounds double, double* %2129, i64 2
  %2132 = bitcast double* %2131 to <2 x double>*
  %wide.load10094 = load <2 x double>, <2 x double>* %2132, align 8, !tbaa !62
  %2133 = getelementptr inbounds double, double* %2129, i64 4
  %2134 = bitcast double* %2133 to <2 x double>*
  %wide.load10095 = load <2 x double>, <2 x double>* %2134, align 8, !tbaa !62
  %2135 = getelementptr inbounds double, double* %2129, i64 6
  %2136 = bitcast double* %2135 to <2 x double>*
  %wide.load10096 = load <2 x double>, <2 x double>* %2136, align 8, !tbaa !62
  %2137 = getelementptr inbounds double, double* %2129, i64 8
  %2138 = bitcast double* %2137 to <2 x double>*
  %wide.load10097 = load <2 x double>, <2 x double>* %2138, align 8, !tbaa !62
  %2139 = getelementptr inbounds double, double* %2129, i64 10
  %2140 = bitcast double* %2139 to <2 x double>*
  %wide.load10098 = load <2 x double>, <2 x double>* %2140, align 8, !tbaa !62
  %2141 = getelementptr inbounds double, double* %2129, i64 12
  %2142 = bitcast double* %2141 to <2 x double>*
  %wide.load10099 = load <2 x double>, <2 x double>* %2142, align 8, !tbaa !62
  %2143 = getelementptr inbounds double, double* %2129, i64 14
  %2144 = bitcast double* %2143 to <2 x double>*
  %wide.load10100 = load <2 x double>, <2 x double>* %2144, align 8, !tbaa !62
  %2145 = getelementptr inbounds double, double* %2129, i64 16
  %2146 = bitcast double* %2145 to <2 x double>*
  %wide.load10101 = load <2 x double>, <2 x double>* %2146, align 8, !tbaa !62
  %2147 = getelementptr inbounds double, double* %2129, i64 18
  %2148 = bitcast double* %2147 to <2 x double>*
  %wide.load10102 = load <2 x double>, <2 x double>* %2148, align 8, !tbaa !62
  %2149 = getelementptr inbounds double, double* %2129, i64 20
  %2150 = bitcast double* %2149 to <2 x double>*
  %wide.load10103 = load <2 x double>, <2 x double>* %2150, align 8, !tbaa !62
  %2151 = getelementptr inbounds double, double* %2129, i64 22
  %2152 = bitcast double* %2151 to <2 x double>*
  %wide.load10104 = load <2 x double>, <2 x double>* %2152, align 8, !tbaa !62
  %2153 = getelementptr inbounds double, double* %1797, i64 %index10079
  %2154 = bitcast double* %2153 to <2 x double>*
  %wide.load10105 = load <2 x double>, <2 x double>* %2154, align 8, !tbaa !62
  %2155 = getelementptr inbounds double, double* %2153, i64 2
  %2156 = bitcast double* %2155 to <2 x double>*
  %wide.load10106 = load <2 x double>, <2 x double>* %2156, align 8, !tbaa !62
  %2157 = getelementptr inbounds double, double* %2153, i64 4
  %2158 = bitcast double* %2157 to <2 x double>*
  %wide.load10107 = load <2 x double>, <2 x double>* %2158, align 8, !tbaa !62
  %2159 = getelementptr inbounds double, double* %2153, i64 6
  %2160 = bitcast double* %2159 to <2 x double>*
  %wide.load10108 = load <2 x double>, <2 x double>* %2160, align 8, !tbaa !62
  %2161 = getelementptr inbounds double, double* %2153, i64 8
  %2162 = bitcast double* %2161 to <2 x double>*
  %wide.load10109 = load <2 x double>, <2 x double>* %2162, align 8, !tbaa !62
  %2163 = getelementptr inbounds double, double* %2153, i64 10
  %2164 = bitcast double* %2163 to <2 x double>*
  %wide.load10110 = load <2 x double>, <2 x double>* %2164, align 8, !tbaa !62
  %2165 = getelementptr inbounds double, double* %2153, i64 12
  %2166 = bitcast double* %2165 to <2 x double>*
  %wide.load10111 = load <2 x double>, <2 x double>* %2166, align 8, !tbaa !62
  %2167 = getelementptr inbounds double, double* %2153, i64 14
  %2168 = bitcast double* %2167 to <2 x double>*
  %wide.load10112 = load <2 x double>, <2 x double>* %2168, align 8, !tbaa !62
  %2169 = getelementptr inbounds double, double* %2153, i64 16
  %2170 = bitcast double* %2169 to <2 x double>*
  %wide.load10113 = load <2 x double>, <2 x double>* %2170, align 8, !tbaa !62
  %2171 = getelementptr inbounds double, double* %2153, i64 18
  %2172 = bitcast double* %2171 to <2 x double>*
  %wide.load10114 = load <2 x double>, <2 x double>* %2172, align 8, !tbaa !62
  %2173 = getelementptr inbounds double, double* %2153, i64 20
  %2174 = bitcast double* %2173 to <2 x double>*
  %wide.load10115 = load <2 x double>, <2 x double>* %2174, align 8, !tbaa !62
  %2175 = getelementptr inbounds double, double* %2153, i64 22
  %2176 = bitcast double* %2175 to <2 x double>*
  %wide.load10116 = load <2 x double>, <2 x double>* %2176, align 8, !tbaa !62
  %2177 = fadd <2 x double> %wide.load, %wide.load10105
  %2178 = fadd <2 x double> %wide.load10094, %wide.load10106
  %2179 = fadd <2 x double> %wide.load10095, %wide.load10107
  %2180 = fadd <2 x double> %wide.load10096, %wide.load10108
  %2181 = fadd <2 x double> %wide.load10097, %wide.load10109
  %2182 = fadd <2 x double> %wide.load10098, %wide.load10110
  %2183 = fadd <2 x double> %wide.load10099, %wide.load10111
  %2184 = fadd <2 x double> %wide.load10100, %wide.load10112
  %2185 = fadd <2 x double> %wide.load10101, %wide.load10113
  %2186 = fadd <2 x double> %wide.load10102, %wide.load10114
  %2187 = fadd <2 x double> %wide.load10103, %wide.load10115
  %2188 = fadd <2 x double> %wide.load10104, %wide.load10116
  %2189 = getelementptr inbounds double, double* %1795, i64 %index10079
  %2190 = bitcast double* %2189 to <2 x double>*
  %wide.load10117 = load <2 x double>, <2 x double>* %2190, align 8, !tbaa !62
  %2191 = getelementptr inbounds double, double* %2189, i64 2
  %2192 = bitcast double* %2191 to <2 x double>*
  %wide.load10118 = load <2 x double>, <2 x double>* %2192, align 8, !tbaa !62
  %2193 = getelementptr inbounds double, double* %2189, i64 4
  %2194 = bitcast double* %2193 to <2 x double>*
  %wide.load10119 = load <2 x double>, <2 x double>* %2194, align 8, !tbaa !62
  %2195 = getelementptr inbounds double, double* %2189, i64 6
  %2196 = bitcast double* %2195 to <2 x double>*
  %wide.load10120 = load <2 x double>, <2 x double>* %2196, align 8, !tbaa !62
  %2197 = getelementptr inbounds double, double* %2189, i64 8
  %2198 = bitcast double* %2197 to <2 x double>*
  %wide.load10121 = load <2 x double>, <2 x double>* %2198, align 8, !tbaa !62
  %2199 = getelementptr inbounds double, double* %2189, i64 10
  %2200 = bitcast double* %2199 to <2 x double>*
  %wide.load10122 = load <2 x double>, <2 x double>* %2200, align 8, !tbaa !62
  %2201 = getelementptr inbounds double, double* %2189, i64 12
  %2202 = bitcast double* %2201 to <2 x double>*
  %wide.load10123 = load <2 x double>, <2 x double>* %2202, align 8, !tbaa !62
  %2203 = getelementptr inbounds double, double* %2189, i64 14
  %2204 = bitcast double* %2203 to <2 x double>*
  %wide.load10124 = load <2 x double>, <2 x double>* %2204, align 8, !tbaa !62
  %2205 = getelementptr inbounds double, double* %2189, i64 16
  %2206 = bitcast double* %2205 to <2 x double>*
  %wide.load10125 = load <2 x double>, <2 x double>* %2206, align 8, !tbaa !62
  %2207 = getelementptr inbounds double, double* %2189, i64 18
  %2208 = bitcast double* %2207 to <2 x double>*
  %wide.load10126 = load <2 x double>, <2 x double>* %2208, align 8, !tbaa !62
  %2209 = getelementptr inbounds double, double* %2189, i64 20
  %2210 = bitcast double* %2209 to <2 x double>*
  %wide.load10127 = load <2 x double>, <2 x double>* %2210, align 8, !tbaa !62
  %2211 = getelementptr inbounds double, double* %2189, i64 22
  %2212 = bitcast double* %2211 to <2 x double>*
  %wide.load10128 = load <2 x double>, <2 x double>* %2212, align 8, !tbaa !62
  %2213 = fadd <2 x double> %2177, %wide.load10117
  %2214 = fadd <2 x double> %2178, %wide.load10118
  %2215 = fadd <2 x double> %2179, %wide.load10119
  %2216 = fadd <2 x double> %2180, %wide.load10120
  %2217 = fadd <2 x double> %2181, %wide.load10121
  %2218 = fadd <2 x double> %2182, %wide.load10122
  %2219 = fadd <2 x double> %2183, %wide.load10123
  %2220 = fadd <2 x double> %2184, %wide.load10124
  %2221 = fadd <2 x double> %2185, %wide.load10125
  %2222 = fadd <2 x double> %2186, %wide.load10126
  %2223 = fadd <2 x double> %2187, %wide.load10127
  %2224 = fadd <2 x double> %2188, %wide.load10128
  store <2 x double> %2213, <2 x double>* %2190, align 8, !tbaa !62
  store <2 x double> %2214, <2 x double>* %2192, align 8, !tbaa !62
  store <2 x double> %2215, <2 x double>* %2194, align 8, !tbaa !62
  store <2 x double> %2216, <2 x double>* %2196, align 8, !tbaa !62
  store <2 x double> %2217, <2 x double>* %2198, align 8, !tbaa !62
  store <2 x double> %2218, <2 x double>* %2200, align 8, !tbaa !62
  store <2 x double> %2219, <2 x double>* %2202, align 8, !tbaa !62
  store <2 x double> %2220, <2 x double>* %2204, align 8, !tbaa !62
  store <2 x double> %2221, <2 x double>* %2206, align 8, !tbaa !62
  store <2 x double> %2222, <2 x double>* %2208, align 8, !tbaa !62
  store <2 x double> %2223, <2 x double>* %2210, align 8, !tbaa !62
  store <2 x double> %2224, <2 x double>* %2212, align 8, !tbaa !62
  %index.next10080 = add nuw nsw i64 %index10079, 24
  %2225 = icmp eq i64 %index.next10080, %n.vec
  br i1 %2225, label %middle.block10076, label %vector.body10075, !llvm.loop !80

middle.block10076:                                ; preds = %vector.body10075
  br i1 %cmp.n10082, label %for.cond.cleanup3792, label %for.body3793

for.cond.cleanup3786:                             ; preds = %for.cond.cleanup3792
  store i8* %call3733, i8** %1831, align 8
  store i8* %call3733, i8** %1832, align 8
  store i64 %2122, i64* %1833, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %1831, i8** nonnull %1832, i64* nonnull %1833, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body3817

for.cond.cleanup3792:                             ; preds = %for.body3793, %middle.block10076
  %inc3806 = add nuw nsw i32 %times.09015, 1
  %exitcond9589 = icmp eq i32 %inc3806, 54
  br i1 %exitcond9589, label %for.cond.cleanup3786, label %for.cond3789.preheader

for.body3793:                                     ; preds = %middle.block10076, %for.body3793
  %indvars.iv9586 = phi i64 [ %indvars.iv.next9587, %for.body3793 ], [ %n.vec, %middle.block10076 ]
  %arrayidx3795 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9586
  %2226 = load double, double* %arrayidx3795, align 8, !tbaa !62
  %arrayidx3797 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9586
  %2227 = load double, double* %arrayidx3797, align 8, !tbaa !62
  %add3798 = fadd double %2226, %2227
  %arrayidx3800 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9586
  %2228 = load double, double* %arrayidx3800, align 8, !tbaa !62
  %add3801 = fadd double %2228, %add3798
  store double %add3801, double* %arrayidx3800, align 8, !tbaa !62
  %indvars.iv.next9587 = add nuw nsw i64 %indvars.iv9586, 1
  %exitcond9588 = icmp eq i64 %indvars.iv.next9587, %indvars.iv9592
  br i1 %exitcond9588, label %for.cond.cleanup3792, label %for.body3793, !llvm.loop !81

for.body3817:                                     ; preds = %for.inc3831.7, %for.cond.cleanup3786
  %indvars.iv9590 = phi i64 [ 0, %for.cond.cleanup3786 ], [ %indvars.iv.next9591.7, %for.inc3831.7 ]
  %arrayidx3819 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9590
  %2229 = load double, double* %arrayidx3819, align 8, !tbaa !62
  %arrayidx3821 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9590
  %2230 = load double, double* %arrayidx3821, align 8, !tbaa !62
  %cmp3822 = fcmp une double %2229, %2230
  br i1 %cmp3822, label %cleanup3838, label %for.inc3831

for.inc3831:                                      ; preds = %for.body3817
  %indvars.iv.next9591 = or i64 %indvars.iv9590, 1
  %arrayidx3819.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9591
  %2231 = load double, double* %arrayidx3819.1, align 8, !tbaa !62
  %arrayidx3821.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9591
  %2232 = load double, double* %arrayidx3821.1, align 8, !tbaa !62
  %cmp3822.1 = fcmp une double %2231, %2232
  br i1 %cmp3822.1, label %cleanup3838, label %for.inc3831.1

for.inc3836:                                      ; preds = %for.inc3831.7
  %indvars.iv.next9593 = add nuw nsw i64 %indvars.iv9592, 5000
  %cmp3744 = icmp ult i64 %indvars.iv.next9593, 25000
  %indvar.next = add nuw nsw i64 %indvar, 1
  br i1 %cmp3744, label %for.body.i.preheader, label %for.end3840

cleanup3838:                                      ; preds = %for.inc3831.6, %for.inc3831.5, %for.inc3831.4, %for.inc3831.3, %for.inc3831.2, %for.inc3831.1, %for.inc3831, %for.body3817
  %indvars.iv9590.lcssa = phi i64 [ %indvars.iv9590, %for.body3817 ], [ %indvars.iv.next9591, %for.inc3831 ], [ %indvars.iv.next9591.1, %for.inc3831.1 ], [ %indvars.iv.next9591.2, %for.inc3831.2 ], [ %indvars.iv.next9591.3, %for.inc3831.3 ], [ %indvars.iv.next9591.4, %for.inc3831.4 ], [ %indvars.iv.next9591.5, %for.inc3831.5 ], [ %indvars.iv.next9591.6, %for.inc3831.6 ]
  %.lcssa11864 = phi double [ %2229, %for.body3817 ], [ %2231, %for.inc3831 ], [ %7920, %for.inc3831.1 ], [ %7922, %for.inc3831.2 ], [ %7924, %for.inc3831.3 ], [ %7926, %for.inc3831.4 ], [ %7928, %for.inc3831.5 ], [ %7930, %for.inc3831.6 ]
  %.lcssa11862 = phi double [ %2230, %for.body3817 ], [ %2232, %for.inc3831 ], [ %7921, %for.inc3831.1 ], [ %7923, %for.inc3831.2 ], [ %7925, %for.inc3831.3 ], [ %7927, %for.inc3831.4 ], [ %7929, %for.inc3831.5 ], [ %7931, %for.inc3831.6 ]
  %2233 = trunc i64 %indvars.iv9592 to i32
  %2234 = trunc i64 %indvars.iv9590.lcssa to i32
  %call3829 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %2233, i32 signext %2234, double %.lcssa11864, double %.lcssa11862)
  br label %cleanup5832

for.end3840:                                      ; preds = %for.inc3836
  %puts8245 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8246 = call i32 @puts(i8* getelementptr inbounds ([25 x i8], [25 x i8]* @str.335, i64 0, i64 0))
  %2235 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3852, i64 0, i64 0
  %2236 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3853, i64 0, i64 0
  %2237 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes3854, i64 0, i64 0
  %2238 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3852, i64 0, i64 1
  %2239 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3853, i64 0, i64 1
  %2240 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes3854, i64 0, i64 1
  %2241 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3852, i64 0, i64 2
  %2242 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3853, i64 0, i64 2
  %2243 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes3854, i64 0, i64 2
  %2244 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3877, i64 0, i64 0
  %2245 = bitcast [6 x i8*]* %.offload_baseptrs3877 to i64*
  %2246 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3878, i64 0, i64 0
  %2247 = bitcast [6 x i8*]* %.offload_ptrs3878 to i64*
  %2248 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3877, i64 0, i64 1
  %2249 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3878, i64 0, i64 1
  %2250 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3877, i64 0, i64 2
  %2251 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3878, i64 0, i64 2
  %2252 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3877, i64 0, i64 3
  %2253 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3878, i64 0, i64 3
  %2254 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3877, i64 0, i64 4
  %2255 = bitcast i8** %2254 to i64*
  %2256 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3878, i64 0, i64 4
  %2257 = bitcast i8** %2256 to i64*
  %2258 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs3877, i64 0, i64 5
  %2259 = bitcast i8** %2258 to i64*
  %2260 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs3878, i64 0, i64 5
  %2261 = bitcast i8** %2260 to i64*
  %2262 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs3918, i64 0, i64 0
  %2263 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs3919, i64 0, i64 0
  %2264 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes3920, i64 0, i64 0
  %2265 = bitcast i8* %arrayidx.i to <2 x double>*
  %2266 = bitcast i8* %arrayidx2.i to <2 x double>*
  %2267 = bitcast i8* %arrayidx5.i to <2 x double>*
  %2268 = bitcast i8* %arrayidx8.i to <2 x double>*
  %2269 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %2270 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %2271 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %2272 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %2273 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %2274 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %2275 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %2276 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %2277 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %2278 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %2279 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %2280 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %2281 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %2282 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %2283 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %2284 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %2285 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %2286 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %2287 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %2288 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %2289 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %2290 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %2291 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %2292 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %2293 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %2294 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %2295 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %2296 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8418.preheader

for.body.i8418.preheader:                         ; preds = %for.end3840, %for.inc3948
  %indvar10168 = phi i64 [ 0, %for.end3840 ], [ %indvar.next10169, %for.inc3948 ]
  %indvars.iv9581 = phi i64 [ 32, %for.end3840 ], [ %indvars.iv.next9582, %for.inc3948 ]
  %2297 = mul nuw nsw i64 %indvar10168, 5000
  br label %vector.body10228

vector.body10228:                                 ; preds = %vector.body10228, %for.body.i8418.preheader
  %index10232 = phi i64 [ 0, %for.body.i8418.preheader ], [ %index.next10233, %vector.body10228 ]
  %vec.ind10250 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8418.preheader ], [ %vec.ind.next10263, %vector.body10228 ]
  %step.add10251 = add <2 x i32> %vec.ind10250, <i32 2, i32 2>
  %step.add10252 = add <2 x i32> %vec.ind10250, <i32 4, i32 4>
  %step.add10253 = add <2 x i32> %vec.ind10250, <i32 6, i32 6>
  %step.add10254 = add <2 x i32> %vec.ind10250, <i32 8, i32 8>
  %step.add10255 = add <2 x i32> %vec.ind10250, <i32 10, i32 10>
  %step.add10256 = add <2 x i32> %vec.ind10250, <i32 12, i32 12>
  %step.add10257 = add <2 x i32> %vec.ind10250, <i32 14, i32 14>
  %step.add10258 = add <2 x i32> %vec.ind10250, <i32 16, i32 16>
  %step.add10259 = add <2 x i32> %vec.ind10250, <i32 18, i32 18>
  %step.add10260 = add <2 x i32> %vec.ind10250, <i32 20, i32 20>
  %step.add10261 = add <2 x i32> %vec.ind10250, <i32 22, i32 22>
  %2298 = sitofp <2 x i32> %vec.ind10250 to <2 x double>
  %2299 = sitofp <2 x i32> %step.add10251 to <2 x double>
  %2300 = sitofp <2 x i32> %step.add10252 to <2 x double>
  %2301 = sitofp <2 x i32> %step.add10253 to <2 x double>
  %2302 = sitofp <2 x i32> %step.add10254 to <2 x double>
  %2303 = sitofp <2 x i32> %step.add10255 to <2 x double>
  %2304 = sitofp <2 x i32> %step.add10256 to <2 x double>
  %2305 = sitofp <2 x i32> %step.add10257 to <2 x double>
  %2306 = sitofp <2 x i32> %step.add10258 to <2 x double>
  %2307 = sitofp <2 x i32> %step.add10259 to <2 x double>
  %2308 = sitofp <2 x i32> %step.add10260 to <2 x double>
  %2309 = sitofp <2 x i32> %step.add10261 to <2 x double>
  %2310 = getelementptr inbounds double, double* %1795, i64 %index10232
  %2311 = bitcast double* %2310 to <2 x double>*
  store <2 x double> %2298, <2 x double>* %2311, align 8, !tbaa !62
  %2312 = getelementptr inbounds double, double* %2310, i64 2
  %2313 = bitcast double* %2312 to <2 x double>*
  store <2 x double> %2299, <2 x double>* %2313, align 8, !tbaa !62
  %2314 = getelementptr inbounds double, double* %2310, i64 4
  %2315 = bitcast double* %2314 to <2 x double>*
  store <2 x double> %2300, <2 x double>* %2315, align 8, !tbaa !62
  %2316 = getelementptr inbounds double, double* %2310, i64 6
  %2317 = bitcast double* %2316 to <2 x double>*
  store <2 x double> %2301, <2 x double>* %2317, align 8, !tbaa !62
  %2318 = getelementptr inbounds double, double* %2310, i64 8
  %2319 = bitcast double* %2318 to <2 x double>*
  store <2 x double> %2302, <2 x double>* %2319, align 8, !tbaa !62
  %2320 = getelementptr inbounds double, double* %2310, i64 10
  %2321 = bitcast double* %2320 to <2 x double>*
  store <2 x double> %2303, <2 x double>* %2321, align 8, !tbaa !62
  %2322 = getelementptr inbounds double, double* %2310, i64 12
  %2323 = bitcast double* %2322 to <2 x double>*
  store <2 x double> %2304, <2 x double>* %2323, align 8, !tbaa !62
  %2324 = getelementptr inbounds double, double* %2310, i64 14
  %2325 = bitcast double* %2324 to <2 x double>*
  store <2 x double> %2305, <2 x double>* %2325, align 8, !tbaa !62
  %2326 = getelementptr inbounds double, double* %2310, i64 16
  %2327 = bitcast double* %2326 to <2 x double>*
  store <2 x double> %2306, <2 x double>* %2327, align 8, !tbaa !62
  %2328 = getelementptr inbounds double, double* %2310, i64 18
  %2329 = bitcast double* %2328 to <2 x double>*
  store <2 x double> %2307, <2 x double>* %2329, align 8, !tbaa !62
  %2330 = getelementptr inbounds double, double* %2310, i64 20
  %2331 = bitcast double* %2330 to <2 x double>*
  store <2 x double> %2308, <2 x double>* %2331, align 8, !tbaa !62
  %2332 = getelementptr inbounds double, double* %2310, i64 22
  %2333 = bitcast double* %2332 to <2 x double>*
  store <2 x double> %2309, <2 x double>* %2333, align 8, !tbaa !62
  %2334 = getelementptr inbounds double, double* %1794, i64 %index10232
  %2335 = bitcast double* %2334 to <2 x double>*
  store <2 x double> %2298, <2 x double>* %2335, align 8, !tbaa !62
  %2336 = getelementptr inbounds double, double* %2334, i64 2
  %2337 = bitcast double* %2336 to <2 x double>*
  store <2 x double> %2299, <2 x double>* %2337, align 8, !tbaa !62
  %2338 = getelementptr inbounds double, double* %2334, i64 4
  %2339 = bitcast double* %2338 to <2 x double>*
  store <2 x double> %2300, <2 x double>* %2339, align 8, !tbaa !62
  %2340 = getelementptr inbounds double, double* %2334, i64 6
  %2341 = bitcast double* %2340 to <2 x double>*
  store <2 x double> %2301, <2 x double>* %2341, align 8, !tbaa !62
  %2342 = getelementptr inbounds double, double* %2334, i64 8
  %2343 = bitcast double* %2342 to <2 x double>*
  store <2 x double> %2302, <2 x double>* %2343, align 8, !tbaa !62
  %2344 = getelementptr inbounds double, double* %2334, i64 10
  %2345 = bitcast double* %2344 to <2 x double>*
  store <2 x double> %2303, <2 x double>* %2345, align 8, !tbaa !62
  %2346 = getelementptr inbounds double, double* %2334, i64 12
  %2347 = bitcast double* %2346 to <2 x double>*
  store <2 x double> %2304, <2 x double>* %2347, align 8, !tbaa !62
  %2348 = getelementptr inbounds double, double* %2334, i64 14
  %2349 = bitcast double* %2348 to <2 x double>*
  store <2 x double> %2305, <2 x double>* %2349, align 8, !tbaa !62
  %2350 = getelementptr inbounds double, double* %2334, i64 16
  %2351 = bitcast double* %2350 to <2 x double>*
  store <2 x double> %2306, <2 x double>* %2351, align 8, !tbaa !62
  %2352 = getelementptr inbounds double, double* %2334, i64 18
  %2353 = bitcast double* %2352 to <2 x double>*
  store <2 x double> %2307, <2 x double>* %2353, align 8, !tbaa !62
  %2354 = getelementptr inbounds double, double* %2334, i64 20
  %2355 = bitcast double* %2354 to <2 x double>*
  store <2 x double> %2308, <2 x double>* %2355, align 8, !tbaa !62
  %2356 = getelementptr inbounds double, double* %2334, i64 22
  %2357 = bitcast double* %2356 to <2 x double>*
  store <2 x double> %2309, <2 x double>* %2357, align 8, !tbaa !62
  %2358 = shl <2 x i32> %vec.ind10250, <i32 1, i32 1>
  %2359 = shl <2 x i32> %step.add10251, <i32 1, i32 1>
  %2360 = shl <2 x i32> %step.add10252, <i32 1, i32 1>
  %2361 = shl <2 x i32> %step.add10253, <i32 1, i32 1>
  %2362 = shl <2 x i32> %step.add10254, <i32 1, i32 1>
  %2363 = shl <2 x i32> %step.add10255, <i32 1, i32 1>
  %2364 = shl <2 x i32> %step.add10256, <i32 1, i32 1>
  %2365 = shl <2 x i32> %step.add10257, <i32 1, i32 1>
  %2366 = shl <2 x i32> %step.add10258, <i32 1, i32 1>
  %2367 = shl <2 x i32> %step.add10259, <i32 1, i32 1>
  %2368 = shl <2 x i32> %step.add10260, <i32 1, i32 1>
  %2369 = shl <2 x i32> %step.add10261, <i32 1, i32 1>
  %2370 = sitofp <2 x i32> %2358 to <2 x double>
  %2371 = sitofp <2 x i32> %2359 to <2 x double>
  %2372 = sitofp <2 x i32> %2360 to <2 x double>
  %2373 = sitofp <2 x i32> %2361 to <2 x double>
  %2374 = sitofp <2 x i32> %2362 to <2 x double>
  %2375 = sitofp <2 x i32> %2363 to <2 x double>
  %2376 = sitofp <2 x i32> %2364 to <2 x double>
  %2377 = sitofp <2 x i32> %2365 to <2 x double>
  %2378 = sitofp <2 x i32> %2366 to <2 x double>
  %2379 = sitofp <2 x i32> %2367 to <2 x double>
  %2380 = sitofp <2 x i32> %2368 to <2 x double>
  %2381 = sitofp <2 x i32> %2369 to <2 x double>
  %2382 = getelementptr inbounds double, double* %1796, i64 %index10232
  %2383 = bitcast double* %2382 to <2 x double>*
  store <2 x double> %2370, <2 x double>* %2383, align 8, !tbaa !62
  %2384 = getelementptr inbounds double, double* %2382, i64 2
  %2385 = bitcast double* %2384 to <2 x double>*
  store <2 x double> %2371, <2 x double>* %2385, align 8, !tbaa !62
  %2386 = getelementptr inbounds double, double* %2382, i64 4
  %2387 = bitcast double* %2386 to <2 x double>*
  store <2 x double> %2372, <2 x double>* %2387, align 8, !tbaa !62
  %2388 = getelementptr inbounds double, double* %2382, i64 6
  %2389 = bitcast double* %2388 to <2 x double>*
  store <2 x double> %2373, <2 x double>* %2389, align 8, !tbaa !62
  %2390 = getelementptr inbounds double, double* %2382, i64 8
  %2391 = bitcast double* %2390 to <2 x double>*
  store <2 x double> %2374, <2 x double>* %2391, align 8, !tbaa !62
  %2392 = getelementptr inbounds double, double* %2382, i64 10
  %2393 = bitcast double* %2392 to <2 x double>*
  store <2 x double> %2375, <2 x double>* %2393, align 8, !tbaa !62
  %2394 = getelementptr inbounds double, double* %2382, i64 12
  %2395 = bitcast double* %2394 to <2 x double>*
  store <2 x double> %2376, <2 x double>* %2395, align 8, !tbaa !62
  %2396 = getelementptr inbounds double, double* %2382, i64 14
  %2397 = bitcast double* %2396 to <2 x double>*
  store <2 x double> %2377, <2 x double>* %2397, align 8, !tbaa !62
  %2398 = getelementptr inbounds double, double* %2382, i64 16
  %2399 = bitcast double* %2398 to <2 x double>*
  store <2 x double> %2378, <2 x double>* %2399, align 8, !tbaa !62
  %2400 = getelementptr inbounds double, double* %2382, i64 18
  %2401 = bitcast double* %2400 to <2 x double>*
  store <2 x double> %2379, <2 x double>* %2401, align 8, !tbaa !62
  %2402 = getelementptr inbounds double, double* %2382, i64 20
  %2403 = bitcast double* %2402 to <2 x double>*
  store <2 x double> %2380, <2 x double>* %2403, align 8, !tbaa !62
  %2404 = getelementptr inbounds double, double* %2382, i64 22
  %2405 = bitcast double* %2404 to <2 x double>*
  store <2 x double> %2381, <2 x double>* %2405, align 8, !tbaa !62
  %2406 = add <2 x i32> %vec.ind10250, <i32 -3, i32 -3>
  %2407 = add <2 x i32> %vec.ind10250, <i32 -1, i32 -1>
  %2408 = add <2 x i32> %vec.ind10250, <i32 1, i32 1>
  %2409 = add <2 x i32> %vec.ind10250, <i32 3, i32 3>
  %2410 = add <2 x i32> %vec.ind10250, <i32 5, i32 5>
  %2411 = add <2 x i32> %vec.ind10250, <i32 7, i32 7>
  %2412 = add <2 x i32> %vec.ind10250, <i32 9, i32 9>
  %2413 = add <2 x i32> %vec.ind10250, <i32 11, i32 11>
  %2414 = add <2 x i32> %vec.ind10250, <i32 13, i32 13>
  %2415 = add <2 x i32> %vec.ind10250, <i32 15, i32 15>
  %2416 = add <2 x i32> %vec.ind10250, <i32 17, i32 17>
  %2417 = add <2 x i32> %vec.ind10250, <i32 19, i32 19>
  %2418 = sitofp <2 x i32> %2406 to <2 x double>
  %2419 = sitofp <2 x i32> %2407 to <2 x double>
  %2420 = sitofp <2 x i32> %2408 to <2 x double>
  %2421 = sitofp <2 x i32> %2409 to <2 x double>
  %2422 = sitofp <2 x i32> %2410 to <2 x double>
  %2423 = sitofp <2 x i32> %2411 to <2 x double>
  %2424 = sitofp <2 x i32> %2412 to <2 x double>
  %2425 = sitofp <2 x i32> %2413 to <2 x double>
  %2426 = sitofp <2 x i32> %2414 to <2 x double>
  %2427 = sitofp <2 x i32> %2415 to <2 x double>
  %2428 = sitofp <2 x i32> %2416 to <2 x double>
  %2429 = sitofp <2 x i32> %2417 to <2 x double>
  %2430 = getelementptr inbounds double, double* %1797, i64 %index10232
  %2431 = bitcast double* %2430 to <2 x double>*
  store <2 x double> %2418, <2 x double>* %2431, align 8, !tbaa !62
  %2432 = getelementptr inbounds double, double* %2430, i64 2
  %2433 = bitcast double* %2432 to <2 x double>*
  store <2 x double> %2419, <2 x double>* %2433, align 8, !tbaa !62
  %2434 = getelementptr inbounds double, double* %2430, i64 4
  %2435 = bitcast double* %2434 to <2 x double>*
  store <2 x double> %2420, <2 x double>* %2435, align 8, !tbaa !62
  %2436 = getelementptr inbounds double, double* %2430, i64 6
  %2437 = bitcast double* %2436 to <2 x double>*
  store <2 x double> %2421, <2 x double>* %2437, align 8, !tbaa !62
  %2438 = getelementptr inbounds double, double* %2430, i64 8
  %2439 = bitcast double* %2438 to <2 x double>*
  store <2 x double> %2422, <2 x double>* %2439, align 8, !tbaa !62
  %2440 = getelementptr inbounds double, double* %2430, i64 10
  %2441 = bitcast double* %2440 to <2 x double>*
  store <2 x double> %2423, <2 x double>* %2441, align 8, !tbaa !62
  %2442 = getelementptr inbounds double, double* %2430, i64 12
  %2443 = bitcast double* %2442 to <2 x double>*
  store <2 x double> %2424, <2 x double>* %2443, align 8, !tbaa !62
  %2444 = getelementptr inbounds double, double* %2430, i64 14
  %2445 = bitcast double* %2444 to <2 x double>*
  store <2 x double> %2425, <2 x double>* %2445, align 8, !tbaa !62
  %2446 = getelementptr inbounds double, double* %2430, i64 16
  %2447 = bitcast double* %2446 to <2 x double>*
  store <2 x double> %2426, <2 x double>* %2447, align 8, !tbaa !62
  %2448 = getelementptr inbounds double, double* %2430, i64 18
  %2449 = bitcast double* %2448 to <2 x double>*
  store <2 x double> %2427, <2 x double>* %2449, align 8, !tbaa !62
  %2450 = getelementptr inbounds double, double* %2430, i64 20
  %2451 = bitcast double* %2450 to <2 x double>*
  store <2 x double> %2428, <2 x double>* %2451, align 8, !tbaa !62
  %2452 = getelementptr inbounds double, double* %2430, i64 22
  %2453 = bitcast double* %2452 to <2 x double>*
  store <2 x double> %2429, <2 x double>* %2453, align 8, !tbaa !62
  %index.next10233 = add nuw nsw i64 %index10232, 24
  %vec.ind.next10263 = add <2 x i32> %vec.ind10250, <i32 24, i32 24>
  %2454 = icmp eq i64 %index.next10233, 24984
  br i1 %2454, label %for.body.i8418, label %vector.body10228, !llvm.loop !83

for.body.i8418:                                   ; preds = %vector.body10228
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %2265, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %2266, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %2267, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %2268, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %2269, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %2270, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %2271, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %2272, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %2273, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %2274, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %2275, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %2276, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %2277, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %2278, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %2279, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %2280, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %2281, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %2282, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %2283, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %2284, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %2285, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %2286, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %2287, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %2288, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %2289, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %2290, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %2291, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %2292, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %2293, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %2294, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %2295, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %2296, align 8, !tbaa !62
  %2455 = shl nuw nsw i64 %indvars.iv9581, 3
  store i8* %call3733, i8** %2235, align 8
  store i8* %call3733, i8** %2236, align 8
  store i64 %2455, i64* %2237, align 8
  store i8* %call3735, i8** %2238, align 8
  store i8* %call3735, i8** %2239, align 8
  store i64 %2455, i64* %2240, align 8
  store i8* %call3736, i8** %2241, align 8
  store i8* %call3736, i8** %2242, align 8
  store i64 %2455, i64* %2243, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %2235, i8** nonnull %2236, i64* nonnull %2237, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  br label %for.cond3863.preheader

for.cond3863.preheader:                           ; preds = %for.body.i8418, %omp_offload.cont3880.5
  %tms3856.09003 = phi i32 [ 1, %for.body.i8418 ], [ %mul3886, %omp_offload.cont3880.5 ]
  %.capture_expr..casted3873.sroa.0.0.insert.ext = zext i32 %tms3856.09003 to i64
  store i64 %indvars.iv9581, i64* %2245, align 8
  store i64 %indvars.iv9581, i64* %2247, align 8
  store i8* %call3733, i8** %2248, align 8
  store i8* %call3733, i8** %2249, align 8
  store i8* %call3735, i8** %2250, align 8
  store i8* %call3735, i8** %2251, align 8
  store i8* %call3736, i8** %2252, align 8
  store i8* %call3736, i8** %2253, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2255, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2257, align 8
  store i64 32, i64* %2259, align 8
  store i64 32, i64* %2261, align 8
  %2456 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1070.region_id, i32 6, i8** nonnull %2244, i8** nonnull %2246, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3856.09003, i32 32) #6
  %2457 = icmp eq i32 %2456, 0
  br i1 %2457, label %omp_offload.cont3880, label %omp_offload.failed3879

omp_offload.failed3879:                           ; preds = %for.cond3863.preheader
  %2458 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %2459 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %2458, i32 %tms3856.09003, i32 32) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..166 to void (i32*, i32*, ...)*), i64 %indvars.iv9581, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3880

omp_offload.cont3880:                             ; preds = %for.cond3863.preheader, %omp_offload.failed3879
  store i64 %indvars.iv9581, i64* %2245, align 8
  store i64 %indvars.iv9581, i64* %2247, align 8
  store i8* %call3733, i8** %2248, align 8
  store i8* %call3733, i8** %2249, align 8
  store i8* %call3735, i8** %2250, align 8
  store i8* %call3735, i8** %2251, align 8
  store i8* %call3736, i8** %2252, align 8
  store i8* %call3736, i8** %2253, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2255, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2257, align 8
  store i64 64, i64* %2259, align 8
  store i64 64, i64* %2261, align 8
  %2460 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1070.region_id, i32 6, i8** nonnull %2244, i8** nonnull %2246, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3856.09003, i32 64) #6
  %2461 = icmp eq i32 %2460, 0
  br i1 %2461, label %omp_offload.cont3880.1, label %omp_offload.failed3879.1

for.cond3896.preheader:                           ; preds = %for.cond3896.preheader.preheader63, %for.cond.cleanup3899
  %times3889.09006 = phi i32 [ %inc3914, %for.cond.cleanup3899 ], [ 0, %for.cond3896.preheader.preheader63 ]
  br label %vector.body10165

vector.body10165:                                 ; preds = %for.cond3896.preheader, %vector.body10165
  %index10174 = phi i64 [ %index.next10175, %vector.body10165 ], [ 0, %for.cond3896.preheader ]
  %2462 = getelementptr inbounds double, double* %1796, i64 %index10174
  %2463 = bitcast double* %2462 to <2 x double>*
  %wide.load10192 = load <2 x double>, <2 x double>* %2463, align 8, !tbaa !62
  %2464 = getelementptr inbounds double, double* %2462, i64 2
  %2465 = bitcast double* %2464 to <2 x double>*
  %wide.load10193 = load <2 x double>, <2 x double>* %2465, align 8, !tbaa !62
  %2466 = getelementptr inbounds double, double* %2462, i64 4
  %2467 = bitcast double* %2466 to <2 x double>*
  %wide.load10194 = load <2 x double>, <2 x double>* %2467, align 8, !tbaa !62
  %2468 = getelementptr inbounds double, double* %2462, i64 6
  %2469 = bitcast double* %2468 to <2 x double>*
  %wide.load10195 = load <2 x double>, <2 x double>* %2469, align 8, !tbaa !62
  %2470 = getelementptr inbounds double, double* %2462, i64 8
  %2471 = bitcast double* %2470 to <2 x double>*
  %wide.load10196 = load <2 x double>, <2 x double>* %2471, align 8, !tbaa !62
  %2472 = getelementptr inbounds double, double* %2462, i64 10
  %2473 = bitcast double* %2472 to <2 x double>*
  %wide.load10197 = load <2 x double>, <2 x double>* %2473, align 8, !tbaa !62
  %2474 = getelementptr inbounds double, double* %2462, i64 12
  %2475 = bitcast double* %2474 to <2 x double>*
  %wide.load10198 = load <2 x double>, <2 x double>* %2475, align 8, !tbaa !62
  %2476 = getelementptr inbounds double, double* %2462, i64 14
  %2477 = bitcast double* %2476 to <2 x double>*
  %wide.load10199 = load <2 x double>, <2 x double>* %2477, align 8, !tbaa !62
  %2478 = getelementptr inbounds double, double* %2462, i64 16
  %2479 = bitcast double* %2478 to <2 x double>*
  %wide.load10200 = load <2 x double>, <2 x double>* %2479, align 8, !tbaa !62
  %2480 = getelementptr inbounds double, double* %2462, i64 18
  %2481 = bitcast double* %2480 to <2 x double>*
  %wide.load10201 = load <2 x double>, <2 x double>* %2481, align 8, !tbaa !62
  %2482 = getelementptr inbounds double, double* %2462, i64 20
  %2483 = bitcast double* %2482 to <2 x double>*
  %wide.load10202 = load <2 x double>, <2 x double>* %2483, align 8, !tbaa !62
  %2484 = getelementptr inbounds double, double* %2462, i64 22
  %2485 = bitcast double* %2484 to <2 x double>*
  %wide.load10203 = load <2 x double>, <2 x double>* %2485, align 8, !tbaa !62
  %2486 = getelementptr inbounds double, double* %1797, i64 %index10174
  %2487 = bitcast double* %2486 to <2 x double>*
  %wide.load10204 = load <2 x double>, <2 x double>* %2487, align 8, !tbaa !62
  %2488 = getelementptr inbounds double, double* %2486, i64 2
  %2489 = bitcast double* %2488 to <2 x double>*
  %wide.load10205 = load <2 x double>, <2 x double>* %2489, align 8, !tbaa !62
  %2490 = getelementptr inbounds double, double* %2486, i64 4
  %2491 = bitcast double* %2490 to <2 x double>*
  %wide.load10206 = load <2 x double>, <2 x double>* %2491, align 8, !tbaa !62
  %2492 = getelementptr inbounds double, double* %2486, i64 6
  %2493 = bitcast double* %2492 to <2 x double>*
  %wide.load10207 = load <2 x double>, <2 x double>* %2493, align 8, !tbaa !62
  %2494 = getelementptr inbounds double, double* %2486, i64 8
  %2495 = bitcast double* %2494 to <2 x double>*
  %wide.load10208 = load <2 x double>, <2 x double>* %2495, align 8, !tbaa !62
  %2496 = getelementptr inbounds double, double* %2486, i64 10
  %2497 = bitcast double* %2496 to <2 x double>*
  %wide.load10209 = load <2 x double>, <2 x double>* %2497, align 8, !tbaa !62
  %2498 = getelementptr inbounds double, double* %2486, i64 12
  %2499 = bitcast double* %2498 to <2 x double>*
  %wide.load10210 = load <2 x double>, <2 x double>* %2499, align 8, !tbaa !62
  %2500 = getelementptr inbounds double, double* %2486, i64 14
  %2501 = bitcast double* %2500 to <2 x double>*
  %wide.load10211 = load <2 x double>, <2 x double>* %2501, align 8, !tbaa !62
  %2502 = getelementptr inbounds double, double* %2486, i64 16
  %2503 = bitcast double* %2502 to <2 x double>*
  %wide.load10212 = load <2 x double>, <2 x double>* %2503, align 8, !tbaa !62
  %2504 = getelementptr inbounds double, double* %2486, i64 18
  %2505 = bitcast double* %2504 to <2 x double>*
  %wide.load10213 = load <2 x double>, <2 x double>* %2505, align 8, !tbaa !62
  %2506 = getelementptr inbounds double, double* %2486, i64 20
  %2507 = bitcast double* %2506 to <2 x double>*
  %wide.load10214 = load <2 x double>, <2 x double>* %2507, align 8, !tbaa !62
  %2508 = getelementptr inbounds double, double* %2486, i64 22
  %2509 = bitcast double* %2508 to <2 x double>*
  %wide.load10215 = load <2 x double>, <2 x double>* %2509, align 8, !tbaa !62
  %2510 = fadd <2 x double> %wide.load10192, %wide.load10204
  %2511 = fadd <2 x double> %wide.load10193, %wide.load10205
  %2512 = fadd <2 x double> %wide.load10194, %wide.load10206
  %2513 = fadd <2 x double> %wide.load10195, %wide.load10207
  %2514 = fadd <2 x double> %wide.load10196, %wide.load10208
  %2515 = fadd <2 x double> %wide.load10197, %wide.load10209
  %2516 = fadd <2 x double> %wide.load10198, %wide.load10210
  %2517 = fadd <2 x double> %wide.load10199, %wide.load10211
  %2518 = fadd <2 x double> %wide.load10200, %wide.load10212
  %2519 = fadd <2 x double> %wide.load10201, %wide.load10213
  %2520 = fadd <2 x double> %wide.load10202, %wide.load10214
  %2521 = fadd <2 x double> %wide.load10203, %wide.load10215
  %2522 = getelementptr inbounds double, double* %1795, i64 %index10174
  %2523 = bitcast double* %2522 to <2 x double>*
  %wide.load10216 = load <2 x double>, <2 x double>* %2523, align 8, !tbaa !62
  %2524 = getelementptr inbounds double, double* %2522, i64 2
  %2525 = bitcast double* %2524 to <2 x double>*
  %wide.load10217 = load <2 x double>, <2 x double>* %2525, align 8, !tbaa !62
  %2526 = getelementptr inbounds double, double* %2522, i64 4
  %2527 = bitcast double* %2526 to <2 x double>*
  %wide.load10218 = load <2 x double>, <2 x double>* %2527, align 8, !tbaa !62
  %2528 = getelementptr inbounds double, double* %2522, i64 6
  %2529 = bitcast double* %2528 to <2 x double>*
  %wide.load10219 = load <2 x double>, <2 x double>* %2529, align 8, !tbaa !62
  %2530 = getelementptr inbounds double, double* %2522, i64 8
  %2531 = bitcast double* %2530 to <2 x double>*
  %wide.load10220 = load <2 x double>, <2 x double>* %2531, align 8, !tbaa !62
  %2532 = getelementptr inbounds double, double* %2522, i64 10
  %2533 = bitcast double* %2532 to <2 x double>*
  %wide.load10221 = load <2 x double>, <2 x double>* %2533, align 8, !tbaa !62
  %2534 = getelementptr inbounds double, double* %2522, i64 12
  %2535 = bitcast double* %2534 to <2 x double>*
  %wide.load10222 = load <2 x double>, <2 x double>* %2535, align 8, !tbaa !62
  %2536 = getelementptr inbounds double, double* %2522, i64 14
  %2537 = bitcast double* %2536 to <2 x double>*
  %wide.load10223 = load <2 x double>, <2 x double>* %2537, align 8, !tbaa !62
  %2538 = getelementptr inbounds double, double* %2522, i64 16
  %2539 = bitcast double* %2538 to <2 x double>*
  %wide.load10224 = load <2 x double>, <2 x double>* %2539, align 8, !tbaa !62
  %2540 = getelementptr inbounds double, double* %2522, i64 18
  %2541 = bitcast double* %2540 to <2 x double>*
  %wide.load10225 = load <2 x double>, <2 x double>* %2541, align 8, !tbaa !62
  %2542 = getelementptr inbounds double, double* %2522, i64 20
  %2543 = bitcast double* %2542 to <2 x double>*
  %wide.load10226 = load <2 x double>, <2 x double>* %2543, align 8, !tbaa !62
  %2544 = getelementptr inbounds double, double* %2522, i64 22
  %2545 = bitcast double* %2544 to <2 x double>*
  %wide.load10227 = load <2 x double>, <2 x double>* %2545, align 8, !tbaa !62
  %2546 = fadd <2 x double> %2510, %wide.load10216
  %2547 = fadd <2 x double> %2511, %wide.load10217
  %2548 = fadd <2 x double> %2512, %wide.load10218
  %2549 = fadd <2 x double> %2513, %wide.load10219
  %2550 = fadd <2 x double> %2514, %wide.load10220
  %2551 = fadd <2 x double> %2515, %wide.load10221
  %2552 = fadd <2 x double> %2516, %wide.load10222
  %2553 = fadd <2 x double> %2517, %wide.load10223
  %2554 = fadd <2 x double> %2518, %wide.load10224
  %2555 = fadd <2 x double> %2519, %wide.load10225
  %2556 = fadd <2 x double> %2520, %wide.load10226
  %2557 = fadd <2 x double> %2521, %wide.load10227
  store <2 x double> %2546, <2 x double>* %2523, align 8, !tbaa !62
  store <2 x double> %2547, <2 x double>* %2525, align 8, !tbaa !62
  store <2 x double> %2548, <2 x double>* %2527, align 8, !tbaa !62
  store <2 x double> %2549, <2 x double>* %2529, align 8, !tbaa !62
  store <2 x double> %2550, <2 x double>* %2531, align 8, !tbaa !62
  store <2 x double> %2551, <2 x double>* %2533, align 8, !tbaa !62
  store <2 x double> %2552, <2 x double>* %2535, align 8, !tbaa !62
  store <2 x double> %2553, <2 x double>* %2537, align 8, !tbaa !62
  store <2 x double> %2554, <2 x double>* %2539, align 8, !tbaa !62
  store <2 x double> %2555, <2 x double>* %2541, align 8, !tbaa !62
  store <2 x double> %2556, <2 x double>* %2543, align 8, !tbaa !62
  store <2 x double> %2557, <2 x double>* %2545, align 8, !tbaa !62
  %index.next10175 = add nuw nsw i64 %index10174, 24
  %2558 = icmp eq i64 %index.next10175, %n.vec10173
  br i1 %2558, label %middle.block10166, label %vector.body10165, !llvm.loop !84

middle.block10166:                                ; preds = %vector.body10165
  br i1 %cmp.n10177, label %for.cond.cleanup3899, label %for.body3900

for.cond.cleanup3893:                             ; preds = %for.cond.cleanup3899
  store i8* %call3733, i8** %2262, align 8
  store i8* %call3733, i8** %2263, align 8
  store i64 %2455, i64* %2264, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %2262, i8** nonnull %2263, i64* nonnull %2264, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body3926

for.cond.cleanup3899:                             ; preds = %for.body3900, %middle.block10166
  %inc3914 = add nuw nsw i32 %times3889.09006, 1
  %exitcond9578 = icmp eq i32 %inc3914, 54
  br i1 %exitcond9578, label %for.cond.cleanup3893, label %for.cond3896.preheader

for.body3900:                                     ; preds = %middle.block10166, %for.body3900
  %indvars.iv9575 = phi i64 [ %indvars.iv.next9576, %for.body3900 ], [ %n.vec10173, %middle.block10166 ]
  %arrayidx3902 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9575
  %2559 = load double, double* %arrayidx3902, align 8, !tbaa !62
  %arrayidx3904 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9575
  %2560 = load double, double* %arrayidx3904, align 8, !tbaa !62
  %add3905 = fadd double %2559, %2560
  %arrayidx3907 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9575
  %2561 = load double, double* %arrayidx3907, align 8, !tbaa !62
  %add3908 = fadd double %2561, %add3905
  store double %add3908, double* %arrayidx3907, align 8, !tbaa !62
  %indvars.iv.next9576 = add nuw nsw i64 %indvars.iv9575, 1
  %exitcond9577 = icmp eq i64 %indvars.iv.next9576, %indvars.iv9581
  br i1 %exitcond9577, label %for.cond.cleanup3899, label %for.body3900, !llvm.loop !85

for.body3926:                                     ; preds = %for.inc3940.7, %for.cond.cleanup3893
  %indvars.iv9579 = phi i64 [ 0, %for.cond.cleanup3893 ], [ %indvars.iv.next9580.7, %for.inc3940.7 ]
  %arrayidx3928 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9579
  %2562 = load double, double* %arrayidx3928, align 8, !tbaa !62
  %arrayidx3930 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9579
  %2563 = load double, double* %arrayidx3930, align 8, !tbaa !62
  %cmp3931 = fcmp une double %2562, %2563
  br i1 %cmp3931, label %cleanup3950, label %for.inc3940

for.inc3940:                                      ; preds = %for.body3926
  %indvars.iv.next9580 = or i64 %indvars.iv9579, 1
  %arrayidx3928.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9580
  %2564 = load double, double* %arrayidx3928.1, align 8, !tbaa !62
  %arrayidx3930.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9580
  %2565 = load double, double* %arrayidx3930.1, align 8, !tbaa !62
  %cmp3931.1 = fcmp une double %2564, %2565
  br i1 %cmp3931.1, label %cleanup3950, label %for.inc3940.1

for.inc3948:                                      ; preds = %for.inc3940.7
  %indvars.iv.next9582 = add nuw nsw i64 %indvars.iv9581, 5000
  %cmp3845 = icmp ult i64 %indvars.iv.next9582, 25000
  %indvar.next10169 = add nuw nsw i64 %indvar10168, 1
  br i1 %cmp3845, label %for.body.i8418.preheader, label %for.end3952

cleanup3950:                                      ; preds = %for.inc3940.6, %for.inc3940.5, %for.inc3940.4, %for.inc3940.3, %for.inc3940.2, %for.inc3940.1, %for.inc3940, %for.body3926
  %indvars.iv9579.lcssa = phi i64 [ %indvars.iv9579, %for.body3926 ], [ %indvars.iv.next9580, %for.inc3940 ], [ %indvars.iv.next9580.1, %for.inc3940.1 ], [ %indvars.iv.next9580.2, %for.inc3940.2 ], [ %indvars.iv.next9580.3, %for.inc3940.3 ], [ %indvars.iv.next9580.4, %for.inc3940.4 ], [ %indvars.iv.next9580.5, %for.inc3940.5 ], [ %indvars.iv.next9580.6, %for.inc3940.6 ]
  %.lcssa11858 = phi double [ %2562, %for.body3926 ], [ %2564, %for.inc3940 ], [ %7908, %for.inc3940.1 ], [ %7910, %for.inc3940.2 ], [ %7912, %for.inc3940.3 ], [ %7914, %for.inc3940.4 ], [ %7916, %for.inc3940.5 ], [ %7918, %for.inc3940.6 ]
  %.lcssa11856 = phi double [ %2563, %for.body3926 ], [ %2565, %for.inc3940 ], [ %7909, %for.inc3940.1 ], [ %7911, %for.inc3940.2 ], [ %7913, %for.inc3940.3 ], [ %7915, %for.inc3940.4 ], [ %7917, %for.inc3940.5 ], [ %7919, %for.inc3940.6 ]
  %2566 = trunc i64 %indvars.iv9581 to i32
  %2567 = trunc i64 %indvars.iv9579.lcssa to i32
  %call3938 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %2566, i32 signext %2567, double %.lcssa11858, double %.lcssa11856)
  br label %cleanup5832

for.end3952:                                      ; preds = %for.inc3948
  %puts8247 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8248 = call i32 @puts(i8* getelementptr inbounds ([22 x i8], [22 x i8]* @str.337, i64 0, i64 0))
  %2568 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3965, i64 0, i64 0
  %2569 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3966, i64 0, i64 0
  %2570 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes3967, i64 0, i64 0
  %2571 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3965, i64 0, i64 1
  %2572 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3966, i64 0, i64 1
  %2573 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes3967, i64 0, i64 1
  %2574 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs3965, i64 0, i64 2
  %2575 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs3966, i64 0, i64 2
  %2576 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes3967, i64 0, i64 2
  %2577 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs3997, i64 0, i64 0
  %2578 = bitcast [7 x i8*]* %.offload_baseptrs3997 to i64*
  %2579 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs3998, i64 0, i64 0
  %2580 = bitcast [7 x i8*]* %.offload_ptrs3998 to i64*
  %2581 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs3997, i64 0, i64 1
  %2582 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs3998, i64 0, i64 1
  %2583 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs3997, i64 0, i64 2
  %2584 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs3998, i64 0, i64 2
  %2585 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs3997, i64 0, i64 3
  %2586 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs3998, i64 0, i64 3
  %2587 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs3997, i64 0, i64 4
  %2588 = bitcast i8** %2587 to i64*
  %2589 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs3998, i64 0, i64 4
  %2590 = bitcast i8** %2589 to i64*
  %2591 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs3997, i64 0, i64 5
  %2592 = bitcast i8** %2591 to i64*
  %2593 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs3998, i64 0, i64 5
  %2594 = bitcast i8** %2593 to i64*
  %2595 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs3997, i64 0, i64 6
  %2596 = bitcast i8** %2595 to i64*
  %2597 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs3998, i64 0, i64 6
  %2598 = bitcast i8** %2597 to i64*
  %2599 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs4042, i64 0, i64 0
  %2600 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs4043, i64 0, i64 0
  %2601 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes4044, i64 0, i64 0
  %2602 = bitcast i8* %arrayidx.i to <2 x double>*
  %2603 = bitcast i8* %arrayidx2.i to <2 x double>*
  %2604 = bitcast i8* %arrayidx5.i to <2 x double>*
  %2605 = bitcast i8* %arrayidx8.i to <2 x double>*
  %2606 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %2607 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %2608 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %2609 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %2610 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %2611 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %2612 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %2613 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %2614 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %2615 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %2616 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %2617 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %2618 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %2619 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %2620 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %2621 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %2622 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %2623 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %2624 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %2625 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %2626 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %2627 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %2628 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %2629 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %2630 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %2631 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %2632 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %2633 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8432.preheader

for.body.i8432.preheader:                         ; preds = %for.end3952, %for.inc4072
  %indvar10267 = phi i64 [ 0, %for.end3952 ], [ %indvar.next10268, %for.inc4072 ]
  %indvars.iv9570 = phi i64 [ 32, %for.end3952 ], [ %indvars.iv.next9571, %for.inc4072 ]
  %2634 = mul nuw nsw i64 %indvar10267, 5000
  br label %vector.body10327

vector.body10327:                                 ; preds = %vector.body10327, %for.body.i8432.preheader
  %index10331 = phi i64 [ 0, %for.body.i8432.preheader ], [ %index.next10332, %vector.body10327 ]
  %vec.ind10349 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8432.preheader ], [ %vec.ind.next10362, %vector.body10327 ]
  %step.add10350 = add <2 x i32> %vec.ind10349, <i32 2, i32 2>
  %step.add10351 = add <2 x i32> %vec.ind10349, <i32 4, i32 4>
  %step.add10352 = add <2 x i32> %vec.ind10349, <i32 6, i32 6>
  %step.add10353 = add <2 x i32> %vec.ind10349, <i32 8, i32 8>
  %step.add10354 = add <2 x i32> %vec.ind10349, <i32 10, i32 10>
  %step.add10355 = add <2 x i32> %vec.ind10349, <i32 12, i32 12>
  %step.add10356 = add <2 x i32> %vec.ind10349, <i32 14, i32 14>
  %step.add10357 = add <2 x i32> %vec.ind10349, <i32 16, i32 16>
  %step.add10358 = add <2 x i32> %vec.ind10349, <i32 18, i32 18>
  %step.add10359 = add <2 x i32> %vec.ind10349, <i32 20, i32 20>
  %step.add10360 = add <2 x i32> %vec.ind10349, <i32 22, i32 22>
  %2635 = sitofp <2 x i32> %vec.ind10349 to <2 x double>
  %2636 = sitofp <2 x i32> %step.add10350 to <2 x double>
  %2637 = sitofp <2 x i32> %step.add10351 to <2 x double>
  %2638 = sitofp <2 x i32> %step.add10352 to <2 x double>
  %2639 = sitofp <2 x i32> %step.add10353 to <2 x double>
  %2640 = sitofp <2 x i32> %step.add10354 to <2 x double>
  %2641 = sitofp <2 x i32> %step.add10355 to <2 x double>
  %2642 = sitofp <2 x i32> %step.add10356 to <2 x double>
  %2643 = sitofp <2 x i32> %step.add10357 to <2 x double>
  %2644 = sitofp <2 x i32> %step.add10358 to <2 x double>
  %2645 = sitofp <2 x i32> %step.add10359 to <2 x double>
  %2646 = sitofp <2 x i32> %step.add10360 to <2 x double>
  %2647 = getelementptr inbounds double, double* %1795, i64 %index10331
  %2648 = bitcast double* %2647 to <2 x double>*
  store <2 x double> %2635, <2 x double>* %2648, align 8, !tbaa !62
  %2649 = getelementptr inbounds double, double* %2647, i64 2
  %2650 = bitcast double* %2649 to <2 x double>*
  store <2 x double> %2636, <2 x double>* %2650, align 8, !tbaa !62
  %2651 = getelementptr inbounds double, double* %2647, i64 4
  %2652 = bitcast double* %2651 to <2 x double>*
  store <2 x double> %2637, <2 x double>* %2652, align 8, !tbaa !62
  %2653 = getelementptr inbounds double, double* %2647, i64 6
  %2654 = bitcast double* %2653 to <2 x double>*
  store <2 x double> %2638, <2 x double>* %2654, align 8, !tbaa !62
  %2655 = getelementptr inbounds double, double* %2647, i64 8
  %2656 = bitcast double* %2655 to <2 x double>*
  store <2 x double> %2639, <2 x double>* %2656, align 8, !tbaa !62
  %2657 = getelementptr inbounds double, double* %2647, i64 10
  %2658 = bitcast double* %2657 to <2 x double>*
  store <2 x double> %2640, <2 x double>* %2658, align 8, !tbaa !62
  %2659 = getelementptr inbounds double, double* %2647, i64 12
  %2660 = bitcast double* %2659 to <2 x double>*
  store <2 x double> %2641, <2 x double>* %2660, align 8, !tbaa !62
  %2661 = getelementptr inbounds double, double* %2647, i64 14
  %2662 = bitcast double* %2661 to <2 x double>*
  store <2 x double> %2642, <2 x double>* %2662, align 8, !tbaa !62
  %2663 = getelementptr inbounds double, double* %2647, i64 16
  %2664 = bitcast double* %2663 to <2 x double>*
  store <2 x double> %2643, <2 x double>* %2664, align 8, !tbaa !62
  %2665 = getelementptr inbounds double, double* %2647, i64 18
  %2666 = bitcast double* %2665 to <2 x double>*
  store <2 x double> %2644, <2 x double>* %2666, align 8, !tbaa !62
  %2667 = getelementptr inbounds double, double* %2647, i64 20
  %2668 = bitcast double* %2667 to <2 x double>*
  store <2 x double> %2645, <2 x double>* %2668, align 8, !tbaa !62
  %2669 = getelementptr inbounds double, double* %2647, i64 22
  %2670 = bitcast double* %2669 to <2 x double>*
  store <2 x double> %2646, <2 x double>* %2670, align 8, !tbaa !62
  %2671 = getelementptr inbounds double, double* %1794, i64 %index10331
  %2672 = bitcast double* %2671 to <2 x double>*
  store <2 x double> %2635, <2 x double>* %2672, align 8, !tbaa !62
  %2673 = getelementptr inbounds double, double* %2671, i64 2
  %2674 = bitcast double* %2673 to <2 x double>*
  store <2 x double> %2636, <2 x double>* %2674, align 8, !tbaa !62
  %2675 = getelementptr inbounds double, double* %2671, i64 4
  %2676 = bitcast double* %2675 to <2 x double>*
  store <2 x double> %2637, <2 x double>* %2676, align 8, !tbaa !62
  %2677 = getelementptr inbounds double, double* %2671, i64 6
  %2678 = bitcast double* %2677 to <2 x double>*
  store <2 x double> %2638, <2 x double>* %2678, align 8, !tbaa !62
  %2679 = getelementptr inbounds double, double* %2671, i64 8
  %2680 = bitcast double* %2679 to <2 x double>*
  store <2 x double> %2639, <2 x double>* %2680, align 8, !tbaa !62
  %2681 = getelementptr inbounds double, double* %2671, i64 10
  %2682 = bitcast double* %2681 to <2 x double>*
  store <2 x double> %2640, <2 x double>* %2682, align 8, !tbaa !62
  %2683 = getelementptr inbounds double, double* %2671, i64 12
  %2684 = bitcast double* %2683 to <2 x double>*
  store <2 x double> %2641, <2 x double>* %2684, align 8, !tbaa !62
  %2685 = getelementptr inbounds double, double* %2671, i64 14
  %2686 = bitcast double* %2685 to <2 x double>*
  store <2 x double> %2642, <2 x double>* %2686, align 8, !tbaa !62
  %2687 = getelementptr inbounds double, double* %2671, i64 16
  %2688 = bitcast double* %2687 to <2 x double>*
  store <2 x double> %2643, <2 x double>* %2688, align 8, !tbaa !62
  %2689 = getelementptr inbounds double, double* %2671, i64 18
  %2690 = bitcast double* %2689 to <2 x double>*
  store <2 x double> %2644, <2 x double>* %2690, align 8, !tbaa !62
  %2691 = getelementptr inbounds double, double* %2671, i64 20
  %2692 = bitcast double* %2691 to <2 x double>*
  store <2 x double> %2645, <2 x double>* %2692, align 8, !tbaa !62
  %2693 = getelementptr inbounds double, double* %2671, i64 22
  %2694 = bitcast double* %2693 to <2 x double>*
  store <2 x double> %2646, <2 x double>* %2694, align 8, !tbaa !62
  %2695 = shl <2 x i32> %vec.ind10349, <i32 1, i32 1>
  %2696 = shl <2 x i32> %step.add10350, <i32 1, i32 1>
  %2697 = shl <2 x i32> %step.add10351, <i32 1, i32 1>
  %2698 = shl <2 x i32> %step.add10352, <i32 1, i32 1>
  %2699 = shl <2 x i32> %step.add10353, <i32 1, i32 1>
  %2700 = shl <2 x i32> %step.add10354, <i32 1, i32 1>
  %2701 = shl <2 x i32> %step.add10355, <i32 1, i32 1>
  %2702 = shl <2 x i32> %step.add10356, <i32 1, i32 1>
  %2703 = shl <2 x i32> %step.add10357, <i32 1, i32 1>
  %2704 = shl <2 x i32> %step.add10358, <i32 1, i32 1>
  %2705 = shl <2 x i32> %step.add10359, <i32 1, i32 1>
  %2706 = shl <2 x i32> %step.add10360, <i32 1, i32 1>
  %2707 = sitofp <2 x i32> %2695 to <2 x double>
  %2708 = sitofp <2 x i32> %2696 to <2 x double>
  %2709 = sitofp <2 x i32> %2697 to <2 x double>
  %2710 = sitofp <2 x i32> %2698 to <2 x double>
  %2711 = sitofp <2 x i32> %2699 to <2 x double>
  %2712 = sitofp <2 x i32> %2700 to <2 x double>
  %2713 = sitofp <2 x i32> %2701 to <2 x double>
  %2714 = sitofp <2 x i32> %2702 to <2 x double>
  %2715 = sitofp <2 x i32> %2703 to <2 x double>
  %2716 = sitofp <2 x i32> %2704 to <2 x double>
  %2717 = sitofp <2 x i32> %2705 to <2 x double>
  %2718 = sitofp <2 x i32> %2706 to <2 x double>
  %2719 = getelementptr inbounds double, double* %1796, i64 %index10331
  %2720 = bitcast double* %2719 to <2 x double>*
  store <2 x double> %2707, <2 x double>* %2720, align 8, !tbaa !62
  %2721 = getelementptr inbounds double, double* %2719, i64 2
  %2722 = bitcast double* %2721 to <2 x double>*
  store <2 x double> %2708, <2 x double>* %2722, align 8, !tbaa !62
  %2723 = getelementptr inbounds double, double* %2719, i64 4
  %2724 = bitcast double* %2723 to <2 x double>*
  store <2 x double> %2709, <2 x double>* %2724, align 8, !tbaa !62
  %2725 = getelementptr inbounds double, double* %2719, i64 6
  %2726 = bitcast double* %2725 to <2 x double>*
  store <2 x double> %2710, <2 x double>* %2726, align 8, !tbaa !62
  %2727 = getelementptr inbounds double, double* %2719, i64 8
  %2728 = bitcast double* %2727 to <2 x double>*
  store <2 x double> %2711, <2 x double>* %2728, align 8, !tbaa !62
  %2729 = getelementptr inbounds double, double* %2719, i64 10
  %2730 = bitcast double* %2729 to <2 x double>*
  store <2 x double> %2712, <2 x double>* %2730, align 8, !tbaa !62
  %2731 = getelementptr inbounds double, double* %2719, i64 12
  %2732 = bitcast double* %2731 to <2 x double>*
  store <2 x double> %2713, <2 x double>* %2732, align 8, !tbaa !62
  %2733 = getelementptr inbounds double, double* %2719, i64 14
  %2734 = bitcast double* %2733 to <2 x double>*
  store <2 x double> %2714, <2 x double>* %2734, align 8, !tbaa !62
  %2735 = getelementptr inbounds double, double* %2719, i64 16
  %2736 = bitcast double* %2735 to <2 x double>*
  store <2 x double> %2715, <2 x double>* %2736, align 8, !tbaa !62
  %2737 = getelementptr inbounds double, double* %2719, i64 18
  %2738 = bitcast double* %2737 to <2 x double>*
  store <2 x double> %2716, <2 x double>* %2738, align 8, !tbaa !62
  %2739 = getelementptr inbounds double, double* %2719, i64 20
  %2740 = bitcast double* %2739 to <2 x double>*
  store <2 x double> %2717, <2 x double>* %2740, align 8, !tbaa !62
  %2741 = getelementptr inbounds double, double* %2719, i64 22
  %2742 = bitcast double* %2741 to <2 x double>*
  store <2 x double> %2718, <2 x double>* %2742, align 8, !tbaa !62
  %2743 = add <2 x i32> %vec.ind10349, <i32 -3, i32 -3>
  %2744 = add <2 x i32> %vec.ind10349, <i32 -1, i32 -1>
  %2745 = add <2 x i32> %vec.ind10349, <i32 1, i32 1>
  %2746 = add <2 x i32> %vec.ind10349, <i32 3, i32 3>
  %2747 = add <2 x i32> %vec.ind10349, <i32 5, i32 5>
  %2748 = add <2 x i32> %vec.ind10349, <i32 7, i32 7>
  %2749 = add <2 x i32> %vec.ind10349, <i32 9, i32 9>
  %2750 = add <2 x i32> %vec.ind10349, <i32 11, i32 11>
  %2751 = add <2 x i32> %vec.ind10349, <i32 13, i32 13>
  %2752 = add <2 x i32> %vec.ind10349, <i32 15, i32 15>
  %2753 = add <2 x i32> %vec.ind10349, <i32 17, i32 17>
  %2754 = add <2 x i32> %vec.ind10349, <i32 19, i32 19>
  %2755 = sitofp <2 x i32> %2743 to <2 x double>
  %2756 = sitofp <2 x i32> %2744 to <2 x double>
  %2757 = sitofp <2 x i32> %2745 to <2 x double>
  %2758 = sitofp <2 x i32> %2746 to <2 x double>
  %2759 = sitofp <2 x i32> %2747 to <2 x double>
  %2760 = sitofp <2 x i32> %2748 to <2 x double>
  %2761 = sitofp <2 x i32> %2749 to <2 x double>
  %2762 = sitofp <2 x i32> %2750 to <2 x double>
  %2763 = sitofp <2 x i32> %2751 to <2 x double>
  %2764 = sitofp <2 x i32> %2752 to <2 x double>
  %2765 = sitofp <2 x i32> %2753 to <2 x double>
  %2766 = sitofp <2 x i32> %2754 to <2 x double>
  %2767 = getelementptr inbounds double, double* %1797, i64 %index10331
  %2768 = bitcast double* %2767 to <2 x double>*
  store <2 x double> %2755, <2 x double>* %2768, align 8, !tbaa !62
  %2769 = getelementptr inbounds double, double* %2767, i64 2
  %2770 = bitcast double* %2769 to <2 x double>*
  store <2 x double> %2756, <2 x double>* %2770, align 8, !tbaa !62
  %2771 = getelementptr inbounds double, double* %2767, i64 4
  %2772 = bitcast double* %2771 to <2 x double>*
  store <2 x double> %2757, <2 x double>* %2772, align 8, !tbaa !62
  %2773 = getelementptr inbounds double, double* %2767, i64 6
  %2774 = bitcast double* %2773 to <2 x double>*
  store <2 x double> %2758, <2 x double>* %2774, align 8, !tbaa !62
  %2775 = getelementptr inbounds double, double* %2767, i64 8
  %2776 = bitcast double* %2775 to <2 x double>*
  store <2 x double> %2759, <2 x double>* %2776, align 8, !tbaa !62
  %2777 = getelementptr inbounds double, double* %2767, i64 10
  %2778 = bitcast double* %2777 to <2 x double>*
  store <2 x double> %2760, <2 x double>* %2778, align 8, !tbaa !62
  %2779 = getelementptr inbounds double, double* %2767, i64 12
  %2780 = bitcast double* %2779 to <2 x double>*
  store <2 x double> %2761, <2 x double>* %2780, align 8, !tbaa !62
  %2781 = getelementptr inbounds double, double* %2767, i64 14
  %2782 = bitcast double* %2781 to <2 x double>*
  store <2 x double> %2762, <2 x double>* %2782, align 8, !tbaa !62
  %2783 = getelementptr inbounds double, double* %2767, i64 16
  %2784 = bitcast double* %2783 to <2 x double>*
  store <2 x double> %2763, <2 x double>* %2784, align 8, !tbaa !62
  %2785 = getelementptr inbounds double, double* %2767, i64 18
  %2786 = bitcast double* %2785 to <2 x double>*
  store <2 x double> %2764, <2 x double>* %2786, align 8, !tbaa !62
  %2787 = getelementptr inbounds double, double* %2767, i64 20
  %2788 = bitcast double* %2787 to <2 x double>*
  store <2 x double> %2765, <2 x double>* %2788, align 8, !tbaa !62
  %2789 = getelementptr inbounds double, double* %2767, i64 22
  %2790 = bitcast double* %2789 to <2 x double>*
  store <2 x double> %2766, <2 x double>* %2790, align 8, !tbaa !62
  %index.next10332 = add nuw nsw i64 %index10331, 24
  %vec.ind.next10362 = add <2 x i32> %vec.ind10349, <i32 24, i32 24>
  %2791 = icmp eq i64 %index.next10332, 24984
  br i1 %2791, label %for.body.i8432, label %vector.body10327, !llvm.loop !86

for.body.i8432:                                   ; preds = %vector.body10327
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %2602, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %2603, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %2604, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %2605, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %2606, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %2607, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %2608, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %2609, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %2610, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %2611, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %2612, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %2613, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %2614, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %2615, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %2616, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %2617, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %2618, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %2619, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %2620, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %2621, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %2622, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %2623, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %2624, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %2625, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %2626, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %2627, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %2628, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %2629, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %2630, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %2631, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %2632, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %2633, align 8, !tbaa !62
  %2792 = shl nuw nsw i64 %indvars.iv9570, 3
  store i8* %call3733, i8** %2568, align 8
  store i8* %call3733, i8** %2569, align 8
  store i64 %2792, i64* %2570, align 8
  store i8* %call3735, i8** %2571, align 8
  store i8* %call3735, i8** %2572, align 8
  store i64 %2792, i64* %2573, align 8
  store i8* %call3736, i8** %2574, align 8
  store i8* %call3736, i8** %2575, align 8
  store i64 %2792, i64* %2576, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %2568, i8** nonnull %2569, i64* nonnull %2570, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  br label %for.cond3975.preheader

for.cond4014.preheader:                           ; preds = %for.cond.cleanup3978
  %2793 = add nuw nsw i64 %2634, 32
  %cmp40158996 = icmp sgt i32 %t3961.28989, -1
  br i1 %cmp40158996, label %for.cond4020.preheader.preheader61, label %for.cond.cleanup4017

for.cond4020.preheader.preheader61:               ; preds = %for.cond4014.preheader
  %n.mod.vf10271 = urem i64 %2793, 24
  %n.vec10272 = sub nuw nsw i64 %2793, %n.mod.vf10271
  %cmp.n10276 = icmp eq i64 %n.mod.vf10271, 0
  br label %for.cond4020.preheader

for.cond3975.preheader:                           ; preds = %for.body.i8432, %for.cond.cleanup3978
  %tms3968.08994 = phi i32 [ 1, %for.body.i8432 ], [ %mul4010, %for.cond.cleanup3978 ]
  %t3961.08993 = phi i32 [ 0, %for.body.i8432 ], [ %inc3985, %for.cond.cleanup3978 ]
  %.capture_expr..casted3993.sroa.0.0.insert.ext = zext i32 %tms3968.08994 to i64
  br label %for.cond3980.preheader

for.cond3980.preheader:                           ; preds = %for.cond3975.preheader, %for.cond.cleanup3983
  %ths3974.08992 = phi i32 [ 32, %for.cond3975.preheader ], [ %mul4006, %for.cond.cleanup3983 ]
  %t3961.18991 = phi i32 [ %t3961.08993, %for.cond3975.preheader ], [ %inc3985, %for.cond.cleanup3983 ]
  %.capture_expr..casted3995.sroa.0.0.insert.ext = zext i32 %ths3974.08992 to i64
  br label %for.body3984

for.cond.cleanup3978:                             ; preds = %for.cond.cleanup3983
  %mul4010 = shl nsw i32 %tms3968.08994, 1
  %cmp3970 = icmp ult i32 %mul4010, 257
  br i1 %cmp3970, label %for.cond3975.preheader, label %for.cond4014.preheader

for.cond.cleanup3983:                             ; preds = %omp_offload.cont4000
  %mul4006 = shl nsw i32 %ths3974.08992, 1
  %cmp3976 = icmp ult i32 %mul4006, 1025
  br i1 %cmp3976, label %for.cond3980.preheader, label %for.cond.cleanup3978

for.body3984:                                     ; preds = %for.cond3980.preheader, %omp_offload.cont4000
  %sch.08990 = phi i32 [ 1, %for.cond3980.preheader ], [ %mul4002, %omp_offload.cont4000 ]
  %t3961.28989 = phi i32 [ %t3961.18991, %for.cond3980.preheader ], [ %inc3985, %omp_offload.cont4000 ]
  %inc3985 = add i32 %t3961.28989, 1
  %.capture_expr..casted3991.sroa.0.0.insert.ext = zext i32 %sch.08990 to i64
  store i64 %indvars.iv9570, i64* %2578, align 8
  store i64 %indvars.iv9570, i64* %2580, align 8
  store i8* %call3733, i8** %2581, align 8
  store i8* %call3733, i8** %2582, align 8
  store i8* %call3735, i8** %2583, align 8
  store i8* %call3735, i8** %2584, align 8
  store i8* %call3736, i8** %2585, align 8
  store i8* %call3736, i8** %2586, align 8
  store i64 %.capture_expr..casted3991.sroa.0.0.insert.ext, i64* %2588, align 8
  store i64 %.capture_expr..casted3991.sroa.0.0.insert.ext, i64* %2590, align 8
  store i64 %.capture_expr..casted3993.sroa.0.0.insert.ext, i64* %2592, align 8
  store i64 %.capture_expr..casted3993.sroa.0.0.insert.ext, i64* %2594, align 8
  store i64 %.capture_expr..casted3995.sroa.0.0.insert.ext, i64* %2596, align 8
  store i64 %.capture_expr..casted3995.sroa.0.0.insert.ext, i64* %2598, align 8
  %2794 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1105.region_id, i32 7, i8** nonnull %2577, i8** nonnull %2579, i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_sizes.273, i64 0, i64 0), i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_maptypes.274, i64 0, i64 0), i32 %tms3968.08994, i32 %ths3974.08992) #6
  %2795 = icmp eq i32 %2794, 0
  br i1 %2795, label %omp_offload.cont4000, label %omp_offload.failed3999

omp_offload.failed3999:                           ; preds = %for.body3984
  %2796 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %2797 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %2796, i32 %tms3968.08994, i32 %ths3974.08992) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 5, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64)* @.omp_outlined..173 to void (i32*, i32*, ...)*), i64 %indvars.iv9570, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted3991.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont4000

omp_offload.cont4000:                             ; preds = %for.body3984, %omp_offload.failed3999
  %mul4002 = mul nsw i32 %sch.08990, 3000
  %2798 = zext i32 %mul4002 to i64
  %cmp3981 = icmp ult i64 %indvars.iv9570, %2798
  br i1 %cmp3981, label %for.cond.cleanup3983, label %for.body3984

for.cond4020.preheader:                           ; preds = %for.cond4020.preheader.preheader61, %for.cond.cleanup4023
  %times4013.08997 = phi i32 [ %inc4038, %for.cond.cleanup4023 ], [ 0, %for.cond4020.preheader.preheader61 ]
  br label %vector.body10264

vector.body10264:                                 ; preds = %for.cond4020.preheader, %vector.body10264
  %index10273 = phi i64 [ %index.next10274, %vector.body10264 ], [ 0, %for.cond4020.preheader ]
  %2799 = getelementptr inbounds double, double* %1796, i64 %index10273
  %2800 = bitcast double* %2799 to <2 x double>*
  %wide.load10291 = load <2 x double>, <2 x double>* %2800, align 8, !tbaa !62
  %2801 = getelementptr inbounds double, double* %2799, i64 2
  %2802 = bitcast double* %2801 to <2 x double>*
  %wide.load10292 = load <2 x double>, <2 x double>* %2802, align 8, !tbaa !62
  %2803 = getelementptr inbounds double, double* %2799, i64 4
  %2804 = bitcast double* %2803 to <2 x double>*
  %wide.load10293 = load <2 x double>, <2 x double>* %2804, align 8, !tbaa !62
  %2805 = getelementptr inbounds double, double* %2799, i64 6
  %2806 = bitcast double* %2805 to <2 x double>*
  %wide.load10294 = load <2 x double>, <2 x double>* %2806, align 8, !tbaa !62
  %2807 = getelementptr inbounds double, double* %2799, i64 8
  %2808 = bitcast double* %2807 to <2 x double>*
  %wide.load10295 = load <2 x double>, <2 x double>* %2808, align 8, !tbaa !62
  %2809 = getelementptr inbounds double, double* %2799, i64 10
  %2810 = bitcast double* %2809 to <2 x double>*
  %wide.load10296 = load <2 x double>, <2 x double>* %2810, align 8, !tbaa !62
  %2811 = getelementptr inbounds double, double* %2799, i64 12
  %2812 = bitcast double* %2811 to <2 x double>*
  %wide.load10297 = load <2 x double>, <2 x double>* %2812, align 8, !tbaa !62
  %2813 = getelementptr inbounds double, double* %2799, i64 14
  %2814 = bitcast double* %2813 to <2 x double>*
  %wide.load10298 = load <2 x double>, <2 x double>* %2814, align 8, !tbaa !62
  %2815 = getelementptr inbounds double, double* %2799, i64 16
  %2816 = bitcast double* %2815 to <2 x double>*
  %wide.load10299 = load <2 x double>, <2 x double>* %2816, align 8, !tbaa !62
  %2817 = getelementptr inbounds double, double* %2799, i64 18
  %2818 = bitcast double* %2817 to <2 x double>*
  %wide.load10300 = load <2 x double>, <2 x double>* %2818, align 8, !tbaa !62
  %2819 = getelementptr inbounds double, double* %2799, i64 20
  %2820 = bitcast double* %2819 to <2 x double>*
  %wide.load10301 = load <2 x double>, <2 x double>* %2820, align 8, !tbaa !62
  %2821 = getelementptr inbounds double, double* %2799, i64 22
  %2822 = bitcast double* %2821 to <2 x double>*
  %wide.load10302 = load <2 x double>, <2 x double>* %2822, align 8, !tbaa !62
  %2823 = getelementptr inbounds double, double* %1797, i64 %index10273
  %2824 = bitcast double* %2823 to <2 x double>*
  %wide.load10303 = load <2 x double>, <2 x double>* %2824, align 8, !tbaa !62
  %2825 = getelementptr inbounds double, double* %2823, i64 2
  %2826 = bitcast double* %2825 to <2 x double>*
  %wide.load10304 = load <2 x double>, <2 x double>* %2826, align 8, !tbaa !62
  %2827 = getelementptr inbounds double, double* %2823, i64 4
  %2828 = bitcast double* %2827 to <2 x double>*
  %wide.load10305 = load <2 x double>, <2 x double>* %2828, align 8, !tbaa !62
  %2829 = getelementptr inbounds double, double* %2823, i64 6
  %2830 = bitcast double* %2829 to <2 x double>*
  %wide.load10306 = load <2 x double>, <2 x double>* %2830, align 8, !tbaa !62
  %2831 = getelementptr inbounds double, double* %2823, i64 8
  %2832 = bitcast double* %2831 to <2 x double>*
  %wide.load10307 = load <2 x double>, <2 x double>* %2832, align 8, !tbaa !62
  %2833 = getelementptr inbounds double, double* %2823, i64 10
  %2834 = bitcast double* %2833 to <2 x double>*
  %wide.load10308 = load <2 x double>, <2 x double>* %2834, align 8, !tbaa !62
  %2835 = getelementptr inbounds double, double* %2823, i64 12
  %2836 = bitcast double* %2835 to <2 x double>*
  %wide.load10309 = load <2 x double>, <2 x double>* %2836, align 8, !tbaa !62
  %2837 = getelementptr inbounds double, double* %2823, i64 14
  %2838 = bitcast double* %2837 to <2 x double>*
  %wide.load10310 = load <2 x double>, <2 x double>* %2838, align 8, !tbaa !62
  %2839 = getelementptr inbounds double, double* %2823, i64 16
  %2840 = bitcast double* %2839 to <2 x double>*
  %wide.load10311 = load <2 x double>, <2 x double>* %2840, align 8, !tbaa !62
  %2841 = getelementptr inbounds double, double* %2823, i64 18
  %2842 = bitcast double* %2841 to <2 x double>*
  %wide.load10312 = load <2 x double>, <2 x double>* %2842, align 8, !tbaa !62
  %2843 = getelementptr inbounds double, double* %2823, i64 20
  %2844 = bitcast double* %2843 to <2 x double>*
  %wide.load10313 = load <2 x double>, <2 x double>* %2844, align 8, !tbaa !62
  %2845 = getelementptr inbounds double, double* %2823, i64 22
  %2846 = bitcast double* %2845 to <2 x double>*
  %wide.load10314 = load <2 x double>, <2 x double>* %2846, align 8, !tbaa !62
  %2847 = fadd <2 x double> %wide.load10291, %wide.load10303
  %2848 = fadd <2 x double> %wide.load10292, %wide.load10304
  %2849 = fadd <2 x double> %wide.load10293, %wide.load10305
  %2850 = fadd <2 x double> %wide.load10294, %wide.load10306
  %2851 = fadd <2 x double> %wide.load10295, %wide.load10307
  %2852 = fadd <2 x double> %wide.load10296, %wide.load10308
  %2853 = fadd <2 x double> %wide.load10297, %wide.load10309
  %2854 = fadd <2 x double> %wide.load10298, %wide.load10310
  %2855 = fadd <2 x double> %wide.load10299, %wide.load10311
  %2856 = fadd <2 x double> %wide.load10300, %wide.load10312
  %2857 = fadd <2 x double> %wide.load10301, %wide.load10313
  %2858 = fadd <2 x double> %wide.load10302, %wide.load10314
  %2859 = getelementptr inbounds double, double* %1795, i64 %index10273
  %2860 = bitcast double* %2859 to <2 x double>*
  %wide.load10315 = load <2 x double>, <2 x double>* %2860, align 8, !tbaa !62
  %2861 = getelementptr inbounds double, double* %2859, i64 2
  %2862 = bitcast double* %2861 to <2 x double>*
  %wide.load10316 = load <2 x double>, <2 x double>* %2862, align 8, !tbaa !62
  %2863 = getelementptr inbounds double, double* %2859, i64 4
  %2864 = bitcast double* %2863 to <2 x double>*
  %wide.load10317 = load <2 x double>, <2 x double>* %2864, align 8, !tbaa !62
  %2865 = getelementptr inbounds double, double* %2859, i64 6
  %2866 = bitcast double* %2865 to <2 x double>*
  %wide.load10318 = load <2 x double>, <2 x double>* %2866, align 8, !tbaa !62
  %2867 = getelementptr inbounds double, double* %2859, i64 8
  %2868 = bitcast double* %2867 to <2 x double>*
  %wide.load10319 = load <2 x double>, <2 x double>* %2868, align 8, !tbaa !62
  %2869 = getelementptr inbounds double, double* %2859, i64 10
  %2870 = bitcast double* %2869 to <2 x double>*
  %wide.load10320 = load <2 x double>, <2 x double>* %2870, align 8, !tbaa !62
  %2871 = getelementptr inbounds double, double* %2859, i64 12
  %2872 = bitcast double* %2871 to <2 x double>*
  %wide.load10321 = load <2 x double>, <2 x double>* %2872, align 8, !tbaa !62
  %2873 = getelementptr inbounds double, double* %2859, i64 14
  %2874 = bitcast double* %2873 to <2 x double>*
  %wide.load10322 = load <2 x double>, <2 x double>* %2874, align 8, !tbaa !62
  %2875 = getelementptr inbounds double, double* %2859, i64 16
  %2876 = bitcast double* %2875 to <2 x double>*
  %wide.load10323 = load <2 x double>, <2 x double>* %2876, align 8, !tbaa !62
  %2877 = getelementptr inbounds double, double* %2859, i64 18
  %2878 = bitcast double* %2877 to <2 x double>*
  %wide.load10324 = load <2 x double>, <2 x double>* %2878, align 8, !tbaa !62
  %2879 = getelementptr inbounds double, double* %2859, i64 20
  %2880 = bitcast double* %2879 to <2 x double>*
  %wide.load10325 = load <2 x double>, <2 x double>* %2880, align 8, !tbaa !62
  %2881 = getelementptr inbounds double, double* %2859, i64 22
  %2882 = bitcast double* %2881 to <2 x double>*
  %wide.load10326 = load <2 x double>, <2 x double>* %2882, align 8, !tbaa !62
  %2883 = fadd <2 x double> %2847, %wide.load10315
  %2884 = fadd <2 x double> %2848, %wide.load10316
  %2885 = fadd <2 x double> %2849, %wide.load10317
  %2886 = fadd <2 x double> %2850, %wide.load10318
  %2887 = fadd <2 x double> %2851, %wide.load10319
  %2888 = fadd <2 x double> %2852, %wide.load10320
  %2889 = fadd <2 x double> %2853, %wide.load10321
  %2890 = fadd <2 x double> %2854, %wide.load10322
  %2891 = fadd <2 x double> %2855, %wide.load10323
  %2892 = fadd <2 x double> %2856, %wide.load10324
  %2893 = fadd <2 x double> %2857, %wide.load10325
  %2894 = fadd <2 x double> %2858, %wide.load10326
  store <2 x double> %2883, <2 x double>* %2860, align 8, !tbaa !62
  store <2 x double> %2884, <2 x double>* %2862, align 8, !tbaa !62
  store <2 x double> %2885, <2 x double>* %2864, align 8, !tbaa !62
  store <2 x double> %2886, <2 x double>* %2866, align 8, !tbaa !62
  store <2 x double> %2887, <2 x double>* %2868, align 8, !tbaa !62
  store <2 x double> %2888, <2 x double>* %2870, align 8, !tbaa !62
  store <2 x double> %2889, <2 x double>* %2872, align 8, !tbaa !62
  store <2 x double> %2890, <2 x double>* %2874, align 8, !tbaa !62
  store <2 x double> %2891, <2 x double>* %2876, align 8, !tbaa !62
  store <2 x double> %2892, <2 x double>* %2878, align 8, !tbaa !62
  store <2 x double> %2893, <2 x double>* %2880, align 8, !tbaa !62
  store <2 x double> %2894, <2 x double>* %2882, align 8, !tbaa !62
  %index.next10274 = add nuw nsw i64 %index10273, 24
  %2895 = icmp eq i64 %index.next10274, %n.vec10272
  br i1 %2895, label %middle.block10265, label %vector.body10264, !llvm.loop !87

middle.block10265:                                ; preds = %vector.body10264
  br i1 %cmp.n10276, label %for.cond.cleanup4023, label %for.body4024

for.cond.cleanup4017:                             ; preds = %for.cond.cleanup4023, %for.cond4014.preheader
  store i8* %call3733, i8** %2599, align 8
  store i8* %call3733, i8** %2600, align 8
  store i64 %2792, i64* %2601, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %2599, i8** nonnull %2600, i64* nonnull %2601, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body4050

for.cond.cleanup4023:                             ; preds = %for.body4024, %middle.block10265
  %inc4038 = add nuw nsw i32 %times4013.08997, 1
  %exitcond9567 = icmp eq i32 %inc4038, %inc3985
  br i1 %exitcond9567, label %for.cond.cleanup4017, label %for.cond4020.preheader

for.body4024:                                     ; preds = %middle.block10265, %for.body4024
  %indvars.iv9564 = phi i64 [ %indvars.iv.next9565, %for.body4024 ], [ %n.vec10272, %middle.block10265 ]
  %arrayidx4026 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9564
  %2896 = load double, double* %arrayidx4026, align 8, !tbaa !62
  %arrayidx4028 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9564
  %2897 = load double, double* %arrayidx4028, align 8, !tbaa !62
  %add4029 = fadd double %2896, %2897
  %arrayidx4031 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9564
  %2898 = load double, double* %arrayidx4031, align 8, !tbaa !62
  %add4032 = fadd double %2898, %add4029
  store double %add4032, double* %arrayidx4031, align 8, !tbaa !62
  %indvars.iv.next9565 = add nuw nsw i64 %indvars.iv9564, 1
  %exitcond9566 = icmp eq i64 %indvars.iv.next9565, %indvars.iv9570
  br i1 %exitcond9566, label %for.cond.cleanup4023, label %for.body4024, !llvm.loop !88

for.body4050:                                     ; preds = %for.inc4064.7, %for.cond.cleanup4017
  %indvars.iv9568 = phi i64 [ 0, %for.cond.cleanup4017 ], [ %indvars.iv.next9569.7, %for.inc4064.7 ]
  %arrayidx4052 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9568
  %2899 = load double, double* %arrayidx4052, align 8, !tbaa !62
  %arrayidx4054 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9568
  %2900 = load double, double* %arrayidx4054, align 8, !tbaa !62
  %cmp4055 = fcmp une double %2899, %2900
  br i1 %cmp4055, label %cleanup4074, label %for.inc4064

for.inc4064:                                      ; preds = %for.body4050
  %indvars.iv.next9569 = or i64 %indvars.iv9568, 1
  %arrayidx4052.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9569
  %2901 = load double, double* %arrayidx4052.1, align 8, !tbaa !62
  %arrayidx4054.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9569
  %2902 = load double, double* %arrayidx4054.1, align 8, !tbaa !62
  %cmp4055.1 = fcmp une double %2901, %2902
  br i1 %cmp4055.1, label %cleanup4074, label %for.inc4064.1

for.inc4072:                                      ; preds = %for.inc4064.7
  %indvars.iv.next9571 = add nuw nsw i64 %indvars.iv9570, 5000
  %cmp3957 = icmp ult i64 %indvars.iv.next9571, 25000
  %indvar.next10268 = add nuw nsw i64 %indvar10267, 1
  br i1 %cmp3957, label %for.body.i8432.preheader, label %for.end4076

cleanup4074:                                      ; preds = %for.inc4064.6, %for.inc4064.5, %for.inc4064.4, %for.inc4064.3, %for.inc4064.2, %for.inc4064.1, %for.inc4064, %for.body4050
  %indvars.iv9568.lcssa = phi i64 [ %indvars.iv9568, %for.body4050 ], [ %indvars.iv.next9569, %for.inc4064 ], [ %indvars.iv.next9569.1, %for.inc4064.1 ], [ %indvars.iv.next9569.2, %for.inc4064.2 ], [ %indvars.iv.next9569.3, %for.inc4064.3 ], [ %indvars.iv.next9569.4, %for.inc4064.4 ], [ %indvars.iv.next9569.5, %for.inc4064.5 ], [ %indvars.iv.next9569.6, %for.inc4064.6 ]
  %.lcssa11852 = phi double [ %2899, %for.body4050 ], [ %2901, %for.inc4064 ], [ %7896, %for.inc4064.1 ], [ %7898, %for.inc4064.2 ], [ %7900, %for.inc4064.3 ], [ %7902, %for.inc4064.4 ], [ %7904, %for.inc4064.5 ], [ %7906, %for.inc4064.6 ]
  %.lcssa11850 = phi double [ %2900, %for.body4050 ], [ %2902, %for.inc4064 ], [ %7897, %for.inc4064.1 ], [ %7899, %for.inc4064.2 ], [ %7901, %for.inc4064.3 ], [ %7903, %for.inc4064.4 ], [ %7905, %for.inc4064.5 ], [ %7907, %for.inc4064.6 ]
  %2903 = trunc i64 %indvars.iv9570 to i32
  %2904 = trunc i64 %indvars.iv9568.lcssa to i32
  %call4062 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %2903, i32 signext %2904, double %.lcssa11852, double %.lcssa11850)
  br label %cleanup5832

for.end4076:                                      ; preds = %for.inc4072
  %puts8249 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8250 = call i32 @puts(i8* getelementptr inbounds ([26 x i8], [26 x i8]* @str.339, i64 0, i64 0))
  %2905 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4089, i64 0, i64 0
  %2906 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4090, i64 0, i64 0
  %2907 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4091, i64 0, i64 0
  %2908 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4089, i64 0, i64 1
  %2909 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4090, i64 0, i64 1
  %2910 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4091, i64 0, i64 1
  %2911 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4089, i64 0, i64 2
  %2912 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4090, i64 0, i64 2
  %2913 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4091, i64 0, i64 2
  %2914 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4113, i64 0, i64 0
  %2915 = bitcast [6 x i8*]* %.offload_baseptrs4113 to i64*
  %2916 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4114, i64 0, i64 0
  %2917 = bitcast [6 x i8*]* %.offload_ptrs4114 to i64*
  %2918 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4113, i64 0, i64 1
  %2919 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4114, i64 0, i64 1
  %2920 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4113, i64 0, i64 2
  %2921 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4114, i64 0, i64 2
  %2922 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4113, i64 0, i64 3
  %2923 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4114, i64 0, i64 3
  %2924 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4113, i64 0, i64 4
  %2925 = bitcast i8** %2924 to i64*
  %2926 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4114, i64 0, i64 4
  %2927 = bitcast i8** %2926 to i64*
  %2928 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4113, i64 0, i64 5
  %2929 = bitcast i8** %2928 to i64*
  %2930 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4114, i64 0, i64 5
  %2931 = bitcast i8** %2930 to i64*
  %2932 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs4154, i64 0, i64 0
  %2933 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs4155, i64 0, i64 0
  %2934 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes4156, i64 0, i64 0
  %2935 = bitcast i8* %arrayidx.i to <2 x double>*
  %2936 = bitcast i8* %arrayidx2.i to <2 x double>*
  %2937 = bitcast i8* %arrayidx5.i to <2 x double>*
  %2938 = bitcast i8* %arrayidx8.i to <2 x double>*
  %2939 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %2940 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %2941 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %2942 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %2943 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %2944 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %2945 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %2946 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %2947 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %2948 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %2949 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %2950 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %2951 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %2952 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %2953 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %2954 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %2955 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %2956 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %2957 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %2958 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %2959 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %2960 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %2961 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %2962 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %2963 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %2964 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %2965 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %2966 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8447.preheader

for.body.i8447.preheader:                         ; preds = %for.end4076, %for.inc4184
  %indvar10366 = phi i64 [ 0, %for.end4076 ], [ %indvar.next10367, %for.inc4184 ]
  %indvars.iv9562 = phi i64 [ 32, %for.end4076 ], [ %indvars.iv.next9563, %for.inc4184 ]
  %2967 = mul nuw nsw i64 %indvar10366, 5000
  br label %vector.body10426

vector.body10426:                                 ; preds = %vector.body10426, %for.body.i8447.preheader
  %index10430 = phi i64 [ 0, %for.body.i8447.preheader ], [ %index.next10431, %vector.body10426 ]
  %vec.ind10448 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8447.preheader ], [ %vec.ind.next10461, %vector.body10426 ]
  %step.add10449 = add <2 x i32> %vec.ind10448, <i32 2, i32 2>
  %step.add10450 = add <2 x i32> %vec.ind10448, <i32 4, i32 4>
  %step.add10451 = add <2 x i32> %vec.ind10448, <i32 6, i32 6>
  %step.add10452 = add <2 x i32> %vec.ind10448, <i32 8, i32 8>
  %step.add10453 = add <2 x i32> %vec.ind10448, <i32 10, i32 10>
  %step.add10454 = add <2 x i32> %vec.ind10448, <i32 12, i32 12>
  %step.add10455 = add <2 x i32> %vec.ind10448, <i32 14, i32 14>
  %step.add10456 = add <2 x i32> %vec.ind10448, <i32 16, i32 16>
  %step.add10457 = add <2 x i32> %vec.ind10448, <i32 18, i32 18>
  %step.add10458 = add <2 x i32> %vec.ind10448, <i32 20, i32 20>
  %step.add10459 = add <2 x i32> %vec.ind10448, <i32 22, i32 22>
  %2968 = sitofp <2 x i32> %vec.ind10448 to <2 x double>
  %2969 = sitofp <2 x i32> %step.add10449 to <2 x double>
  %2970 = sitofp <2 x i32> %step.add10450 to <2 x double>
  %2971 = sitofp <2 x i32> %step.add10451 to <2 x double>
  %2972 = sitofp <2 x i32> %step.add10452 to <2 x double>
  %2973 = sitofp <2 x i32> %step.add10453 to <2 x double>
  %2974 = sitofp <2 x i32> %step.add10454 to <2 x double>
  %2975 = sitofp <2 x i32> %step.add10455 to <2 x double>
  %2976 = sitofp <2 x i32> %step.add10456 to <2 x double>
  %2977 = sitofp <2 x i32> %step.add10457 to <2 x double>
  %2978 = sitofp <2 x i32> %step.add10458 to <2 x double>
  %2979 = sitofp <2 x i32> %step.add10459 to <2 x double>
  %2980 = getelementptr inbounds double, double* %1795, i64 %index10430
  %2981 = bitcast double* %2980 to <2 x double>*
  store <2 x double> %2968, <2 x double>* %2981, align 8, !tbaa !62
  %2982 = getelementptr inbounds double, double* %2980, i64 2
  %2983 = bitcast double* %2982 to <2 x double>*
  store <2 x double> %2969, <2 x double>* %2983, align 8, !tbaa !62
  %2984 = getelementptr inbounds double, double* %2980, i64 4
  %2985 = bitcast double* %2984 to <2 x double>*
  store <2 x double> %2970, <2 x double>* %2985, align 8, !tbaa !62
  %2986 = getelementptr inbounds double, double* %2980, i64 6
  %2987 = bitcast double* %2986 to <2 x double>*
  store <2 x double> %2971, <2 x double>* %2987, align 8, !tbaa !62
  %2988 = getelementptr inbounds double, double* %2980, i64 8
  %2989 = bitcast double* %2988 to <2 x double>*
  store <2 x double> %2972, <2 x double>* %2989, align 8, !tbaa !62
  %2990 = getelementptr inbounds double, double* %2980, i64 10
  %2991 = bitcast double* %2990 to <2 x double>*
  store <2 x double> %2973, <2 x double>* %2991, align 8, !tbaa !62
  %2992 = getelementptr inbounds double, double* %2980, i64 12
  %2993 = bitcast double* %2992 to <2 x double>*
  store <2 x double> %2974, <2 x double>* %2993, align 8, !tbaa !62
  %2994 = getelementptr inbounds double, double* %2980, i64 14
  %2995 = bitcast double* %2994 to <2 x double>*
  store <2 x double> %2975, <2 x double>* %2995, align 8, !tbaa !62
  %2996 = getelementptr inbounds double, double* %2980, i64 16
  %2997 = bitcast double* %2996 to <2 x double>*
  store <2 x double> %2976, <2 x double>* %2997, align 8, !tbaa !62
  %2998 = getelementptr inbounds double, double* %2980, i64 18
  %2999 = bitcast double* %2998 to <2 x double>*
  store <2 x double> %2977, <2 x double>* %2999, align 8, !tbaa !62
  %3000 = getelementptr inbounds double, double* %2980, i64 20
  %3001 = bitcast double* %3000 to <2 x double>*
  store <2 x double> %2978, <2 x double>* %3001, align 8, !tbaa !62
  %3002 = getelementptr inbounds double, double* %2980, i64 22
  %3003 = bitcast double* %3002 to <2 x double>*
  store <2 x double> %2979, <2 x double>* %3003, align 8, !tbaa !62
  %3004 = getelementptr inbounds double, double* %1794, i64 %index10430
  %3005 = bitcast double* %3004 to <2 x double>*
  store <2 x double> %2968, <2 x double>* %3005, align 8, !tbaa !62
  %3006 = getelementptr inbounds double, double* %3004, i64 2
  %3007 = bitcast double* %3006 to <2 x double>*
  store <2 x double> %2969, <2 x double>* %3007, align 8, !tbaa !62
  %3008 = getelementptr inbounds double, double* %3004, i64 4
  %3009 = bitcast double* %3008 to <2 x double>*
  store <2 x double> %2970, <2 x double>* %3009, align 8, !tbaa !62
  %3010 = getelementptr inbounds double, double* %3004, i64 6
  %3011 = bitcast double* %3010 to <2 x double>*
  store <2 x double> %2971, <2 x double>* %3011, align 8, !tbaa !62
  %3012 = getelementptr inbounds double, double* %3004, i64 8
  %3013 = bitcast double* %3012 to <2 x double>*
  store <2 x double> %2972, <2 x double>* %3013, align 8, !tbaa !62
  %3014 = getelementptr inbounds double, double* %3004, i64 10
  %3015 = bitcast double* %3014 to <2 x double>*
  store <2 x double> %2973, <2 x double>* %3015, align 8, !tbaa !62
  %3016 = getelementptr inbounds double, double* %3004, i64 12
  %3017 = bitcast double* %3016 to <2 x double>*
  store <2 x double> %2974, <2 x double>* %3017, align 8, !tbaa !62
  %3018 = getelementptr inbounds double, double* %3004, i64 14
  %3019 = bitcast double* %3018 to <2 x double>*
  store <2 x double> %2975, <2 x double>* %3019, align 8, !tbaa !62
  %3020 = getelementptr inbounds double, double* %3004, i64 16
  %3021 = bitcast double* %3020 to <2 x double>*
  store <2 x double> %2976, <2 x double>* %3021, align 8, !tbaa !62
  %3022 = getelementptr inbounds double, double* %3004, i64 18
  %3023 = bitcast double* %3022 to <2 x double>*
  store <2 x double> %2977, <2 x double>* %3023, align 8, !tbaa !62
  %3024 = getelementptr inbounds double, double* %3004, i64 20
  %3025 = bitcast double* %3024 to <2 x double>*
  store <2 x double> %2978, <2 x double>* %3025, align 8, !tbaa !62
  %3026 = getelementptr inbounds double, double* %3004, i64 22
  %3027 = bitcast double* %3026 to <2 x double>*
  store <2 x double> %2979, <2 x double>* %3027, align 8, !tbaa !62
  %3028 = shl <2 x i32> %vec.ind10448, <i32 1, i32 1>
  %3029 = shl <2 x i32> %step.add10449, <i32 1, i32 1>
  %3030 = shl <2 x i32> %step.add10450, <i32 1, i32 1>
  %3031 = shl <2 x i32> %step.add10451, <i32 1, i32 1>
  %3032 = shl <2 x i32> %step.add10452, <i32 1, i32 1>
  %3033 = shl <2 x i32> %step.add10453, <i32 1, i32 1>
  %3034 = shl <2 x i32> %step.add10454, <i32 1, i32 1>
  %3035 = shl <2 x i32> %step.add10455, <i32 1, i32 1>
  %3036 = shl <2 x i32> %step.add10456, <i32 1, i32 1>
  %3037 = shl <2 x i32> %step.add10457, <i32 1, i32 1>
  %3038 = shl <2 x i32> %step.add10458, <i32 1, i32 1>
  %3039 = shl <2 x i32> %step.add10459, <i32 1, i32 1>
  %3040 = sitofp <2 x i32> %3028 to <2 x double>
  %3041 = sitofp <2 x i32> %3029 to <2 x double>
  %3042 = sitofp <2 x i32> %3030 to <2 x double>
  %3043 = sitofp <2 x i32> %3031 to <2 x double>
  %3044 = sitofp <2 x i32> %3032 to <2 x double>
  %3045 = sitofp <2 x i32> %3033 to <2 x double>
  %3046 = sitofp <2 x i32> %3034 to <2 x double>
  %3047 = sitofp <2 x i32> %3035 to <2 x double>
  %3048 = sitofp <2 x i32> %3036 to <2 x double>
  %3049 = sitofp <2 x i32> %3037 to <2 x double>
  %3050 = sitofp <2 x i32> %3038 to <2 x double>
  %3051 = sitofp <2 x i32> %3039 to <2 x double>
  %3052 = getelementptr inbounds double, double* %1796, i64 %index10430
  %3053 = bitcast double* %3052 to <2 x double>*
  store <2 x double> %3040, <2 x double>* %3053, align 8, !tbaa !62
  %3054 = getelementptr inbounds double, double* %3052, i64 2
  %3055 = bitcast double* %3054 to <2 x double>*
  store <2 x double> %3041, <2 x double>* %3055, align 8, !tbaa !62
  %3056 = getelementptr inbounds double, double* %3052, i64 4
  %3057 = bitcast double* %3056 to <2 x double>*
  store <2 x double> %3042, <2 x double>* %3057, align 8, !tbaa !62
  %3058 = getelementptr inbounds double, double* %3052, i64 6
  %3059 = bitcast double* %3058 to <2 x double>*
  store <2 x double> %3043, <2 x double>* %3059, align 8, !tbaa !62
  %3060 = getelementptr inbounds double, double* %3052, i64 8
  %3061 = bitcast double* %3060 to <2 x double>*
  store <2 x double> %3044, <2 x double>* %3061, align 8, !tbaa !62
  %3062 = getelementptr inbounds double, double* %3052, i64 10
  %3063 = bitcast double* %3062 to <2 x double>*
  store <2 x double> %3045, <2 x double>* %3063, align 8, !tbaa !62
  %3064 = getelementptr inbounds double, double* %3052, i64 12
  %3065 = bitcast double* %3064 to <2 x double>*
  store <2 x double> %3046, <2 x double>* %3065, align 8, !tbaa !62
  %3066 = getelementptr inbounds double, double* %3052, i64 14
  %3067 = bitcast double* %3066 to <2 x double>*
  store <2 x double> %3047, <2 x double>* %3067, align 8, !tbaa !62
  %3068 = getelementptr inbounds double, double* %3052, i64 16
  %3069 = bitcast double* %3068 to <2 x double>*
  store <2 x double> %3048, <2 x double>* %3069, align 8, !tbaa !62
  %3070 = getelementptr inbounds double, double* %3052, i64 18
  %3071 = bitcast double* %3070 to <2 x double>*
  store <2 x double> %3049, <2 x double>* %3071, align 8, !tbaa !62
  %3072 = getelementptr inbounds double, double* %3052, i64 20
  %3073 = bitcast double* %3072 to <2 x double>*
  store <2 x double> %3050, <2 x double>* %3073, align 8, !tbaa !62
  %3074 = getelementptr inbounds double, double* %3052, i64 22
  %3075 = bitcast double* %3074 to <2 x double>*
  store <2 x double> %3051, <2 x double>* %3075, align 8, !tbaa !62
  %3076 = add <2 x i32> %vec.ind10448, <i32 -3, i32 -3>
  %3077 = add <2 x i32> %vec.ind10448, <i32 -1, i32 -1>
  %3078 = add <2 x i32> %vec.ind10448, <i32 1, i32 1>
  %3079 = add <2 x i32> %vec.ind10448, <i32 3, i32 3>
  %3080 = add <2 x i32> %vec.ind10448, <i32 5, i32 5>
  %3081 = add <2 x i32> %vec.ind10448, <i32 7, i32 7>
  %3082 = add <2 x i32> %vec.ind10448, <i32 9, i32 9>
  %3083 = add <2 x i32> %vec.ind10448, <i32 11, i32 11>
  %3084 = add <2 x i32> %vec.ind10448, <i32 13, i32 13>
  %3085 = add <2 x i32> %vec.ind10448, <i32 15, i32 15>
  %3086 = add <2 x i32> %vec.ind10448, <i32 17, i32 17>
  %3087 = add <2 x i32> %vec.ind10448, <i32 19, i32 19>
  %3088 = sitofp <2 x i32> %3076 to <2 x double>
  %3089 = sitofp <2 x i32> %3077 to <2 x double>
  %3090 = sitofp <2 x i32> %3078 to <2 x double>
  %3091 = sitofp <2 x i32> %3079 to <2 x double>
  %3092 = sitofp <2 x i32> %3080 to <2 x double>
  %3093 = sitofp <2 x i32> %3081 to <2 x double>
  %3094 = sitofp <2 x i32> %3082 to <2 x double>
  %3095 = sitofp <2 x i32> %3083 to <2 x double>
  %3096 = sitofp <2 x i32> %3084 to <2 x double>
  %3097 = sitofp <2 x i32> %3085 to <2 x double>
  %3098 = sitofp <2 x i32> %3086 to <2 x double>
  %3099 = sitofp <2 x i32> %3087 to <2 x double>
  %3100 = getelementptr inbounds double, double* %1797, i64 %index10430
  %3101 = bitcast double* %3100 to <2 x double>*
  store <2 x double> %3088, <2 x double>* %3101, align 8, !tbaa !62
  %3102 = getelementptr inbounds double, double* %3100, i64 2
  %3103 = bitcast double* %3102 to <2 x double>*
  store <2 x double> %3089, <2 x double>* %3103, align 8, !tbaa !62
  %3104 = getelementptr inbounds double, double* %3100, i64 4
  %3105 = bitcast double* %3104 to <2 x double>*
  store <2 x double> %3090, <2 x double>* %3105, align 8, !tbaa !62
  %3106 = getelementptr inbounds double, double* %3100, i64 6
  %3107 = bitcast double* %3106 to <2 x double>*
  store <2 x double> %3091, <2 x double>* %3107, align 8, !tbaa !62
  %3108 = getelementptr inbounds double, double* %3100, i64 8
  %3109 = bitcast double* %3108 to <2 x double>*
  store <2 x double> %3092, <2 x double>* %3109, align 8, !tbaa !62
  %3110 = getelementptr inbounds double, double* %3100, i64 10
  %3111 = bitcast double* %3110 to <2 x double>*
  store <2 x double> %3093, <2 x double>* %3111, align 8, !tbaa !62
  %3112 = getelementptr inbounds double, double* %3100, i64 12
  %3113 = bitcast double* %3112 to <2 x double>*
  store <2 x double> %3094, <2 x double>* %3113, align 8, !tbaa !62
  %3114 = getelementptr inbounds double, double* %3100, i64 14
  %3115 = bitcast double* %3114 to <2 x double>*
  store <2 x double> %3095, <2 x double>* %3115, align 8, !tbaa !62
  %3116 = getelementptr inbounds double, double* %3100, i64 16
  %3117 = bitcast double* %3116 to <2 x double>*
  store <2 x double> %3096, <2 x double>* %3117, align 8, !tbaa !62
  %3118 = getelementptr inbounds double, double* %3100, i64 18
  %3119 = bitcast double* %3118 to <2 x double>*
  store <2 x double> %3097, <2 x double>* %3119, align 8, !tbaa !62
  %3120 = getelementptr inbounds double, double* %3100, i64 20
  %3121 = bitcast double* %3120 to <2 x double>*
  store <2 x double> %3098, <2 x double>* %3121, align 8, !tbaa !62
  %3122 = getelementptr inbounds double, double* %3100, i64 22
  %3123 = bitcast double* %3122 to <2 x double>*
  store <2 x double> %3099, <2 x double>* %3123, align 8, !tbaa !62
  %index.next10431 = add nuw nsw i64 %index10430, 24
  %vec.ind.next10461 = add <2 x i32> %vec.ind10448, <i32 24, i32 24>
  %3124 = icmp eq i64 %index.next10431, 24984
  br i1 %3124, label %for.body.i8447, label %vector.body10426, !llvm.loop !89

for.body.i8447:                                   ; preds = %vector.body10426
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %2935, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %2936, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %2937, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %2938, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %2939, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %2940, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %2941, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %2942, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %2943, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %2944, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %2945, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %2946, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %2947, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %2948, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %2949, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %2950, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %2951, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %2952, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %2953, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %2954, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %2955, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %2956, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %2957, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %2958, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %2959, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %2960, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %2961, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %2962, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %2963, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %2964, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %2965, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %2966, align 8, !tbaa !62
  %3125 = shl nuw nsw i64 %indvars.iv9562, 3
  store i8* %call3733, i8** %2905, align 8
  store i8* %call3733, i8** %2906, align 8
  store i64 %3125, i64* %2907, align 8
  store i8* %call3735, i8** %2908, align 8
  store i8* %call3735, i8** %2909, align 8
  store i64 %3125, i64* %2910, align 8
  store i8* %call3736, i8** %2911, align 8
  store i8* %call3736, i8** %2912, align 8
  store i64 %3125, i64* %2913, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %2905, i8** nonnull %2906, i64* nonnull %2907, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  br label %for.cond4099.preheader

for.cond4099.preheader:                           ; preds = %for.body.i8447, %omp_offload.cont4116.5
  %tms4092.08983 = phi i32 [ 1, %for.body.i8447 ], [ %mul4122, %omp_offload.cont4116.5 ]
  %.capture_expr..casted4109.sroa.0.0.insert.ext = zext i32 %tms4092.08983 to i64
  store i64 %indvars.iv9562, i64* %2915, align 8
  store i64 %indvars.iv9562, i64* %2917, align 8
  store i8* %call3733, i8** %2918, align 8
  store i8* %call3733, i8** %2919, align 8
  store i8* %call3735, i8** %2920, align 8
  store i8* %call3735, i8** %2921, align 8
  store i8* %call3736, i8** %2922, align 8
  store i8* %call3736, i8** %2923, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2925, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2927, align 8
  store i64 32, i64* %2929, align 8
  store i64 32, i64* %2931, align 8
  %3126 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1140.region_id, i32 6, i8** nonnull %2914, i8** nonnull %2916, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4092.08983, i32 32) #6
  %3127 = icmp eq i32 %3126, 0
  br i1 %3127, label %omp_offload.cont4116, label %omp_offload.failed4115

omp_offload.failed4115:                           ; preds = %for.cond4099.preheader
  %3128 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %3129 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %3128, i32 %tms4092.08983, i32 32) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..180 to void (i32*, i32*, ...)*), i64 %indvars.iv9562, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4116

omp_offload.cont4116:                             ; preds = %for.cond4099.preheader, %omp_offload.failed4115
  store i64 %indvars.iv9562, i64* %2915, align 8
  store i64 %indvars.iv9562, i64* %2917, align 8
  store i8* %call3733, i8** %2918, align 8
  store i8* %call3733, i8** %2919, align 8
  store i8* %call3735, i8** %2920, align 8
  store i8* %call3735, i8** %2921, align 8
  store i8* %call3736, i8** %2922, align 8
  store i8* %call3736, i8** %2923, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2925, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2927, align 8
  store i64 64, i64* %2929, align 8
  store i64 64, i64* %2931, align 8
  %3130 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1140.region_id, i32 6, i8** nonnull %2914, i8** nonnull %2916, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4092.08983, i32 64) #6
  %3131 = icmp eq i32 %3130, 0
  br i1 %3131, label %omp_offload.cont4116.1, label %omp_offload.failed4115.1

for.cond4132.preheader:                           ; preds = %for.cond4132.preheader.preheader59, %for.cond.cleanup4135
  %times4125.08986 = phi i32 [ %inc4150, %for.cond.cleanup4135 ], [ 0, %for.cond4132.preheader.preheader59 ]
  br label %vector.body10363

vector.body10363:                                 ; preds = %for.cond4132.preheader, %vector.body10363
  %index10372 = phi i64 [ %index.next10373, %vector.body10363 ], [ 0, %for.cond4132.preheader ]
  %3132 = getelementptr inbounds double, double* %1796, i64 %index10372
  %3133 = bitcast double* %3132 to <2 x double>*
  %wide.load10390 = load <2 x double>, <2 x double>* %3133, align 8, !tbaa !62
  %3134 = getelementptr inbounds double, double* %3132, i64 2
  %3135 = bitcast double* %3134 to <2 x double>*
  %wide.load10391 = load <2 x double>, <2 x double>* %3135, align 8, !tbaa !62
  %3136 = getelementptr inbounds double, double* %3132, i64 4
  %3137 = bitcast double* %3136 to <2 x double>*
  %wide.load10392 = load <2 x double>, <2 x double>* %3137, align 8, !tbaa !62
  %3138 = getelementptr inbounds double, double* %3132, i64 6
  %3139 = bitcast double* %3138 to <2 x double>*
  %wide.load10393 = load <2 x double>, <2 x double>* %3139, align 8, !tbaa !62
  %3140 = getelementptr inbounds double, double* %3132, i64 8
  %3141 = bitcast double* %3140 to <2 x double>*
  %wide.load10394 = load <2 x double>, <2 x double>* %3141, align 8, !tbaa !62
  %3142 = getelementptr inbounds double, double* %3132, i64 10
  %3143 = bitcast double* %3142 to <2 x double>*
  %wide.load10395 = load <2 x double>, <2 x double>* %3143, align 8, !tbaa !62
  %3144 = getelementptr inbounds double, double* %3132, i64 12
  %3145 = bitcast double* %3144 to <2 x double>*
  %wide.load10396 = load <2 x double>, <2 x double>* %3145, align 8, !tbaa !62
  %3146 = getelementptr inbounds double, double* %3132, i64 14
  %3147 = bitcast double* %3146 to <2 x double>*
  %wide.load10397 = load <2 x double>, <2 x double>* %3147, align 8, !tbaa !62
  %3148 = getelementptr inbounds double, double* %3132, i64 16
  %3149 = bitcast double* %3148 to <2 x double>*
  %wide.load10398 = load <2 x double>, <2 x double>* %3149, align 8, !tbaa !62
  %3150 = getelementptr inbounds double, double* %3132, i64 18
  %3151 = bitcast double* %3150 to <2 x double>*
  %wide.load10399 = load <2 x double>, <2 x double>* %3151, align 8, !tbaa !62
  %3152 = getelementptr inbounds double, double* %3132, i64 20
  %3153 = bitcast double* %3152 to <2 x double>*
  %wide.load10400 = load <2 x double>, <2 x double>* %3153, align 8, !tbaa !62
  %3154 = getelementptr inbounds double, double* %3132, i64 22
  %3155 = bitcast double* %3154 to <2 x double>*
  %wide.load10401 = load <2 x double>, <2 x double>* %3155, align 8, !tbaa !62
  %3156 = getelementptr inbounds double, double* %1797, i64 %index10372
  %3157 = bitcast double* %3156 to <2 x double>*
  %wide.load10402 = load <2 x double>, <2 x double>* %3157, align 8, !tbaa !62
  %3158 = getelementptr inbounds double, double* %3156, i64 2
  %3159 = bitcast double* %3158 to <2 x double>*
  %wide.load10403 = load <2 x double>, <2 x double>* %3159, align 8, !tbaa !62
  %3160 = getelementptr inbounds double, double* %3156, i64 4
  %3161 = bitcast double* %3160 to <2 x double>*
  %wide.load10404 = load <2 x double>, <2 x double>* %3161, align 8, !tbaa !62
  %3162 = getelementptr inbounds double, double* %3156, i64 6
  %3163 = bitcast double* %3162 to <2 x double>*
  %wide.load10405 = load <2 x double>, <2 x double>* %3163, align 8, !tbaa !62
  %3164 = getelementptr inbounds double, double* %3156, i64 8
  %3165 = bitcast double* %3164 to <2 x double>*
  %wide.load10406 = load <2 x double>, <2 x double>* %3165, align 8, !tbaa !62
  %3166 = getelementptr inbounds double, double* %3156, i64 10
  %3167 = bitcast double* %3166 to <2 x double>*
  %wide.load10407 = load <2 x double>, <2 x double>* %3167, align 8, !tbaa !62
  %3168 = getelementptr inbounds double, double* %3156, i64 12
  %3169 = bitcast double* %3168 to <2 x double>*
  %wide.load10408 = load <2 x double>, <2 x double>* %3169, align 8, !tbaa !62
  %3170 = getelementptr inbounds double, double* %3156, i64 14
  %3171 = bitcast double* %3170 to <2 x double>*
  %wide.load10409 = load <2 x double>, <2 x double>* %3171, align 8, !tbaa !62
  %3172 = getelementptr inbounds double, double* %3156, i64 16
  %3173 = bitcast double* %3172 to <2 x double>*
  %wide.load10410 = load <2 x double>, <2 x double>* %3173, align 8, !tbaa !62
  %3174 = getelementptr inbounds double, double* %3156, i64 18
  %3175 = bitcast double* %3174 to <2 x double>*
  %wide.load10411 = load <2 x double>, <2 x double>* %3175, align 8, !tbaa !62
  %3176 = getelementptr inbounds double, double* %3156, i64 20
  %3177 = bitcast double* %3176 to <2 x double>*
  %wide.load10412 = load <2 x double>, <2 x double>* %3177, align 8, !tbaa !62
  %3178 = getelementptr inbounds double, double* %3156, i64 22
  %3179 = bitcast double* %3178 to <2 x double>*
  %wide.load10413 = load <2 x double>, <2 x double>* %3179, align 8, !tbaa !62
  %3180 = fadd <2 x double> %wide.load10390, %wide.load10402
  %3181 = fadd <2 x double> %wide.load10391, %wide.load10403
  %3182 = fadd <2 x double> %wide.load10392, %wide.load10404
  %3183 = fadd <2 x double> %wide.load10393, %wide.load10405
  %3184 = fadd <2 x double> %wide.load10394, %wide.load10406
  %3185 = fadd <2 x double> %wide.load10395, %wide.load10407
  %3186 = fadd <2 x double> %wide.load10396, %wide.load10408
  %3187 = fadd <2 x double> %wide.load10397, %wide.load10409
  %3188 = fadd <2 x double> %wide.load10398, %wide.load10410
  %3189 = fadd <2 x double> %wide.load10399, %wide.load10411
  %3190 = fadd <2 x double> %wide.load10400, %wide.load10412
  %3191 = fadd <2 x double> %wide.load10401, %wide.load10413
  %3192 = getelementptr inbounds double, double* %1795, i64 %index10372
  %3193 = bitcast double* %3192 to <2 x double>*
  %wide.load10414 = load <2 x double>, <2 x double>* %3193, align 8, !tbaa !62
  %3194 = getelementptr inbounds double, double* %3192, i64 2
  %3195 = bitcast double* %3194 to <2 x double>*
  %wide.load10415 = load <2 x double>, <2 x double>* %3195, align 8, !tbaa !62
  %3196 = getelementptr inbounds double, double* %3192, i64 4
  %3197 = bitcast double* %3196 to <2 x double>*
  %wide.load10416 = load <2 x double>, <2 x double>* %3197, align 8, !tbaa !62
  %3198 = getelementptr inbounds double, double* %3192, i64 6
  %3199 = bitcast double* %3198 to <2 x double>*
  %wide.load10417 = load <2 x double>, <2 x double>* %3199, align 8, !tbaa !62
  %3200 = getelementptr inbounds double, double* %3192, i64 8
  %3201 = bitcast double* %3200 to <2 x double>*
  %wide.load10418 = load <2 x double>, <2 x double>* %3201, align 8, !tbaa !62
  %3202 = getelementptr inbounds double, double* %3192, i64 10
  %3203 = bitcast double* %3202 to <2 x double>*
  %wide.load10419 = load <2 x double>, <2 x double>* %3203, align 8, !tbaa !62
  %3204 = getelementptr inbounds double, double* %3192, i64 12
  %3205 = bitcast double* %3204 to <2 x double>*
  %wide.load10420 = load <2 x double>, <2 x double>* %3205, align 8, !tbaa !62
  %3206 = getelementptr inbounds double, double* %3192, i64 14
  %3207 = bitcast double* %3206 to <2 x double>*
  %wide.load10421 = load <2 x double>, <2 x double>* %3207, align 8, !tbaa !62
  %3208 = getelementptr inbounds double, double* %3192, i64 16
  %3209 = bitcast double* %3208 to <2 x double>*
  %wide.load10422 = load <2 x double>, <2 x double>* %3209, align 8, !tbaa !62
  %3210 = getelementptr inbounds double, double* %3192, i64 18
  %3211 = bitcast double* %3210 to <2 x double>*
  %wide.load10423 = load <2 x double>, <2 x double>* %3211, align 8, !tbaa !62
  %3212 = getelementptr inbounds double, double* %3192, i64 20
  %3213 = bitcast double* %3212 to <2 x double>*
  %wide.load10424 = load <2 x double>, <2 x double>* %3213, align 8, !tbaa !62
  %3214 = getelementptr inbounds double, double* %3192, i64 22
  %3215 = bitcast double* %3214 to <2 x double>*
  %wide.load10425 = load <2 x double>, <2 x double>* %3215, align 8, !tbaa !62
  %3216 = fadd <2 x double> %3180, %wide.load10414
  %3217 = fadd <2 x double> %3181, %wide.load10415
  %3218 = fadd <2 x double> %3182, %wide.load10416
  %3219 = fadd <2 x double> %3183, %wide.load10417
  %3220 = fadd <2 x double> %3184, %wide.load10418
  %3221 = fadd <2 x double> %3185, %wide.load10419
  %3222 = fadd <2 x double> %3186, %wide.load10420
  %3223 = fadd <2 x double> %3187, %wide.load10421
  %3224 = fadd <2 x double> %3188, %wide.load10422
  %3225 = fadd <2 x double> %3189, %wide.load10423
  %3226 = fadd <2 x double> %3190, %wide.load10424
  %3227 = fadd <2 x double> %3191, %wide.load10425
  store <2 x double> %3216, <2 x double>* %3193, align 8, !tbaa !62
  store <2 x double> %3217, <2 x double>* %3195, align 8, !tbaa !62
  store <2 x double> %3218, <2 x double>* %3197, align 8, !tbaa !62
  store <2 x double> %3219, <2 x double>* %3199, align 8, !tbaa !62
  store <2 x double> %3220, <2 x double>* %3201, align 8, !tbaa !62
  store <2 x double> %3221, <2 x double>* %3203, align 8, !tbaa !62
  store <2 x double> %3222, <2 x double>* %3205, align 8, !tbaa !62
  store <2 x double> %3223, <2 x double>* %3207, align 8, !tbaa !62
  store <2 x double> %3224, <2 x double>* %3209, align 8, !tbaa !62
  store <2 x double> %3225, <2 x double>* %3211, align 8, !tbaa !62
  store <2 x double> %3226, <2 x double>* %3213, align 8, !tbaa !62
  store <2 x double> %3227, <2 x double>* %3215, align 8, !tbaa !62
  %index.next10373 = add nuw nsw i64 %index10372, 24
  %3228 = icmp eq i64 %index.next10373, %n.vec10371
  br i1 %3228, label %middle.block10364, label %vector.body10363, !llvm.loop !90

middle.block10364:                                ; preds = %vector.body10363
  br i1 %cmp.n10375, label %for.cond.cleanup4135, label %for.body4136

for.cond.cleanup4129:                             ; preds = %for.cond.cleanup4135
  store i8* %call3733, i8** %2932, align 8
  store i8* %call3733, i8** %2933, align 8
  store i64 %3125, i64* %2934, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %2932, i8** nonnull %2933, i64* nonnull %2934, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body4162

for.cond.cleanup4135:                             ; preds = %for.body4136, %middle.block10364
  %inc4150 = add nuw nsw i32 %times4125.08986, 1
  %exitcond9559 = icmp eq i32 %inc4150, 54
  br i1 %exitcond9559, label %for.cond.cleanup4129, label %for.cond4132.preheader

for.body4136:                                     ; preds = %middle.block10364, %for.body4136
  %indvars.iv9556 = phi i64 [ %indvars.iv.next9557, %for.body4136 ], [ %n.vec10371, %middle.block10364 ]
  %arrayidx4138 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9556
  %3229 = load double, double* %arrayidx4138, align 8, !tbaa !62
  %arrayidx4140 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9556
  %3230 = load double, double* %arrayidx4140, align 8, !tbaa !62
  %add4141 = fadd double %3229, %3230
  %arrayidx4143 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9556
  %3231 = load double, double* %arrayidx4143, align 8, !tbaa !62
  %add4144 = fadd double %3231, %add4141
  store double %add4144, double* %arrayidx4143, align 8, !tbaa !62
  %indvars.iv.next9557 = add nuw nsw i64 %indvars.iv9556, 1
  %exitcond9558 = icmp eq i64 %indvars.iv.next9557, %indvars.iv9562
  br i1 %exitcond9558, label %for.cond.cleanup4135, label %for.body4136, !llvm.loop !91

for.body4162:                                     ; preds = %for.inc4176.7, %for.cond.cleanup4129
  %indvars.iv9560 = phi i64 [ 0, %for.cond.cleanup4129 ], [ %indvars.iv.next9561.7, %for.inc4176.7 ]
  %arrayidx4164 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9560
  %3232 = load double, double* %arrayidx4164, align 8, !tbaa !62
  %arrayidx4166 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9560
  %3233 = load double, double* %arrayidx4166, align 8, !tbaa !62
  %cmp4167 = fcmp une double %3232, %3233
  br i1 %cmp4167, label %cleanup4186, label %for.inc4176

for.inc4176:                                      ; preds = %for.body4162
  %indvars.iv.next9561 = or i64 %indvars.iv9560, 1
  %arrayidx4164.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9561
  %3234 = load double, double* %arrayidx4164.1, align 8, !tbaa !62
  %arrayidx4166.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9561
  %3235 = load double, double* %arrayidx4166.1, align 8, !tbaa !62
  %cmp4167.1 = fcmp une double %3234, %3235
  br i1 %cmp4167.1, label %cleanup4186, label %for.inc4176.1

for.inc4184:                                      ; preds = %for.inc4176.7
  %indvars.iv.next9563 = add nuw nsw i64 %indvars.iv9562, 5000
  %cmp4081 = icmp ult i64 %indvars.iv.next9563, 25000
  %indvar.next10367 = add nuw nsw i64 %indvar10366, 1
  br i1 %cmp4081, label %for.body.i8447.preheader, label %for.end4188

cleanup4186:                                      ; preds = %for.inc4176.6, %for.inc4176.5, %for.inc4176.4, %for.inc4176.3, %for.inc4176.2, %for.inc4176.1, %for.inc4176, %for.body4162
  %indvars.iv9560.lcssa = phi i64 [ %indvars.iv9560, %for.body4162 ], [ %indvars.iv.next9561, %for.inc4176 ], [ %indvars.iv.next9561.1, %for.inc4176.1 ], [ %indvars.iv.next9561.2, %for.inc4176.2 ], [ %indvars.iv.next9561.3, %for.inc4176.3 ], [ %indvars.iv.next9561.4, %for.inc4176.4 ], [ %indvars.iv.next9561.5, %for.inc4176.5 ], [ %indvars.iv.next9561.6, %for.inc4176.6 ]
  %.lcssa11846 = phi double [ %3232, %for.body4162 ], [ %3234, %for.inc4176 ], [ %7884, %for.inc4176.1 ], [ %7886, %for.inc4176.2 ], [ %7888, %for.inc4176.3 ], [ %7890, %for.inc4176.4 ], [ %7892, %for.inc4176.5 ], [ %7894, %for.inc4176.6 ]
  %.lcssa11844 = phi double [ %3233, %for.body4162 ], [ %3235, %for.inc4176 ], [ %7885, %for.inc4176.1 ], [ %7887, %for.inc4176.2 ], [ %7889, %for.inc4176.3 ], [ %7891, %for.inc4176.4 ], [ %7893, %for.inc4176.5 ], [ %7895, %for.inc4176.6 ]
  %3236 = trunc i64 %indvars.iv9562 to i32
  %3237 = trunc i64 %indvars.iv9560.lcssa to i32
  %call4174 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %3236, i32 signext %3237, double %.lcssa11846, double %.lcssa11844)
  br label %cleanup5832

for.end4188:                                      ; preds = %for.inc4184
  %puts8251 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8252 = call i32 @puts(i8* getelementptr inbounds ([23 x i8], [23 x i8]* @str.341, i64 0, i64 0))
  %3238 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4201, i64 0, i64 0
  %3239 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4202, i64 0, i64 0
  %3240 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4203, i64 0, i64 0
  %3241 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4201, i64 0, i64 1
  %3242 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4202, i64 0, i64 1
  %3243 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4203, i64 0, i64 1
  %3244 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4201, i64 0, i64 2
  %3245 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4202, i64 0, i64 2
  %3246 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4203, i64 0, i64 2
  %3247 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4234, i64 0, i64 0
  %3248 = bitcast [7 x i8*]* %.offload_baseptrs4234 to i64*
  %3249 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4235, i64 0, i64 0
  %3250 = bitcast [7 x i8*]* %.offload_ptrs4235 to i64*
  %3251 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4234, i64 0, i64 1
  %3252 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4235, i64 0, i64 1
  %3253 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4234, i64 0, i64 2
  %3254 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4235, i64 0, i64 2
  %3255 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4234, i64 0, i64 3
  %3256 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4235, i64 0, i64 3
  %3257 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4234, i64 0, i64 4
  %3258 = bitcast i8** %3257 to i64*
  %3259 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4235, i64 0, i64 4
  %3260 = bitcast i8** %3259 to i64*
  %3261 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4234, i64 0, i64 5
  %3262 = bitcast i8** %3261 to i64*
  %3263 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4235, i64 0, i64 5
  %3264 = bitcast i8** %3263 to i64*
  %3265 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4234, i64 0, i64 6
  %3266 = bitcast i8** %3265 to i64*
  %3267 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4235, i64 0, i64 6
  %3268 = bitcast i8** %3267 to i64*
  %3269 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs4279, i64 0, i64 0
  %3270 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs4280, i64 0, i64 0
  %3271 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes4281, i64 0, i64 0
  %3272 = bitcast i8* %arrayidx.i to <2 x double>*
  %3273 = bitcast i8* %arrayidx2.i to <2 x double>*
  %3274 = bitcast i8* %arrayidx5.i to <2 x double>*
  %3275 = bitcast i8* %arrayidx8.i to <2 x double>*
  %3276 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %3277 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %3278 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %3279 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %3280 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %3281 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %3282 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %3283 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %3284 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %3285 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %3286 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %3287 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %3288 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %3289 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %3290 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %3291 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %3292 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %3293 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %3294 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %3295 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %3296 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %3297 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %3298 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %3299 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %3300 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %3301 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %3302 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %3303 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8461.preheader

for.body.i8461.preheader:                         ; preds = %for.end4188, %for.inc4309
  %indvar10465 = phi i64 [ 0, %for.end4188 ], [ %indvar.next10466, %for.inc4309 ]
  %indvars.iv9551 = phi i64 [ 32, %for.end4188 ], [ %indvars.iv.next9552, %for.inc4309 ]
  %3304 = mul nuw nsw i64 %indvar10465, 5000
  br label %vector.body10525

vector.body10525:                                 ; preds = %vector.body10525, %for.body.i8461.preheader
  %index10529 = phi i64 [ 0, %for.body.i8461.preheader ], [ %index.next10530, %vector.body10525 ]
  %vec.ind10547 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8461.preheader ], [ %vec.ind.next10560, %vector.body10525 ]
  %step.add10548 = add <2 x i32> %vec.ind10547, <i32 2, i32 2>
  %step.add10549 = add <2 x i32> %vec.ind10547, <i32 4, i32 4>
  %step.add10550 = add <2 x i32> %vec.ind10547, <i32 6, i32 6>
  %step.add10551 = add <2 x i32> %vec.ind10547, <i32 8, i32 8>
  %step.add10552 = add <2 x i32> %vec.ind10547, <i32 10, i32 10>
  %step.add10553 = add <2 x i32> %vec.ind10547, <i32 12, i32 12>
  %step.add10554 = add <2 x i32> %vec.ind10547, <i32 14, i32 14>
  %step.add10555 = add <2 x i32> %vec.ind10547, <i32 16, i32 16>
  %step.add10556 = add <2 x i32> %vec.ind10547, <i32 18, i32 18>
  %step.add10557 = add <2 x i32> %vec.ind10547, <i32 20, i32 20>
  %step.add10558 = add <2 x i32> %vec.ind10547, <i32 22, i32 22>
  %3305 = sitofp <2 x i32> %vec.ind10547 to <2 x double>
  %3306 = sitofp <2 x i32> %step.add10548 to <2 x double>
  %3307 = sitofp <2 x i32> %step.add10549 to <2 x double>
  %3308 = sitofp <2 x i32> %step.add10550 to <2 x double>
  %3309 = sitofp <2 x i32> %step.add10551 to <2 x double>
  %3310 = sitofp <2 x i32> %step.add10552 to <2 x double>
  %3311 = sitofp <2 x i32> %step.add10553 to <2 x double>
  %3312 = sitofp <2 x i32> %step.add10554 to <2 x double>
  %3313 = sitofp <2 x i32> %step.add10555 to <2 x double>
  %3314 = sitofp <2 x i32> %step.add10556 to <2 x double>
  %3315 = sitofp <2 x i32> %step.add10557 to <2 x double>
  %3316 = sitofp <2 x i32> %step.add10558 to <2 x double>
  %3317 = getelementptr inbounds double, double* %1795, i64 %index10529
  %3318 = bitcast double* %3317 to <2 x double>*
  store <2 x double> %3305, <2 x double>* %3318, align 8, !tbaa !62
  %3319 = getelementptr inbounds double, double* %3317, i64 2
  %3320 = bitcast double* %3319 to <2 x double>*
  store <2 x double> %3306, <2 x double>* %3320, align 8, !tbaa !62
  %3321 = getelementptr inbounds double, double* %3317, i64 4
  %3322 = bitcast double* %3321 to <2 x double>*
  store <2 x double> %3307, <2 x double>* %3322, align 8, !tbaa !62
  %3323 = getelementptr inbounds double, double* %3317, i64 6
  %3324 = bitcast double* %3323 to <2 x double>*
  store <2 x double> %3308, <2 x double>* %3324, align 8, !tbaa !62
  %3325 = getelementptr inbounds double, double* %3317, i64 8
  %3326 = bitcast double* %3325 to <2 x double>*
  store <2 x double> %3309, <2 x double>* %3326, align 8, !tbaa !62
  %3327 = getelementptr inbounds double, double* %3317, i64 10
  %3328 = bitcast double* %3327 to <2 x double>*
  store <2 x double> %3310, <2 x double>* %3328, align 8, !tbaa !62
  %3329 = getelementptr inbounds double, double* %3317, i64 12
  %3330 = bitcast double* %3329 to <2 x double>*
  store <2 x double> %3311, <2 x double>* %3330, align 8, !tbaa !62
  %3331 = getelementptr inbounds double, double* %3317, i64 14
  %3332 = bitcast double* %3331 to <2 x double>*
  store <2 x double> %3312, <2 x double>* %3332, align 8, !tbaa !62
  %3333 = getelementptr inbounds double, double* %3317, i64 16
  %3334 = bitcast double* %3333 to <2 x double>*
  store <2 x double> %3313, <2 x double>* %3334, align 8, !tbaa !62
  %3335 = getelementptr inbounds double, double* %3317, i64 18
  %3336 = bitcast double* %3335 to <2 x double>*
  store <2 x double> %3314, <2 x double>* %3336, align 8, !tbaa !62
  %3337 = getelementptr inbounds double, double* %3317, i64 20
  %3338 = bitcast double* %3337 to <2 x double>*
  store <2 x double> %3315, <2 x double>* %3338, align 8, !tbaa !62
  %3339 = getelementptr inbounds double, double* %3317, i64 22
  %3340 = bitcast double* %3339 to <2 x double>*
  store <2 x double> %3316, <2 x double>* %3340, align 8, !tbaa !62
  %3341 = getelementptr inbounds double, double* %1794, i64 %index10529
  %3342 = bitcast double* %3341 to <2 x double>*
  store <2 x double> %3305, <2 x double>* %3342, align 8, !tbaa !62
  %3343 = getelementptr inbounds double, double* %3341, i64 2
  %3344 = bitcast double* %3343 to <2 x double>*
  store <2 x double> %3306, <2 x double>* %3344, align 8, !tbaa !62
  %3345 = getelementptr inbounds double, double* %3341, i64 4
  %3346 = bitcast double* %3345 to <2 x double>*
  store <2 x double> %3307, <2 x double>* %3346, align 8, !tbaa !62
  %3347 = getelementptr inbounds double, double* %3341, i64 6
  %3348 = bitcast double* %3347 to <2 x double>*
  store <2 x double> %3308, <2 x double>* %3348, align 8, !tbaa !62
  %3349 = getelementptr inbounds double, double* %3341, i64 8
  %3350 = bitcast double* %3349 to <2 x double>*
  store <2 x double> %3309, <2 x double>* %3350, align 8, !tbaa !62
  %3351 = getelementptr inbounds double, double* %3341, i64 10
  %3352 = bitcast double* %3351 to <2 x double>*
  store <2 x double> %3310, <2 x double>* %3352, align 8, !tbaa !62
  %3353 = getelementptr inbounds double, double* %3341, i64 12
  %3354 = bitcast double* %3353 to <2 x double>*
  store <2 x double> %3311, <2 x double>* %3354, align 8, !tbaa !62
  %3355 = getelementptr inbounds double, double* %3341, i64 14
  %3356 = bitcast double* %3355 to <2 x double>*
  store <2 x double> %3312, <2 x double>* %3356, align 8, !tbaa !62
  %3357 = getelementptr inbounds double, double* %3341, i64 16
  %3358 = bitcast double* %3357 to <2 x double>*
  store <2 x double> %3313, <2 x double>* %3358, align 8, !tbaa !62
  %3359 = getelementptr inbounds double, double* %3341, i64 18
  %3360 = bitcast double* %3359 to <2 x double>*
  store <2 x double> %3314, <2 x double>* %3360, align 8, !tbaa !62
  %3361 = getelementptr inbounds double, double* %3341, i64 20
  %3362 = bitcast double* %3361 to <2 x double>*
  store <2 x double> %3315, <2 x double>* %3362, align 8, !tbaa !62
  %3363 = getelementptr inbounds double, double* %3341, i64 22
  %3364 = bitcast double* %3363 to <2 x double>*
  store <2 x double> %3316, <2 x double>* %3364, align 8, !tbaa !62
  %3365 = shl <2 x i32> %vec.ind10547, <i32 1, i32 1>
  %3366 = shl <2 x i32> %step.add10548, <i32 1, i32 1>
  %3367 = shl <2 x i32> %step.add10549, <i32 1, i32 1>
  %3368 = shl <2 x i32> %step.add10550, <i32 1, i32 1>
  %3369 = shl <2 x i32> %step.add10551, <i32 1, i32 1>
  %3370 = shl <2 x i32> %step.add10552, <i32 1, i32 1>
  %3371 = shl <2 x i32> %step.add10553, <i32 1, i32 1>
  %3372 = shl <2 x i32> %step.add10554, <i32 1, i32 1>
  %3373 = shl <2 x i32> %step.add10555, <i32 1, i32 1>
  %3374 = shl <2 x i32> %step.add10556, <i32 1, i32 1>
  %3375 = shl <2 x i32> %step.add10557, <i32 1, i32 1>
  %3376 = shl <2 x i32> %step.add10558, <i32 1, i32 1>
  %3377 = sitofp <2 x i32> %3365 to <2 x double>
  %3378 = sitofp <2 x i32> %3366 to <2 x double>
  %3379 = sitofp <2 x i32> %3367 to <2 x double>
  %3380 = sitofp <2 x i32> %3368 to <2 x double>
  %3381 = sitofp <2 x i32> %3369 to <2 x double>
  %3382 = sitofp <2 x i32> %3370 to <2 x double>
  %3383 = sitofp <2 x i32> %3371 to <2 x double>
  %3384 = sitofp <2 x i32> %3372 to <2 x double>
  %3385 = sitofp <2 x i32> %3373 to <2 x double>
  %3386 = sitofp <2 x i32> %3374 to <2 x double>
  %3387 = sitofp <2 x i32> %3375 to <2 x double>
  %3388 = sitofp <2 x i32> %3376 to <2 x double>
  %3389 = getelementptr inbounds double, double* %1796, i64 %index10529
  %3390 = bitcast double* %3389 to <2 x double>*
  store <2 x double> %3377, <2 x double>* %3390, align 8, !tbaa !62
  %3391 = getelementptr inbounds double, double* %3389, i64 2
  %3392 = bitcast double* %3391 to <2 x double>*
  store <2 x double> %3378, <2 x double>* %3392, align 8, !tbaa !62
  %3393 = getelementptr inbounds double, double* %3389, i64 4
  %3394 = bitcast double* %3393 to <2 x double>*
  store <2 x double> %3379, <2 x double>* %3394, align 8, !tbaa !62
  %3395 = getelementptr inbounds double, double* %3389, i64 6
  %3396 = bitcast double* %3395 to <2 x double>*
  store <2 x double> %3380, <2 x double>* %3396, align 8, !tbaa !62
  %3397 = getelementptr inbounds double, double* %3389, i64 8
  %3398 = bitcast double* %3397 to <2 x double>*
  store <2 x double> %3381, <2 x double>* %3398, align 8, !tbaa !62
  %3399 = getelementptr inbounds double, double* %3389, i64 10
  %3400 = bitcast double* %3399 to <2 x double>*
  store <2 x double> %3382, <2 x double>* %3400, align 8, !tbaa !62
  %3401 = getelementptr inbounds double, double* %3389, i64 12
  %3402 = bitcast double* %3401 to <2 x double>*
  store <2 x double> %3383, <2 x double>* %3402, align 8, !tbaa !62
  %3403 = getelementptr inbounds double, double* %3389, i64 14
  %3404 = bitcast double* %3403 to <2 x double>*
  store <2 x double> %3384, <2 x double>* %3404, align 8, !tbaa !62
  %3405 = getelementptr inbounds double, double* %3389, i64 16
  %3406 = bitcast double* %3405 to <2 x double>*
  store <2 x double> %3385, <2 x double>* %3406, align 8, !tbaa !62
  %3407 = getelementptr inbounds double, double* %3389, i64 18
  %3408 = bitcast double* %3407 to <2 x double>*
  store <2 x double> %3386, <2 x double>* %3408, align 8, !tbaa !62
  %3409 = getelementptr inbounds double, double* %3389, i64 20
  %3410 = bitcast double* %3409 to <2 x double>*
  store <2 x double> %3387, <2 x double>* %3410, align 8, !tbaa !62
  %3411 = getelementptr inbounds double, double* %3389, i64 22
  %3412 = bitcast double* %3411 to <2 x double>*
  store <2 x double> %3388, <2 x double>* %3412, align 8, !tbaa !62
  %3413 = add <2 x i32> %vec.ind10547, <i32 -3, i32 -3>
  %3414 = add <2 x i32> %vec.ind10547, <i32 -1, i32 -1>
  %3415 = add <2 x i32> %vec.ind10547, <i32 1, i32 1>
  %3416 = add <2 x i32> %vec.ind10547, <i32 3, i32 3>
  %3417 = add <2 x i32> %vec.ind10547, <i32 5, i32 5>
  %3418 = add <2 x i32> %vec.ind10547, <i32 7, i32 7>
  %3419 = add <2 x i32> %vec.ind10547, <i32 9, i32 9>
  %3420 = add <2 x i32> %vec.ind10547, <i32 11, i32 11>
  %3421 = add <2 x i32> %vec.ind10547, <i32 13, i32 13>
  %3422 = add <2 x i32> %vec.ind10547, <i32 15, i32 15>
  %3423 = add <2 x i32> %vec.ind10547, <i32 17, i32 17>
  %3424 = add <2 x i32> %vec.ind10547, <i32 19, i32 19>
  %3425 = sitofp <2 x i32> %3413 to <2 x double>
  %3426 = sitofp <2 x i32> %3414 to <2 x double>
  %3427 = sitofp <2 x i32> %3415 to <2 x double>
  %3428 = sitofp <2 x i32> %3416 to <2 x double>
  %3429 = sitofp <2 x i32> %3417 to <2 x double>
  %3430 = sitofp <2 x i32> %3418 to <2 x double>
  %3431 = sitofp <2 x i32> %3419 to <2 x double>
  %3432 = sitofp <2 x i32> %3420 to <2 x double>
  %3433 = sitofp <2 x i32> %3421 to <2 x double>
  %3434 = sitofp <2 x i32> %3422 to <2 x double>
  %3435 = sitofp <2 x i32> %3423 to <2 x double>
  %3436 = sitofp <2 x i32> %3424 to <2 x double>
  %3437 = getelementptr inbounds double, double* %1797, i64 %index10529
  %3438 = bitcast double* %3437 to <2 x double>*
  store <2 x double> %3425, <2 x double>* %3438, align 8, !tbaa !62
  %3439 = getelementptr inbounds double, double* %3437, i64 2
  %3440 = bitcast double* %3439 to <2 x double>*
  store <2 x double> %3426, <2 x double>* %3440, align 8, !tbaa !62
  %3441 = getelementptr inbounds double, double* %3437, i64 4
  %3442 = bitcast double* %3441 to <2 x double>*
  store <2 x double> %3427, <2 x double>* %3442, align 8, !tbaa !62
  %3443 = getelementptr inbounds double, double* %3437, i64 6
  %3444 = bitcast double* %3443 to <2 x double>*
  store <2 x double> %3428, <2 x double>* %3444, align 8, !tbaa !62
  %3445 = getelementptr inbounds double, double* %3437, i64 8
  %3446 = bitcast double* %3445 to <2 x double>*
  store <2 x double> %3429, <2 x double>* %3446, align 8, !tbaa !62
  %3447 = getelementptr inbounds double, double* %3437, i64 10
  %3448 = bitcast double* %3447 to <2 x double>*
  store <2 x double> %3430, <2 x double>* %3448, align 8, !tbaa !62
  %3449 = getelementptr inbounds double, double* %3437, i64 12
  %3450 = bitcast double* %3449 to <2 x double>*
  store <2 x double> %3431, <2 x double>* %3450, align 8, !tbaa !62
  %3451 = getelementptr inbounds double, double* %3437, i64 14
  %3452 = bitcast double* %3451 to <2 x double>*
  store <2 x double> %3432, <2 x double>* %3452, align 8, !tbaa !62
  %3453 = getelementptr inbounds double, double* %3437, i64 16
  %3454 = bitcast double* %3453 to <2 x double>*
  store <2 x double> %3433, <2 x double>* %3454, align 8, !tbaa !62
  %3455 = getelementptr inbounds double, double* %3437, i64 18
  %3456 = bitcast double* %3455 to <2 x double>*
  store <2 x double> %3434, <2 x double>* %3456, align 8, !tbaa !62
  %3457 = getelementptr inbounds double, double* %3437, i64 20
  %3458 = bitcast double* %3457 to <2 x double>*
  store <2 x double> %3435, <2 x double>* %3458, align 8, !tbaa !62
  %3459 = getelementptr inbounds double, double* %3437, i64 22
  %3460 = bitcast double* %3459 to <2 x double>*
  store <2 x double> %3436, <2 x double>* %3460, align 8, !tbaa !62
  %index.next10530 = add nuw nsw i64 %index10529, 24
  %vec.ind.next10560 = add <2 x i32> %vec.ind10547, <i32 24, i32 24>
  %3461 = icmp eq i64 %index.next10530, 24984
  br i1 %3461, label %for.body.i8461, label %vector.body10525, !llvm.loop !92

for.body.i8461:                                   ; preds = %vector.body10525
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %3272, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %3273, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %3274, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %3275, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %3276, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %3277, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %3278, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %3279, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %3280, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %3281, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %3282, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %3283, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %3284, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %3285, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %3286, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %3287, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %3288, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %3289, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %3290, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %3291, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %3292, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %3293, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %3294, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %3295, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %3296, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %3297, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %3298, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %3299, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %3300, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %3301, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %3302, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %3303, align 8, !tbaa !62
  %3462 = shl nuw nsw i64 %indvars.iv9551, 3
  store i8* %call3733, i8** %3238, align 8
  store i8* %call3733, i8** %3239, align 8
  store i64 %3462, i64* %3240, align 8
  store i8* %call3735, i8** %3241, align 8
  store i8* %call3735, i8** %3242, align 8
  store i64 %3462, i64* %3243, align 8
  store i8* %call3736, i8** %3244, align 8
  store i8* %call3736, i8** %3245, align 8
  store i64 %3462, i64* %3246, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %3238, i8** nonnull %3239, i64* nonnull %3240, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  br label %for.cond4211.preheader

for.cond4251.preheader:                           ; preds = %for.cond.cleanup4214
  %3463 = add nuw nsw i64 %3304, 32
  %cmp42528976 = icmp sgt i32 %t4197.28969, -1
  br i1 %cmp42528976, label %for.cond4257.preheader.preheader57, label %for.cond.cleanup4254

for.cond4257.preheader.preheader57:               ; preds = %for.cond4251.preheader
  %n.mod.vf10469 = urem i64 %3463, 24
  %n.vec10470 = sub nuw nsw i64 %3463, %n.mod.vf10469
  %cmp.n10474 = icmp eq i64 %n.mod.vf10469, 0
  br label %for.cond4257.preheader

for.cond4211.preheader:                           ; preds = %for.body.i8461, %for.cond.cleanup4214
  %tms4204.08974 = phi i32 [ 1, %for.body.i8461 ], [ %mul4247, %for.cond.cleanup4214 ]
  %t4197.08973 = phi i32 [ 0, %for.body.i8461 ], [ %inc4222, %for.cond.cleanup4214 ]
  %.capture_expr..casted4230.sroa.0.0.insert.ext = zext i32 %tms4204.08974 to i64
  br label %for.cond4217.preheader

for.cond4217.preheader:                           ; preds = %for.cond4211.preheader, %for.cond.cleanup4220
  %ths4210.08972 = phi i32 [ 32, %for.cond4211.preheader ], [ %mul4243, %for.cond.cleanup4220 ]
  %t4197.18971 = phi i32 [ %t4197.08973, %for.cond4211.preheader ], [ %inc4222, %for.cond.cleanup4220 ]
  %.capture_expr..casted4232.sroa.0.0.insert.ext = zext i32 %ths4210.08972 to i64
  br label %for.body4221

for.cond.cleanup4214:                             ; preds = %for.cond.cleanup4220
  %mul4247 = shl nsw i32 %tms4204.08974, 1
  %cmp4206 = icmp ult i32 %mul4247, 257
  br i1 %cmp4206, label %for.cond4211.preheader, label %for.cond4251.preheader

for.cond.cleanup4220:                             ; preds = %omp_offload.cont4237
  %mul4243 = shl nsw i32 %ths4210.08972, 1
  %cmp4212 = icmp ult i32 %mul4243, 1025
  br i1 %cmp4212, label %for.cond4217.preheader, label %for.cond.cleanup4214

for.body4221:                                     ; preds = %for.cond4217.preheader, %omp_offload.cont4237
  %sch4216.08970 = phi i32 [ 1, %for.cond4217.preheader ], [ %mul4239, %omp_offload.cont4237 ]
  %t4197.28969 = phi i32 [ %t4197.18971, %for.cond4217.preheader ], [ %inc4222, %omp_offload.cont4237 ]
  %inc4222 = add i32 %t4197.28969, 1
  %.capture_expr..casted4228.sroa.0.0.insert.ext = zext i32 %sch4216.08970 to i64
  store i64 %indvars.iv9551, i64* %3248, align 8
  store i64 %indvars.iv9551, i64* %3250, align 8
  store i8* %call3733, i8** %3251, align 8
  store i8* %call3733, i8** %3252, align 8
  store i8* %call3735, i8** %3253, align 8
  store i8* %call3735, i8** %3254, align 8
  store i8* %call3736, i8** %3255, align 8
  store i8* %call3736, i8** %3256, align 8
  store i64 %.capture_expr..casted4228.sroa.0.0.insert.ext, i64* %3258, align 8
  store i64 %.capture_expr..casted4228.sroa.0.0.insert.ext, i64* %3260, align 8
  store i64 %.capture_expr..casted4230.sroa.0.0.insert.ext, i64* %3262, align 8
  store i64 %.capture_expr..casted4230.sroa.0.0.insert.ext, i64* %3264, align 8
  store i64 %.capture_expr..casted4232.sroa.0.0.insert.ext, i64* %3266, align 8
  store i64 %.capture_expr..casted4232.sroa.0.0.insert.ext, i64* %3268, align 8
  %3464 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1177.region_id, i32 7, i8** nonnull %3247, i8** nonnull %3249, i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_sizes.273, i64 0, i64 0), i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_maptypes.274, i64 0, i64 0), i32 %tms4204.08974, i32 %ths4210.08972) #6
  %3465 = icmp eq i32 %3464, 0
  br i1 %3465, label %omp_offload.cont4237, label %omp_offload.failed4236

omp_offload.failed4236:                           ; preds = %for.body4221
  %3466 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %3467 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %3466, i32 %tms4204.08974, i32 %ths4210.08972) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 5, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64)* @.omp_outlined..187 to void (i32*, i32*, ...)*), i64 %indvars.iv9551, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted4228.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont4237

omp_offload.cont4237:                             ; preds = %for.body4221, %omp_offload.failed4236
  %mul4239 = mul nsw i32 %sch4216.08970, 1200
  %3468 = zext i32 %mul4239 to i64
  %cmp4218 = icmp ult i64 %indvars.iv9551, %3468
  br i1 %cmp4218, label %for.cond.cleanup4220, label %for.body4221

for.cond4257.preheader:                           ; preds = %for.cond4257.preheader.preheader57, %for.cond.cleanup4260
  %times4250.08977 = phi i32 [ %inc4275, %for.cond.cleanup4260 ], [ 0, %for.cond4257.preheader.preheader57 ]
  br label %vector.body10462

vector.body10462:                                 ; preds = %for.cond4257.preheader, %vector.body10462
  %index10471 = phi i64 [ %index.next10472, %vector.body10462 ], [ 0, %for.cond4257.preheader ]
  %3469 = getelementptr inbounds double, double* %1796, i64 %index10471
  %3470 = bitcast double* %3469 to <2 x double>*
  %wide.load10489 = load <2 x double>, <2 x double>* %3470, align 8, !tbaa !62
  %3471 = getelementptr inbounds double, double* %3469, i64 2
  %3472 = bitcast double* %3471 to <2 x double>*
  %wide.load10490 = load <2 x double>, <2 x double>* %3472, align 8, !tbaa !62
  %3473 = getelementptr inbounds double, double* %3469, i64 4
  %3474 = bitcast double* %3473 to <2 x double>*
  %wide.load10491 = load <2 x double>, <2 x double>* %3474, align 8, !tbaa !62
  %3475 = getelementptr inbounds double, double* %3469, i64 6
  %3476 = bitcast double* %3475 to <2 x double>*
  %wide.load10492 = load <2 x double>, <2 x double>* %3476, align 8, !tbaa !62
  %3477 = getelementptr inbounds double, double* %3469, i64 8
  %3478 = bitcast double* %3477 to <2 x double>*
  %wide.load10493 = load <2 x double>, <2 x double>* %3478, align 8, !tbaa !62
  %3479 = getelementptr inbounds double, double* %3469, i64 10
  %3480 = bitcast double* %3479 to <2 x double>*
  %wide.load10494 = load <2 x double>, <2 x double>* %3480, align 8, !tbaa !62
  %3481 = getelementptr inbounds double, double* %3469, i64 12
  %3482 = bitcast double* %3481 to <2 x double>*
  %wide.load10495 = load <2 x double>, <2 x double>* %3482, align 8, !tbaa !62
  %3483 = getelementptr inbounds double, double* %3469, i64 14
  %3484 = bitcast double* %3483 to <2 x double>*
  %wide.load10496 = load <2 x double>, <2 x double>* %3484, align 8, !tbaa !62
  %3485 = getelementptr inbounds double, double* %3469, i64 16
  %3486 = bitcast double* %3485 to <2 x double>*
  %wide.load10497 = load <2 x double>, <2 x double>* %3486, align 8, !tbaa !62
  %3487 = getelementptr inbounds double, double* %3469, i64 18
  %3488 = bitcast double* %3487 to <2 x double>*
  %wide.load10498 = load <2 x double>, <2 x double>* %3488, align 8, !tbaa !62
  %3489 = getelementptr inbounds double, double* %3469, i64 20
  %3490 = bitcast double* %3489 to <2 x double>*
  %wide.load10499 = load <2 x double>, <2 x double>* %3490, align 8, !tbaa !62
  %3491 = getelementptr inbounds double, double* %3469, i64 22
  %3492 = bitcast double* %3491 to <2 x double>*
  %wide.load10500 = load <2 x double>, <2 x double>* %3492, align 8, !tbaa !62
  %3493 = getelementptr inbounds double, double* %1797, i64 %index10471
  %3494 = bitcast double* %3493 to <2 x double>*
  %wide.load10501 = load <2 x double>, <2 x double>* %3494, align 8, !tbaa !62
  %3495 = getelementptr inbounds double, double* %3493, i64 2
  %3496 = bitcast double* %3495 to <2 x double>*
  %wide.load10502 = load <2 x double>, <2 x double>* %3496, align 8, !tbaa !62
  %3497 = getelementptr inbounds double, double* %3493, i64 4
  %3498 = bitcast double* %3497 to <2 x double>*
  %wide.load10503 = load <2 x double>, <2 x double>* %3498, align 8, !tbaa !62
  %3499 = getelementptr inbounds double, double* %3493, i64 6
  %3500 = bitcast double* %3499 to <2 x double>*
  %wide.load10504 = load <2 x double>, <2 x double>* %3500, align 8, !tbaa !62
  %3501 = getelementptr inbounds double, double* %3493, i64 8
  %3502 = bitcast double* %3501 to <2 x double>*
  %wide.load10505 = load <2 x double>, <2 x double>* %3502, align 8, !tbaa !62
  %3503 = getelementptr inbounds double, double* %3493, i64 10
  %3504 = bitcast double* %3503 to <2 x double>*
  %wide.load10506 = load <2 x double>, <2 x double>* %3504, align 8, !tbaa !62
  %3505 = getelementptr inbounds double, double* %3493, i64 12
  %3506 = bitcast double* %3505 to <2 x double>*
  %wide.load10507 = load <2 x double>, <2 x double>* %3506, align 8, !tbaa !62
  %3507 = getelementptr inbounds double, double* %3493, i64 14
  %3508 = bitcast double* %3507 to <2 x double>*
  %wide.load10508 = load <2 x double>, <2 x double>* %3508, align 8, !tbaa !62
  %3509 = getelementptr inbounds double, double* %3493, i64 16
  %3510 = bitcast double* %3509 to <2 x double>*
  %wide.load10509 = load <2 x double>, <2 x double>* %3510, align 8, !tbaa !62
  %3511 = getelementptr inbounds double, double* %3493, i64 18
  %3512 = bitcast double* %3511 to <2 x double>*
  %wide.load10510 = load <2 x double>, <2 x double>* %3512, align 8, !tbaa !62
  %3513 = getelementptr inbounds double, double* %3493, i64 20
  %3514 = bitcast double* %3513 to <2 x double>*
  %wide.load10511 = load <2 x double>, <2 x double>* %3514, align 8, !tbaa !62
  %3515 = getelementptr inbounds double, double* %3493, i64 22
  %3516 = bitcast double* %3515 to <2 x double>*
  %wide.load10512 = load <2 x double>, <2 x double>* %3516, align 8, !tbaa !62
  %3517 = fadd <2 x double> %wide.load10489, %wide.load10501
  %3518 = fadd <2 x double> %wide.load10490, %wide.load10502
  %3519 = fadd <2 x double> %wide.load10491, %wide.load10503
  %3520 = fadd <2 x double> %wide.load10492, %wide.load10504
  %3521 = fadd <2 x double> %wide.load10493, %wide.load10505
  %3522 = fadd <2 x double> %wide.load10494, %wide.load10506
  %3523 = fadd <2 x double> %wide.load10495, %wide.load10507
  %3524 = fadd <2 x double> %wide.load10496, %wide.load10508
  %3525 = fadd <2 x double> %wide.load10497, %wide.load10509
  %3526 = fadd <2 x double> %wide.load10498, %wide.load10510
  %3527 = fadd <2 x double> %wide.load10499, %wide.load10511
  %3528 = fadd <2 x double> %wide.load10500, %wide.load10512
  %3529 = getelementptr inbounds double, double* %1795, i64 %index10471
  %3530 = bitcast double* %3529 to <2 x double>*
  %wide.load10513 = load <2 x double>, <2 x double>* %3530, align 8, !tbaa !62
  %3531 = getelementptr inbounds double, double* %3529, i64 2
  %3532 = bitcast double* %3531 to <2 x double>*
  %wide.load10514 = load <2 x double>, <2 x double>* %3532, align 8, !tbaa !62
  %3533 = getelementptr inbounds double, double* %3529, i64 4
  %3534 = bitcast double* %3533 to <2 x double>*
  %wide.load10515 = load <2 x double>, <2 x double>* %3534, align 8, !tbaa !62
  %3535 = getelementptr inbounds double, double* %3529, i64 6
  %3536 = bitcast double* %3535 to <2 x double>*
  %wide.load10516 = load <2 x double>, <2 x double>* %3536, align 8, !tbaa !62
  %3537 = getelementptr inbounds double, double* %3529, i64 8
  %3538 = bitcast double* %3537 to <2 x double>*
  %wide.load10517 = load <2 x double>, <2 x double>* %3538, align 8, !tbaa !62
  %3539 = getelementptr inbounds double, double* %3529, i64 10
  %3540 = bitcast double* %3539 to <2 x double>*
  %wide.load10518 = load <2 x double>, <2 x double>* %3540, align 8, !tbaa !62
  %3541 = getelementptr inbounds double, double* %3529, i64 12
  %3542 = bitcast double* %3541 to <2 x double>*
  %wide.load10519 = load <2 x double>, <2 x double>* %3542, align 8, !tbaa !62
  %3543 = getelementptr inbounds double, double* %3529, i64 14
  %3544 = bitcast double* %3543 to <2 x double>*
  %wide.load10520 = load <2 x double>, <2 x double>* %3544, align 8, !tbaa !62
  %3545 = getelementptr inbounds double, double* %3529, i64 16
  %3546 = bitcast double* %3545 to <2 x double>*
  %wide.load10521 = load <2 x double>, <2 x double>* %3546, align 8, !tbaa !62
  %3547 = getelementptr inbounds double, double* %3529, i64 18
  %3548 = bitcast double* %3547 to <2 x double>*
  %wide.load10522 = load <2 x double>, <2 x double>* %3548, align 8, !tbaa !62
  %3549 = getelementptr inbounds double, double* %3529, i64 20
  %3550 = bitcast double* %3549 to <2 x double>*
  %wide.load10523 = load <2 x double>, <2 x double>* %3550, align 8, !tbaa !62
  %3551 = getelementptr inbounds double, double* %3529, i64 22
  %3552 = bitcast double* %3551 to <2 x double>*
  %wide.load10524 = load <2 x double>, <2 x double>* %3552, align 8, !tbaa !62
  %3553 = fadd <2 x double> %3517, %wide.load10513
  %3554 = fadd <2 x double> %3518, %wide.load10514
  %3555 = fadd <2 x double> %3519, %wide.load10515
  %3556 = fadd <2 x double> %3520, %wide.load10516
  %3557 = fadd <2 x double> %3521, %wide.load10517
  %3558 = fadd <2 x double> %3522, %wide.load10518
  %3559 = fadd <2 x double> %3523, %wide.load10519
  %3560 = fadd <2 x double> %3524, %wide.load10520
  %3561 = fadd <2 x double> %3525, %wide.load10521
  %3562 = fadd <2 x double> %3526, %wide.load10522
  %3563 = fadd <2 x double> %3527, %wide.load10523
  %3564 = fadd <2 x double> %3528, %wide.load10524
  store <2 x double> %3553, <2 x double>* %3530, align 8, !tbaa !62
  store <2 x double> %3554, <2 x double>* %3532, align 8, !tbaa !62
  store <2 x double> %3555, <2 x double>* %3534, align 8, !tbaa !62
  store <2 x double> %3556, <2 x double>* %3536, align 8, !tbaa !62
  store <2 x double> %3557, <2 x double>* %3538, align 8, !tbaa !62
  store <2 x double> %3558, <2 x double>* %3540, align 8, !tbaa !62
  store <2 x double> %3559, <2 x double>* %3542, align 8, !tbaa !62
  store <2 x double> %3560, <2 x double>* %3544, align 8, !tbaa !62
  store <2 x double> %3561, <2 x double>* %3546, align 8, !tbaa !62
  store <2 x double> %3562, <2 x double>* %3548, align 8, !tbaa !62
  store <2 x double> %3563, <2 x double>* %3550, align 8, !tbaa !62
  store <2 x double> %3564, <2 x double>* %3552, align 8, !tbaa !62
  %index.next10472 = add nuw nsw i64 %index10471, 24
  %3565 = icmp eq i64 %index.next10472, %n.vec10470
  br i1 %3565, label %middle.block10463, label %vector.body10462, !llvm.loop !93

middle.block10463:                                ; preds = %vector.body10462
  br i1 %cmp.n10474, label %for.cond.cleanup4260, label %for.body4261

for.cond.cleanup4254:                             ; preds = %for.cond.cleanup4260, %for.cond4251.preheader
  store i8* %call3733, i8** %3269, align 8
  store i8* %call3733, i8** %3270, align 8
  store i64 %3462, i64* %3271, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %3269, i8** nonnull %3270, i64* nonnull %3271, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body4287

for.cond.cleanup4260:                             ; preds = %for.body4261, %middle.block10463
  %inc4275 = add nuw nsw i32 %times4250.08977, 1
  %exitcond9548 = icmp eq i32 %inc4275, %inc4222
  br i1 %exitcond9548, label %for.cond.cleanup4254, label %for.cond4257.preheader

for.body4261:                                     ; preds = %middle.block10463, %for.body4261
  %indvars.iv9545 = phi i64 [ %indvars.iv.next9546, %for.body4261 ], [ %n.vec10470, %middle.block10463 ]
  %arrayidx4263 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9545
  %3566 = load double, double* %arrayidx4263, align 8, !tbaa !62
  %arrayidx4265 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9545
  %3567 = load double, double* %arrayidx4265, align 8, !tbaa !62
  %add4266 = fadd double %3566, %3567
  %arrayidx4268 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9545
  %3568 = load double, double* %arrayidx4268, align 8, !tbaa !62
  %add4269 = fadd double %3568, %add4266
  store double %add4269, double* %arrayidx4268, align 8, !tbaa !62
  %indvars.iv.next9546 = add nuw nsw i64 %indvars.iv9545, 1
  %exitcond9547 = icmp eq i64 %indvars.iv.next9546, %indvars.iv9551
  br i1 %exitcond9547, label %for.cond.cleanup4260, label %for.body4261, !llvm.loop !94

for.body4287:                                     ; preds = %for.inc4301.7, %for.cond.cleanup4254
  %indvars.iv9549 = phi i64 [ 0, %for.cond.cleanup4254 ], [ %indvars.iv.next9550.7, %for.inc4301.7 ]
  %arrayidx4289 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9549
  %3569 = load double, double* %arrayidx4289, align 8, !tbaa !62
  %arrayidx4291 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9549
  %3570 = load double, double* %arrayidx4291, align 8, !tbaa !62
  %cmp4292 = fcmp une double %3569, %3570
  br i1 %cmp4292, label %cleanup4311, label %for.inc4301

for.inc4301:                                      ; preds = %for.body4287
  %indvars.iv.next9550 = or i64 %indvars.iv9549, 1
  %arrayidx4289.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9550
  %3571 = load double, double* %arrayidx4289.1, align 8, !tbaa !62
  %arrayidx4291.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9550
  %3572 = load double, double* %arrayidx4291.1, align 8, !tbaa !62
  %cmp4292.1 = fcmp une double %3571, %3572
  br i1 %cmp4292.1, label %cleanup4311, label %for.inc4301.1

for.inc4309:                                      ; preds = %for.inc4301.7
  %indvars.iv.next9552 = add nuw nsw i64 %indvars.iv9551, 5000
  %cmp4193 = icmp ult i64 %indvars.iv.next9552, 25000
  %indvar.next10466 = add nuw nsw i64 %indvar10465, 1
  br i1 %cmp4193, label %for.body.i8461.preheader, label %for.end4313

cleanup4311:                                      ; preds = %for.inc4301.6, %for.inc4301.5, %for.inc4301.4, %for.inc4301.3, %for.inc4301.2, %for.inc4301.1, %for.inc4301, %for.body4287
  %indvars.iv9549.lcssa = phi i64 [ %indvars.iv9549, %for.body4287 ], [ %indvars.iv.next9550, %for.inc4301 ], [ %indvars.iv.next9550.1, %for.inc4301.1 ], [ %indvars.iv.next9550.2, %for.inc4301.2 ], [ %indvars.iv.next9550.3, %for.inc4301.3 ], [ %indvars.iv.next9550.4, %for.inc4301.4 ], [ %indvars.iv.next9550.5, %for.inc4301.5 ], [ %indvars.iv.next9550.6, %for.inc4301.6 ]
  %.lcssa11840 = phi double [ %3569, %for.body4287 ], [ %3571, %for.inc4301 ], [ %7872, %for.inc4301.1 ], [ %7874, %for.inc4301.2 ], [ %7876, %for.inc4301.3 ], [ %7878, %for.inc4301.4 ], [ %7880, %for.inc4301.5 ], [ %7882, %for.inc4301.6 ]
  %.lcssa11838 = phi double [ %3570, %for.body4287 ], [ %3572, %for.inc4301 ], [ %7873, %for.inc4301.1 ], [ %7875, %for.inc4301.2 ], [ %7877, %for.inc4301.3 ], [ %7879, %for.inc4301.4 ], [ %7881, %for.inc4301.5 ], [ %7883, %for.inc4301.6 ]
  %3573 = trunc i64 %indvars.iv9551 to i32
  %3574 = trunc i64 %indvars.iv9549.lcssa to i32
  %call4299 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %3573, i32 signext %3574, double %.lcssa11840, double %.lcssa11838)
  br label %cleanup5832

for.end4313:                                      ; preds = %for.inc4309
  %puts8253 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8254 = call i32 @puts(i8* getelementptr inbounds ([30 x i8], [30 x i8]* @str.343, i64 0, i64 0))
  %3575 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4325, i64 0, i64 0
  %3576 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4326, i64 0, i64 0
  %3577 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4327, i64 0, i64 0
  %3578 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4325, i64 0, i64 1
  %3579 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4326, i64 0, i64 1
  %3580 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4327, i64 0, i64 1
  %3581 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4325, i64 0, i64 2
  %3582 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4326, i64 0, i64 2
  %3583 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4327, i64 0, i64 2
  %3584 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4350, i64 0, i64 0
  %3585 = bitcast [6 x i8*]* %.offload_baseptrs4350 to i64*
  %3586 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4351, i64 0, i64 0
  %3587 = bitcast [6 x i8*]* %.offload_ptrs4351 to i64*
  %3588 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4350, i64 0, i64 1
  %3589 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4351, i64 0, i64 1
  %3590 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4350, i64 0, i64 2
  %3591 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4351, i64 0, i64 2
  %3592 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4350, i64 0, i64 3
  %3593 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4351, i64 0, i64 3
  %3594 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4350, i64 0, i64 4
  %3595 = bitcast i8** %3594 to i64*
  %3596 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4351, i64 0, i64 4
  %3597 = bitcast i8** %3596 to i64*
  %3598 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4350, i64 0, i64 5
  %3599 = bitcast i8** %3598 to i64*
  %3600 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4351, i64 0, i64 5
  %3601 = bitcast i8** %3600 to i64*
  %3602 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs4391, i64 0, i64 0
  %3603 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs4392, i64 0, i64 0
  %3604 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes4393, i64 0, i64 0
  %3605 = bitcast i8* %arrayidx.i to <2 x double>*
  %3606 = bitcast i8* %arrayidx2.i to <2 x double>*
  %3607 = bitcast i8* %arrayidx5.i to <2 x double>*
  %3608 = bitcast i8* %arrayidx8.i to <2 x double>*
  %3609 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %3610 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %3611 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %3612 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %3613 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %3614 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %3615 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %3616 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %3617 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %3618 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %3619 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %3620 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %3621 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %3622 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %3623 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %3624 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %3625 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %3626 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %3627 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %3628 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %3629 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %3630 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %3631 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %3632 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %3633 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %3634 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %3635 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %3636 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8476.preheader

for.body.i8476.preheader:                         ; preds = %for.end4313, %for.inc4421
  %indvar10564 = phi i64 [ 0, %for.end4313 ], [ %indvar.next10565, %for.inc4421 ]
  %indvars.iv9543 = phi i64 [ 32, %for.end4313 ], [ %indvars.iv.next9544, %for.inc4421 ]
  %3637 = mul nuw nsw i64 %indvar10564, 5000
  br label %vector.body10624

vector.body10624:                                 ; preds = %vector.body10624, %for.body.i8476.preheader
  %index10628 = phi i64 [ 0, %for.body.i8476.preheader ], [ %index.next10629, %vector.body10624 ]
  %vec.ind10646 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8476.preheader ], [ %vec.ind.next10659, %vector.body10624 ]
  %step.add10647 = add <2 x i32> %vec.ind10646, <i32 2, i32 2>
  %step.add10648 = add <2 x i32> %vec.ind10646, <i32 4, i32 4>
  %step.add10649 = add <2 x i32> %vec.ind10646, <i32 6, i32 6>
  %step.add10650 = add <2 x i32> %vec.ind10646, <i32 8, i32 8>
  %step.add10651 = add <2 x i32> %vec.ind10646, <i32 10, i32 10>
  %step.add10652 = add <2 x i32> %vec.ind10646, <i32 12, i32 12>
  %step.add10653 = add <2 x i32> %vec.ind10646, <i32 14, i32 14>
  %step.add10654 = add <2 x i32> %vec.ind10646, <i32 16, i32 16>
  %step.add10655 = add <2 x i32> %vec.ind10646, <i32 18, i32 18>
  %step.add10656 = add <2 x i32> %vec.ind10646, <i32 20, i32 20>
  %step.add10657 = add <2 x i32> %vec.ind10646, <i32 22, i32 22>
  %3638 = sitofp <2 x i32> %vec.ind10646 to <2 x double>
  %3639 = sitofp <2 x i32> %step.add10647 to <2 x double>
  %3640 = sitofp <2 x i32> %step.add10648 to <2 x double>
  %3641 = sitofp <2 x i32> %step.add10649 to <2 x double>
  %3642 = sitofp <2 x i32> %step.add10650 to <2 x double>
  %3643 = sitofp <2 x i32> %step.add10651 to <2 x double>
  %3644 = sitofp <2 x i32> %step.add10652 to <2 x double>
  %3645 = sitofp <2 x i32> %step.add10653 to <2 x double>
  %3646 = sitofp <2 x i32> %step.add10654 to <2 x double>
  %3647 = sitofp <2 x i32> %step.add10655 to <2 x double>
  %3648 = sitofp <2 x i32> %step.add10656 to <2 x double>
  %3649 = sitofp <2 x i32> %step.add10657 to <2 x double>
  %3650 = getelementptr inbounds double, double* %1795, i64 %index10628
  %3651 = bitcast double* %3650 to <2 x double>*
  store <2 x double> %3638, <2 x double>* %3651, align 8, !tbaa !62
  %3652 = getelementptr inbounds double, double* %3650, i64 2
  %3653 = bitcast double* %3652 to <2 x double>*
  store <2 x double> %3639, <2 x double>* %3653, align 8, !tbaa !62
  %3654 = getelementptr inbounds double, double* %3650, i64 4
  %3655 = bitcast double* %3654 to <2 x double>*
  store <2 x double> %3640, <2 x double>* %3655, align 8, !tbaa !62
  %3656 = getelementptr inbounds double, double* %3650, i64 6
  %3657 = bitcast double* %3656 to <2 x double>*
  store <2 x double> %3641, <2 x double>* %3657, align 8, !tbaa !62
  %3658 = getelementptr inbounds double, double* %3650, i64 8
  %3659 = bitcast double* %3658 to <2 x double>*
  store <2 x double> %3642, <2 x double>* %3659, align 8, !tbaa !62
  %3660 = getelementptr inbounds double, double* %3650, i64 10
  %3661 = bitcast double* %3660 to <2 x double>*
  store <2 x double> %3643, <2 x double>* %3661, align 8, !tbaa !62
  %3662 = getelementptr inbounds double, double* %3650, i64 12
  %3663 = bitcast double* %3662 to <2 x double>*
  store <2 x double> %3644, <2 x double>* %3663, align 8, !tbaa !62
  %3664 = getelementptr inbounds double, double* %3650, i64 14
  %3665 = bitcast double* %3664 to <2 x double>*
  store <2 x double> %3645, <2 x double>* %3665, align 8, !tbaa !62
  %3666 = getelementptr inbounds double, double* %3650, i64 16
  %3667 = bitcast double* %3666 to <2 x double>*
  store <2 x double> %3646, <2 x double>* %3667, align 8, !tbaa !62
  %3668 = getelementptr inbounds double, double* %3650, i64 18
  %3669 = bitcast double* %3668 to <2 x double>*
  store <2 x double> %3647, <2 x double>* %3669, align 8, !tbaa !62
  %3670 = getelementptr inbounds double, double* %3650, i64 20
  %3671 = bitcast double* %3670 to <2 x double>*
  store <2 x double> %3648, <2 x double>* %3671, align 8, !tbaa !62
  %3672 = getelementptr inbounds double, double* %3650, i64 22
  %3673 = bitcast double* %3672 to <2 x double>*
  store <2 x double> %3649, <2 x double>* %3673, align 8, !tbaa !62
  %3674 = getelementptr inbounds double, double* %1794, i64 %index10628
  %3675 = bitcast double* %3674 to <2 x double>*
  store <2 x double> %3638, <2 x double>* %3675, align 8, !tbaa !62
  %3676 = getelementptr inbounds double, double* %3674, i64 2
  %3677 = bitcast double* %3676 to <2 x double>*
  store <2 x double> %3639, <2 x double>* %3677, align 8, !tbaa !62
  %3678 = getelementptr inbounds double, double* %3674, i64 4
  %3679 = bitcast double* %3678 to <2 x double>*
  store <2 x double> %3640, <2 x double>* %3679, align 8, !tbaa !62
  %3680 = getelementptr inbounds double, double* %3674, i64 6
  %3681 = bitcast double* %3680 to <2 x double>*
  store <2 x double> %3641, <2 x double>* %3681, align 8, !tbaa !62
  %3682 = getelementptr inbounds double, double* %3674, i64 8
  %3683 = bitcast double* %3682 to <2 x double>*
  store <2 x double> %3642, <2 x double>* %3683, align 8, !tbaa !62
  %3684 = getelementptr inbounds double, double* %3674, i64 10
  %3685 = bitcast double* %3684 to <2 x double>*
  store <2 x double> %3643, <2 x double>* %3685, align 8, !tbaa !62
  %3686 = getelementptr inbounds double, double* %3674, i64 12
  %3687 = bitcast double* %3686 to <2 x double>*
  store <2 x double> %3644, <2 x double>* %3687, align 8, !tbaa !62
  %3688 = getelementptr inbounds double, double* %3674, i64 14
  %3689 = bitcast double* %3688 to <2 x double>*
  store <2 x double> %3645, <2 x double>* %3689, align 8, !tbaa !62
  %3690 = getelementptr inbounds double, double* %3674, i64 16
  %3691 = bitcast double* %3690 to <2 x double>*
  store <2 x double> %3646, <2 x double>* %3691, align 8, !tbaa !62
  %3692 = getelementptr inbounds double, double* %3674, i64 18
  %3693 = bitcast double* %3692 to <2 x double>*
  store <2 x double> %3647, <2 x double>* %3693, align 8, !tbaa !62
  %3694 = getelementptr inbounds double, double* %3674, i64 20
  %3695 = bitcast double* %3694 to <2 x double>*
  store <2 x double> %3648, <2 x double>* %3695, align 8, !tbaa !62
  %3696 = getelementptr inbounds double, double* %3674, i64 22
  %3697 = bitcast double* %3696 to <2 x double>*
  store <2 x double> %3649, <2 x double>* %3697, align 8, !tbaa !62
  %3698 = shl <2 x i32> %vec.ind10646, <i32 1, i32 1>
  %3699 = shl <2 x i32> %step.add10647, <i32 1, i32 1>
  %3700 = shl <2 x i32> %step.add10648, <i32 1, i32 1>
  %3701 = shl <2 x i32> %step.add10649, <i32 1, i32 1>
  %3702 = shl <2 x i32> %step.add10650, <i32 1, i32 1>
  %3703 = shl <2 x i32> %step.add10651, <i32 1, i32 1>
  %3704 = shl <2 x i32> %step.add10652, <i32 1, i32 1>
  %3705 = shl <2 x i32> %step.add10653, <i32 1, i32 1>
  %3706 = shl <2 x i32> %step.add10654, <i32 1, i32 1>
  %3707 = shl <2 x i32> %step.add10655, <i32 1, i32 1>
  %3708 = shl <2 x i32> %step.add10656, <i32 1, i32 1>
  %3709 = shl <2 x i32> %step.add10657, <i32 1, i32 1>
  %3710 = sitofp <2 x i32> %3698 to <2 x double>
  %3711 = sitofp <2 x i32> %3699 to <2 x double>
  %3712 = sitofp <2 x i32> %3700 to <2 x double>
  %3713 = sitofp <2 x i32> %3701 to <2 x double>
  %3714 = sitofp <2 x i32> %3702 to <2 x double>
  %3715 = sitofp <2 x i32> %3703 to <2 x double>
  %3716 = sitofp <2 x i32> %3704 to <2 x double>
  %3717 = sitofp <2 x i32> %3705 to <2 x double>
  %3718 = sitofp <2 x i32> %3706 to <2 x double>
  %3719 = sitofp <2 x i32> %3707 to <2 x double>
  %3720 = sitofp <2 x i32> %3708 to <2 x double>
  %3721 = sitofp <2 x i32> %3709 to <2 x double>
  %3722 = getelementptr inbounds double, double* %1796, i64 %index10628
  %3723 = bitcast double* %3722 to <2 x double>*
  store <2 x double> %3710, <2 x double>* %3723, align 8, !tbaa !62
  %3724 = getelementptr inbounds double, double* %3722, i64 2
  %3725 = bitcast double* %3724 to <2 x double>*
  store <2 x double> %3711, <2 x double>* %3725, align 8, !tbaa !62
  %3726 = getelementptr inbounds double, double* %3722, i64 4
  %3727 = bitcast double* %3726 to <2 x double>*
  store <2 x double> %3712, <2 x double>* %3727, align 8, !tbaa !62
  %3728 = getelementptr inbounds double, double* %3722, i64 6
  %3729 = bitcast double* %3728 to <2 x double>*
  store <2 x double> %3713, <2 x double>* %3729, align 8, !tbaa !62
  %3730 = getelementptr inbounds double, double* %3722, i64 8
  %3731 = bitcast double* %3730 to <2 x double>*
  store <2 x double> %3714, <2 x double>* %3731, align 8, !tbaa !62
  %3732 = getelementptr inbounds double, double* %3722, i64 10
  %3733 = bitcast double* %3732 to <2 x double>*
  store <2 x double> %3715, <2 x double>* %3733, align 8, !tbaa !62
  %3734 = getelementptr inbounds double, double* %3722, i64 12
  %3735 = bitcast double* %3734 to <2 x double>*
  store <2 x double> %3716, <2 x double>* %3735, align 8, !tbaa !62
  %3736 = getelementptr inbounds double, double* %3722, i64 14
  %3737 = bitcast double* %3736 to <2 x double>*
  store <2 x double> %3717, <2 x double>* %3737, align 8, !tbaa !62
  %3738 = getelementptr inbounds double, double* %3722, i64 16
  %3739 = bitcast double* %3738 to <2 x double>*
  store <2 x double> %3718, <2 x double>* %3739, align 8, !tbaa !62
  %3740 = getelementptr inbounds double, double* %3722, i64 18
  %3741 = bitcast double* %3740 to <2 x double>*
  store <2 x double> %3719, <2 x double>* %3741, align 8, !tbaa !62
  %3742 = getelementptr inbounds double, double* %3722, i64 20
  %3743 = bitcast double* %3742 to <2 x double>*
  store <2 x double> %3720, <2 x double>* %3743, align 8, !tbaa !62
  %3744 = getelementptr inbounds double, double* %3722, i64 22
  %3745 = bitcast double* %3744 to <2 x double>*
  store <2 x double> %3721, <2 x double>* %3745, align 8, !tbaa !62
  %3746 = add <2 x i32> %vec.ind10646, <i32 -3, i32 -3>
  %3747 = add <2 x i32> %vec.ind10646, <i32 -1, i32 -1>
  %3748 = add <2 x i32> %vec.ind10646, <i32 1, i32 1>
  %3749 = add <2 x i32> %vec.ind10646, <i32 3, i32 3>
  %3750 = add <2 x i32> %vec.ind10646, <i32 5, i32 5>
  %3751 = add <2 x i32> %vec.ind10646, <i32 7, i32 7>
  %3752 = add <2 x i32> %vec.ind10646, <i32 9, i32 9>
  %3753 = add <2 x i32> %vec.ind10646, <i32 11, i32 11>
  %3754 = add <2 x i32> %vec.ind10646, <i32 13, i32 13>
  %3755 = add <2 x i32> %vec.ind10646, <i32 15, i32 15>
  %3756 = add <2 x i32> %vec.ind10646, <i32 17, i32 17>
  %3757 = add <2 x i32> %vec.ind10646, <i32 19, i32 19>
  %3758 = sitofp <2 x i32> %3746 to <2 x double>
  %3759 = sitofp <2 x i32> %3747 to <2 x double>
  %3760 = sitofp <2 x i32> %3748 to <2 x double>
  %3761 = sitofp <2 x i32> %3749 to <2 x double>
  %3762 = sitofp <2 x i32> %3750 to <2 x double>
  %3763 = sitofp <2 x i32> %3751 to <2 x double>
  %3764 = sitofp <2 x i32> %3752 to <2 x double>
  %3765 = sitofp <2 x i32> %3753 to <2 x double>
  %3766 = sitofp <2 x i32> %3754 to <2 x double>
  %3767 = sitofp <2 x i32> %3755 to <2 x double>
  %3768 = sitofp <2 x i32> %3756 to <2 x double>
  %3769 = sitofp <2 x i32> %3757 to <2 x double>
  %3770 = getelementptr inbounds double, double* %1797, i64 %index10628
  %3771 = bitcast double* %3770 to <2 x double>*
  store <2 x double> %3758, <2 x double>* %3771, align 8, !tbaa !62
  %3772 = getelementptr inbounds double, double* %3770, i64 2
  %3773 = bitcast double* %3772 to <2 x double>*
  store <2 x double> %3759, <2 x double>* %3773, align 8, !tbaa !62
  %3774 = getelementptr inbounds double, double* %3770, i64 4
  %3775 = bitcast double* %3774 to <2 x double>*
  store <2 x double> %3760, <2 x double>* %3775, align 8, !tbaa !62
  %3776 = getelementptr inbounds double, double* %3770, i64 6
  %3777 = bitcast double* %3776 to <2 x double>*
  store <2 x double> %3761, <2 x double>* %3777, align 8, !tbaa !62
  %3778 = getelementptr inbounds double, double* %3770, i64 8
  %3779 = bitcast double* %3778 to <2 x double>*
  store <2 x double> %3762, <2 x double>* %3779, align 8, !tbaa !62
  %3780 = getelementptr inbounds double, double* %3770, i64 10
  %3781 = bitcast double* %3780 to <2 x double>*
  store <2 x double> %3763, <2 x double>* %3781, align 8, !tbaa !62
  %3782 = getelementptr inbounds double, double* %3770, i64 12
  %3783 = bitcast double* %3782 to <2 x double>*
  store <2 x double> %3764, <2 x double>* %3783, align 8, !tbaa !62
  %3784 = getelementptr inbounds double, double* %3770, i64 14
  %3785 = bitcast double* %3784 to <2 x double>*
  store <2 x double> %3765, <2 x double>* %3785, align 8, !tbaa !62
  %3786 = getelementptr inbounds double, double* %3770, i64 16
  %3787 = bitcast double* %3786 to <2 x double>*
  store <2 x double> %3766, <2 x double>* %3787, align 8, !tbaa !62
  %3788 = getelementptr inbounds double, double* %3770, i64 18
  %3789 = bitcast double* %3788 to <2 x double>*
  store <2 x double> %3767, <2 x double>* %3789, align 8, !tbaa !62
  %3790 = getelementptr inbounds double, double* %3770, i64 20
  %3791 = bitcast double* %3790 to <2 x double>*
  store <2 x double> %3768, <2 x double>* %3791, align 8, !tbaa !62
  %3792 = getelementptr inbounds double, double* %3770, i64 22
  %3793 = bitcast double* %3792 to <2 x double>*
  store <2 x double> %3769, <2 x double>* %3793, align 8, !tbaa !62
  %index.next10629 = add nuw nsw i64 %index10628, 24
  %vec.ind.next10659 = add <2 x i32> %vec.ind10646, <i32 24, i32 24>
  %3794 = icmp eq i64 %index.next10629, 24984
  br i1 %3794, label %for.body.i8476, label %vector.body10624, !llvm.loop !95

for.body.i8476:                                   ; preds = %vector.body10624
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %3605, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %3606, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %3607, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %3608, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %3609, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %3610, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %3611, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %3612, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %3613, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %3614, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %3615, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %3616, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %3617, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %3618, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %3619, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %3620, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %3621, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %3622, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %3623, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %3624, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %3625, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %3626, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %3627, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %3628, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %3629, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %3630, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %3631, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %3632, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %3633, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %3634, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %3635, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %3636, align 8, !tbaa !62
  %3795 = shl nuw nsw i64 %indvars.iv9543, 3
  store i8* %call3733, i8** %3575, align 8
  store i8* %call3733, i8** %3576, align 8
  store i64 %3795, i64* %3577, align 8
  store i8* %call3735, i8** %3578, align 8
  store i8* %call3735, i8** %3579, align 8
  store i64 %3795, i64* %3580, align 8
  store i8* %call3736, i8** %3581, align 8
  store i8* %call3736, i8** %3582, align 8
  store i64 %3795, i64* %3583, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %3575, i8** nonnull %3576, i64* nonnull %3577, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  br label %for.cond4336.preheader

for.cond4336.preheader:                           ; preds = %for.body.i8476, %omp_offload.cont4353.5
  %tms4329.08963 = phi i32 [ 1, %for.body.i8476 ], [ %mul4359, %omp_offload.cont4353.5 ]
  %.capture_expr..casted4346.sroa.0.0.insert.ext = zext i32 %tms4329.08963 to i64
  store i64 %indvars.iv9543, i64* %3585, align 8
  store i64 %indvars.iv9543, i64* %3587, align 8
  store i8* %call3733, i8** %3588, align 8
  store i8* %call3733, i8** %3589, align 8
  store i8* %call3735, i8** %3590, align 8
  store i8* %call3735, i8** %3591, align 8
  store i8* %call3736, i8** %3592, align 8
  store i8* %call3736, i8** %3593, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3595, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3597, align 8
  store i64 32, i64* %3599, align 8
  store i64 32, i64* %3601, align 8
  %3796 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1213.region_id, i32 6, i8** nonnull %3584, i8** nonnull %3586, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4329.08963, i32 32) #6
  %3797 = icmp eq i32 %3796, 0
  br i1 %3797, label %omp_offload.cont4353, label %omp_offload.failed4352

omp_offload.failed4352:                           ; preds = %for.cond4336.preheader
  %3798 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %3799 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %3798, i32 %tms4329.08963, i32 32) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..194 to void (i32*, i32*, ...)*), i64 %indvars.iv9543, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4353

omp_offload.cont4353:                             ; preds = %for.cond4336.preheader, %omp_offload.failed4352
  store i64 %indvars.iv9543, i64* %3585, align 8
  store i64 %indvars.iv9543, i64* %3587, align 8
  store i8* %call3733, i8** %3588, align 8
  store i8* %call3733, i8** %3589, align 8
  store i8* %call3735, i8** %3590, align 8
  store i8* %call3735, i8** %3591, align 8
  store i8* %call3736, i8** %3592, align 8
  store i8* %call3736, i8** %3593, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3595, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3597, align 8
  store i64 64, i64* %3599, align 8
  store i64 64, i64* %3601, align 8
  %3800 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1213.region_id, i32 6, i8** nonnull %3584, i8** nonnull %3586, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4329.08963, i32 64) #6
  %3801 = icmp eq i32 %3800, 0
  br i1 %3801, label %omp_offload.cont4353.1, label %omp_offload.failed4352.1

for.cond4369.preheader:                           ; preds = %for.cond4369.preheader.preheader55, %for.cond.cleanup4372
  %times4362.08966 = phi i32 [ %inc4387, %for.cond.cleanup4372 ], [ 0, %for.cond4369.preheader.preheader55 ]
  br label %vector.body10561

vector.body10561:                                 ; preds = %for.cond4369.preheader, %vector.body10561
  %index10570 = phi i64 [ %index.next10571, %vector.body10561 ], [ 0, %for.cond4369.preheader ]
  %3802 = getelementptr inbounds double, double* %1796, i64 %index10570
  %3803 = bitcast double* %3802 to <2 x double>*
  %wide.load10588 = load <2 x double>, <2 x double>* %3803, align 8, !tbaa !62
  %3804 = getelementptr inbounds double, double* %3802, i64 2
  %3805 = bitcast double* %3804 to <2 x double>*
  %wide.load10589 = load <2 x double>, <2 x double>* %3805, align 8, !tbaa !62
  %3806 = getelementptr inbounds double, double* %3802, i64 4
  %3807 = bitcast double* %3806 to <2 x double>*
  %wide.load10590 = load <2 x double>, <2 x double>* %3807, align 8, !tbaa !62
  %3808 = getelementptr inbounds double, double* %3802, i64 6
  %3809 = bitcast double* %3808 to <2 x double>*
  %wide.load10591 = load <2 x double>, <2 x double>* %3809, align 8, !tbaa !62
  %3810 = getelementptr inbounds double, double* %3802, i64 8
  %3811 = bitcast double* %3810 to <2 x double>*
  %wide.load10592 = load <2 x double>, <2 x double>* %3811, align 8, !tbaa !62
  %3812 = getelementptr inbounds double, double* %3802, i64 10
  %3813 = bitcast double* %3812 to <2 x double>*
  %wide.load10593 = load <2 x double>, <2 x double>* %3813, align 8, !tbaa !62
  %3814 = getelementptr inbounds double, double* %3802, i64 12
  %3815 = bitcast double* %3814 to <2 x double>*
  %wide.load10594 = load <2 x double>, <2 x double>* %3815, align 8, !tbaa !62
  %3816 = getelementptr inbounds double, double* %3802, i64 14
  %3817 = bitcast double* %3816 to <2 x double>*
  %wide.load10595 = load <2 x double>, <2 x double>* %3817, align 8, !tbaa !62
  %3818 = getelementptr inbounds double, double* %3802, i64 16
  %3819 = bitcast double* %3818 to <2 x double>*
  %wide.load10596 = load <2 x double>, <2 x double>* %3819, align 8, !tbaa !62
  %3820 = getelementptr inbounds double, double* %3802, i64 18
  %3821 = bitcast double* %3820 to <2 x double>*
  %wide.load10597 = load <2 x double>, <2 x double>* %3821, align 8, !tbaa !62
  %3822 = getelementptr inbounds double, double* %3802, i64 20
  %3823 = bitcast double* %3822 to <2 x double>*
  %wide.load10598 = load <2 x double>, <2 x double>* %3823, align 8, !tbaa !62
  %3824 = getelementptr inbounds double, double* %3802, i64 22
  %3825 = bitcast double* %3824 to <2 x double>*
  %wide.load10599 = load <2 x double>, <2 x double>* %3825, align 8, !tbaa !62
  %3826 = getelementptr inbounds double, double* %1797, i64 %index10570
  %3827 = bitcast double* %3826 to <2 x double>*
  %wide.load10600 = load <2 x double>, <2 x double>* %3827, align 8, !tbaa !62
  %3828 = getelementptr inbounds double, double* %3826, i64 2
  %3829 = bitcast double* %3828 to <2 x double>*
  %wide.load10601 = load <2 x double>, <2 x double>* %3829, align 8, !tbaa !62
  %3830 = getelementptr inbounds double, double* %3826, i64 4
  %3831 = bitcast double* %3830 to <2 x double>*
  %wide.load10602 = load <2 x double>, <2 x double>* %3831, align 8, !tbaa !62
  %3832 = getelementptr inbounds double, double* %3826, i64 6
  %3833 = bitcast double* %3832 to <2 x double>*
  %wide.load10603 = load <2 x double>, <2 x double>* %3833, align 8, !tbaa !62
  %3834 = getelementptr inbounds double, double* %3826, i64 8
  %3835 = bitcast double* %3834 to <2 x double>*
  %wide.load10604 = load <2 x double>, <2 x double>* %3835, align 8, !tbaa !62
  %3836 = getelementptr inbounds double, double* %3826, i64 10
  %3837 = bitcast double* %3836 to <2 x double>*
  %wide.load10605 = load <2 x double>, <2 x double>* %3837, align 8, !tbaa !62
  %3838 = getelementptr inbounds double, double* %3826, i64 12
  %3839 = bitcast double* %3838 to <2 x double>*
  %wide.load10606 = load <2 x double>, <2 x double>* %3839, align 8, !tbaa !62
  %3840 = getelementptr inbounds double, double* %3826, i64 14
  %3841 = bitcast double* %3840 to <2 x double>*
  %wide.load10607 = load <2 x double>, <2 x double>* %3841, align 8, !tbaa !62
  %3842 = getelementptr inbounds double, double* %3826, i64 16
  %3843 = bitcast double* %3842 to <2 x double>*
  %wide.load10608 = load <2 x double>, <2 x double>* %3843, align 8, !tbaa !62
  %3844 = getelementptr inbounds double, double* %3826, i64 18
  %3845 = bitcast double* %3844 to <2 x double>*
  %wide.load10609 = load <2 x double>, <2 x double>* %3845, align 8, !tbaa !62
  %3846 = getelementptr inbounds double, double* %3826, i64 20
  %3847 = bitcast double* %3846 to <2 x double>*
  %wide.load10610 = load <2 x double>, <2 x double>* %3847, align 8, !tbaa !62
  %3848 = getelementptr inbounds double, double* %3826, i64 22
  %3849 = bitcast double* %3848 to <2 x double>*
  %wide.load10611 = load <2 x double>, <2 x double>* %3849, align 8, !tbaa !62
  %3850 = fadd <2 x double> %wide.load10588, %wide.load10600
  %3851 = fadd <2 x double> %wide.load10589, %wide.load10601
  %3852 = fadd <2 x double> %wide.load10590, %wide.load10602
  %3853 = fadd <2 x double> %wide.load10591, %wide.load10603
  %3854 = fadd <2 x double> %wide.load10592, %wide.load10604
  %3855 = fadd <2 x double> %wide.load10593, %wide.load10605
  %3856 = fadd <2 x double> %wide.load10594, %wide.load10606
  %3857 = fadd <2 x double> %wide.load10595, %wide.load10607
  %3858 = fadd <2 x double> %wide.load10596, %wide.load10608
  %3859 = fadd <2 x double> %wide.load10597, %wide.load10609
  %3860 = fadd <2 x double> %wide.load10598, %wide.load10610
  %3861 = fadd <2 x double> %wide.load10599, %wide.load10611
  %3862 = getelementptr inbounds double, double* %1795, i64 %index10570
  %3863 = bitcast double* %3862 to <2 x double>*
  %wide.load10612 = load <2 x double>, <2 x double>* %3863, align 8, !tbaa !62
  %3864 = getelementptr inbounds double, double* %3862, i64 2
  %3865 = bitcast double* %3864 to <2 x double>*
  %wide.load10613 = load <2 x double>, <2 x double>* %3865, align 8, !tbaa !62
  %3866 = getelementptr inbounds double, double* %3862, i64 4
  %3867 = bitcast double* %3866 to <2 x double>*
  %wide.load10614 = load <2 x double>, <2 x double>* %3867, align 8, !tbaa !62
  %3868 = getelementptr inbounds double, double* %3862, i64 6
  %3869 = bitcast double* %3868 to <2 x double>*
  %wide.load10615 = load <2 x double>, <2 x double>* %3869, align 8, !tbaa !62
  %3870 = getelementptr inbounds double, double* %3862, i64 8
  %3871 = bitcast double* %3870 to <2 x double>*
  %wide.load10616 = load <2 x double>, <2 x double>* %3871, align 8, !tbaa !62
  %3872 = getelementptr inbounds double, double* %3862, i64 10
  %3873 = bitcast double* %3872 to <2 x double>*
  %wide.load10617 = load <2 x double>, <2 x double>* %3873, align 8, !tbaa !62
  %3874 = getelementptr inbounds double, double* %3862, i64 12
  %3875 = bitcast double* %3874 to <2 x double>*
  %wide.load10618 = load <2 x double>, <2 x double>* %3875, align 8, !tbaa !62
  %3876 = getelementptr inbounds double, double* %3862, i64 14
  %3877 = bitcast double* %3876 to <2 x double>*
  %wide.load10619 = load <2 x double>, <2 x double>* %3877, align 8, !tbaa !62
  %3878 = getelementptr inbounds double, double* %3862, i64 16
  %3879 = bitcast double* %3878 to <2 x double>*
  %wide.load10620 = load <2 x double>, <2 x double>* %3879, align 8, !tbaa !62
  %3880 = getelementptr inbounds double, double* %3862, i64 18
  %3881 = bitcast double* %3880 to <2 x double>*
  %wide.load10621 = load <2 x double>, <2 x double>* %3881, align 8, !tbaa !62
  %3882 = getelementptr inbounds double, double* %3862, i64 20
  %3883 = bitcast double* %3882 to <2 x double>*
  %wide.load10622 = load <2 x double>, <2 x double>* %3883, align 8, !tbaa !62
  %3884 = getelementptr inbounds double, double* %3862, i64 22
  %3885 = bitcast double* %3884 to <2 x double>*
  %wide.load10623 = load <2 x double>, <2 x double>* %3885, align 8, !tbaa !62
  %3886 = fadd <2 x double> %3850, %wide.load10612
  %3887 = fadd <2 x double> %3851, %wide.load10613
  %3888 = fadd <2 x double> %3852, %wide.load10614
  %3889 = fadd <2 x double> %3853, %wide.load10615
  %3890 = fadd <2 x double> %3854, %wide.load10616
  %3891 = fadd <2 x double> %3855, %wide.load10617
  %3892 = fadd <2 x double> %3856, %wide.load10618
  %3893 = fadd <2 x double> %3857, %wide.load10619
  %3894 = fadd <2 x double> %3858, %wide.load10620
  %3895 = fadd <2 x double> %3859, %wide.load10621
  %3896 = fadd <2 x double> %3860, %wide.load10622
  %3897 = fadd <2 x double> %3861, %wide.load10623
  store <2 x double> %3886, <2 x double>* %3863, align 8, !tbaa !62
  store <2 x double> %3887, <2 x double>* %3865, align 8, !tbaa !62
  store <2 x double> %3888, <2 x double>* %3867, align 8, !tbaa !62
  store <2 x double> %3889, <2 x double>* %3869, align 8, !tbaa !62
  store <2 x double> %3890, <2 x double>* %3871, align 8, !tbaa !62
  store <2 x double> %3891, <2 x double>* %3873, align 8, !tbaa !62
  store <2 x double> %3892, <2 x double>* %3875, align 8, !tbaa !62
  store <2 x double> %3893, <2 x double>* %3877, align 8, !tbaa !62
  store <2 x double> %3894, <2 x double>* %3879, align 8, !tbaa !62
  store <2 x double> %3895, <2 x double>* %3881, align 8, !tbaa !62
  store <2 x double> %3896, <2 x double>* %3883, align 8, !tbaa !62
  store <2 x double> %3897, <2 x double>* %3885, align 8, !tbaa !62
  %index.next10571 = add nuw nsw i64 %index10570, 24
  %3898 = icmp eq i64 %index.next10571, %n.vec10569
  br i1 %3898, label %middle.block10562, label %vector.body10561, !llvm.loop !96

middle.block10562:                                ; preds = %vector.body10561
  br i1 %cmp.n10573, label %for.cond.cleanup4372, label %for.body4373

for.cond.cleanup4366:                             ; preds = %for.cond.cleanup4372
  store i8* %call3733, i8** %3602, align 8
  store i8* %call3733, i8** %3603, align 8
  store i64 %3795, i64* %3604, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %3602, i8** nonnull %3603, i64* nonnull %3604, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body4399

for.cond.cleanup4372:                             ; preds = %for.body4373, %middle.block10562
  %inc4387 = add nuw nsw i32 %times4362.08966, 1
  %exitcond9540 = icmp eq i32 %inc4387, 54
  br i1 %exitcond9540, label %for.cond.cleanup4366, label %for.cond4369.preheader

for.body4373:                                     ; preds = %middle.block10562, %for.body4373
  %indvars.iv9537 = phi i64 [ %indvars.iv.next9538, %for.body4373 ], [ %n.vec10569, %middle.block10562 ]
  %arrayidx4375 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9537
  %3899 = load double, double* %arrayidx4375, align 8, !tbaa !62
  %arrayidx4377 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9537
  %3900 = load double, double* %arrayidx4377, align 8, !tbaa !62
  %add4378 = fadd double %3899, %3900
  %arrayidx4380 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9537
  %3901 = load double, double* %arrayidx4380, align 8, !tbaa !62
  %add4381 = fadd double %3901, %add4378
  store double %add4381, double* %arrayidx4380, align 8, !tbaa !62
  %indvars.iv.next9538 = add nuw nsw i64 %indvars.iv9537, 1
  %exitcond9539 = icmp eq i64 %indvars.iv.next9538, %indvars.iv9543
  br i1 %exitcond9539, label %for.cond.cleanup4372, label %for.body4373, !llvm.loop !97

for.body4399:                                     ; preds = %for.inc4413.7, %for.cond.cleanup4366
  %indvars.iv9541 = phi i64 [ 0, %for.cond.cleanup4366 ], [ %indvars.iv.next9542.7, %for.inc4413.7 ]
  %arrayidx4401 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9541
  %3902 = load double, double* %arrayidx4401, align 8, !tbaa !62
  %arrayidx4403 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9541
  %3903 = load double, double* %arrayidx4403, align 8, !tbaa !62
  %cmp4404 = fcmp une double %3902, %3903
  br i1 %cmp4404, label %cleanup4423, label %for.inc4413

for.inc4413:                                      ; preds = %for.body4399
  %indvars.iv.next9542 = or i64 %indvars.iv9541, 1
  %arrayidx4401.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9542
  %3904 = load double, double* %arrayidx4401.1, align 8, !tbaa !62
  %arrayidx4403.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9542
  %3905 = load double, double* %arrayidx4403.1, align 8, !tbaa !62
  %cmp4404.1 = fcmp une double %3904, %3905
  br i1 %cmp4404.1, label %cleanup4423, label %for.inc4413.1

for.inc4421:                                      ; preds = %for.inc4413.7
  %indvars.iv.next9544 = add nuw nsw i64 %indvars.iv9543, 5000
  %cmp4318 = icmp ult i64 %indvars.iv.next9544, 25000
  %indvar.next10565 = add nuw nsw i64 %indvar10564, 1
  br i1 %cmp4318, label %for.body.i8476.preheader, label %for.end4425

cleanup4423:                                      ; preds = %for.inc4413.6, %for.inc4413.5, %for.inc4413.4, %for.inc4413.3, %for.inc4413.2, %for.inc4413.1, %for.inc4413, %for.body4399
  %indvars.iv9541.lcssa = phi i64 [ %indvars.iv9541, %for.body4399 ], [ %indvars.iv.next9542, %for.inc4413 ], [ %indvars.iv.next9542.1, %for.inc4413.1 ], [ %indvars.iv.next9542.2, %for.inc4413.2 ], [ %indvars.iv.next9542.3, %for.inc4413.3 ], [ %indvars.iv.next9542.4, %for.inc4413.4 ], [ %indvars.iv.next9542.5, %for.inc4413.5 ], [ %indvars.iv.next9542.6, %for.inc4413.6 ]
  %.lcssa11834 = phi double [ %3902, %for.body4399 ], [ %3904, %for.inc4413 ], [ %7860, %for.inc4413.1 ], [ %7862, %for.inc4413.2 ], [ %7864, %for.inc4413.3 ], [ %7866, %for.inc4413.4 ], [ %7868, %for.inc4413.5 ], [ %7870, %for.inc4413.6 ]
  %.lcssa11832 = phi double [ %3903, %for.body4399 ], [ %3905, %for.inc4413 ], [ %7861, %for.inc4413.1 ], [ %7863, %for.inc4413.2 ], [ %7865, %for.inc4413.3 ], [ %7867, %for.inc4413.4 ], [ %7869, %for.inc4413.5 ], [ %7871, %for.inc4413.6 ]
  %3906 = trunc i64 %indvars.iv9543 to i32
  %3907 = trunc i64 %indvars.iv9541.lcssa to i32
  %call4411 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %3906, i32 signext %3907, double %.lcssa11834, double %.lcssa11832)
  br label %cleanup5832

for.end4425:                                      ; preds = %for.inc4421
  %puts8255 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8256 = call i32 @puts(i8* getelementptr inbounds ([27 x i8], [27 x i8]* @str.345, i64 0, i64 0))
  %3908 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4438, i64 0, i64 0
  %3909 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4439, i64 0, i64 0
  %3910 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4440, i64 0, i64 0
  %3911 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4438, i64 0, i64 1
  %3912 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4439, i64 0, i64 1
  %3913 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4440, i64 0, i64 1
  %3914 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4438, i64 0, i64 2
  %3915 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4439, i64 0, i64 2
  %3916 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4440, i64 0, i64 2
  %3917 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4471, i64 0, i64 0
  %3918 = bitcast [7 x i8*]* %.offload_baseptrs4471 to i64*
  %3919 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4472, i64 0, i64 0
  %3920 = bitcast [7 x i8*]* %.offload_ptrs4472 to i64*
  %3921 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4471, i64 0, i64 1
  %3922 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4472, i64 0, i64 1
  %3923 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4471, i64 0, i64 2
  %3924 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4472, i64 0, i64 2
  %3925 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4471, i64 0, i64 3
  %3926 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4472, i64 0, i64 3
  %3927 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4471, i64 0, i64 4
  %3928 = bitcast i8** %3927 to i64*
  %3929 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4472, i64 0, i64 4
  %3930 = bitcast i8** %3929 to i64*
  %3931 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4471, i64 0, i64 5
  %3932 = bitcast i8** %3931 to i64*
  %3933 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4472, i64 0, i64 5
  %3934 = bitcast i8** %3933 to i64*
  %3935 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4471, i64 0, i64 6
  %3936 = bitcast i8** %3935 to i64*
  %3937 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4472, i64 0, i64 6
  %3938 = bitcast i8** %3937 to i64*
  %3939 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs4516, i64 0, i64 0
  %3940 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs4517, i64 0, i64 0
  %3941 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes4518, i64 0, i64 0
  %3942 = bitcast i8* %arrayidx.i to <2 x double>*
  %3943 = bitcast i8* %arrayidx2.i to <2 x double>*
  %3944 = bitcast i8* %arrayidx5.i to <2 x double>*
  %3945 = bitcast i8* %arrayidx8.i to <2 x double>*
  %3946 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %3947 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %3948 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %3949 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %3950 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %3951 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %3952 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %3953 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %3954 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %3955 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %3956 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %3957 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %3958 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %3959 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %3960 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %3961 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %3962 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %3963 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %3964 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %3965 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %3966 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %3967 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %3968 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %3969 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %3970 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %3971 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %3972 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %3973 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8490.preheader

for.body.i8490.preheader:                         ; preds = %for.end4425, %for.inc4546
  %indvar10663 = phi i64 [ 0, %for.end4425 ], [ %indvar.next10664, %for.inc4546 ]
  %indvars.iv9532 = phi i64 [ 32, %for.end4425 ], [ %indvars.iv.next9533, %for.inc4546 ]
  %3974 = mul nuw nsw i64 %indvar10663, 5000
  br label %vector.body10723

vector.body10723:                                 ; preds = %vector.body10723, %for.body.i8490.preheader
  %index10727 = phi i64 [ 0, %for.body.i8490.preheader ], [ %index.next10728, %vector.body10723 ]
  %vec.ind10745 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8490.preheader ], [ %vec.ind.next10758, %vector.body10723 ]
  %step.add10746 = add <2 x i32> %vec.ind10745, <i32 2, i32 2>
  %step.add10747 = add <2 x i32> %vec.ind10745, <i32 4, i32 4>
  %step.add10748 = add <2 x i32> %vec.ind10745, <i32 6, i32 6>
  %step.add10749 = add <2 x i32> %vec.ind10745, <i32 8, i32 8>
  %step.add10750 = add <2 x i32> %vec.ind10745, <i32 10, i32 10>
  %step.add10751 = add <2 x i32> %vec.ind10745, <i32 12, i32 12>
  %step.add10752 = add <2 x i32> %vec.ind10745, <i32 14, i32 14>
  %step.add10753 = add <2 x i32> %vec.ind10745, <i32 16, i32 16>
  %step.add10754 = add <2 x i32> %vec.ind10745, <i32 18, i32 18>
  %step.add10755 = add <2 x i32> %vec.ind10745, <i32 20, i32 20>
  %step.add10756 = add <2 x i32> %vec.ind10745, <i32 22, i32 22>
  %3975 = sitofp <2 x i32> %vec.ind10745 to <2 x double>
  %3976 = sitofp <2 x i32> %step.add10746 to <2 x double>
  %3977 = sitofp <2 x i32> %step.add10747 to <2 x double>
  %3978 = sitofp <2 x i32> %step.add10748 to <2 x double>
  %3979 = sitofp <2 x i32> %step.add10749 to <2 x double>
  %3980 = sitofp <2 x i32> %step.add10750 to <2 x double>
  %3981 = sitofp <2 x i32> %step.add10751 to <2 x double>
  %3982 = sitofp <2 x i32> %step.add10752 to <2 x double>
  %3983 = sitofp <2 x i32> %step.add10753 to <2 x double>
  %3984 = sitofp <2 x i32> %step.add10754 to <2 x double>
  %3985 = sitofp <2 x i32> %step.add10755 to <2 x double>
  %3986 = sitofp <2 x i32> %step.add10756 to <2 x double>
  %3987 = getelementptr inbounds double, double* %1795, i64 %index10727
  %3988 = bitcast double* %3987 to <2 x double>*
  store <2 x double> %3975, <2 x double>* %3988, align 8, !tbaa !62
  %3989 = getelementptr inbounds double, double* %3987, i64 2
  %3990 = bitcast double* %3989 to <2 x double>*
  store <2 x double> %3976, <2 x double>* %3990, align 8, !tbaa !62
  %3991 = getelementptr inbounds double, double* %3987, i64 4
  %3992 = bitcast double* %3991 to <2 x double>*
  store <2 x double> %3977, <2 x double>* %3992, align 8, !tbaa !62
  %3993 = getelementptr inbounds double, double* %3987, i64 6
  %3994 = bitcast double* %3993 to <2 x double>*
  store <2 x double> %3978, <2 x double>* %3994, align 8, !tbaa !62
  %3995 = getelementptr inbounds double, double* %3987, i64 8
  %3996 = bitcast double* %3995 to <2 x double>*
  store <2 x double> %3979, <2 x double>* %3996, align 8, !tbaa !62
  %3997 = getelementptr inbounds double, double* %3987, i64 10
  %3998 = bitcast double* %3997 to <2 x double>*
  store <2 x double> %3980, <2 x double>* %3998, align 8, !tbaa !62
  %3999 = getelementptr inbounds double, double* %3987, i64 12
  %4000 = bitcast double* %3999 to <2 x double>*
  store <2 x double> %3981, <2 x double>* %4000, align 8, !tbaa !62
  %4001 = getelementptr inbounds double, double* %3987, i64 14
  %4002 = bitcast double* %4001 to <2 x double>*
  store <2 x double> %3982, <2 x double>* %4002, align 8, !tbaa !62
  %4003 = getelementptr inbounds double, double* %3987, i64 16
  %4004 = bitcast double* %4003 to <2 x double>*
  store <2 x double> %3983, <2 x double>* %4004, align 8, !tbaa !62
  %4005 = getelementptr inbounds double, double* %3987, i64 18
  %4006 = bitcast double* %4005 to <2 x double>*
  store <2 x double> %3984, <2 x double>* %4006, align 8, !tbaa !62
  %4007 = getelementptr inbounds double, double* %3987, i64 20
  %4008 = bitcast double* %4007 to <2 x double>*
  store <2 x double> %3985, <2 x double>* %4008, align 8, !tbaa !62
  %4009 = getelementptr inbounds double, double* %3987, i64 22
  %4010 = bitcast double* %4009 to <2 x double>*
  store <2 x double> %3986, <2 x double>* %4010, align 8, !tbaa !62
  %4011 = getelementptr inbounds double, double* %1794, i64 %index10727
  %4012 = bitcast double* %4011 to <2 x double>*
  store <2 x double> %3975, <2 x double>* %4012, align 8, !tbaa !62
  %4013 = getelementptr inbounds double, double* %4011, i64 2
  %4014 = bitcast double* %4013 to <2 x double>*
  store <2 x double> %3976, <2 x double>* %4014, align 8, !tbaa !62
  %4015 = getelementptr inbounds double, double* %4011, i64 4
  %4016 = bitcast double* %4015 to <2 x double>*
  store <2 x double> %3977, <2 x double>* %4016, align 8, !tbaa !62
  %4017 = getelementptr inbounds double, double* %4011, i64 6
  %4018 = bitcast double* %4017 to <2 x double>*
  store <2 x double> %3978, <2 x double>* %4018, align 8, !tbaa !62
  %4019 = getelementptr inbounds double, double* %4011, i64 8
  %4020 = bitcast double* %4019 to <2 x double>*
  store <2 x double> %3979, <2 x double>* %4020, align 8, !tbaa !62
  %4021 = getelementptr inbounds double, double* %4011, i64 10
  %4022 = bitcast double* %4021 to <2 x double>*
  store <2 x double> %3980, <2 x double>* %4022, align 8, !tbaa !62
  %4023 = getelementptr inbounds double, double* %4011, i64 12
  %4024 = bitcast double* %4023 to <2 x double>*
  store <2 x double> %3981, <2 x double>* %4024, align 8, !tbaa !62
  %4025 = getelementptr inbounds double, double* %4011, i64 14
  %4026 = bitcast double* %4025 to <2 x double>*
  store <2 x double> %3982, <2 x double>* %4026, align 8, !tbaa !62
  %4027 = getelementptr inbounds double, double* %4011, i64 16
  %4028 = bitcast double* %4027 to <2 x double>*
  store <2 x double> %3983, <2 x double>* %4028, align 8, !tbaa !62
  %4029 = getelementptr inbounds double, double* %4011, i64 18
  %4030 = bitcast double* %4029 to <2 x double>*
  store <2 x double> %3984, <2 x double>* %4030, align 8, !tbaa !62
  %4031 = getelementptr inbounds double, double* %4011, i64 20
  %4032 = bitcast double* %4031 to <2 x double>*
  store <2 x double> %3985, <2 x double>* %4032, align 8, !tbaa !62
  %4033 = getelementptr inbounds double, double* %4011, i64 22
  %4034 = bitcast double* %4033 to <2 x double>*
  store <2 x double> %3986, <2 x double>* %4034, align 8, !tbaa !62
  %4035 = shl <2 x i32> %vec.ind10745, <i32 1, i32 1>
  %4036 = shl <2 x i32> %step.add10746, <i32 1, i32 1>
  %4037 = shl <2 x i32> %step.add10747, <i32 1, i32 1>
  %4038 = shl <2 x i32> %step.add10748, <i32 1, i32 1>
  %4039 = shl <2 x i32> %step.add10749, <i32 1, i32 1>
  %4040 = shl <2 x i32> %step.add10750, <i32 1, i32 1>
  %4041 = shl <2 x i32> %step.add10751, <i32 1, i32 1>
  %4042 = shl <2 x i32> %step.add10752, <i32 1, i32 1>
  %4043 = shl <2 x i32> %step.add10753, <i32 1, i32 1>
  %4044 = shl <2 x i32> %step.add10754, <i32 1, i32 1>
  %4045 = shl <2 x i32> %step.add10755, <i32 1, i32 1>
  %4046 = shl <2 x i32> %step.add10756, <i32 1, i32 1>
  %4047 = sitofp <2 x i32> %4035 to <2 x double>
  %4048 = sitofp <2 x i32> %4036 to <2 x double>
  %4049 = sitofp <2 x i32> %4037 to <2 x double>
  %4050 = sitofp <2 x i32> %4038 to <2 x double>
  %4051 = sitofp <2 x i32> %4039 to <2 x double>
  %4052 = sitofp <2 x i32> %4040 to <2 x double>
  %4053 = sitofp <2 x i32> %4041 to <2 x double>
  %4054 = sitofp <2 x i32> %4042 to <2 x double>
  %4055 = sitofp <2 x i32> %4043 to <2 x double>
  %4056 = sitofp <2 x i32> %4044 to <2 x double>
  %4057 = sitofp <2 x i32> %4045 to <2 x double>
  %4058 = sitofp <2 x i32> %4046 to <2 x double>
  %4059 = getelementptr inbounds double, double* %1796, i64 %index10727
  %4060 = bitcast double* %4059 to <2 x double>*
  store <2 x double> %4047, <2 x double>* %4060, align 8, !tbaa !62
  %4061 = getelementptr inbounds double, double* %4059, i64 2
  %4062 = bitcast double* %4061 to <2 x double>*
  store <2 x double> %4048, <2 x double>* %4062, align 8, !tbaa !62
  %4063 = getelementptr inbounds double, double* %4059, i64 4
  %4064 = bitcast double* %4063 to <2 x double>*
  store <2 x double> %4049, <2 x double>* %4064, align 8, !tbaa !62
  %4065 = getelementptr inbounds double, double* %4059, i64 6
  %4066 = bitcast double* %4065 to <2 x double>*
  store <2 x double> %4050, <2 x double>* %4066, align 8, !tbaa !62
  %4067 = getelementptr inbounds double, double* %4059, i64 8
  %4068 = bitcast double* %4067 to <2 x double>*
  store <2 x double> %4051, <2 x double>* %4068, align 8, !tbaa !62
  %4069 = getelementptr inbounds double, double* %4059, i64 10
  %4070 = bitcast double* %4069 to <2 x double>*
  store <2 x double> %4052, <2 x double>* %4070, align 8, !tbaa !62
  %4071 = getelementptr inbounds double, double* %4059, i64 12
  %4072 = bitcast double* %4071 to <2 x double>*
  store <2 x double> %4053, <2 x double>* %4072, align 8, !tbaa !62
  %4073 = getelementptr inbounds double, double* %4059, i64 14
  %4074 = bitcast double* %4073 to <2 x double>*
  store <2 x double> %4054, <2 x double>* %4074, align 8, !tbaa !62
  %4075 = getelementptr inbounds double, double* %4059, i64 16
  %4076 = bitcast double* %4075 to <2 x double>*
  store <2 x double> %4055, <2 x double>* %4076, align 8, !tbaa !62
  %4077 = getelementptr inbounds double, double* %4059, i64 18
  %4078 = bitcast double* %4077 to <2 x double>*
  store <2 x double> %4056, <2 x double>* %4078, align 8, !tbaa !62
  %4079 = getelementptr inbounds double, double* %4059, i64 20
  %4080 = bitcast double* %4079 to <2 x double>*
  store <2 x double> %4057, <2 x double>* %4080, align 8, !tbaa !62
  %4081 = getelementptr inbounds double, double* %4059, i64 22
  %4082 = bitcast double* %4081 to <2 x double>*
  store <2 x double> %4058, <2 x double>* %4082, align 8, !tbaa !62
  %4083 = add <2 x i32> %vec.ind10745, <i32 -3, i32 -3>
  %4084 = add <2 x i32> %vec.ind10745, <i32 -1, i32 -1>
  %4085 = add <2 x i32> %vec.ind10745, <i32 1, i32 1>
  %4086 = add <2 x i32> %vec.ind10745, <i32 3, i32 3>
  %4087 = add <2 x i32> %vec.ind10745, <i32 5, i32 5>
  %4088 = add <2 x i32> %vec.ind10745, <i32 7, i32 7>
  %4089 = add <2 x i32> %vec.ind10745, <i32 9, i32 9>
  %4090 = add <2 x i32> %vec.ind10745, <i32 11, i32 11>
  %4091 = add <2 x i32> %vec.ind10745, <i32 13, i32 13>
  %4092 = add <2 x i32> %vec.ind10745, <i32 15, i32 15>
  %4093 = add <2 x i32> %vec.ind10745, <i32 17, i32 17>
  %4094 = add <2 x i32> %vec.ind10745, <i32 19, i32 19>
  %4095 = sitofp <2 x i32> %4083 to <2 x double>
  %4096 = sitofp <2 x i32> %4084 to <2 x double>
  %4097 = sitofp <2 x i32> %4085 to <2 x double>
  %4098 = sitofp <2 x i32> %4086 to <2 x double>
  %4099 = sitofp <2 x i32> %4087 to <2 x double>
  %4100 = sitofp <2 x i32> %4088 to <2 x double>
  %4101 = sitofp <2 x i32> %4089 to <2 x double>
  %4102 = sitofp <2 x i32> %4090 to <2 x double>
  %4103 = sitofp <2 x i32> %4091 to <2 x double>
  %4104 = sitofp <2 x i32> %4092 to <2 x double>
  %4105 = sitofp <2 x i32> %4093 to <2 x double>
  %4106 = sitofp <2 x i32> %4094 to <2 x double>
  %4107 = getelementptr inbounds double, double* %1797, i64 %index10727
  %4108 = bitcast double* %4107 to <2 x double>*
  store <2 x double> %4095, <2 x double>* %4108, align 8, !tbaa !62
  %4109 = getelementptr inbounds double, double* %4107, i64 2
  %4110 = bitcast double* %4109 to <2 x double>*
  store <2 x double> %4096, <2 x double>* %4110, align 8, !tbaa !62
  %4111 = getelementptr inbounds double, double* %4107, i64 4
  %4112 = bitcast double* %4111 to <2 x double>*
  store <2 x double> %4097, <2 x double>* %4112, align 8, !tbaa !62
  %4113 = getelementptr inbounds double, double* %4107, i64 6
  %4114 = bitcast double* %4113 to <2 x double>*
  store <2 x double> %4098, <2 x double>* %4114, align 8, !tbaa !62
  %4115 = getelementptr inbounds double, double* %4107, i64 8
  %4116 = bitcast double* %4115 to <2 x double>*
  store <2 x double> %4099, <2 x double>* %4116, align 8, !tbaa !62
  %4117 = getelementptr inbounds double, double* %4107, i64 10
  %4118 = bitcast double* %4117 to <2 x double>*
  store <2 x double> %4100, <2 x double>* %4118, align 8, !tbaa !62
  %4119 = getelementptr inbounds double, double* %4107, i64 12
  %4120 = bitcast double* %4119 to <2 x double>*
  store <2 x double> %4101, <2 x double>* %4120, align 8, !tbaa !62
  %4121 = getelementptr inbounds double, double* %4107, i64 14
  %4122 = bitcast double* %4121 to <2 x double>*
  store <2 x double> %4102, <2 x double>* %4122, align 8, !tbaa !62
  %4123 = getelementptr inbounds double, double* %4107, i64 16
  %4124 = bitcast double* %4123 to <2 x double>*
  store <2 x double> %4103, <2 x double>* %4124, align 8, !tbaa !62
  %4125 = getelementptr inbounds double, double* %4107, i64 18
  %4126 = bitcast double* %4125 to <2 x double>*
  store <2 x double> %4104, <2 x double>* %4126, align 8, !tbaa !62
  %4127 = getelementptr inbounds double, double* %4107, i64 20
  %4128 = bitcast double* %4127 to <2 x double>*
  store <2 x double> %4105, <2 x double>* %4128, align 8, !tbaa !62
  %4129 = getelementptr inbounds double, double* %4107, i64 22
  %4130 = bitcast double* %4129 to <2 x double>*
  store <2 x double> %4106, <2 x double>* %4130, align 8, !tbaa !62
  %index.next10728 = add nuw nsw i64 %index10727, 24
  %vec.ind.next10758 = add <2 x i32> %vec.ind10745, <i32 24, i32 24>
  %4131 = icmp eq i64 %index.next10728, 24984
  br i1 %4131, label %for.body.i8490, label %vector.body10723, !llvm.loop !98

for.body.i8490:                                   ; preds = %vector.body10723
  %4132 = add nuw nsw i64 %3974, 32
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %3942, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %3943, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %3944, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %3945, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %3946, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %3947, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %3948, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %3949, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %3950, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %3951, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %3952, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %3953, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %3954, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %3955, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %3956, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %3957, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %3958, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %3959, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %3960, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %3961, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %3962, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %3963, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %3964, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %3965, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %3966, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %3967, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %3968, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %3969, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %3970, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %3971, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %3972, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %3973, align 8, !tbaa !62
  %4133 = shl nuw nsw i64 %indvars.iv9532, 3
  store i8* %call3733, i8** %3908, align 8
  store i8* %call3733, i8** %3909, align 8
  store i64 %4133, i64* %3910, align 8
  store i8* %call3735, i8** %3911, align 8
  store i8* %call3735, i8** %3912, align 8
  store i64 %4133, i64* %3913, align 8
  store i8* %call3736, i8** %3914, align 8
  store i8* %call3736, i8** %3915, align 8
  store i64 %4133, i64* %3916, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %3908, i8** nonnull %3909, i64* nonnull %3910, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  %cmp44558945 = icmp ult i64 %indvars.iv9532, 128
  br i1 %cmp44558945, label %for.cond.cleanup4491, label %for.cond4448.preheader

for.cond4488.preheader:                           ; preds = %for.cond.cleanup4451
  %cmp44898956 = icmp sgt i32 %t4434.28946, -1
  br i1 %cmp44898956, label %for.cond4494.preheader.preheader52, label %for.cond.cleanup4491

for.cond4494.preheader.preheader52:               ; preds = %for.cond4488.preheader
  %n.mod.vf10667 = urem i64 %4132, 24
  %n.vec10668 = sub nuw nsw i64 %4132, %n.mod.vf10667
  %cmp.n10672 = icmp eq i64 %n.mod.vf10667, 0
  br label %for.cond4494.preheader

for.cond4448.preheader:                           ; preds = %for.body.i8490, %for.cond.cleanup4451
  %tms4441.08953 = phi i32 [ %mul4484, %for.cond.cleanup4451 ], [ 1, %for.body.i8490 ]
  %t4434.08952 = phi i32 [ %inc4459, %for.cond.cleanup4451 ], [ 0, %for.body.i8490 ]
  %.capture_expr..casted4467.sroa.0.0.insert.ext = zext i32 %tms4441.08953 to i64
  br label %for.cond4454.preheader

for.cond4454.preheader:                           ; preds = %for.cond4448.preheader, %for.cond.cleanup4457
  %ths4447.08950 = phi i32 [ %mul4480, %for.cond.cleanup4457 ], [ 32, %for.cond4448.preheader ]
  %t4434.18949 = phi i32 [ %inc4459, %for.cond.cleanup4457 ], [ %t4434.08952, %for.cond4448.preheader ]
  %.capture_expr..casted4469.sroa.0.0.insert.ext = zext i32 %ths4447.08950 to i64
  br label %for.body4458

for.cond.cleanup4451:                             ; preds = %for.cond.cleanup4457
  %mul4484 = shl nsw i32 %tms4441.08953, 1
  %cmp4443 = icmp ult i32 %mul4484, 257
  br i1 %cmp4443, label %for.cond4448.preheader, label %for.cond4488.preheader

for.cond.cleanup4457:                             ; preds = %omp_offload.cont4474
  %mul4480 = shl nsw i32 %ths4447.08950, 1
  %cmp4449 = icmp ult i32 %mul4480, 1025
  br i1 %cmp4449, label %for.cond4454.preheader, label %for.cond.cleanup4451

for.body4458:                                     ; preds = %for.cond4454.preheader, %omp_offload.cont4474
  %sch4453.08947 = phi i32 [ 128, %for.cond4454.preheader ], [ %mul4476, %omp_offload.cont4474 ]
  %t4434.28946 = phi i32 [ %t4434.18949, %for.cond4454.preheader ], [ %inc4459, %omp_offload.cont4474 ]
  %inc4459 = add nsw i32 %t4434.28946, 1
  %.capture_expr..casted4465.sroa.0.0.insert.ext = zext i32 %sch4453.08947 to i64
  store i64 %indvars.iv9532, i64* %3918, align 8
  store i64 %indvars.iv9532, i64* %3920, align 8
  store i8* %call3733, i8** %3921, align 8
  store i8* %call3733, i8** %3922, align 8
  store i8* %call3735, i8** %3923, align 8
  store i8* %call3735, i8** %3924, align 8
  store i8* %call3736, i8** %3925, align 8
  store i8* %call3736, i8** %3926, align 8
  store i64 %.capture_expr..casted4465.sroa.0.0.insert.ext, i64* %3928, align 8
  store i64 %.capture_expr..casted4465.sroa.0.0.insert.ext, i64* %3930, align 8
  store i64 %.capture_expr..casted4467.sroa.0.0.insert.ext, i64* %3932, align 8
  store i64 %.capture_expr..casted4467.sroa.0.0.insert.ext, i64* %3934, align 8
  store i64 %.capture_expr..casted4469.sroa.0.0.insert.ext, i64* %3936, align 8
  store i64 %.capture_expr..casted4469.sroa.0.0.insert.ext, i64* %3938, align 8
  %4134 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1248.region_id, i32 7, i8** nonnull %3917, i8** nonnull %3919, i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_sizes.273, i64 0, i64 0), i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_maptypes.274, i64 0, i64 0), i32 %tms4441.08953, i32 %ths4447.08950) #6
  %4135 = icmp eq i32 %4134, 0
  br i1 %4135, label %omp_offload.cont4474, label %omp_offload.failed4473

omp_offload.failed4473:                           ; preds = %for.body4458
  %4136 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %4137 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %4136, i32 %tms4441.08953, i32 %ths4447.08950) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 5, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64)* @.omp_outlined..201 to void (i32*, i32*, ...)*), i64 %indvars.iv9532, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted4465.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont4474

omp_offload.cont4474:                             ; preds = %for.body4458, %omp_offload.failed4473
  %mul4476 = mul nsw i32 %sch4453.08947, 10000
  %4138 = zext i32 %mul4476 to i64
  %cmp4455 = icmp ult i64 %indvars.iv9532, %4138
  br i1 %cmp4455, label %for.cond.cleanup4457, label %for.body4458

for.cond4494.preheader:                           ; preds = %for.cond4494.preheader.preheader52, %for.cond.cleanup4497
  %times4487.08957 = phi i32 [ %inc4512, %for.cond.cleanup4497 ], [ 0, %for.cond4494.preheader.preheader52 ]
  br label %vector.body10660

vector.body10660:                                 ; preds = %for.cond4494.preheader, %vector.body10660
  %index10669 = phi i64 [ %index.next10670, %vector.body10660 ], [ 0, %for.cond4494.preheader ]
  %4139 = getelementptr inbounds double, double* %1796, i64 %index10669
  %4140 = bitcast double* %4139 to <2 x double>*
  %wide.load10687 = load <2 x double>, <2 x double>* %4140, align 8, !tbaa !62
  %4141 = getelementptr inbounds double, double* %4139, i64 2
  %4142 = bitcast double* %4141 to <2 x double>*
  %wide.load10688 = load <2 x double>, <2 x double>* %4142, align 8, !tbaa !62
  %4143 = getelementptr inbounds double, double* %4139, i64 4
  %4144 = bitcast double* %4143 to <2 x double>*
  %wide.load10689 = load <2 x double>, <2 x double>* %4144, align 8, !tbaa !62
  %4145 = getelementptr inbounds double, double* %4139, i64 6
  %4146 = bitcast double* %4145 to <2 x double>*
  %wide.load10690 = load <2 x double>, <2 x double>* %4146, align 8, !tbaa !62
  %4147 = getelementptr inbounds double, double* %4139, i64 8
  %4148 = bitcast double* %4147 to <2 x double>*
  %wide.load10691 = load <2 x double>, <2 x double>* %4148, align 8, !tbaa !62
  %4149 = getelementptr inbounds double, double* %4139, i64 10
  %4150 = bitcast double* %4149 to <2 x double>*
  %wide.load10692 = load <2 x double>, <2 x double>* %4150, align 8, !tbaa !62
  %4151 = getelementptr inbounds double, double* %4139, i64 12
  %4152 = bitcast double* %4151 to <2 x double>*
  %wide.load10693 = load <2 x double>, <2 x double>* %4152, align 8, !tbaa !62
  %4153 = getelementptr inbounds double, double* %4139, i64 14
  %4154 = bitcast double* %4153 to <2 x double>*
  %wide.load10694 = load <2 x double>, <2 x double>* %4154, align 8, !tbaa !62
  %4155 = getelementptr inbounds double, double* %4139, i64 16
  %4156 = bitcast double* %4155 to <2 x double>*
  %wide.load10695 = load <2 x double>, <2 x double>* %4156, align 8, !tbaa !62
  %4157 = getelementptr inbounds double, double* %4139, i64 18
  %4158 = bitcast double* %4157 to <2 x double>*
  %wide.load10696 = load <2 x double>, <2 x double>* %4158, align 8, !tbaa !62
  %4159 = getelementptr inbounds double, double* %4139, i64 20
  %4160 = bitcast double* %4159 to <2 x double>*
  %wide.load10697 = load <2 x double>, <2 x double>* %4160, align 8, !tbaa !62
  %4161 = getelementptr inbounds double, double* %4139, i64 22
  %4162 = bitcast double* %4161 to <2 x double>*
  %wide.load10698 = load <2 x double>, <2 x double>* %4162, align 8, !tbaa !62
  %4163 = getelementptr inbounds double, double* %1797, i64 %index10669
  %4164 = bitcast double* %4163 to <2 x double>*
  %wide.load10699 = load <2 x double>, <2 x double>* %4164, align 8, !tbaa !62
  %4165 = getelementptr inbounds double, double* %4163, i64 2
  %4166 = bitcast double* %4165 to <2 x double>*
  %wide.load10700 = load <2 x double>, <2 x double>* %4166, align 8, !tbaa !62
  %4167 = getelementptr inbounds double, double* %4163, i64 4
  %4168 = bitcast double* %4167 to <2 x double>*
  %wide.load10701 = load <2 x double>, <2 x double>* %4168, align 8, !tbaa !62
  %4169 = getelementptr inbounds double, double* %4163, i64 6
  %4170 = bitcast double* %4169 to <2 x double>*
  %wide.load10702 = load <2 x double>, <2 x double>* %4170, align 8, !tbaa !62
  %4171 = getelementptr inbounds double, double* %4163, i64 8
  %4172 = bitcast double* %4171 to <2 x double>*
  %wide.load10703 = load <2 x double>, <2 x double>* %4172, align 8, !tbaa !62
  %4173 = getelementptr inbounds double, double* %4163, i64 10
  %4174 = bitcast double* %4173 to <2 x double>*
  %wide.load10704 = load <2 x double>, <2 x double>* %4174, align 8, !tbaa !62
  %4175 = getelementptr inbounds double, double* %4163, i64 12
  %4176 = bitcast double* %4175 to <2 x double>*
  %wide.load10705 = load <2 x double>, <2 x double>* %4176, align 8, !tbaa !62
  %4177 = getelementptr inbounds double, double* %4163, i64 14
  %4178 = bitcast double* %4177 to <2 x double>*
  %wide.load10706 = load <2 x double>, <2 x double>* %4178, align 8, !tbaa !62
  %4179 = getelementptr inbounds double, double* %4163, i64 16
  %4180 = bitcast double* %4179 to <2 x double>*
  %wide.load10707 = load <2 x double>, <2 x double>* %4180, align 8, !tbaa !62
  %4181 = getelementptr inbounds double, double* %4163, i64 18
  %4182 = bitcast double* %4181 to <2 x double>*
  %wide.load10708 = load <2 x double>, <2 x double>* %4182, align 8, !tbaa !62
  %4183 = getelementptr inbounds double, double* %4163, i64 20
  %4184 = bitcast double* %4183 to <2 x double>*
  %wide.load10709 = load <2 x double>, <2 x double>* %4184, align 8, !tbaa !62
  %4185 = getelementptr inbounds double, double* %4163, i64 22
  %4186 = bitcast double* %4185 to <2 x double>*
  %wide.load10710 = load <2 x double>, <2 x double>* %4186, align 8, !tbaa !62
  %4187 = fadd <2 x double> %wide.load10687, %wide.load10699
  %4188 = fadd <2 x double> %wide.load10688, %wide.load10700
  %4189 = fadd <2 x double> %wide.load10689, %wide.load10701
  %4190 = fadd <2 x double> %wide.load10690, %wide.load10702
  %4191 = fadd <2 x double> %wide.load10691, %wide.load10703
  %4192 = fadd <2 x double> %wide.load10692, %wide.load10704
  %4193 = fadd <2 x double> %wide.load10693, %wide.load10705
  %4194 = fadd <2 x double> %wide.load10694, %wide.load10706
  %4195 = fadd <2 x double> %wide.load10695, %wide.load10707
  %4196 = fadd <2 x double> %wide.load10696, %wide.load10708
  %4197 = fadd <2 x double> %wide.load10697, %wide.load10709
  %4198 = fadd <2 x double> %wide.load10698, %wide.load10710
  %4199 = getelementptr inbounds double, double* %1795, i64 %index10669
  %4200 = bitcast double* %4199 to <2 x double>*
  %wide.load10711 = load <2 x double>, <2 x double>* %4200, align 8, !tbaa !62
  %4201 = getelementptr inbounds double, double* %4199, i64 2
  %4202 = bitcast double* %4201 to <2 x double>*
  %wide.load10712 = load <2 x double>, <2 x double>* %4202, align 8, !tbaa !62
  %4203 = getelementptr inbounds double, double* %4199, i64 4
  %4204 = bitcast double* %4203 to <2 x double>*
  %wide.load10713 = load <2 x double>, <2 x double>* %4204, align 8, !tbaa !62
  %4205 = getelementptr inbounds double, double* %4199, i64 6
  %4206 = bitcast double* %4205 to <2 x double>*
  %wide.load10714 = load <2 x double>, <2 x double>* %4206, align 8, !tbaa !62
  %4207 = getelementptr inbounds double, double* %4199, i64 8
  %4208 = bitcast double* %4207 to <2 x double>*
  %wide.load10715 = load <2 x double>, <2 x double>* %4208, align 8, !tbaa !62
  %4209 = getelementptr inbounds double, double* %4199, i64 10
  %4210 = bitcast double* %4209 to <2 x double>*
  %wide.load10716 = load <2 x double>, <2 x double>* %4210, align 8, !tbaa !62
  %4211 = getelementptr inbounds double, double* %4199, i64 12
  %4212 = bitcast double* %4211 to <2 x double>*
  %wide.load10717 = load <2 x double>, <2 x double>* %4212, align 8, !tbaa !62
  %4213 = getelementptr inbounds double, double* %4199, i64 14
  %4214 = bitcast double* %4213 to <2 x double>*
  %wide.load10718 = load <2 x double>, <2 x double>* %4214, align 8, !tbaa !62
  %4215 = getelementptr inbounds double, double* %4199, i64 16
  %4216 = bitcast double* %4215 to <2 x double>*
  %wide.load10719 = load <2 x double>, <2 x double>* %4216, align 8, !tbaa !62
  %4217 = getelementptr inbounds double, double* %4199, i64 18
  %4218 = bitcast double* %4217 to <2 x double>*
  %wide.load10720 = load <2 x double>, <2 x double>* %4218, align 8, !tbaa !62
  %4219 = getelementptr inbounds double, double* %4199, i64 20
  %4220 = bitcast double* %4219 to <2 x double>*
  %wide.load10721 = load <2 x double>, <2 x double>* %4220, align 8, !tbaa !62
  %4221 = getelementptr inbounds double, double* %4199, i64 22
  %4222 = bitcast double* %4221 to <2 x double>*
  %wide.load10722 = load <2 x double>, <2 x double>* %4222, align 8, !tbaa !62
  %4223 = fadd <2 x double> %4187, %wide.load10711
  %4224 = fadd <2 x double> %4188, %wide.load10712
  %4225 = fadd <2 x double> %4189, %wide.load10713
  %4226 = fadd <2 x double> %4190, %wide.load10714
  %4227 = fadd <2 x double> %4191, %wide.load10715
  %4228 = fadd <2 x double> %4192, %wide.load10716
  %4229 = fadd <2 x double> %4193, %wide.load10717
  %4230 = fadd <2 x double> %4194, %wide.load10718
  %4231 = fadd <2 x double> %4195, %wide.load10719
  %4232 = fadd <2 x double> %4196, %wide.load10720
  %4233 = fadd <2 x double> %4197, %wide.load10721
  %4234 = fadd <2 x double> %4198, %wide.load10722
  store <2 x double> %4223, <2 x double>* %4200, align 8, !tbaa !62
  store <2 x double> %4224, <2 x double>* %4202, align 8, !tbaa !62
  store <2 x double> %4225, <2 x double>* %4204, align 8, !tbaa !62
  store <2 x double> %4226, <2 x double>* %4206, align 8, !tbaa !62
  store <2 x double> %4227, <2 x double>* %4208, align 8, !tbaa !62
  store <2 x double> %4228, <2 x double>* %4210, align 8, !tbaa !62
  store <2 x double> %4229, <2 x double>* %4212, align 8, !tbaa !62
  store <2 x double> %4230, <2 x double>* %4214, align 8, !tbaa !62
  store <2 x double> %4231, <2 x double>* %4216, align 8, !tbaa !62
  store <2 x double> %4232, <2 x double>* %4218, align 8, !tbaa !62
  store <2 x double> %4233, <2 x double>* %4220, align 8, !tbaa !62
  store <2 x double> %4234, <2 x double>* %4222, align 8, !tbaa !62
  %index.next10670 = add nuw nsw i64 %index10669, 24
  %4235 = icmp eq i64 %index.next10670, %n.vec10668
  br i1 %4235, label %middle.block10661, label %vector.body10660, !llvm.loop !99

middle.block10661:                                ; preds = %vector.body10660
  br i1 %cmp.n10672, label %for.cond.cleanup4497, label %for.body4498

for.cond.cleanup4491:                             ; preds = %for.cond.cleanup4497, %for.body.i8490, %for.cond4488.preheader
  store i8* %call3733, i8** %3939, align 8
  store i8* %call3733, i8** %3940, align 8
  store i64 %4133, i64* %3941, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %3939, i8** nonnull %3940, i64* nonnull %3941, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body4524

for.cond.cleanup4497:                             ; preds = %for.body4498, %middle.block10661
  %inc4512 = add nuw nsw i32 %times4487.08957, 1
  %exitcond9529 = icmp eq i32 %inc4512, %inc4459
  br i1 %exitcond9529, label %for.cond.cleanup4491, label %for.cond4494.preheader

for.body4498:                                     ; preds = %middle.block10661, %for.body4498
  %indvars.iv9526 = phi i64 [ %indvars.iv.next9527, %for.body4498 ], [ %n.vec10668, %middle.block10661 ]
  %arrayidx4500 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9526
  %4236 = load double, double* %arrayidx4500, align 8, !tbaa !62
  %arrayidx4502 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9526
  %4237 = load double, double* %arrayidx4502, align 8, !tbaa !62
  %add4503 = fadd double %4236, %4237
  %arrayidx4505 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9526
  %4238 = load double, double* %arrayidx4505, align 8, !tbaa !62
  %add4506 = fadd double %4238, %add4503
  store double %add4506, double* %arrayidx4505, align 8, !tbaa !62
  %indvars.iv.next9527 = add nuw nsw i64 %indvars.iv9526, 1
  %exitcond9528 = icmp eq i64 %indvars.iv.next9527, %indvars.iv9532
  br i1 %exitcond9528, label %for.cond.cleanup4497, label %for.body4498, !llvm.loop !100

for.body4524:                                     ; preds = %for.inc4538.7, %for.cond.cleanup4491
  %indvars.iv9530 = phi i64 [ 0, %for.cond.cleanup4491 ], [ %indvars.iv.next9531.7, %for.inc4538.7 ]
  %arrayidx4526 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9530
  %4239 = load double, double* %arrayidx4526, align 8, !tbaa !62
  %arrayidx4528 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9530
  %4240 = load double, double* %arrayidx4528, align 8, !tbaa !62
  %cmp4529 = fcmp une double %4239, %4240
  br i1 %cmp4529, label %cleanup4548, label %for.inc4538

for.inc4538:                                      ; preds = %for.body4524
  %indvars.iv.next9531 = or i64 %indvars.iv9530, 1
  %arrayidx4526.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9531
  %4241 = load double, double* %arrayidx4526.1, align 8, !tbaa !62
  %arrayidx4528.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9531
  %4242 = load double, double* %arrayidx4528.1, align 8, !tbaa !62
  %cmp4529.1 = fcmp une double %4241, %4242
  br i1 %cmp4529.1, label %cleanup4548, label %for.inc4538.1

for.inc4546:                                      ; preds = %for.inc4538.7
  %indvars.iv.next9533 = add nuw nsw i64 %indvars.iv9532, 5000
  %cmp4430 = icmp ult i64 %indvars.iv.next9533, 25000
  %indvar.next10664 = add nuw nsw i64 %indvar10663, 1
  br i1 %cmp4430, label %for.body.i8490.preheader, label %for.end4550

cleanup4548:                                      ; preds = %for.inc4538.6, %for.inc4538.5, %for.inc4538.4, %for.inc4538.3, %for.inc4538.2, %for.inc4538.1, %for.inc4538, %for.body4524
  %indvars.iv9530.lcssa = phi i64 [ %indvars.iv9530, %for.body4524 ], [ %indvars.iv.next9531, %for.inc4538 ], [ %indvars.iv.next9531.1, %for.inc4538.1 ], [ %indvars.iv.next9531.2, %for.inc4538.2 ], [ %indvars.iv.next9531.3, %for.inc4538.3 ], [ %indvars.iv.next9531.4, %for.inc4538.4 ], [ %indvars.iv.next9531.5, %for.inc4538.5 ], [ %indvars.iv.next9531.6, %for.inc4538.6 ]
  %.lcssa11828 = phi double [ %4239, %for.body4524 ], [ %4241, %for.inc4538 ], [ %7848, %for.inc4538.1 ], [ %7850, %for.inc4538.2 ], [ %7852, %for.inc4538.3 ], [ %7854, %for.inc4538.4 ], [ %7856, %for.inc4538.5 ], [ %7858, %for.inc4538.6 ]
  %.lcssa11826 = phi double [ %4240, %for.body4524 ], [ %4242, %for.inc4538 ], [ %7849, %for.inc4538.1 ], [ %7851, %for.inc4538.2 ], [ %7853, %for.inc4538.3 ], [ %7855, %for.inc4538.4 ], [ %7857, %for.inc4538.5 ], [ %7859, %for.inc4538.6 ]
  %4243 = trunc i64 %indvars.iv9532 to i32
  %4244 = trunc i64 %indvars.iv9530.lcssa to i32
  %call4536 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %4243, i32 signext %4244, double %.lcssa11828, double %.lcssa11826)
  br label %cleanup5832

for.end4550:                                      ; preds = %for.inc4546
  %puts8257 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8258 = call i32 @puts(i8* getelementptr inbounds ([56 x i8], [56 x i8]* @str.347, i64 0, i64 0))
  %4245 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4562, i64 0, i64 0
  %4246 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4563, i64 0, i64 0
  %4247 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4564, i64 0, i64 0
  %4248 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4562, i64 0, i64 1
  %4249 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4563, i64 0, i64 1
  %4250 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4564, i64 0, i64 1
  %4251 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4562, i64 0, i64 2
  %4252 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4563, i64 0, i64 2
  %4253 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4564, i64 0, i64 2
  %4254 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4587, i64 0, i64 0
  %4255 = bitcast [6 x i8*]* %.offload_baseptrs4587 to i64*
  %4256 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4588, i64 0, i64 0
  %4257 = bitcast [6 x i8*]* %.offload_ptrs4588 to i64*
  %4258 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4587, i64 0, i64 1
  %4259 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4588, i64 0, i64 1
  %4260 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4587, i64 0, i64 2
  %4261 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4588, i64 0, i64 2
  %4262 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4587, i64 0, i64 3
  %4263 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4588, i64 0, i64 3
  %4264 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4587, i64 0, i64 4
  %4265 = bitcast i8** %4264 to i64*
  %4266 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4588, i64 0, i64 4
  %4267 = bitcast i8** %4266 to i64*
  %4268 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_baseptrs4587, i64 0, i64 5
  %4269 = bitcast i8** %4268 to i64*
  %4270 = getelementptr inbounds [6 x i8*], [6 x i8*]* %.offload_ptrs4588, i64 0, i64 5
  %4271 = bitcast i8** %4270 to i64*
  %4272 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs4628, i64 0, i64 0
  %4273 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs4629, i64 0, i64 0
  %4274 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes4630, i64 0, i64 0
  %4275 = bitcast i8* %arrayidx.i to <2 x double>*
  %4276 = bitcast i8* %arrayidx2.i to <2 x double>*
  %4277 = bitcast i8* %arrayidx5.i to <2 x double>*
  %4278 = bitcast i8* %arrayidx8.i to <2 x double>*
  %4279 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %4280 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %4281 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %4282 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %4283 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %4284 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %4285 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %4286 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %4287 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %4288 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %4289 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %4290 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %4291 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %4292 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %4293 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %4294 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %4295 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %4296 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %4297 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %4298 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %4299 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %4300 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %4301 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %4302 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %4303 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %4304 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %4305 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %4306 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8505.preheader

for.body.i8505.preheader:                         ; preds = %for.end4550, %for.inc4658
  %indvar10762 = phi i64 [ 0, %for.end4550 ], [ %indvar.next10763, %for.inc4658 ]
  %indvars.iv9524 = phi i64 [ 32, %for.end4550 ], [ %indvars.iv.next9525, %for.inc4658 ]
  %4307 = mul nuw nsw i64 %indvar10762, 5000
  br label %vector.body10822

vector.body10822:                                 ; preds = %vector.body10822, %for.body.i8505.preheader
  %index10826 = phi i64 [ 0, %for.body.i8505.preheader ], [ %index.next10827, %vector.body10822 ]
  %vec.ind10844 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8505.preheader ], [ %vec.ind.next10857, %vector.body10822 ]
  %step.add10845 = add <2 x i32> %vec.ind10844, <i32 2, i32 2>
  %step.add10846 = add <2 x i32> %vec.ind10844, <i32 4, i32 4>
  %step.add10847 = add <2 x i32> %vec.ind10844, <i32 6, i32 6>
  %step.add10848 = add <2 x i32> %vec.ind10844, <i32 8, i32 8>
  %step.add10849 = add <2 x i32> %vec.ind10844, <i32 10, i32 10>
  %step.add10850 = add <2 x i32> %vec.ind10844, <i32 12, i32 12>
  %step.add10851 = add <2 x i32> %vec.ind10844, <i32 14, i32 14>
  %step.add10852 = add <2 x i32> %vec.ind10844, <i32 16, i32 16>
  %step.add10853 = add <2 x i32> %vec.ind10844, <i32 18, i32 18>
  %step.add10854 = add <2 x i32> %vec.ind10844, <i32 20, i32 20>
  %step.add10855 = add <2 x i32> %vec.ind10844, <i32 22, i32 22>
  %4308 = sitofp <2 x i32> %vec.ind10844 to <2 x double>
  %4309 = sitofp <2 x i32> %step.add10845 to <2 x double>
  %4310 = sitofp <2 x i32> %step.add10846 to <2 x double>
  %4311 = sitofp <2 x i32> %step.add10847 to <2 x double>
  %4312 = sitofp <2 x i32> %step.add10848 to <2 x double>
  %4313 = sitofp <2 x i32> %step.add10849 to <2 x double>
  %4314 = sitofp <2 x i32> %step.add10850 to <2 x double>
  %4315 = sitofp <2 x i32> %step.add10851 to <2 x double>
  %4316 = sitofp <2 x i32> %step.add10852 to <2 x double>
  %4317 = sitofp <2 x i32> %step.add10853 to <2 x double>
  %4318 = sitofp <2 x i32> %step.add10854 to <2 x double>
  %4319 = sitofp <2 x i32> %step.add10855 to <2 x double>
  %4320 = getelementptr inbounds double, double* %1795, i64 %index10826
  %4321 = bitcast double* %4320 to <2 x double>*
  store <2 x double> %4308, <2 x double>* %4321, align 8, !tbaa !62
  %4322 = getelementptr inbounds double, double* %4320, i64 2
  %4323 = bitcast double* %4322 to <2 x double>*
  store <2 x double> %4309, <2 x double>* %4323, align 8, !tbaa !62
  %4324 = getelementptr inbounds double, double* %4320, i64 4
  %4325 = bitcast double* %4324 to <2 x double>*
  store <2 x double> %4310, <2 x double>* %4325, align 8, !tbaa !62
  %4326 = getelementptr inbounds double, double* %4320, i64 6
  %4327 = bitcast double* %4326 to <2 x double>*
  store <2 x double> %4311, <2 x double>* %4327, align 8, !tbaa !62
  %4328 = getelementptr inbounds double, double* %4320, i64 8
  %4329 = bitcast double* %4328 to <2 x double>*
  store <2 x double> %4312, <2 x double>* %4329, align 8, !tbaa !62
  %4330 = getelementptr inbounds double, double* %4320, i64 10
  %4331 = bitcast double* %4330 to <2 x double>*
  store <2 x double> %4313, <2 x double>* %4331, align 8, !tbaa !62
  %4332 = getelementptr inbounds double, double* %4320, i64 12
  %4333 = bitcast double* %4332 to <2 x double>*
  store <2 x double> %4314, <2 x double>* %4333, align 8, !tbaa !62
  %4334 = getelementptr inbounds double, double* %4320, i64 14
  %4335 = bitcast double* %4334 to <2 x double>*
  store <2 x double> %4315, <2 x double>* %4335, align 8, !tbaa !62
  %4336 = getelementptr inbounds double, double* %4320, i64 16
  %4337 = bitcast double* %4336 to <2 x double>*
  store <2 x double> %4316, <2 x double>* %4337, align 8, !tbaa !62
  %4338 = getelementptr inbounds double, double* %4320, i64 18
  %4339 = bitcast double* %4338 to <2 x double>*
  store <2 x double> %4317, <2 x double>* %4339, align 8, !tbaa !62
  %4340 = getelementptr inbounds double, double* %4320, i64 20
  %4341 = bitcast double* %4340 to <2 x double>*
  store <2 x double> %4318, <2 x double>* %4341, align 8, !tbaa !62
  %4342 = getelementptr inbounds double, double* %4320, i64 22
  %4343 = bitcast double* %4342 to <2 x double>*
  store <2 x double> %4319, <2 x double>* %4343, align 8, !tbaa !62
  %4344 = getelementptr inbounds double, double* %1794, i64 %index10826
  %4345 = bitcast double* %4344 to <2 x double>*
  store <2 x double> %4308, <2 x double>* %4345, align 8, !tbaa !62
  %4346 = getelementptr inbounds double, double* %4344, i64 2
  %4347 = bitcast double* %4346 to <2 x double>*
  store <2 x double> %4309, <2 x double>* %4347, align 8, !tbaa !62
  %4348 = getelementptr inbounds double, double* %4344, i64 4
  %4349 = bitcast double* %4348 to <2 x double>*
  store <2 x double> %4310, <2 x double>* %4349, align 8, !tbaa !62
  %4350 = getelementptr inbounds double, double* %4344, i64 6
  %4351 = bitcast double* %4350 to <2 x double>*
  store <2 x double> %4311, <2 x double>* %4351, align 8, !tbaa !62
  %4352 = getelementptr inbounds double, double* %4344, i64 8
  %4353 = bitcast double* %4352 to <2 x double>*
  store <2 x double> %4312, <2 x double>* %4353, align 8, !tbaa !62
  %4354 = getelementptr inbounds double, double* %4344, i64 10
  %4355 = bitcast double* %4354 to <2 x double>*
  store <2 x double> %4313, <2 x double>* %4355, align 8, !tbaa !62
  %4356 = getelementptr inbounds double, double* %4344, i64 12
  %4357 = bitcast double* %4356 to <2 x double>*
  store <2 x double> %4314, <2 x double>* %4357, align 8, !tbaa !62
  %4358 = getelementptr inbounds double, double* %4344, i64 14
  %4359 = bitcast double* %4358 to <2 x double>*
  store <2 x double> %4315, <2 x double>* %4359, align 8, !tbaa !62
  %4360 = getelementptr inbounds double, double* %4344, i64 16
  %4361 = bitcast double* %4360 to <2 x double>*
  store <2 x double> %4316, <2 x double>* %4361, align 8, !tbaa !62
  %4362 = getelementptr inbounds double, double* %4344, i64 18
  %4363 = bitcast double* %4362 to <2 x double>*
  store <2 x double> %4317, <2 x double>* %4363, align 8, !tbaa !62
  %4364 = getelementptr inbounds double, double* %4344, i64 20
  %4365 = bitcast double* %4364 to <2 x double>*
  store <2 x double> %4318, <2 x double>* %4365, align 8, !tbaa !62
  %4366 = getelementptr inbounds double, double* %4344, i64 22
  %4367 = bitcast double* %4366 to <2 x double>*
  store <2 x double> %4319, <2 x double>* %4367, align 8, !tbaa !62
  %4368 = shl <2 x i32> %vec.ind10844, <i32 1, i32 1>
  %4369 = shl <2 x i32> %step.add10845, <i32 1, i32 1>
  %4370 = shl <2 x i32> %step.add10846, <i32 1, i32 1>
  %4371 = shl <2 x i32> %step.add10847, <i32 1, i32 1>
  %4372 = shl <2 x i32> %step.add10848, <i32 1, i32 1>
  %4373 = shl <2 x i32> %step.add10849, <i32 1, i32 1>
  %4374 = shl <2 x i32> %step.add10850, <i32 1, i32 1>
  %4375 = shl <2 x i32> %step.add10851, <i32 1, i32 1>
  %4376 = shl <2 x i32> %step.add10852, <i32 1, i32 1>
  %4377 = shl <2 x i32> %step.add10853, <i32 1, i32 1>
  %4378 = shl <2 x i32> %step.add10854, <i32 1, i32 1>
  %4379 = shl <2 x i32> %step.add10855, <i32 1, i32 1>
  %4380 = sitofp <2 x i32> %4368 to <2 x double>
  %4381 = sitofp <2 x i32> %4369 to <2 x double>
  %4382 = sitofp <2 x i32> %4370 to <2 x double>
  %4383 = sitofp <2 x i32> %4371 to <2 x double>
  %4384 = sitofp <2 x i32> %4372 to <2 x double>
  %4385 = sitofp <2 x i32> %4373 to <2 x double>
  %4386 = sitofp <2 x i32> %4374 to <2 x double>
  %4387 = sitofp <2 x i32> %4375 to <2 x double>
  %4388 = sitofp <2 x i32> %4376 to <2 x double>
  %4389 = sitofp <2 x i32> %4377 to <2 x double>
  %4390 = sitofp <2 x i32> %4378 to <2 x double>
  %4391 = sitofp <2 x i32> %4379 to <2 x double>
  %4392 = getelementptr inbounds double, double* %1796, i64 %index10826
  %4393 = bitcast double* %4392 to <2 x double>*
  store <2 x double> %4380, <2 x double>* %4393, align 8, !tbaa !62
  %4394 = getelementptr inbounds double, double* %4392, i64 2
  %4395 = bitcast double* %4394 to <2 x double>*
  store <2 x double> %4381, <2 x double>* %4395, align 8, !tbaa !62
  %4396 = getelementptr inbounds double, double* %4392, i64 4
  %4397 = bitcast double* %4396 to <2 x double>*
  store <2 x double> %4382, <2 x double>* %4397, align 8, !tbaa !62
  %4398 = getelementptr inbounds double, double* %4392, i64 6
  %4399 = bitcast double* %4398 to <2 x double>*
  store <2 x double> %4383, <2 x double>* %4399, align 8, !tbaa !62
  %4400 = getelementptr inbounds double, double* %4392, i64 8
  %4401 = bitcast double* %4400 to <2 x double>*
  store <2 x double> %4384, <2 x double>* %4401, align 8, !tbaa !62
  %4402 = getelementptr inbounds double, double* %4392, i64 10
  %4403 = bitcast double* %4402 to <2 x double>*
  store <2 x double> %4385, <2 x double>* %4403, align 8, !tbaa !62
  %4404 = getelementptr inbounds double, double* %4392, i64 12
  %4405 = bitcast double* %4404 to <2 x double>*
  store <2 x double> %4386, <2 x double>* %4405, align 8, !tbaa !62
  %4406 = getelementptr inbounds double, double* %4392, i64 14
  %4407 = bitcast double* %4406 to <2 x double>*
  store <2 x double> %4387, <2 x double>* %4407, align 8, !tbaa !62
  %4408 = getelementptr inbounds double, double* %4392, i64 16
  %4409 = bitcast double* %4408 to <2 x double>*
  store <2 x double> %4388, <2 x double>* %4409, align 8, !tbaa !62
  %4410 = getelementptr inbounds double, double* %4392, i64 18
  %4411 = bitcast double* %4410 to <2 x double>*
  store <2 x double> %4389, <2 x double>* %4411, align 8, !tbaa !62
  %4412 = getelementptr inbounds double, double* %4392, i64 20
  %4413 = bitcast double* %4412 to <2 x double>*
  store <2 x double> %4390, <2 x double>* %4413, align 8, !tbaa !62
  %4414 = getelementptr inbounds double, double* %4392, i64 22
  %4415 = bitcast double* %4414 to <2 x double>*
  store <2 x double> %4391, <2 x double>* %4415, align 8, !tbaa !62
  %4416 = add <2 x i32> %vec.ind10844, <i32 -3, i32 -3>
  %4417 = add <2 x i32> %vec.ind10844, <i32 -1, i32 -1>
  %4418 = add <2 x i32> %vec.ind10844, <i32 1, i32 1>
  %4419 = add <2 x i32> %vec.ind10844, <i32 3, i32 3>
  %4420 = add <2 x i32> %vec.ind10844, <i32 5, i32 5>
  %4421 = add <2 x i32> %vec.ind10844, <i32 7, i32 7>
  %4422 = add <2 x i32> %vec.ind10844, <i32 9, i32 9>
  %4423 = add <2 x i32> %vec.ind10844, <i32 11, i32 11>
  %4424 = add <2 x i32> %vec.ind10844, <i32 13, i32 13>
  %4425 = add <2 x i32> %vec.ind10844, <i32 15, i32 15>
  %4426 = add <2 x i32> %vec.ind10844, <i32 17, i32 17>
  %4427 = add <2 x i32> %vec.ind10844, <i32 19, i32 19>
  %4428 = sitofp <2 x i32> %4416 to <2 x double>
  %4429 = sitofp <2 x i32> %4417 to <2 x double>
  %4430 = sitofp <2 x i32> %4418 to <2 x double>
  %4431 = sitofp <2 x i32> %4419 to <2 x double>
  %4432 = sitofp <2 x i32> %4420 to <2 x double>
  %4433 = sitofp <2 x i32> %4421 to <2 x double>
  %4434 = sitofp <2 x i32> %4422 to <2 x double>
  %4435 = sitofp <2 x i32> %4423 to <2 x double>
  %4436 = sitofp <2 x i32> %4424 to <2 x double>
  %4437 = sitofp <2 x i32> %4425 to <2 x double>
  %4438 = sitofp <2 x i32> %4426 to <2 x double>
  %4439 = sitofp <2 x i32> %4427 to <2 x double>
  %4440 = getelementptr inbounds double, double* %1797, i64 %index10826
  %4441 = bitcast double* %4440 to <2 x double>*
  store <2 x double> %4428, <2 x double>* %4441, align 8, !tbaa !62
  %4442 = getelementptr inbounds double, double* %4440, i64 2
  %4443 = bitcast double* %4442 to <2 x double>*
  store <2 x double> %4429, <2 x double>* %4443, align 8, !tbaa !62
  %4444 = getelementptr inbounds double, double* %4440, i64 4
  %4445 = bitcast double* %4444 to <2 x double>*
  store <2 x double> %4430, <2 x double>* %4445, align 8, !tbaa !62
  %4446 = getelementptr inbounds double, double* %4440, i64 6
  %4447 = bitcast double* %4446 to <2 x double>*
  store <2 x double> %4431, <2 x double>* %4447, align 8, !tbaa !62
  %4448 = getelementptr inbounds double, double* %4440, i64 8
  %4449 = bitcast double* %4448 to <2 x double>*
  store <2 x double> %4432, <2 x double>* %4449, align 8, !tbaa !62
  %4450 = getelementptr inbounds double, double* %4440, i64 10
  %4451 = bitcast double* %4450 to <2 x double>*
  store <2 x double> %4433, <2 x double>* %4451, align 8, !tbaa !62
  %4452 = getelementptr inbounds double, double* %4440, i64 12
  %4453 = bitcast double* %4452 to <2 x double>*
  store <2 x double> %4434, <2 x double>* %4453, align 8, !tbaa !62
  %4454 = getelementptr inbounds double, double* %4440, i64 14
  %4455 = bitcast double* %4454 to <2 x double>*
  store <2 x double> %4435, <2 x double>* %4455, align 8, !tbaa !62
  %4456 = getelementptr inbounds double, double* %4440, i64 16
  %4457 = bitcast double* %4456 to <2 x double>*
  store <2 x double> %4436, <2 x double>* %4457, align 8, !tbaa !62
  %4458 = getelementptr inbounds double, double* %4440, i64 18
  %4459 = bitcast double* %4458 to <2 x double>*
  store <2 x double> %4437, <2 x double>* %4459, align 8, !tbaa !62
  %4460 = getelementptr inbounds double, double* %4440, i64 20
  %4461 = bitcast double* %4460 to <2 x double>*
  store <2 x double> %4438, <2 x double>* %4461, align 8, !tbaa !62
  %4462 = getelementptr inbounds double, double* %4440, i64 22
  %4463 = bitcast double* %4462 to <2 x double>*
  store <2 x double> %4439, <2 x double>* %4463, align 8, !tbaa !62
  %index.next10827 = add nuw nsw i64 %index10826, 24
  %vec.ind.next10857 = add <2 x i32> %vec.ind10844, <i32 24, i32 24>
  %4464 = icmp eq i64 %index.next10827, 24984
  br i1 %4464, label %for.body.i8505, label %vector.body10822, !llvm.loop !101

for.body.i8505:                                   ; preds = %vector.body10822
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %4275, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %4276, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %4277, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %4278, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %4279, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %4280, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %4281, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %4282, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %4283, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %4284, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %4285, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %4286, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %4287, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %4288, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %4289, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %4290, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %4291, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %4292, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %4293, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %4294, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %4295, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %4296, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %4297, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %4298, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %4299, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %4300, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %4301, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %4302, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %4303, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %4304, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %4305, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %4306, align 8, !tbaa !62
  %4465 = shl nuw nsw i64 %indvars.iv9524, 3
  store i8* %call3733, i8** %4245, align 8
  store i8* %call3733, i8** %4246, align 8
  store i64 %4465, i64* %4247, align 8
  store i8* %call3735, i8** %4248, align 8
  store i8* %call3735, i8** %4249, align 8
  store i64 %4465, i64* %4250, align 8
  store i8* %call3736, i8** %4251, align 8
  store i8* %call3736, i8** %4252, align 8
  store i64 %4465, i64* %4253, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %4245, i8** nonnull %4246, i64* nonnull %4247, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  br label %for.cond4573.preheader

for.cond4573.preheader:                           ; preds = %for.body.i8505, %omp_offload.cont4590.5
  %tms4566.08939 = phi i32 [ 1, %for.body.i8505 ], [ %mul4596, %omp_offload.cont4590.5 ]
  %.capture_expr..casted4583.sroa.0.0.insert.ext = zext i32 %tms4566.08939 to i64
  store i64 %indvars.iv9524, i64* %4255, align 8
  store i64 %indvars.iv9524, i64* %4257, align 8
  store i8* %call3733, i8** %4258, align 8
  store i8* %call3733, i8** %4259, align 8
  store i8* %call3735, i8** %4260, align 8
  store i8* %call3735, i8** %4261, align 8
  store i8* %call3736, i8** %4262, align 8
  store i8* %call3736, i8** %4263, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4265, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4267, align 8
  store i64 32, i64* %4269, align 8
  store i64 32, i64* %4271, align 8
  %4466 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1283.region_id, i32 6, i8** nonnull %4254, i8** nonnull %4256, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4566.08939, i32 32) #6
  %4467 = icmp eq i32 %4466, 0
  br i1 %4467, label %omp_offload.cont4590, label %omp_offload.failed4589

omp_offload.failed4589:                           ; preds = %for.cond4573.preheader
  %4468 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %4469 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %4468, i32 %tms4566.08939, i32 32) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..208 to void (i32*, i32*, ...)*), i64 %indvars.iv9524, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4590

omp_offload.cont4590:                             ; preds = %for.cond4573.preheader, %omp_offload.failed4589
  store i64 %indvars.iv9524, i64* %4255, align 8
  store i64 %indvars.iv9524, i64* %4257, align 8
  store i8* %call3733, i8** %4258, align 8
  store i8* %call3733, i8** %4259, align 8
  store i8* %call3735, i8** %4260, align 8
  store i8* %call3735, i8** %4261, align 8
  store i8* %call3736, i8** %4262, align 8
  store i8* %call3736, i8** %4263, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4265, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4267, align 8
  store i64 64, i64* %4269, align 8
  store i64 64, i64* %4271, align 8
  %4470 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1283.region_id, i32 6, i8** nonnull %4254, i8** nonnull %4256, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4566.08939, i32 64) #6
  %4471 = icmp eq i32 %4470, 0
  br i1 %4471, label %omp_offload.cont4590.1, label %omp_offload.failed4589.1

for.cond4606.preheader:                           ; preds = %for.cond4606.preheader.preheader50, %for.cond.cleanup4609
  %times4599.08942 = phi i32 [ %inc4624, %for.cond.cleanup4609 ], [ 0, %for.cond4606.preheader.preheader50 ]
  br label %vector.body10759

vector.body10759:                                 ; preds = %for.cond4606.preheader, %vector.body10759
  %index10768 = phi i64 [ %index.next10769, %vector.body10759 ], [ 0, %for.cond4606.preheader ]
  %4472 = getelementptr inbounds double, double* %1796, i64 %index10768
  %4473 = bitcast double* %4472 to <2 x double>*
  %wide.load10786 = load <2 x double>, <2 x double>* %4473, align 8, !tbaa !62
  %4474 = getelementptr inbounds double, double* %4472, i64 2
  %4475 = bitcast double* %4474 to <2 x double>*
  %wide.load10787 = load <2 x double>, <2 x double>* %4475, align 8, !tbaa !62
  %4476 = getelementptr inbounds double, double* %4472, i64 4
  %4477 = bitcast double* %4476 to <2 x double>*
  %wide.load10788 = load <2 x double>, <2 x double>* %4477, align 8, !tbaa !62
  %4478 = getelementptr inbounds double, double* %4472, i64 6
  %4479 = bitcast double* %4478 to <2 x double>*
  %wide.load10789 = load <2 x double>, <2 x double>* %4479, align 8, !tbaa !62
  %4480 = getelementptr inbounds double, double* %4472, i64 8
  %4481 = bitcast double* %4480 to <2 x double>*
  %wide.load10790 = load <2 x double>, <2 x double>* %4481, align 8, !tbaa !62
  %4482 = getelementptr inbounds double, double* %4472, i64 10
  %4483 = bitcast double* %4482 to <2 x double>*
  %wide.load10791 = load <2 x double>, <2 x double>* %4483, align 8, !tbaa !62
  %4484 = getelementptr inbounds double, double* %4472, i64 12
  %4485 = bitcast double* %4484 to <2 x double>*
  %wide.load10792 = load <2 x double>, <2 x double>* %4485, align 8, !tbaa !62
  %4486 = getelementptr inbounds double, double* %4472, i64 14
  %4487 = bitcast double* %4486 to <2 x double>*
  %wide.load10793 = load <2 x double>, <2 x double>* %4487, align 8, !tbaa !62
  %4488 = getelementptr inbounds double, double* %4472, i64 16
  %4489 = bitcast double* %4488 to <2 x double>*
  %wide.load10794 = load <2 x double>, <2 x double>* %4489, align 8, !tbaa !62
  %4490 = getelementptr inbounds double, double* %4472, i64 18
  %4491 = bitcast double* %4490 to <2 x double>*
  %wide.load10795 = load <2 x double>, <2 x double>* %4491, align 8, !tbaa !62
  %4492 = getelementptr inbounds double, double* %4472, i64 20
  %4493 = bitcast double* %4492 to <2 x double>*
  %wide.load10796 = load <2 x double>, <2 x double>* %4493, align 8, !tbaa !62
  %4494 = getelementptr inbounds double, double* %4472, i64 22
  %4495 = bitcast double* %4494 to <2 x double>*
  %wide.load10797 = load <2 x double>, <2 x double>* %4495, align 8, !tbaa !62
  %4496 = getelementptr inbounds double, double* %1797, i64 %index10768
  %4497 = bitcast double* %4496 to <2 x double>*
  %wide.load10798 = load <2 x double>, <2 x double>* %4497, align 8, !tbaa !62
  %4498 = getelementptr inbounds double, double* %4496, i64 2
  %4499 = bitcast double* %4498 to <2 x double>*
  %wide.load10799 = load <2 x double>, <2 x double>* %4499, align 8, !tbaa !62
  %4500 = getelementptr inbounds double, double* %4496, i64 4
  %4501 = bitcast double* %4500 to <2 x double>*
  %wide.load10800 = load <2 x double>, <2 x double>* %4501, align 8, !tbaa !62
  %4502 = getelementptr inbounds double, double* %4496, i64 6
  %4503 = bitcast double* %4502 to <2 x double>*
  %wide.load10801 = load <2 x double>, <2 x double>* %4503, align 8, !tbaa !62
  %4504 = getelementptr inbounds double, double* %4496, i64 8
  %4505 = bitcast double* %4504 to <2 x double>*
  %wide.load10802 = load <2 x double>, <2 x double>* %4505, align 8, !tbaa !62
  %4506 = getelementptr inbounds double, double* %4496, i64 10
  %4507 = bitcast double* %4506 to <2 x double>*
  %wide.load10803 = load <2 x double>, <2 x double>* %4507, align 8, !tbaa !62
  %4508 = getelementptr inbounds double, double* %4496, i64 12
  %4509 = bitcast double* %4508 to <2 x double>*
  %wide.load10804 = load <2 x double>, <2 x double>* %4509, align 8, !tbaa !62
  %4510 = getelementptr inbounds double, double* %4496, i64 14
  %4511 = bitcast double* %4510 to <2 x double>*
  %wide.load10805 = load <2 x double>, <2 x double>* %4511, align 8, !tbaa !62
  %4512 = getelementptr inbounds double, double* %4496, i64 16
  %4513 = bitcast double* %4512 to <2 x double>*
  %wide.load10806 = load <2 x double>, <2 x double>* %4513, align 8, !tbaa !62
  %4514 = getelementptr inbounds double, double* %4496, i64 18
  %4515 = bitcast double* %4514 to <2 x double>*
  %wide.load10807 = load <2 x double>, <2 x double>* %4515, align 8, !tbaa !62
  %4516 = getelementptr inbounds double, double* %4496, i64 20
  %4517 = bitcast double* %4516 to <2 x double>*
  %wide.load10808 = load <2 x double>, <2 x double>* %4517, align 8, !tbaa !62
  %4518 = getelementptr inbounds double, double* %4496, i64 22
  %4519 = bitcast double* %4518 to <2 x double>*
  %wide.load10809 = load <2 x double>, <2 x double>* %4519, align 8, !tbaa !62
  %4520 = fadd <2 x double> %wide.load10786, %wide.load10798
  %4521 = fadd <2 x double> %wide.load10787, %wide.load10799
  %4522 = fadd <2 x double> %wide.load10788, %wide.load10800
  %4523 = fadd <2 x double> %wide.load10789, %wide.load10801
  %4524 = fadd <2 x double> %wide.load10790, %wide.load10802
  %4525 = fadd <2 x double> %wide.load10791, %wide.load10803
  %4526 = fadd <2 x double> %wide.load10792, %wide.load10804
  %4527 = fadd <2 x double> %wide.load10793, %wide.load10805
  %4528 = fadd <2 x double> %wide.load10794, %wide.load10806
  %4529 = fadd <2 x double> %wide.load10795, %wide.load10807
  %4530 = fadd <2 x double> %wide.load10796, %wide.load10808
  %4531 = fadd <2 x double> %wide.load10797, %wide.load10809
  %4532 = getelementptr inbounds double, double* %1795, i64 %index10768
  %4533 = bitcast double* %4532 to <2 x double>*
  %wide.load10810 = load <2 x double>, <2 x double>* %4533, align 8, !tbaa !62
  %4534 = getelementptr inbounds double, double* %4532, i64 2
  %4535 = bitcast double* %4534 to <2 x double>*
  %wide.load10811 = load <2 x double>, <2 x double>* %4535, align 8, !tbaa !62
  %4536 = getelementptr inbounds double, double* %4532, i64 4
  %4537 = bitcast double* %4536 to <2 x double>*
  %wide.load10812 = load <2 x double>, <2 x double>* %4537, align 8, !tbaa !62
  %4538 = getelementptr inbounds double, double* %4532, i64 6
  %4539 = bitcast double* %4538 to <2 x double>*
  %wide.load10813 = load <2 x double>, <2 x double>* %4539, align 8, !tbaa !62
  %4540 = getelementptr inbounds double, double* %4532, i64 8
  %4541 = bitcast double* %4540 to <2 x double>*
  %wide.load10814 = load <2 x double>, <2 x double>* %4541, align 8, !tbaa !62
  %4542 = getelementptr inbounds double, double* %4532, i64 10
  %4543 = bitcast double* %4542 to <2 x double>*
  %wide.load10815 = load <2 x double>, <2 x double>* %4543, align 8, !tbaa !62
  %4544 = getelementptr inbounds double, double* %4532, i64 12
  %4545 = bitcast double* %4544 to <2 x double>*
  %wide.load10816 = load <2 x double>, <2 x double>* %4545, align 8, !tbaa !62
  %4546 = getelementptr inbounds double, double* %4532, i64 14
  %4547 = bitcast double* %4546 to <2 x double>*
  %wide.load10817 = load <2 x double>, <2 x double>* %4547, align 8, !tbaa !62
  %4548 = getelementptr inbounds double, double* %4532, i64 16
  %4549 = bitcast double* %4548 to <2 x double>*
  %wide.load10818 = load <2 x double>, <2 x double>* %4549, align 8, !tbaa !62
  %4550 = getelementptr inbounds double, double* %4532, i64 18
  %4551 = bitcast double* %4550 to <2 x double>*
  %wide.load10819 = load <2 x double>, <2 x double>* %4551, align 8, !tbaa !62
  %4552 = getelementptr inbounds double, double* %4532, i64 20
  %4553 = bitcast double* %4552 to <2 x double>*
  %wide.load10820 = load <2 x double>, <2 x double>* %4553, align 8, !tbaa !62
  %4554 = getelementptr inbounds double, double* %4532, i64 22
  %4555 = bitcast double* %4554 to <2 x double>*
  %wide.load10821 = load <2 x double>, <2 x double>* %4555, align 8, !tbaa !62
  %4556 = fadd <2 x double> %4520, %wide.load10810
  %4557 = fadd <2 x double> %4521, %wide.load10811
  %4558 = fadd <2 x double> %4522, %wide.load10812
  %4559 = fadd <2 x double> %4523, %wide.load10813
  %4560 = fadd <2 x double> %4524, %wide.load10814
  %4561 = fadd <2 x double> %4525, %wide.load10815
  %4562 = fadd <2 x double> %4526, %wide.load10816
  %4563 = fadd <2 x double> %4527, %wide.load10817
  %4564 = fadd <2 x double> %4528, %wide.load10818
  %4565 = fadd <2 x double> %4529, %wide.load10819
  %4566 = fadd <2 x double> %4530, %wide.load10820
  %4567 = fadd <2 x double> %4531, %wide.load10821
  store <2 x double> %4556, <2 x double>* %4533, align 8, !tbaa !62
  store <2 x double> %4557, <2 x double>* %4535, align 8, !tbaa !62
  store <2 x double> %4558, <2 x double>* %4537, align 8, !tbaa !62
  store <2 x double> %4559, <2 x double>* %4539, align 8, !tbaa !62
  store <2 x double> %4560, <2 x double>* %4541, align 8, !tbaa !62
  store <2 x double> %4561, <2 x double>* %4543, align 8, !tbaa !62
  store <2 x double> %4562, <2 x double>* %4545, align 8, !tbaa !62
  store <2 x double> %4563, <2 x double>* %4547, align 8, !tbaa !62
  store <2 x double> %4564, <2 x double>* %4549, align 8, !tbaa !62
  store <2 x double> %4565, <2 x double>* %4551, align 8, !tbaa !62
  store <2 x double> %4566, <2 x double>* %4553, align 8, !tbaa !62
  store <2 x double> %4567, <2 x double>* %4555, align 8, !tbaa !62
  %index.next10769 = add nuw nsw i64 %index10768, 24
  %4568 = icmp eq i64 %index.next10769, %n.vec10767
  br i1 %4568, label %middle.block10760, label %vector.body10759, !llvm.loop !102

middle.block10760:                                ; preds = %vector.body10759
  br i1 %cmp.n10771, label %for.cond.cleanup4609, label %for.body4610

for.cond.cleanup4603:                             ; preds = %for.cond.cleanup4609
  store i8* %call3733, i8** %4272, align 8
  store i8* %call3733, i8** %4273, align 8
  store i64 %4465, i64* %4274, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %4272, i8** nonnull %4273, i64* nonnull %4274, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body4636

for.cond.cleanup4609:                             ; preds = %for.body4610, %middle.block10760
  %inc4624 = add nuw nsw i32 %times4599.08942, 1
  %exitcond9521 = icmp eq i32 %inc4624, 54
  br i1 %exitcond9521, label %for.cond.cleanup4603, label %for.cond4606.preheader

for.body4610:                                     ; preds = %middle.block10760, %for.body4610
  %indvars.iv9518 = phi i64 [ %indvars.iv.next9519, %for.body4610 ], [ %n.vec10767, %middle.block10760 ]
  %arrayidx4612 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9518
  %4569 = load double, double* %arrayidx4612, align 8, !tbaa !62
  %arrayidx4614 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9518
  %4570 = load double, double* %arrayidx4614, align 8, !tbaa !62
  %add4615 = fadd double %4569, %4570
  %arrayidx4617 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9518
  %4571 = load double, double* %arrayidx4617, align 8, !tbaa !62
  %add4618 = fadd double %4571, %add4615
  store double %add4618, double* %arrayidx4617, align 8, !tbaa !62
  %indvars.iv.next9519 = add nuw nsw i64 %indvars.iv9518, 1
  %exitcond9520 = icmp eq i64 %indvars.iv.next9519, %indvars.iv9524
  br i1 %exitcond9520, label %for.cond.cleanup4609, label %for.body4610, !llvm.loop !103

for.body4636:                                     ; preds = %for.inc4650.7, %for.cond.cleanup4603
  %indvars.iv9522 = phi i64 [ 0, %for.cond.cleanup4603 ], [ %indvars.iv.next9523.7, %for.inc4650.7 ]
  %arrayidx4638 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9522
  %4572 = load double, double* %arrayidx4638, align 8, !tbaa !62
  %arrayidx4640 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9522
  %4573 = load double, double* %arrayidx4640, align 8, !tbaa !62
  %cmp4641 = fcmp une double %4572, %4573
  br i1 %cmp4641, label %cleanup4660, label %for.inc4650

for.inc4650:                                      ; preds = %for.body4636
  %indvars.iv.next9523 = or i64 %indvars.iv9522, 1
  %arrayidx4638.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9523
  %4574 = load double, double* %arrayidx4638.1, align 8, !tbaa !62
  %arrayidx4640.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9523
  %4575 = load double, double* %arrayidx4640.1, align 8, !tbaa !62
  %cmp4641.1 = fcmp une double %4574, %4575
  br i1 %cmp4641.1, label %cleanup4660, label %for.inc4650.1

for.inc4658:                                      ; preds = %for.inc4650.7
  %indvars.iv.next9525 = add nuw nsw i64 %indvars.iv9524, 5000
  %cmp4555 = icmp ult i64 %indvars.iv.next9525, 25000
  %indvar.next10763 = add nuw nsw i64 %indvar10762, 1
  br i1 %cmp4555, label %for.body.i8505.preheader, label %for.end4662

cleanup4660:                                      ; preds = %for.inc4650.6, %for.inc4650.5, %for.inc4650.4, %for.inc4650.3, %for.inc4650.2, %for.inc4650.1, %for.inc4650, %for.body4636
  %indvars.iv9522.lcssa = phi i64 [ %indvars.iv9522, %for.body4636 ], [ %indvars.iv.next9523, %for.inc4650 ], [ %indvars.iv.next9523.1, %for.inc4650.1 ], [ %indvars.iv.next9523.2, %for.inc4650.2 ], [ %indvars.iv.next9523.3, %for.inc4650.3 ], [ %indvars.iv.next9523.4, %for.inc4650.4 ], [ %indvars.iv.next9523.5, %for.inc4650.5 ], [ %indvars.iv.next9523.6, %for.inc4650.6 ]
  %.lcssa11822 = phi double [ %4572, %for.body4636 ], [ %4574, %for.inc4650 ], [ %7836, %for.inc4650.1 ], [ %7838, %for.inc4650.2 ], [ %7840, %for.inc4650.3 ], [ %7842, %for.inc4650.4 ], [ %7844, %for.inc4650.5 ], [ %7846, %for.inc4650.6 ]
  %.lcssa11820 = phi double [ %4573, %for.body4636 ], [ %4575, %for.inc4650 ], [ %7837, %for.inc4650.1 ], [ %7839, %for.inc4650.2 ], [ %7841, %for.inc4650.3 ], [ %7843, %for.inc4650.4 ], [ %7845, %for.inc4650.5 ], [ %7847, %for.inc4650.6 ]
  %4576 = trunc i64 %indvars.iv9524 to i32
  %4577 = trunc i64 %indvars.iv9522.lcssa to i32
  %call4648 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %4576, i32 signext %4577, double %.lcssa11822, double %.lcssa11820)
  br label %cleanup5832

for.end4662:                                      ; preds = %for.inc4658
  %puts8259 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8260 = call i32 @puts(i8* getelementptr inbounds ([53 x i8], [53 x i8]* @str.349, i64 0, i64 0))
  %4578 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4675, i64 0, i64 0
  %4579 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4676, i64 0, i64 0
  %4580 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4677, i64 0, i64 0
  %4581 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4675, i64 0, i64 1
  %4582 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4676, i64 0, i64 1
  %4583 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4677, i64 0, i64 1
  %4584 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4675, i64 0, i64 2
  %4585 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4676, i64 0, i64 2
  %4586 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4677, i64 0, i64 2
  %4587 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4708, i64 0, i64 0
  %4588 = bitcast [7 x i8*]* %.offload_baseptrs4708 to i64*
  %4589 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4709, i64 0, i64 0
  %4590 = bitcast [7 x i8*]* %.offload_ptrs4709 to i64*
  %4591 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4708, i64 0, i64 1
  %4592 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4709, i64 0, i64 1
  %4593 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4708, i64 0, i64 2
  %4594 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4709, i64 0, i64 2
  %4595 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4708, i64 0, i64 3
  %4596 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4709, i64 0, i64 3
  %4597 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4708, i64 0, i64 4
  %4598 = bitcast i8** %4597 to i64*
  %4599 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4709, i64 0, i64 4
  %4600 = bitcast i8** %4599 to i64*
  %4601 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4708, i64 0, i64 5
  %4602 = bitcast i8** %4601 to i64*
  %4603 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4709, i64 0, i64 5
  %4604 = bitcast i8** %4603 to i64*
  %4605 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4708, i64 0, i64 6
  %4606 = bitcast i8** %4605 to i64*
  %4607 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4709, i64 0, i64 6
  %4608 = bitcast i8** %4607 to i64*
  %4609 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs4753, i64 0, i64 0
  %4610 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs4754, i64 0, i64 0
  %4611 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes4755, i64 0, i64 0
  %4612 = bitcast i8* %arrayidx.i to <2 x double>*
  %4613 = bitcast i8* %arrayidx2.i to <2 x double>*
  %4614 = bitcast i8* %arrayidx5.i to <2 x double>*
  %4615 = bitcast i8* %arrayidx8.i to <2 x double>*
  %4616 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %4617 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %4618 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %4619 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %4620 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %4621 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %4622 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %4623 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %4624 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %4625 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %4626 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %4627 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %4628 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %4629 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %4630 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %4631 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %4632 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %4633 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %4634 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %4635 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %4636 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %4637 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %4638 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %4639 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %4640 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %4641 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %4642 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %4643 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8519.preheader

for.body.i8519.preheader:                         ; preds = %for.end4662, %for.inc4783
  %indvar10861 = phi i64 [ 0, %for.end4662 ], [ %indvar.next10862, %for.inc4783 ]
  %indvars.iv9513 = phi i64 [ 32, %for.end4662 ], [ %indvars.iv.next9514, %for.inc4783 ]
  %4644 = mul nuw nsw i64 %indvar10861, 5000
  br label %vector.body10921

vector.body10921:                                 ; preds = %vector.body10921, %for.body.i8519.preheader
  %index10925 = phi i64 [ 0, %for.body.i8519.preheader ], [ %index.next10926, %vector.body10921 ]
  %vec.ind10943 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8519.preheader ], [ %vec.ind.next10956, %vector.body10921 ]
  %step.add10944 = add <2 x i32> %vec.ind10943, <i32 2, i32 2>
  %step.add10945 = add <2 x i32> %vec.ind10943, <i32 4, i32 4>
  %step.add10946 = add <2 x i32> %vec.ind10943, <i32 6, i32 6>
  %step.add10947 = add <2 x i32> %vec.ind10943, <i32 8, i32 8>
  %step.add10948 = add <2 x i32> %vec.ind10943, <i32 10, i32 10>
  %step.add10949 = add <2 x i32> %vec.ind10943, <i32 12, i32 12>
  %step.add10950 = add <2 x i32> %vec.ind10943, <i32 14, i32 14>
  %step.add10951 = add <2 x i32> %vec.ind10943, <i32 16, i32 16>
  %step.add10952 = add <2 x i32> %vec.ind10943, <i32 18, i32 18>
  %step.add10953 = add <2 x i32> %vec.ind10943, <i32 20, i32 20>
  %step.add10954 = add <2 x i32> %vec.ind10943, <i32 22, i32 22>
  %4645 = sitofp <2 x i32> %vec.ind10943 to <2 x double>
  %4646 = sitofp <2 x i32> %step.add10944 to <2 x double>
  %4647 = sitofp <2 x i32> %step.add10945 to <2 x double>
  %4648 = sitofp <2 x i32> %step.add10946 to <2 x double>
  %4649 = sitofp <2 x i32> %step.add10947 to <2 x double>
  %4650 = sitofp <2 x i32> %step.add10948 to <2 x double>
  %4651 = sitofp <2 x i32> %step.add10949 to <2 x double>
  %4652 = sitofp <2 x i32> %step.add10950 to <2 x double>
  %4653 = sitofp <2 x i32> %step.add10951 to <2 x double>
  %4654 = sitofp <2 x i32> %step.add10952 to <2 x double>
  %4655 = sitofp <2 x i32> %step.add10953 to <2 x double>
  %4656 = sitofp <2 x i32> %step.add10954 to <2 x double>
  %4657 = getelementptr inbounds double, double* %1795, i64 %index10925
  %4658 = bitcast double* %4657 to <2 x double>*
  store <2 x double> %4645, <2 x double>* %4658, align 8, !tbaa !62
  %4659 = getelementptr inbounds double, double* %4657, i64 2
  %4660 = bitcast double* %4659 to <2 x double>*
  store <2 x double> %4646, <2 x double>* %4660, align 8, !tbaa !62
  %4661 = getelementptr inbounds double, double* %4657, i64 4
  %4662 = bitcast double* %4661 to <2 x double>*
  store <2 x double> %4647, <2 x double>* %4662, align 8, !tbaa !62
  %4663 = getelementptr inbounds double, double* %4657, i64 6
  %4664 = bitcast double* %4663 to <2 x double>*
  store <2 x double> %4648, <2 x double>* %4664, align 8, !tbaa !62
  %4665 = getelementptr inbounds double, double* %4657, i64 8
  %4666 = bitcast double* %4665 to <2 x double>*
  store <2 x double> %4649, <2 x double>* %4666, align 8, !tbaa !62
  %4667 = getelementptr inbounds double, double* %4657, i64 10
  %4668 = bitcast double* %4667 to <2 x double>*
  store <2 x double> %4650, <2 x double>* %4668, align 8, !tbaa !62
  %4669 = getelementptr inbounds double, double* %4657, i64 12
  %4670 = bitcast double* %4669 to <2 x double>*
  store <2 x double> %4651, <2 x double>* %4670, align 8, !tbaa !62
  %4671 = getelementptr inbounds double, double* %4657, i64 14
  %4672 = bitcast double* %4671 to <2 x double>*
  store <2 x double> %4652, <2 x double>* %4672, align 8, !tbaa !62
  %4673 = getelementptr inbounds double, double* %4657, i64 16
  %4674 = bitcast double* %4673 to <2 x double>*
  store <2 x double> %4653, <2 x double>* %4674, align 8, !tbaa !62
  %4675 = getelementptr inbounds double, double* %4657, i64 18
  %4676 = bitcast double* %4675 to <2 x double>*
  store <2 x double> %4654, <2 x double>* %4676, align 8, !tbaa !62
  %4677 = getelementptr inbounds double, double* %4657, i64 20
  %4678 = bitcast double* %4677 to <2 x double>*
  store <2 x double> %4655, <2 x double>* %4678, align 8, !tbaa !62
  %4679 = getelementptr inbounds double, double* %4657, i64 22
  %4680 = bitcast double* %4679 to <2 x double>*
  store <2 x double> %4656, <2 x double>* %4680, align 8, !tbaa !62
  %4681 = getelementptr inbounds double, double* %1794, i64 %index10925
  %4682 = bitcast double* %4681 to <2 x double>*
  store <2 x double> %4645, <2 x double>* %4682, align 8, !tbaa !62
  %4683 = getelementptr inbounds double, double* %4681, i64 2
  %4684 = bitcast double* %4683 to <2 x double>*
  store <2 x double> %4646, <2 x double>* %4684, align 8, !tbaa !62
  %4685 = getelementptr inbounds double, double* %4681, i64 4
  %4686 = bitcast double* %4685 to <2 x double>*
  store <2 x double> %4647, <2 x double>* %4686, align 8, !tbaa !62
  %4687 = getelementptr inbounds double, double* %4681, i64 6
  %4688 = bitcast double* %4687 to <2 x double>*
  store <2 x double> %4648, <2 x double>* %4688, align 8, !tbaa !62
  %4689 = getelementptr inbounds double, double* %4681, i64 8
  %4690 = bitcast double* %4689 to <2 x double>*
  store <2 x double> %4649, <2 x double>* %4690, align 8, !tbaa !62
  %4691 = getelementptr inbounds double, double* %4681, i64 10
  %4692 = bitcast double* %4691 to <2 x double>*
  store <2 x double> %4650, <2 x double>* %4692, align 8, !tbaa !62
  %4693 = getelementptr inbounds double, double* %4681, i64 12
  %4694 = bitcast double* %4693 to <2 x double>*
  store <2 x double> %4651, <2 x double>* %4694, align 8, !tbaa !62
  %4695 = getelementptr inbounds double, double* %4681, i64 14
  %4696 = bitcast double* %4695 to <2 x double>*
  store <2 x double> %4652, <2 x double>* %4696, align 8, !tbaa !62
  %4697 = getelementptr inbounds double, double* %4681, i64 16
  %4698 = bitcast double* %4697 to <2 x double>*
  store <2 x double> %4653, <2 x double>* %4698, align 8, !tbaa !62
  %4699 = getelementptr inbounds double, double* %4681, i64 18
  %4700 = bitcast double* %4699 to <2 x double>*
  store <2 x double> %4654, <2 x double>* %4700, align 8, !tbaa !62
  %4701 = getelementptr inbounds double, double* %4681, i64 20
  %4702 = bitcast double* %4701 to <2 x double>*
  store <2 x double> %4655, <2 x double>* %4702, align 8, !tbaa !62
  %4703 = getelementptr inbounds double, double* %4681, i64 22
  %4704 = bitcast double* %4703 to <2 x double>*
  store <2 x double> %4656, <2 x double>* %4704, align 8, !tbaa !62
  %4705 = shl <2 x i32> %vec.ind10943, <i32 1, i32 1>
  %4706 = shl <2 x i32> %step.add10944, <i32 1, i32 1>
  %4707 = shl <2 x i32> %step.add10945, <i32 1, i32 1>
  %4708 = shl <2 x i32> %step.add10946, <i32 1, i32 1>
  %4709 = shl <2 x i32> %step.add10947, <i32 1, i32 1>
  %4710 = shl <2 x i32> %step.add10948, <i32 1, i32 1>
  %4711 = shl <2 x i32> %step.add10949, <i32 1, i32 1>
  %4712 = shl <2 x i32> %step.add10950, <i32 1, i32 1>
  %4713 = shl <2 x i32> %step.add10951, <i32 1, i32 1>
  %4714 = shl <2 x i32> %step.add10952, <i32 1, i32 1>
  %4715 = shl <2 x i32> %step.add10953, <i32 1, i32 1>
  %4716 = shl <2 x i32> %step.add10954, <i32 1, i32 1>
  %4717 = sitofp <2 x i32> %4705 to <2 x double>
  %4718 = sitofp <2 x i32> %4706 to <2 x double>
  %4719 = sitofp <2 x i32> %4707 to <2 x double>
  %4720 = sitofp <2 x i32> %4708 to <2 x double>
  %4721 = sitofp <2 x i32> %4709 to <2 x double>
  %4722 = sitofp <2 x i32> %4710 to <2 x double>
  %4723 = sitofp <2 x i32> %4711 to <2 x double>
  %4724 = sitofp <2 x i32> %4712 to <2 x double>
  %4725 = sitofp <2 x i32> %4713 to <2 x double>
  %4726 = sitofp <2 x i32> %4714 to <2 x double>
  %4727 = sitofp <2 x i32> %4715 to <2 x double>
  %4728 = sitofp <2 x i32> %4716 to <2 x double>
  %4729 = getelementptr inbounds double, double* %1796, i64 %index10925
  %4730 = bitcast double* %4729 to <2 x double>*
  store <2 x double> %4717, <2 x double>* %4730, align 8, !tbaa !62
  %4731 = getelementptr inbounds double, double* %4729, i64 2
  %4732 = bitcast double* %4731 to <2 x double>*
  store <2 x double> %4718, <2 x double>* %4732, align 8, !tbaa !62
  %4733 = getelementptr inbounds double, double* %4729, i64 4
  %4734 = bitcast double* %4733 to <2 x double>*
  store <2 x double> %4719, <2 x double>* %4734, align 8, !tbaa !62
  %4735 = getelementptr inbounds double, double* %4729, i64 6
  %4736 = bitcast double* %4735 to <2 x double>*
  store <2 x double> %4720, <2 x double>* %4736, align 8, !tbaa !62
  %4737 = getelementptr inbounds double, double* %4729, i64 8
  %4738 = bitcast double* %4737 to <2 x double>*
  store <2 x double> %4721, <2 x double>* %4738, align 8, !tbaa !62
  %4739 = getelementptr inbounds double, double* %4729, i64 10
  %4740 = bitcast double* %4739 to <2 x double>*
  store <2 x double> %4722, <2 x double>* %4740, align 8, !tbaa !62
  %4741 = getelementptr inbounds double, double* %4729, i64 12
  %4742 = bitcast double* %4741 to <2 x double>*
  store <2 x double> %4723, <2 x double>* %4742, align 8, !tbaa !62
  %4743 = getelementptr inbounds double, double* %4729, i64 14
  %4744 = bitcast double* %4743 to <2 x double>*
  store <2 x double> %4724, <2 x double>* %4744, align 8, !tbaa !62
  %4745 = getelementptr inbounds double, double* %4729, i64 16
  %4746 = bitcast double* %4745 to <2 x double>*
  store <2 x double> %4725, <2 x double>* %4746, align 8, !tbaa !62
  %4747 = getelementptr inbounds double, double* %4729, i64 18
  %4748 = bitcast double* %4747 to <2 x double>*
  store <2 x double> %4726, <2 x double>* %4748, align 8, !tbaa !62
  %4749 = getelementptr inbounds double, double* %4729, i64 20
  %4750 = bitcast double* %4749 to <2 x double>*
  store <2 x double> %4727, <2 x double>* %4750, align 8, !tbaa !62
  %4751 = getelementptr inbounds double, double* %4729, i64 22
  %4752 = bitcast double* %4751 to <2 x double>*
  store <2 x double> %4728, <2 x double>* %4752, align 8, !tbaa !62
  %4753 = add <2 x i32> %vec.ind10943, <i32 -3, i32 -3>
  %4754 = add <2 x i32> %vec.ind10943, <i32 -1, i32 -1>
  %4755 = add <2 x i32> %vec.ind10943, <i32 1, i32 1>
  %4756 = add <2 x i32> %vec.ind10943, <i32 3, i32 3>
  %4757 = add <2 x i32> %vec.ind10943, <i32 5, i32 5>
  %4758 = add <2 x i32> %vec.ind10943, <i32 7, i32 7>
  %4759 = add <2 x i32> %vec.ind10943, <i32 9, i32 9>
  %4760 = add <2 x i32> %vec.ind10943, <i32 11, i32 11>
  %4761 = add <2 x i32> %vec.ind10943, <i32 13, i32 13>
  %4762 = add <2 x i32> %vec.ind10943, <i32 15, i32 15>
  %4763 = add <2 x i32> %vec.ind10943, <i32 17, i32 17>
  %4764 = add <2 x i32> %vec.ind10943, <i32 19, i32 19>
  %4765 = sitofp <2 x i32> %4753 to <2 x double>
  %4766 = sitofp <2 x i32> %4754 to <2 x double>
  %4767 = sitofp <2 x i32> %4755 to <2 x double>
  %4768 = sitofp <2 x i32> %4756 to <2 x double>
  %4769 = sitofp <2 x i32> %4757 to <2 x double>
  %4770 = sitofp <2 x i32> %4758 to <2 x double>
  %4771 = sitofp <2 x i32> %4759 to <2 x double>
  %4772 = sitofp <2 x i32> %4760 to <2 x double>
  %4773 = sitofp <2 x i32> %4761 to <2 x double>
  %4774 = sitofp <2 x i32> %4762 to <2 x double>
  %4775 = sitofp <2 x i32> %4763 to <2 x double>
  %4776 = sitofp <2 x i32> %4764 to <2 x double>
  %4777 = getelementptr inbounds double, double* %1797, i64 %index10925
  %4778 = bitcast double* %4777 to <2 x double>*
  store <2 x double> %4765, <2 x double>* %4778, align 8, !tbaa !62
  %4779 = getelementptr inbounds double, double* %4777, i64 2
  %4780 = bitcast double* %4779 to <2 x double>*
  store <2 x double> %4766, <2 x double>* %4780, align 8, !tbaa !62
  %4781 = getelementptr inbounds double, double* %4777, i64 4
  %4782 = bitcast double* %4781 to <2 x double>*
  store <2 x double> %4767, <2 x double>* %4782, align 8, !tbaa !62
  %4783 = getelementptr inbounds double, double* %4777, i64 6
  %4784 = bitcast double* %4783 to <2 x double>*
  store <2 x double> %4768, <2 x double>* %4784, align 8, !tbaa !62
  %4785 = getelementptr inbounds double, double* %4777, i64 8
  %4786 = bitcast double* %4785 to <2 x double>*
  store <2 x double> %4769, <2 x double>* %4786, align 8, !tbaa !62
  %4787 = getelementptr inbounds double, double* %4777, i64 10
  %4788 = bitcast double* %4787 to <2 x double>*
  store <2 x double> %4770, <2 x double>* %4788, align 8, !tbaa !62
  %4789 = getelementptr inbounds double, double* %4777, i64 12
  %4790 = bitcast double* %4789 to <2 x double>*
  store <2 x double> %4771, <2 x double>* %4790, align 8, !tbaa !62
  %4791 = getelementptr inbounds double, double* %4777, i64 14
  %4792 = bitcast double* %4791 to <2 x double>*
  store <2 x double> %4772, <2 x double>* %4792, align 8, !tbaa !62
  %4793 = getelementptr inbounds double, double* %4777, i64 16
  %4794 = bitcast double* %4793 to <2 x double>*
  store <2 x double> %4773, <2 x double>* %4794, align 8, !tbaa !62
  %4795 = getelementptr inbounds double, double* %4777, i64 18
  %4796 = bitcast double* %4795 to <2 x double>*
  store <2 x double> %4774, <2 x double>* %4796, align 8, !tbaa !62
  %4797 = getelementptr inbounds double, double* %4777, i64 20
  %4798 = bitcast double* %4797 to <2 x double>*
  store <2 x double> %4775, <2 x double>* %4798, align 8, !tbaa !62
  %4799 = getelementptr inbounds double, double* %4777, i64 22
  %4800 = bitcast double* %4799 to <2 x double>*
  store <2 x double> %4776, <2 x double>* %4800, align 8, !tbaa !62
  %index.next10926 = add nuw nsw i64 %index10925, 24
  %vec.ind.next10956 = add <2 x i32> %vec.ind10943, <i32 24, i32 24>
  %4801 = icmp eq i64 %index.next10926, 24984
  br i1 %4801, label %for.body.i8519, label %vector.body10921, !llvm.loop !104

for.body.i8519:                                   ; preds = %vector.body10921
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %4612, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %4613, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %4614, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %4615, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %4616, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %4617, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %4618, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %4619, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %4620, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %4621, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %4622, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %4623, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %4624, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %4625, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %4626, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %4627, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %4628, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %4629, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %4630, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %4631, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %4632, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %4633, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %4634, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %4635, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %4636, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %4637, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %4638, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %4639, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %4640, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %4641, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %4642, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %4643, align 8, !tbaa !62
  %4802 = shl nuw nsw i64 %indvars.iv9513, 3
  store i8* %call3733, i8** %4578, align 8
  store i8* %call3733, i8** %4579, align 8
  store i64 %4802, i64* %4580, align 8
  store i8* %call3735, i8** %4581, align 8
  store i8* %call3735, i8** %4582, align 8
  store i64 %4802, i64* %4583, align 8
  store i8* %call3736, i8** %4584, align 8
  store i8* %call3736, i8** %4585, align 8
  store i64 %4802, i64* %4586, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %4578, i8** nonnull %4579, i64* nonnull %4580, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  br label %for.cond4685.preheader

for.cond4725.preheader:                           ; preds = %for.cond.cleanup4688
  %4803 = add nuw nsw i64 %4644, 32
  %cmp47268932 = icmp sgt i32 %t4671.28925, -1
  br i1 %cmp47268932, label %for.cond4731.preheader.preheader48, label %for.cond.cleanup4728

for.cond4731.preheader.preheader48:               ; preds = %for.cond4725.preheader
  %n.mod.vf10865 = urem i64 %4803, 24
  %n.vec10866 = sub nuw nsw i64 %4803, %n.mod.vf10865
  %cmp.n10870 = icmp eq i64 %n.mod.vf10865, 0
  br label %for.cond4731.preheader

for.cond4685.preheader:                           ; preds = %for.body.i8519, %for.cond.cleanup4688
  %tms4678.08930 = phi i32 [ 1, %for.body.i8519 ], [ %mul4721, %for.cond.cleanup4688 ]
  %t4671.08929 = phi i32 [ 0, %for.body.i8519 ], [ %inc4696, %for.cond.cleanup4688 ]
  %.capture_expr..casted4704.sroa.0.0.insert.ext = zext i32 %tms4678.08930 to i64
  br label %for.cond4691.preheader

for.cond4691.preheader:                           ; preds = %for.cond4685.preheader, %for.cond.cleanup4694
  %ths4684.08928 = phi i32 [ 32, %for.cond4685.preheader ], [ %mul4717, %for.cond.cleanup4694 ]
  %t4671.18927 = phi i32 [ %t4671.08929, %for.cond4685.preheader ], [ %inc4696, %for.cond.cleanup4694 ]
  %.capture_expr..casted4706.sroa.0.0.insert.ext = zext i32 %ths4684.08928 to i64
  br label %for.body4695

for.cond.cleanup4688:                             ; preds = %for.cond.cleanup4694
  %mul4721 = shl nsw i32 %tms4678.08930, 1
  %cmp4680 = icmp ult i32 %mul4721, 257
  br i1 %cmp4680, label %for.cond4685.preheader, label %for.cond4725.preheader

for.cond.cleanup4694:                             ; preds = %omp_offload.cont4711
  %mul4717 = shl nsw i32 %ths4684.08928, 1
  %cmp4686 = icmp ult i32 %mul4717, 1025
  br i1 %cmp4686, label %for.cond4691.preheader, label %for.cond.cleanup4688

for.body4695:                                     ; preds = %for.cond4691.preheader, %omp_offload.cont4711
  %sch4690.08926 = phi i32 [ 1, %for.cond4691.preheader ], [ %mul4713, %omp_offload.cont4711 ]
  %t4671.28925 = phi i32 [ %t4671.18927, %for.cond4691.preheader ], [ %inc4696, %omp_offload.cont4711 ]
  %inc4696 = add i32 %t4671.28925, 1
  %.capture_expr..casted4702.sroa.0.0.insert.ext = zext i32 %sch4690.08926 to i64
  store i64 %indvars.iv9513, i64* %4588, align 8
  store i64 %indvars.iv9513, i64* %4590, align 8
  store i8* %call3733, i8** %4591, align 8
  store i8* %call3733, i8** %4592, align 8
  store i8* %call3735, i8** %4593, align 8
  store i8* %call3735, i8** %4594, align 8
  store i8* %call3736, i8** %4595, align 8
  store i8* %call3736, i8** %4596, align 8
  store i64 %.capture_expr..casted4702.sroa.0.0.insert.ext, i64* %4598, align 8
  store i64 %.capture_expr..casted4702.sroa.0.0.insert.ext, i64* %4600, align 8
  store i64 %.capture_expr..casted4704.sroa.0.0.insert.ext, i64* %4602, align 8
  store i64 %.capture_expr..casted4704.sroa.0.0.insert.ext, i64* %4604, align 8
  store i64 %.capture_expr..casted4706.sroa.0.0.insert.ext, i64* %4606, align 8
  store i64 %.capture_expr..casted4706.sroa.0.0.insert.ext, i64* %4608, align 8
  %4804 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1318.region_id, i32 7, i8** nonnull %4587, i8** nonnull %4589, i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_sizes.273, i64 0, i64 0), i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_maptypes.274, i64 0, i64 0), i32 %tms4678.08930, i32 %ths4684.08928) #6
  %4805 = icmp eq i32 %4804, 0
  br i1 %4805, label %omp_offload.cont4711, label %omp_offload.failed4710

omp_offload.failed4710:                           ; preds = %for.body4695
  %4806 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %4807 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %4806, i32 %tms4678.08930, i32 %ths4684.08928) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 5, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64)* @.omp_outlined..215 to void (i32*, i32*, ...)*), i64 %indvars.iv9513, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted4702.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont4711

omp_offload.cont4711:                             ; preds = %for.body4695, %omp_offload.failed4710
  %mul4713 = mul nsw i32 %sch4690.08926, 1000
  %4808 = zext i32 %mul4713 to i64
  %cmp4692 = icmp ult i64 %indvars.iv9513, %4808
  br i1 %cmp4692, label %for.cond.cleanup4694, label %for.body4695

for.cond4731.preheader:                           ; preds = %for.cond4731.preheader.preheader48, %for.cond.cleanup4734
  %times4724.08933 = phi i32 [ %inc4749, %for.cond.cleanup4734 ], [ 0, %for.cond4731.preheader.preheader48 ]
  br label %vector.body10858

vector.body10858:                                 ; preds = %for.cond4731.preheader, %vector.body10858
  %index10867 = phi i64 [ %index.next10868, %vector.body10858 ], [ 0, %for.cond4731.preheader ]
  %4809 = getelementptr inbounds double, double* %1796, i64 %index10867
  %4810 = bitcast double* %4809 to <2 x double>*
  %wide.load10885 = load <2 x double>, <2 x double>* %4810, align 8, !tbaa !62
  %4811 = getelementptr inbounds double, double* %4809, i64 2
  %4812 = bitcast double* %4811 to <2 x double>*
  %wide.load10886 = load <2 x double>, <2 x double>* %4812, align 8, !tbaa !62
  %4813 = getelementptr inbounds double, double* %4809, i64 4
  %4814 = bitcast double* %4813 to <2 x double>*
  %wide.load10887 = load <2 x double>, <2 x double>* %4814, align 8, !tbaa !62
  %4815 = getelementptr inbounds double, double* %4809, i64 6
  %4816 = bitcast double* %4815 to <2 x double>*
  %wide.load10888 = load <2 x double>, <2 x double>* %4816, align 8, !tbaa !62
  %4817 = getelementptr inbounds double, double* %4809, i64 8
  %4818 = bitcast double* %4817 to <2 x double>*
  %wide.load10889 = load <2 x double>, <2 x double>* %4818, align 8, !tbaa !62
  %4819 = getelementptr inbounds double, double* %4809, i64 10
  %4820 = bitcast double* %4819 to <2 x double>*
  %wide.load10890 = load <2 x double>, <2 x double>* %4820, align 8, !tbaa !62
  %4821 = getelementptr inbounds double, double* %4809, i64 12
  %4822 = bitcast double* %4821 to <2 x double>*
  %wide.load10891 = load <2 x double>, <2 x double>* %4822, align 8, !tbaa !62
  %4823 = getelementptr inbounds double, double* %4809, i64 14
  %4824 = bitcast double* %4823 to <2 x double>*
  %wide.load10892 = load <2 x double>, <2 x double>* %4824, align 8, !tbaa !62
  %4825 = getelementptr inbounds double, double* %4809, i64 16
  %4826 = bitcast double* %4825 to <2 x double>*
  %wide.load10893 = load <2 x double>, <2 x double>* %4826, align 8, !tbaa !62
  %4827 = getelementptr inbounds double, double* %4809, i64 18
  %4828 = bitcast double* %4827 to <2 x double>*
  %wide.load10894 = load <2 x double>, <2 x double>* %4828, align 8, !tbaa !62
  %4829 = getelementptr inbounds double, double* %4809, i64 20
  %4830 = bitcast double* %4829 to <2 x double>*
  %wide.load10895 = load <2 x double>, <2 x double>* %4830, align 8, !tbaa !62
  %4831 = getelementptr inbounds double, double* %4809, i64 22
  %4832 = bitcast double* %4831 to <2 x double>*
  %wide.load10896 = load <2 x double>, <2 x double>* %4832, align 8, !tbaa !62
  %4833 = getelementptr inbounds double, double* %1797, i64 %index10867
  %4834 = bitcast double* %4833 to <2 x double>*
  %wide.load10897 = load <2 x double>, <2 x double>* %4834, align 8, !tbaa !62
  %4835 = getelementptr inbounds double, double* %4833, i64 2
  %4836 = bitcast double* %4835 to <2 x double>*
  %wide.load10898 = load <2 x double>, <2 x double>* %4836, align 8, !tbaa !62
  %4837 = getelementptr inbounds double, double* %4833, i64 4
  %4838 = bitcast double* %4837 to <2 x double>*
  %wide.load10899 = load <2 x double>, <2 x double>* %4838, align 8, !tbaa !62
  %4839 = getelementptr inbounds double, double* %4833, i64 6
  %4840 = bitcast double* %4839 to <2 x double>*
  %wide.load10900 = load <2 x double>, <2 x double>* %4840, align 8, !tbaa !62
  %4841 = getelementptr inbounds double, double* %4833, i64 8
  %4842 = bitcast double* %4841 to <2 x double>*
  %wide.load10901 = load <2 x double>, <2 x double>* %4842, align 8, !tbaa !62
  %4843 = getelementptr inbounds double, double* %4833, i64 10
  %4844 = bitcast double* %4843 to <2 x double>*
  %wide.load10902 = load <2 x double>, <2 x double>* %4844, align 8, !tbaa !62
  %4845 = getelementptr inbounds double, double* %4833, i64 12
  %4846 = bitcast double* %4845 to <2 x double>*
  %wide.load10903 = load <2 x double>, <2 x double>* %4846, align 8, !tbaa !62
  %4847 = getelementptr inbounds double, double* %4833, i64 14
  %4848 = bitcast double* %4847 to <2 x double>*
  %wide.load10904 = load <2 x double>, <2 x double>* %4848, align 8, !tbaa !62
  %4849 = getelementptr inbounds double, double* %4833, i64 16
  %4850 = bitcast double* %4849 to <2 x double>*
  %wide.load10905 = load <2 x double>, <2 x double>* %4850, align 8, !tbaa !62
  %4851 = getelementptr inbounds double, double* %4833, i64 18
  %4852 = bitcast double* %4851 to <2 x double>*
  %wide.load10906 = load <2 x double>, <2 x double>* %4852, align 8, !tbaa !62
  %4853 = getelementptr inbounds double, double* %4833, i64 20
  %4854 = bitcast double* %4853 to <2 x double>*
  %wide.load10907 = load <2 x double>, <2 x double>* %4854, align 8, !tbaa !62
  %4855 = getelementptr inbounds double, double* %4833, i64 22
  %4856 = bitcast double* %4855 to <2 x double>*
  %wide.load10908 = load <2 x double>, <2 x double>* %4856, align 8, !tbaa !62
  %4857 = fadd <2 x double> %wide.load10885, %wide.load10897
  %4858 = fadd <2 x double> %wide.load10886, %wide.load10898
  %4859 = fadd <2 x double> %wide.load10887, %wide.load10899
  %4860 = fadd <2 x double> %wide.load10888, %wide.load10900
  %4861 = fadd <2 x double> %wide.load10889, %wide.load10901
  %4862 = fadd <2 x double> %wide.load10890, %wide.load10902
  %4863 = fadd <2 x double> %wide.load10891, %wide.load10903
  %4864 = fadd <2 x double> %wide.load10892, %wide.load10904
  %4865 = fadd <2 x double> %wide.load10893, %wide.load10905
  %4866 = fadd <2 x double> %wide.load10894, %wide.load10906
  %4867 = fadd <2 x double> %wide.load10895, %wide.load10907
  %4868 = fadd <2 x double> %wide.load10896, %wide.load10908
  %4869 = getelementptr inbounds double, double* %1795, i64 %index10867
  %4870 = bitcast double* %4869 to <2 x double>*
  %wide.load10909 = load <2 x double>, <2 x double>* %4870, align 8, !tbaa !62
  %4871 = getelementptr inbounds double, double* %4869, i64 2
  %4872 = bitcast double* %4871 to <2 x double>*
  %wide.load10910 = load <2 x double>, <2 x double>* %4872, align 8, !tbaa !62
  %4873 = getelementptr inbounds double, double* %4869, i64 4
  %4874 = bitcast double* %4873 to <2 x double>*
  %wide.load10911 = load <2 x double>, <2 x double>* %4874, align 8, !tbaa !62
  %4875 = getelementptr inbounds double, double* %4869, i64 6
  %4876 = bitcast double* %4875 to <2 x double>*
  %wide.load10912 = load <2 x double>, <2 x double>* %4876, align 8, !tbaa !62
  %4877 = getelementptr inbounds double, double* %4869, i64 8
  %4878 = bitcast double* %4877 to <2 x double>*
  %wide.load10913 = load <2 x double>, <2 x double>* %4878, align 8, !tbaa !62
  %4879 = getelementptr inbounds double, double* %4869, i64 10
  %4880 = bitcast double* %4879 to <2 x double>*
  %wide.load10914 = load <2 x double>, <2 x double>* %4880, align 8, !tbaa !62
  %4881 = getelementptr inbounds double, double* %4869, i64 12
  %4882 = bitcast double* %4881 to <2 x double>*
  %wide.load10915 = load <2 x double>, <2 x double>* %4882, align 8, !tbaa !62
  %4883 = getelementptr inbounds double, double* %4869, i64 14
  %4884 = bitcast double* %4883 to <2 x double>*
  %wide.load10916 = load <2 x double>, <2 x double>* %4884, align 8, !tbaa !62
  %4885 = getelementptr inbounds double, double* %4869, i64 16
  %4886 = bitcast double* %4885 to <2 x double>*
  %wide.load10917 = load <2 x double>, <2 x double>* %4886, align 8, !tbaa !62
  %4887 = getelementptr inbounds double, double* %4869, i64 18
  %4888 = bitcast double* %4887 to <2 x double>*
  %wide.load10918 = load <2 x double>, <2 x double>* %4888, align 8, !tbaa !62
  %4889 = getelementptr inbounds double, double* %4869, i64 20
  %4890 = bitcast double* %4889 to <2 x double>*
  %wide.load10919 = load <2 x double>, <2 x double>* %4890, align 8, !tbaa !62
  %4891 = getelementptr inbounds double, double* %4869, i64 22
  %4892 = bitcast double* %4891 to <2 x double>*
  %wide.load10920 = load <2 x double>, <2 x double>* %4892, align 8, !tbaa !62
  %4893 = fadd <2 x double> %4857, %wide.load10909
  %4894 = fadd <2 x double> %4858, %wide.load10910
  %4895 = fadd <2 x double> %4859, %wide.load10911
  %4896 = fadd <2 x double> %4860, %wide.load10912
  %4897 = fadd <2 x double> %4861, %wide.load10913
  %4898 = fadd <2 x double> %4862, %wide.load10914
  %4899 = fadd <2 x double> %4863, %wide.load10915
  %4900 = fadd <2 x double> %4864, %wide.load10916
  %4901 = fadd <2 x double> %4865, %wide.load10917
  %4902 = fadd <2 x double> %4866, %wide.load10918
  %4903 = fadd <2 x double> %4867, %wide.load10919
  %4904 = fadd <2 x double> %4868, %wide.load10920
  store <2 x double> %4893, <2 x double>* %4870, align 8, !tbaa !62
  store <2 x double> %4894, <2 x double>* %4872, align 8, !tbaa !62
  store <2 x double> %4895, <2 x double>* %4874, align 8, !tbaa !62
  store <2 x double> %4896, <2 x double>* %4876, align 8, !tbaa !62
  store <2 x double> %4897, <2 x double>* %4878, align 8, !tbaa !62
  store <2 x double> %4898, <2 x double>* %4880, align 8, !tbaa !62
  store <2 x double> %4899, <2 x double>* %4882, align 8, !tbaa !62
  store <2 x double> %4900, <2 x double>* %4884, align 8, !tbaa !62
  store <2 x double> %4901, <2 x double>* %4886, align 8, !tbaa !62
  store <2 x double> %4902, <2 x double>* %4888, align 8, !tbaa !62
  store <2 x double> %4903, <2 x double>* %4890, align 8, !tbaa !62
  store <2 x double> %4904, <2 x double>* %4892, align 8, !tbaa !62
  %index.next10868 = add nuw nsw i64 %index10867, 24
  %4905 = icmp eq i64 %index.next10868, %n.vec10866
  br i1 %4905, label %middle.block10859, label %vector.body10858, !llvm.loop !105

middle.block10859:                                ; preds = %vector.body10858
  br i1 %cmp.n10870, label %for.cond.cleanup4734, label %for.body4735

for.cond.cleanup4728:                             ; preds = %for.cond.cleanup4734, %for.cond4725.preheader
  store i8* %call3733, i8** %4609, align 8
  store i8* %call3733, i8** %4610, align 8
  store i64 %4802, i64* %4611, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %4609, i8** nonnull %4610, i64* nonnull %4611, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body4761

for.cond.cleanup4734:                             ; preds = %for.body4735, %middle.block10859
  %inc4749 = add nuw nsw i32 %times4724.08933, 1
  %exitcond9510 = icmp eq i32 %inc4749, %inc4696
  br i1 %exitcond9510, label %for.cond.cleanup4728, label %for.cond4731.preheader

for.body4735:                                     ; preds = %middle.block10859, %for.body4735
  %indvars.iv9507 = phi i64 [ %indvars.iv.next9508, %for.body4735 ], [ %n.vec10866, %middle.block10859 ]
  %arrayidx4737 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9507
  %4906 = load double, double* %arrayidx4737, align 8, !tbaa !62
  %arrayidx4739 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9507
  %4907 = load double, double* %arrayidx4739, align 8, !tbaa !62
  %add4740 = fadd double %4906, %4907
  %arrayidx4742 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9507
  %4908 = load double, double* %arrayidx4742, align 8, !tbaa !62
  %add4743 = fadd double %4908, %add4740
  store double %add4743, double* %arrayidx4742, align 8, !tbaa !62
  %indvars.iv.next9508 = add nuw nsw i64 %indvars.iv9507, 1
  %exitcond9509 = icmp eq i64 %indvars.iv.next9508, %indvars.iv9513
  br i1 %exitcond9509, label %for.cond.cleanup4734, label %for.body4735, !llvm.loop !106

for.body4761:                                     ; preds = %for.inc4775.7, %for.cond.cleanup4728
  %indvars.iv9511 = phi i64 [ 0, %for.cond.cleanup4728 ], [ %indvars.iv.next9512.7, %for.inc4775.7 ]
  %arrayidx4763 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9511
  %4909 = load double, double* %arrayidx4763, align 8, !tbaa !62
  %arrayidx4765 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9511
  %4910 = load double, double* %arrayidx4765, align 8, !tbaa !62
  %cmp4766 = fcmp une double %4909, %4910
  br i1 %cmp4766, label %cleanup4785, label %for.inc4775

for.inc4775:                                      ; preds = %for.body4761
  %indvars.iv.next9512 = or i64 %indvars.iv9511, 1
  %arrayidx4763.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9512
  %4911 = load double, double* %arrayidx4763.1, align 8, !tbaa !62
  %arrayidx4765.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9512
  %4912 = load double, double* %arrayidx4765.1, align 8, !tbaa !62
  %cmp4766.1 = fcmp une double %4911, %4912
  br i1 %cmp4766.1, label %cleanup4785, label %for.inc4775.1

for.inc4783:                                      ; preds = %for.inc4775.7
  %indvars.iv.next9514 = add nuw nsw i64 %indvars.iv9513, 5000
  %cmp4667 = icmp ult i64 %indvars.iv.next9514, 25000
  %indvar.next10862 = add nuw nsw i64 %indvar10861, 1
  br i1 %cmp4667, label %for.body.i8519.preheader, label %for.end4787

cleanup4785:                                      ; preds = %for.inc4775.6, %for.inc4775.5, %for.inc4775.4, %for.inc4775.3, %for.inc4775.2, %for.inc4775.1, %for.inc4775, %for.body4761
  %indvars.iv9511.lcssa = phi i64 [ %indvars.iv9511, %for.body4761 ], [ %indvars.iv.next9512, %for.inc4775 ], [ %indvars.iv.next9512.1, %for.inc4775.1 ], [ %indvars.iv.next9512.2, %for.inc4775.2 ], [ %indvars.iv.next9512.3, %for.inc4775.3 ], [ %indvars.iv.next9512.4, %for.inc4775.4 ], [ %indvars.iv.next9512.5, %for.inc4775.5 ], [ %indvars.iv.next9512.6, %for.inc4775.6 ]
  %.lcssa11816 = phi double [ %4909, %for.body4761 ], [ %4911, %for.inc4775 ], [ %7824, %for.inc4775.1 ], [ %7826, %for.inc4775.2 ], [ %7828, %for.inc4775.3 ], [ %7830, %for.inc4775.4 ], [ %7832, %for.inc4775.5 ], [ %7834, %for.inc4775.6 ]
  %.lcssa11814 = phi double [ %4910, %for.body4761 ], [ %4912, %for.inc4775 ], [ %7825, %for.inc4775.1 ], [ %7827, %for.inc4775.2 ], [ %7829, %for.inc4775.3 ], [ %7831, %for.inc4775.4 ], [ %7833, %for.inc4775.5 ], [ %7835, %for.inc4775.6 ]
  %4913 = trunc i64 %indvars.iv9513 to i32
  %4914 = trunc i64 %indvars.iv9511.lcssa to i32
  %call4773 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %4913, i32 signext %4914, double %.lcssa11816, double %.lcssa11814)
  br label %cleanup5832

for.end4787:                                      ; preds = %for.inc4783
  %puts8261 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8262 = call i32 @puts(i8* getelementptr inbounds ([53 x i8], [53 x i8]* @str.351, i64 0, i64 0))
  %4915 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4800, i64 0, i64 0
  %4916 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4801, i64 0, i64 0
  %4917 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4802, i64 0, i64 0
  %4918 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4800, i64 0, i64 1
  %4919 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4801, i64 0, i64 1
  %4920 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4802, i64 0, i64 1
  %4921 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4800, i64 0, i64 2
  %4922 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4801, i64 0, i64 2
  %4923 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4802, i64 0, i64 2
  %4924 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4833, i64 0, i64 0
  %4925 = bitcast [7 x i8*]* %.offload_baseptrs4833 to i64*
  %4926 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4834, i64 0, i64 0
  %4927 = bitcast [7 x i8*]* %.offload_ptrs4834 to i64*
  %4928 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4833, i64 0, i64 1
  %4929 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4834, i64 0, i64 1
  %4930 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4833, i64 0, i64 2
  %4931 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4834, i64 0, i64 2
  %4932 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4833, i64 0, i64 3
  %4933 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4834, i64 0, i64 3
  %4934 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4833, i64 0, i64 4
  %4935 = bitcast i8** %4934 to i64*
  %4936 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4834, i64 0, i64 4
  %4937 = bitcast i8** %4936 to i64*
  %4938 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4833, i64 0, i64 5
  %4939 = bitcast i8** %4938 to i64*
  %4940 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4834, i64 0, i64 5
  %4941 = bitcast i8** %4940 to i64*
  %4942 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs4833, i64 0, i64 6
  %4943 = bitcast i8** %4942 to i64*
  %4944 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs4834, i64 0, i64 6
  %4945 = bitcast i8** %4944 to i64*
  %4946 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs4878, i64 0, i64 0
  %4947 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs4879, i64 0, i64 0
  %4948 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes4880, i64 0, i64 0
  %4949 = bitcast i8* %arrayidx.i to <2 x double>*
  %4950 = bitcast i8* %arrayidx2.i to <2 x double>*
  %4951 = bitcast i8* %arrayidx5.i to <2 x double>*
  %4952 = bitcast i8* %arrayidx8.i to <2 x double>*
  %4953 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %4954 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %4955 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %4956 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %4957 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %4958 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %4959 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %4960 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %4961 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %4962 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %4963 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %4964 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %4965 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %4966 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %4967 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %4968 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %4969 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %4970 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %4971 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %4972 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %4973 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %4974 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %4975 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %4976 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %4977 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %4978 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %4979 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %4980 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8534.preheader

for.body.i8534.preheader:                         ; preds = %for.end4787, %for.inc4908
  %indvar10960 = phi i64 [ 0, %for.end4787 ], [ %indvar.next10961, %for.inc4908 ]
  %indvars.iv9505 = phi i64 [ 32, %for.end4787 ], [ %indvars.iv.next9506, %for.inc4908 ]
  %4981 = mul nuw nsw i64 %indvar10960, 5000
  br label %vector.body11020

vector.body11020:                                 ; preds = %vector.body11020, %for.body.i8534.preheader
  %index11024 = phi i64 [ 0, %for.body.i8534.preheader ], [ %index.next11025, %vector.body11020 ]
  %vec.ind11042 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8534.preheader ], [ %vec.ind.next11055, %vector.body11020 ]
  %step.add11043 = add <2 x i32> %vec.ind11042, <i32 2, i32 2>
  %step.add11044 = add <2 x i32> %vec.ind11042, <i32 4, i32 4>
  %step.add11045 = add <2 x i32> %vec.ind11042, <i32 6, i32 6>
  %step.add11046 = add <2 x i32> %vec.ind11042, <i32 8, i32 8>
  %step.add11047 = add <2 x i32> %vec.ind11042, <i32 10, i32 10>
  %step.add11048 = add <2 x i32> %vec.ind11042, <i32 12, i32 12>
  %step.add11049 = add <2 x i32> %vec.ind11042, <i32 14, i32 14>
  %step.add11050 = add <2 x i32> %vec.ind11042, <i32 16, i32 16>
  %step.add11051 = add <2 x i32> %vec.ind11042, <i32 18, i32 18>
  %step.add11052 = add <2 x i32> %vec.ind11042, <i32 20, i32 20>
  %step.add11053 = add <2 x i32> %vec.ind11042, <i32 22, i32 22>
  %4982 = sitofp <2 x i32> %vec.ind11042 to <2 x double>
  %4983 = sitofp <2 x i32> %step.add11043 to <2 x double>
  %4984 = sitofp <2 x i32> %step.add11044 to <2 x double>
  %4985 = sitofp <2 x i32> %step.add11045 to <2 x double>
  %4986 = sitofp <2 x i32> %step.add11046 to <2 x double>
  %4987 = sitofp <2 x i32> %step.add11047 to <2 x double>
  %4988 = sitofp <2 x i32> %step.add11048 to <2 x double>
  %4989 = sitofp <2 x i32> %step.add11049 to <2 x double>
  %4990 = sitofp <2 x i32> %step.add11050 to <2 x double>
  %4991 = sitofp <2 x i32> %step.add11051 to <2 x double>
  %4992 = sitofp <2 x i32> %step.add11052 to <2 x double>
  %4993 = sitofp <2 x i32> %step.add11053 to <2 x double>
  %4994 = getelementptr inbounds double, double* %1795, i64 %index11024
  %4995 = bitcast double* %4994 to <2 x double>*
  store <2 x double> %4982, <2 x double>* %4995, align 8, !tbaa !62
  %4996 = getelementptr inbounds double, double* %4994, i64 2
  %4997 = bitcast double* %4996 to <2 x double>*
  store <2 x double> %4983, <2 x double>* %4997, align 8, !tbaa !62
  %4998 = getelementptr inbounds double, double* %4994, i64 4
  %4999 = bitcast double* %4998 to <2 x double>*
  store <2 x double> %4984, <2 x double>* %4999, align 8, !tbaa !62
  %5000 = getelementptr inbounds double, double* %4994, i64 6
  %5001 = bitcast double* %5000 to <2 x double>*
  store <2 x double> %4985, <2 x double>* %5001, align 8, !tbaa !62
  %5002 = getelementptr inbounds double, double* %4994, i64 8
  %5003 = bitcast double* %5002 to <2 x double>*
  store <2 x double> %4986, <2 x double>* %5003, align 8, !tbaa !62
  %5004 = getelementptr inbounds double, double* %4994, i64 10
  %5005 = bitcast double* %5004 to <2 x double>*
  store <2 x double> %4987, <2 x double>* %5005, align 8, !tbaa !62
  %5006 = getelementptr inbounds double, double* %4994, i64 12
  %5007 = bitcast double* %5006 to <2 x double>*
  store <2 x double> %4988, <2 x double>* %5007, align 8, !tbaa !62
  %5008 = getelementptr inbounds double, double* %4994, i64 14
  %5009 = bitcast double* %5008 to <2 x double>*
  store <2 x double> %4989, <2 x double>* %5009, align 8, !tbaa !62
  %5010 = getelementptr inbounds double, double* %4994, i64 16
  %5011 = bitcast double* %5010 to <2 x double>*
  store <2 x double> %4990, <2 x double>* %5011, align 8, !tbaa !62
  %5012 = getelementptr inbounds double, double* %4994, i64 18
  %5013 = bitcast double* %5012 to <2 x double>*
  store <2 x double> %4991, <2 x double>* %5013, align 8, !tbaa !62
  %5014 = getelementptr inbounds double, double* %4994, i64 20
  %5015 = bitcast double* %5014 to <2 x double>*
  store <2 x double> %4992, <2 x double>* %5015, align 8, !tbaa !62
  %5016 = getelementptr inbounds double, double* %4994, i64 22
  %5017 = bitcast double* %5016 to <2 x double>*
  store <2 x double> %4993, <2 x double>* %5017, align 8, !tbaa !62
  %5018 = getelementptr inbounds double, double* %1794, i64 %index11024
  %5019 = bitcast double* %5018 to <2 x double>*
  store <2 x double> %4982, <2 x double>* %5019, align 8, !tbaa !62
  %5020 = getelementptr inbounds double, double* %5018, i64 2
  %5021 = bitcast double* %5020 to <2 x double>*
  store <2 x double> %4983, <2 x double>* %5021, align 8, !tbaa !62
  %5022 = getelementptr inbounds double, double* %5018, i64 4
  %5023 = bitcast double* %5022 to <2 x double>*
  store <2 x double> %4984, <2 x double>* %5023, align 8, !tbaa !62
  %5024 = getelementptr inbounds double, double* %5018, i64 6
  %5025 = bitcast double* %5024 to <2 x double>*
  store <2 x double> %4985, <2 x double>* %5025, align 8, !tbaa !62
  %5026 = getelementptr inbounds double, double* %5018, i64 8
  %5027 = bitcast double* %5026 to <2 x double>*
  store <2 x double> %4986, <2 x double>* %5027, align 8, !tbaa !62
  %5028 = getelementptr inbounds double, double* %5018, i64 10
  %5029 = bitcast double* %5028 to <2 x double>*
  store <2 x double> %4987, <2 x double>* %5029, align 8, !tbaa !62
  %5030 = getelementptr inbounds double, double* %5018, i64 12
  %5031 = bitcast double* %5030 to <2 x double>*
  store <2 x double> %4988, <2 x double>* %5031, align 8, !tbaa !62
  %5032 = getelementptr inbounds double, double* %5018, i64 14
  %5033 = bitcast double* %5032 to <2 x double>*
  store <2 x double> %4989, <2 x double>* %5033, align 8, !tbaa !62
  %5034 = getelementptr inbounds double, double* %5018, i64 16
  %5035 = bitcast double* %5034 to <2 x double>*
  store <2 x double> %4990, <2 x double>* %5035, align 8, !tbaa !62
  %5036 = getelementptr inbounds double, double* %5018, i64 18
  %5037 = bitcast double* %5036 to <2 x double>*
  store <2 x double> %4991, <2 x double>* %5037, align 8, !tbaa !62
  %5038 = getelementptr inbounds double, double* %5018, i64 20
  %5039 = bitcast double* %5038 to <2 x double>*
  store <2 x double> %4992, <2 x double>* %5039, align 8, !tbaa !62
  %5040 = getelementptr inbounds double, double* %5018, i64 22
  %5041 = bitcast double* %5040 to <2 x double>*
  store <2 x double> %4993, <2 x double>* %5041, align 8, !tbaa !62
  %5042 = shl <2 x i32> %vec.ind11042, <i32 1, i32 1>
  %5043 = shl <2 x i32> %step.add11043, <i32 1, i32 1>
  %5044 = shl <2 x i32> %step.add11044, <i32 1, i32 1>
  %5045 = shl <2 x i32> %step.add11045, <i32 1, i32 1>
  %5046 = shl <2 x i32> %step.add11046, <i32 1, i32 1>
  %5047 = shl <2 x i32> %step.add11047, <i32 1, i32 1>
  %5048 = shl <2 x i32> %step.add11048, <i32 1, i32 1>
  %5049 = shl <2 x i32> %step.add11049, <i32 1, i32 1>
  %5050 = shl <2 x i32> %step.add11050, <i32 1, i32 1>
  %5051 = shl <2 x i32> %step.add11051, <i32 1, i32 1>
  %5052 = shl <2 x i32> %step.add11052, <i32 1, i32 1>
  %5053 = shl <2 x i32> %step.add11053, <i32 1, i32 1>
  %5054 = sitofp <2 x i32> %5042 to <2 x double>
  %5055 = sitofp <2 x i32> %5043 to <2 x double>
  %5056 = sitofp <2 x i32> %5044 to <2 x double>
  %5057 = sitofp <2 x i32> %5045 to <2 x double>
  %5058 = sitofp <2 x i32> %5046 to <2 x double>
  %5059 = sitofp <2 x i32> %5047 to <2 x double>
  %5060 = sitofp <2 x i32> %5048 to <2 x double>
  %5061 = sitofp <2 x i32> %5049 to <2 x double>
  %5062 = sitofp <2 x i32> %5050 to <2 x double>
  %5063 = sitofp <2 x i32> %5051 to <2 x double>
  %5064 = sitofp <2 x i32> %5052 to <2 x double>
  %5065 = sitofp <2 x i32> %5053 to <2 x double>
  %5066 = getelementptr inbounds double, double* %1796, i64 %index11024
  %5067 = bitcast double* %5066 to <2 x double>*
  store <2 x double> %5054, <2 x double>* %5067, align 8, !tbaa !62
  %5068 = getelementptr inbounds double, double* %5066, i64 2
  %5069 = bitcast double* %5068 to <2 x double>*
  store <2 x double> %5055, <2 x double>* %5069, align 8, !tbaa !62
  %5070 = getelementptr inbounds double, double* %5066, i64 4
  %5071 = bitcast double* %5070 to <2 x double>*
  store <2 x double> %5056, <2 x double>* %5071, align 8, !tbaa !62
  %5072 = getelementptr inbounds double, double* %5066, i64 6
  %5073 = bitcast double* %5072 to <2 x double>*
  store <2 x double> %5057, <2 x double>* %5073, align 8, !tbaa !62
  %5074 = getelementptr inbounds double, double* %5066, i64 8
  %5075 = bitcast double* %5074 to <2 x double>*
  store <2 x double> %5058, <2 x double>* %5075, align 8, !tbaa !62
  %5076 = getelementptr inbounds double, double* %5066, i64 10
  %5077 = bitcast double* %5076 to <2 x double>*
  store <2 x double> %5059, <2 x double>* %5077, align 8, !tbaa !62
  %5078 = getelementptr inbounds double, double* %5066, i64 12
  %5079 = bitcast double* %5078 to <2 x double>*
  store <2 x double> %5060, <2 x double>* %5079, align 8, !tbaa !62
  %5080 = getelementptr inbounds double, double* %5066, i64 14
  %5081 = bitcast double* %5080 to <2 x double>*
  store <2 x double> %5061, <2 x double>* %5081, align 8, !tbaa !62
  %5082 = getelementptr inbounds double, double* %5066, i64 16
  %5083 = bitcast double* %5082 to <2 x double>*
  store <2 x double> %5062, <2 x double>* %5083, align 8, !tbaa !62
  %5084 = getelementptr inbounds double, double* %5066, i64 18
  %5085 = bitcast double* %5084 to <2 x double>*
  store <2 x double> %5063, <2 x double>* %5085, align 8, !tbaa !62
  %5086 = getelementptr inbounds double, double* %5066, i64 20
  %5087 = bitcast double* %5086 to <2 x double>*
  store <2 x double> %5064, <2 x double>* %5087, align 8, !tbaa !62
  %5088 = getelementptr inbounds double, double* %5066, i64 22
  %5089 = bitcast double* %5088 to <2 x double>*
  store <2 x double> %5065, <2 x double>* %5089, align 8, !tbaa !62
  %5090 = add <2 x i32> %vec.ind11042, <i32 -3, i32 -3>
  %5091 = add <2 x i32> %vec.ind11042, <i32 -1, i32 -1>
  %5092 = add <2 x i32> %vec.ind11042, <i32 1, i32 1>
  %5093 = add <2 x i32> %vec.ind11042, <i32 3, i32 3>
  %5094 = add <2 x i32> %vec.ind11042, <i32 5, i32 5>
  %5095 = add <2 x i32> %vec.ind11042, <i32 7, i32 7>
  %5096 = add <2 x i32> %vec.ind11042, <i32 9, i32 9>
  %5097 = add <2 x i32> %vec.ind11042, <i32 11, i32 11>
  %5098 = add <2 x i32> %vec.ind11042, <i32 13, i32 13>
  %5099 = add <2 x i32> %vec.ind11042, <i32 15, i32 15>
  %5100 = add <2 x i32> %vec.ind11042, <i32 17, i32 17>
  %5101 = add <2 x i32> %vec.ind11042, <i32 19, i32 19>
  %5102 = sitofp <2 x i32> %5090 to <2 x double>
  %5103 = sitofp <2 x i32> %5091 to <2 x double>
  %5104 = sitofp <2 x i32> %5092 to <2 x double>
  %5105 = sitofp <2 x i32> %5093 to <2 x double>
  %5106 = sitofp <2 x i32> %5094 to <2 x double>
  %5107 = sitofp <2 x i32> %5095 to <2 x double>
  %5108 = sitofp <2 x i32> %5096 to <2 x double>
  %5109 = sitofp <2 x i32> %5097 to <2 x double>
  %5110 = sitofp <2 x i32> %5098 to <2 x double>
  %5111 = sitofp <2 x i32> %5099 to <2 x double>
  %5112 = sitofp <2 x i32> %5100 to <2 x double>
  %5113 = sitofp <2 x i32> %5101 to <2 x double>
  %5114 = getelementptr inbounds double, double* %1797, i64 %index11024
  %5115 = bitcast double* %5114 to <2 x double>*
  store <2 x double> %5102, <2 x double>* %5115, align 8, !tbaa !62
  %5116 = getelementptr inbounds double, double* %5114, i64 2
  %5117 = bitcast double* %5116 to <2 x double>*
  store <2 x double> %5103, <2 x double>* %5117, align 8, !tbaa !62
  %5118 = getelementptr inbounds double, double* %5114, i64 4
  %5119 = bitcast double* %5118 to <2 x double>*
  store <2 x double> %5104, <2 x double>* %5119, align 8, !tbaa !62
  %5120 = getelementptr inbounds double, double* %5114, i64 6
  %5121 = bitcast double* %5120 to <2 x double>*
  store <2 x double> %5105, <2 x double>* %5121, align 8, !tbaa !62
  %5122 = getelementptr inbounds double, double* %5114, i64 8
  %5123 = bitcast double* %5122 to <2 x double>*
  store <2 x double> %5106, <2 x double>* %5123, align 8, !tbaa !62
  %5124 = getelementptr inbounds double, double* %5114, i64 10
  %5125 = bitcast double* %5124 to <2 x double>*
  store <2 x double> %5107, <2 x double>* %5125, align 8, !tbaa !62
  %5126 = getelementptr inbounds double, double* %5114, i64 12
  %5127 = bitcast double* %5126 to <2 x double>*
  store <2 x double> %5108, <2 x double>* %5127, align 8, !tbaa !62
  %5128 = getelementptr inbounds double, double* %5114, i64 14
  %5129 = bitcast double* %5128 to <2 x double>*
  store <2 x double> %5109, <2 x double>* %5129, align 8, !tbaa !62
  %5130 = getelementptr inbounds double, double* %5114, i64 16
  %5131 = bitcast double* %5130 to <2 x double>*
  store <2 x double> %5110, <2 x double>* %5131, align 8, !tbaa !62
  %5132 = getelementptr inbounds double, double* %5114, i64 18
  %5133 = bitcast double* %5132 to <2 x double>*
  store <2 x double> %5111, <2 x double>* %5133, align 8, !tbaa !62
  %5134 = getelementptr inbounds double, double* %5114, i64 20
  %5135 = bitcast double* %5134 to <2 x double>*
  store <2 x double> %5112, <2 x double>* %5135, align 8, !tbaa !62
  %5136 = getelementptr inbounds double, double* %5114, i64 22
  %5137 = bitcast double* %5136 to <2 x double>*
  store <2 x double> %5113, <2 x double>* %5137, align 8, !tbaa !62
  %index.next11025 = add nuw nsw i64 %index11024, 24
  %vec.ind.next11055 = add <2 x i32> %vec.ind11042, <i32 24, i32 24>
  %5138 = icmp eq i64 %index.next11025, 24984
  br i1 %5138, label %for.body.i8534, label %vector.body11020, !llvm.loop !107

for.body.i8534:                                   ; preds = %vector.body11020
  %5139 = add nuw nsw i64 %4981, 32
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %4949, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %4950, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %4951, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %4952, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %4953, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %4954, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %4955, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %4956, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %4957, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %4958, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %4959, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %4960, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %4961, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %4962, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %4963, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %4964, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %4965, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %4966, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %4967, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %4968, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %4969, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %4970, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %4971, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %4972, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %4973, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %4974, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %4975, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %4976, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %4977, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %4978, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %4979, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %4980, align 8, !tbaa !62
  %5140 = shl nuw nsw i64 %indvars.iv9505, 3
  store i8* %call3733, i8** %4915, align 8
  store i8* %call3733, i8** %4916, align 8
  store i64 %5140, i64* %4917, align 8
  store i8* %call3735, i8** %4918, align 8
  store i8* %call3735, i8** %4919, align 8
  store i64 %5140, i64* %4920, align 8
  store i8* %call3736, i8** %4921, align 8
  store i8* %call3736, i8** %4922, align 8
  store i64 %5140, i64* %4923, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %4915, i8** nonnull %4916, i64* nonnull %4917, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  %cmp48178910 = icmp ult i64 %indvars.iv9505, 128
  br i1 %cmp48178910, label %for.cond.cleanup4853, label %for.cond4810.preheader

for.cond4850.preheader:                           ; preds = %for.cond.cleanup4813
  %cmp48518921 = icmp sgt i32 %t4796.28911, -1
  br i1 %cmp48518921, label %for.cond4856.preheader.preheader45, label %for.cond.cleanup4853

for.cond4856.preheader.preheader45:               ; preds = %for.cond4850.preheader
  %n.mod.vf10964 = urem i64 %5139, 24
  %n.vec10965 = sub nuw nsw i64 %5139, %n.mod.vf10964
  %cmp.n10969 = icmp eq i64 %n.mod.vf10964, 0
  br label %for.cond4856.preheader

for.cond4810.preheader:                           ; preds = %for.body.i8534, %for.cond.cleanup4813
  %tms4803.08918 = phi i32 [ %mul4846, %for.cond.cleanup4813 ], [ 1, %for.body.i8534 ]
  %t4796.08917 = phi i32 [ %inc4821, %for.cond.cleanup4813 ], [ 0, %for.body.i8534 ]
  %.capture_expr..casted4829.sroa.0.0.insert.ext = zext i32 %tms4803.08918 to i64
  br label %for.cond4816.preheader

for.cond4816.preheader:                           ; preds = %for.cond4810.preheader, %for.cond.cleanup4819
  %ths4809.08915 = phi i32 [ %mul4842, %for.cond.cleanup4819 ], [ 32, %for.cond4810.preheader ]
  %t4796.18914 = phi i32 [ %inc4821, %for.cond.cleanup4819 ], [ %t4796.08917, %for.cond4810.preheader ]
  %.capture_expr..casted4831.sroa.0.0.insert.ext = zext i32 %ths4809.08915 to i64
  br label %for.body4820

for.cond.cleanup4813:                             ; preds = %for.cond.cleanup4819
  %mul4846 = shl nsw i32 %tms4803.08918, 1
  %cmp4805 = icmp ult i32 %mul4846, 257
  br i1 %cmp4805, label %for.cond4810.preheader, label %for.cond4850.preheader

for.cond.cleanup4819:                             ; preds = %omp_offload.cont4836
  %mul4842 = shl nsw i32 %ths4809.08915, 1
  %cmp4811 = icmp ult i32 %mul4842, 1025
  br i1 %cmp4811, label %for.cond4816.preheader, label %for.cond.cleanup4813

for.body4820:                                     ; preds = %for.cond4816.preheader, %omp_offload.cont4836
  %sch4815.08912 = phi i32 [ 128, %for.cond4816.preheader ], [ %mul4838, %omp_offload.cont4836 ]
  %t4796.28911 = phi i32 [ %t4796.18914, %for.cond4816.preheader ], [ %inc4821, %omp_offload.cont4836 ]
  %inc4821 = add nsw i32 %t4796.28911, 1
  %.capture_expr..casted4827.sroa.0.0.insert.ext = zext i32 %sch4815.08912 to i64
  store i64 %indvars.iv9505, i64* %4925, align 8
  store i64 %indvars.iv9505, i64* %4927, align 8
  store i8* %call3733, i8** %4928, align 8
  store i8* %call3733, i8** %4929, align 8
  store i8* %call3735, i8** %4930, align 8
  store i8* %call3735, i8** %4931, align 8
  store i8* %call3736, i8** %4932, align 8
  store i8* %call3736, i8** %4933, align 8
  store i64 %.capture_expr..casted4827.sroa.0.0.insert.ext, i64* %4935, align 8
  store i64 %.capture_expr..casted4827.sroa.0.0.insert.ext, i64* %4937, align 8
  store i64 %.capture_expr..casted4829.sroa.0.0.insert.ext, i64* %4939, align 8
  store i64 %.capture_expr..casted4829.sroa.0.0.insert.ext, i64* %4941, align 8
  store i64 %.capture_expr..casted4831.sroa.0.0.insert.ext, i64* %4943, align 8
  store i64 %.capture_expr..casted4831.sroa.0.0.insert.ext, i64* %4945, align 8
  %5141 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1354.region_id, i32 7, i8** nonnull %4924, i8** nonnull %4926, i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_sizes.273, i64 0, i64 0), i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_maptypes.274, i64 0, i64 0), i32 %tms4803.08918, i32 %ths4809.08915) #6
  %5142 = icmp eq i32 %5141, 0
  br i1 %5142, label %omp_offload.cont4836, label %omp_offload.failed4835

omp_offload.failed4835:                           ; preds = %for.body4820
  %5143 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %5144 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %5143, i32 %tms4803.08918, i32 %ths4809.08915) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 5, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64)* @.omp_outlined..222 to void (i32*, i32*, ...)*), i64 %indvars.iv9505, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted4827.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont4836

omp_offload.cont4836:                             ; preds = %for.body4820, %omp_offload.failed4835
  %mul4838 = mul nsw i32 %sch4815.08912, 1200
  %5145 = zext i32 %mul4838 to i64
  %cmp4817 = icmp ult i64 %indvars.iv9505, %5145
  br i1 %cmp4817, label %for.cond.cleanup4819, label %for.body4820

for.cond4856.preheader:                           ; preds = %for.cond4856.preheader.preheader45, %for.cond.cleanup4859
  %times4849.08922 = phi i32 [ %inc4874, %for.cond.cleanup4859 ], [ 0, %for.cond4856.preheader.preheader45 ]
  br label %vector.body10957

vector.body10957:                                 ; preds = %for.cond4856.preheader, %vector.body10957
  %index10966 = phi i64 [ %index.next10967, %vector.body10957 ], [ 0, %for.cond4856.preheader ]
  %5146 = getelementptr inbounds double, double* %1796, i64 %index10966
  %5147 = bitcast double* %5146 to <2 x double>*
  %wide.load10984 = load <2 x double>, <2 x double>* %5147, align 8, !tbaa !62
  %5148 = getelementptr inbounds double, double* %5146, i64 2
  %5149 = bitcast double* %5148 to <2 x double>*
  %wide.load10985 = load <2 x double>, <2 x double>* %5149, align 8, !tbaa !62
  %5150 = getelementptr inbounds double, double* %5146, i64 4
  %5151 = bitcast double* %5150 to <2 x double>*
  %wide.load10986 = load <2 x double>, <2 x double>* %5151, align 8, !tbaa !62
  %5152 = getelementptr inbounds double, double* %5146, i64 6
  %5153 = bitcast double* %5152 to <2 x double>*
  %wide.load10987 = load <2 x double>, <2 x double>* %5153, align 8, !tbaa !62
  %5154 = getelementptr inbounds double, double* %5146, i64 8
  %5155 = bitcast double* %5154 to <2 x double>*
  %wide.load10988 = load <2 x double>, <2 x double>* %5155, align 8, !tbaa !62
  %5156 = getelementptr inbounds double, double* %5146, i64 10
  %5157 = bitcast double* %5156 to <2 x double>*
  %wide.load10989 = load <2 x double>, <2 x double>* %5157, align 8, !tbaa !62
  %5158 = getelementptr inbounds double, double* %5146, i64 12
  %5159 = bitcast double* %5158 to <2 x double>*
  %wide.load10990 = load <2 x double>, <2 x double>* %5159, align 8, !tbaa !62
  %5160 = getelementptr inbounds double, double* %5146, i64 14
  %5161 = bitcast double* %5160 to <2 x double>*
  %wide.load10991 = load <2 x double>, <2 x double>* %5161, align 8, !tbaa !62
  %5162 = getelementptr inbounds double, double* %5146, i64 16
  %5163 = bitcast double* %5162 to <2 x double>*
  %wide.load10992 = load <2 x double>, <2 x double>* %5163, align 8, !tbaa !62
  %5164 = getelementptr inbounds double, double* %5146, i64 18
  %5165 = bitcast double* %5164 to <2 x double>*
  %wide.load10993 = load <2 x double>, <2 x double>* %5165, align 8, !tbaa !62
  %5166 = getelementptr inbounds double, double* %5146, i64 20
  %5167 = bitcast double* %5166 to <2 x double>*
  %wide.load10994 = load <2 x double>, <2 x double>* %5167, align 8, !tbaa !62
  %5168 = getelementptr inbounds double, double* %5146, i64 22
  %5169 = bitcast double* %5168 to <2 x double>*
  %wide.load10995 = load <2 x double>, <2 x double>* %5169, align 8, !tbaa !62
  %5170 = getelementptr inbounds double, double* %1797, i64 %index10966
  %5171 = bitcast double* %5170 to <2 x double>*
  %wide.load10996 = load <2 x double>, <2 x double>* %5171, align 8, !tbaa !62
  %5172 = getelementptr inbounds double, double* %5170, i64 2
  %5173 = bitcast double* %5172 to <2 x double>*
  %wide.load10997 = load <2 x double>, <2 x double>* %5173, align 8, !tbaa !62
  %5174 = getelementptr inbounds double, double* %5170, i64 4
  %5175 = bitcast double* %5174 to <2 x double>*
  %wide.load10998 = load <2 x double>, <2 x double>* %5175, align 8, !tbaa !62
  %5176 = getelementptr inbounds double, double* %5170, i64 6
  %5177 = bitcast double* %5176 to <2 x double>*
  %wide.load10999 = load <2 x double>, <2 x double>* %5177, align 8, !tbaa !62
  %5178 = getelementptr inbounds double, double* %5170, i64 8
  %5179 = bitcast double* %5178 to <2 x double>*
  %wide.load11000 = load <2 x double>, <2 x double>* %5179, align 8, !tbaa !62
  %5180 = getelementptr inbounds double, double* %5170, i64 10
  %5181 = bitcast double* %5180 to <2 x double>*
  %wide.load11001 = load <2 x double>, <2 x double>* %5181, align 8, !tbaa !62
  %5182 = getelementptr inbounds double, double* %5170, i64 12
  %5183 = bitcast double* %5182 to <2 x double>*
  %wide.load11002 = load <2 x double>, <2 x double>* %5183, align 8, !tbaa !62
  %5184 = getelementptr inbounds double, double* %5170, i64 14
  %5185 = bitcast double* %5184 to <2 x double>*
  %wide.load11003 = load <2 x double>, <2 x double>* %5185, align 8, !tbaa !62
  %5186 = getelementptr inbounds double, double* %5170, i64 16
  %5187 = bitcast double* %5186 to <2 x double>*
  %wide.load11004 = load <2 x double>, <2 x double>* %5187, align 8, !tbaa !62
  %5188 = getelementptr inbounds double, double* %5170, i64 18
  %5189 = bitcast double* %5188 to <2 x double>*
  %wide.load11005 = load <2 x double>, <2 x double>* %5189, align 8, !tbaa !62
  %5190 = getelementptr inbounds double, double* %5170, i64 20
  %5191 = bitcast double* %5190 to <2 x double>*
  %wide.load11006 = load <2 x double>, <2 x double>* %5191, align 8, !tbaa !62
  %5192 = getelementptr inbounds double, double* %5170, i64 22
  %5193 = bitcast double* %5192 to <2 x double>*
  %wide.load11007 = load <2 x double>, <2 x double>* %5193, align 8, !tbaa !62
  %5194 = fadd <2 x double> %wide.load10984, %wide.load10996
  %5195 = fadd <2 x double> %wide.load10985, %wide.load10997
  %5196 = fadd <2 x double> %wide.load10986, %wide.load10998
  %5197 = fadd <2 x double> %wide.load10987, %wide.load10999
  %5198 = fadd <2 x double> %wide.load10988, %wide.load11000
  %5199 = fadd <2 x double> %wide.load10989, %wide.load11001
  %5200 = fadd <2 x double> %wide.load10990, %wide.load11002
  %5201 = fadd <2 x double> %wide.load10991, %wide.load11003
  %5202 = fadd <2 x double> %wide.load10992, %wide.load11004
  %5203 = fadd <2 x double> %wide.load10993, %wide.load11005
  %5204 = fadd <2 x double> %wide.load10994, %wide.load11006
  %5205 = fadd <2 x double> %wide.load10995, %wide.load11007
  %5206 = getelementptr inbounds double, double* %1795, i64 %index10966
  %5207 = bitcast double* %5206 to <2 x double>*
  %wide.load11008 = load <2 x double>, <2 x double>* %5207, align 8, !tbaa !62
  %5208 = getelementptr inbounds double, double* %5206, i64 2
  %5209 = bitcast double* %5208 to <2 x double>*
  %wide.load11009 = load <2 x double>, <2 x double>* %5209, align 8, !tbaa !62
  %5210 = getelementptr inbounds double, double* %5206, i64 4
  %5211 = bitcast double* %5210 to <2 x double>*
  %wide.load11010 = load <2 x double>, <2 x double>* %5211, align 8, !tbaa !62
  %5212 = getelementptr inbounds double, double* %5206, i64 6
  %5213 = bitcast double* %5212 to <2 x double>*
  %wide.load11011 = load <2 x double>, <2 x double>* %5213, align 8, !tbaa !62
  %5214 = getelementptr inbounds double, double* %5206, i64 8
  %5215 = bitcast double* %5214 to <2 x double>*
  %wide.load11012 = load <2 x double>, <2 x double>* %5215, align 8, !tbaa !62
  %5216 = getelementptr inbounds double, double* %5206, i64 10
  %5217 = bitcast double* %5216 to <2 x double>*
  %wide.load11013 = load <2 x double>, <2 x double>* %5217, align 8, !tbaa !62
  %5218 = getelementptr inbounds double, double* %5206, i64 12
  %5219 = bitcast double* %5218 to <2 x double>*
  %wide.load11014 = load <2 x double>, <2 x double>* %5219, align 8, !tbaa !62
  %5220 = getelementptr inbounds double, double* %5206, i64 14
  %5221 = bitcast double* %5220 to <2 x double>*
  %wide.load11015 = load <2 x double>, <2 x double>* %5221, align 8, !tbaa !62
  %5222 = getelementptr inbounds double, double* %5206, i64 16
  %5223 = bitcast double* %5222 to <2 x double>*
  %wide.load11016 = load <2 x double>, <2 x double>* %5223, align 8, !tbaa !62
  %5224 = getelementptr inbounds double, double* %5206, i64 18
  %5225 = bitcast double* %5224 to <2 x double>*
  %wide.load11017 = load <2 x double>, <2 x double>* %5225, align 8, !tbaa !62
  %5226 = getelementptr inbounds double, double* %5206, i64 20
  %5227 = bitcast double* %5226 to <2 x double>*
  %wide.load11018 = load <2 x double>, <2 x double>* %5227, align 8, !tbaa !62
  %5228 = getelementptr inbounds double, double* %5206, i64 22
  %5229 = bitcast double* %5228 to <2 x double>*
  %wide.load11019 = load <2 x double>, <2 x double>* %5229, align 8, !tbaa !62
  %5230 = fadd <2 x double> %5194, %wide.load11008
  %5231 = fadd <2 x double> %5195, %wide.load11009
  %5232 = fadd <2 x double> %5196, %wide.load11010
  %5233 = fadd <2 x double> %5197, %wide.load11011
  %5234 = fadd <2 x double> %5198, %wide.load11012
  %5235 = fadd <2 x double> %5199, %wide.load11013
  %5236 = fadd <2 x double> %5200, %wide.load11014
  %5237 = fadd <2 x double> %5201, %wide.load11015
  %5238 = fadd <2 x double> %5202, %wide.load11016
  %5239 = fadd <2 x double> %5203, %wide.load11017
  %5240 = fadd <2 x double> %5204, %wide.load11018
  %5241 = fadd <2 x double> %5205, %wide.load11019
  store <2 x double> %5230, <2 x double>* %5207, align 8, !tbaa !62
  store <2 x double> %5231, <2 x double>* %5209, align 8, !tbaa !62
  store <2 x double> %5232, <2 x double>* %5211, align 8, !tbaa !62
  store <2 x double> %5233, <2 x double>* %5213, align 8, !tbaa !62
  store <2 x double> %5234, <2 x double>* %5215, align 8, !tbaa !62
  store <2 x double> %5235, <2 x double>* %5217, align 8, !tbaa !62
  store <2 x double> %5236, <2 x double>* %5219, align 8, !tbaa !62
  store <2 x double> %5237, <2 x double>* %5221, align 8, !tbaa !62
  store <2 x double> %5238, <2 x double>* %5223, align 8, !tbaa !62
  store <2 x double> %5239, <2 x double>* %5225, align 8, !tbaa !62
  store <2 x double> %5240, <2 x double>* %5227, align 8, !tbaa !62
  store <2 x double> %5241, <2 x double>* %5229, align 8, !tbaa !62
  %index.next10967 = add nuw nsw i64 %index10966, 24
  %5242 = icmp eq i64 %index.next10967, %n.vec10965
  br i1 %5242, label %middle.block10958, label %vector.body10957, !llvm.loop !108

middle.block10958:                                ; preds = %vector.body10957
  br i1 %cmp.n10969, label %for.cond.cleanup4859, label %for.body4860

for.cond.cleanup4853:                             ; preds = %for.cond.cleanup4859, %for.body.i8534, %for.cond4850.preheader
  store i8* %call3733, i8** %4946, align 8
  store i8* %call3733, i8** %4947, align 8
  store i64 %5140, i64* %4948, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %4946, i8** nonnull %4947, i64* nonnull %4948, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body4886

for.cond.cleanup4859:                             ; preds = %for.body4860, %middle.block10958
  %inc4874 = add nuw nsw i32 %times4849.08922, 1
  %exitcond9502 = icmp eq i32 %inc4874, %inc4821
  br i1 %exitcond9502, label %for.cond.cleanup4853, label %for.cond4856.preheader

for.body4860:                                     ; preds = %middle.block10958, %for.body4860
  %indvars.iv9499 = phi i64 [ %indvars.iv.next9500, %for.body4860 ], [ %n.vec10965, %middle.block10958 ]
  %arrayidx4862 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9499
  %5243 = load double, double* %arrayidx4862, align 8, !tbaa !62
  %arrayidx4864 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9499
  %5244 = load double, double* %arrayidx4864, align 8, !tbaa !62
  %add4865 = fadd double %5243, %5244
  %arrayidx4867 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9499
  %5245 = load double, double* %arrayidx4867, align 8, !tbaa !62
  %add4868 = fadd double %5245, %add4865
  store double %add4868, double* %arrayidx4867, align 8, !tbaa !62
  %indvars.iv.next9500 = add nuw nsw i64 %indvars.iv9499, 1
  %exitcond9501 = icmp eq i64 %indvars.iv.next9500, %indvars.iv9505
  br i1 %exitcond9501, label %for.cond.cleanup4859, label %for.body4860, !llvm.loop !109

for.body4886:                                     ; preds = %for.inc4900.7, %for.cond.cleanup4853
  %indvars.iv9503 = phi i64 [ 0, %for.cond.cleanup4853 ], [ %indvars.iv.next9504.7, %for.inc4900.7 ]
  %arrayidx4888 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9503
  %5246 = load double, double* %arrayidx4888, align 8, !tbaa !62
  %arrayidx4890 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9503
  %5247 = load double, double* %arrayidx4890, align 8, !tbaa !62
  %cmp4891 = fcmp une double %5246, %5247
  br i1 %cmp4891, label %cleanup4910, label %for.inc4900

for.inc4900:                                      ; preds = %for.body4886
  %indvars.iv.next9504 = or i64 %indvars.iv9503, 1
  %arrayidx4888.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9504
  %5248 = load double, double* %arrayidx4888.1, align 8, !tbaa !62
  %arrayidx4890.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9504
  %5249 = load double, double* %arrayidx4890.1, align 8, !tbaa !62
  %cmp4891.1 = fcmp une double %5248, %5249
  br i1 %cmp4891.1, label %cleanup4910, label %for.inc4900.1

for.inc4908:                                      ; preds = %for.inc4900.7
  %indvars.iv.next9506 = add nuw nsw i64 %indvars.iv9505, 5000
  %cmp4792 = icmp ult i64 %indvars.iv.next9506, 25000
  %indvar.next10961 = add nuw nsw i64 %indvar10960, 1
  br i1 %cmp4792, label %for.body.i8534.preheader, label %for.end4912

cleanup4910:                                      ; preds = %for.inc4900.6, %for.inc4900.5, %for.inc4900.4, %for.inc4900.3, %for.inc4900.2, %for.inc4900.1, %for.inc4900, %for.body4886
  %indvars.iv9503.lcssa = phi i64 [ %indvars.iv9503, %for.body4886 ], [ %indvars.iv.next9504, %for.inc4900 ], [ %indvars.iv.next9504.1, %for.inc4900.1 ], [ %indvars.iv.next9504.2, %for.inc4900.2 ], [ %indvars.iv.next9504.3, %for.inc4900.3 ], [ %indvars.iv.next9504.4, %for.inc4900.4 ], [ %indvars.iv.next9504.5, %for.inc4900.5 ], [ %indvars.iv.next9504.6, %for.inc4900.6 ]
  %.lcssa11810 = phi double [ %5246, %for.body4886 ], [ %5248, %for.inc4900 ], [ %7812, %for.inc4900.1 ], [ %7814, %for.inc4900.2 ], [ %7816, %for.inc4900.3 ], [ %7818, %for.inc4900.4 ], [ %7820, %for.inc4900.5 ], [ %7822, %for.inc4900.6 ]
  %.lcssa11808 = phi double [ %5247, %for.body4886 ], [ %5249, %for.inc4900 ], [ %7813, %for.inc4900.1 ], [ %7815, %for.inc4900.2 ], [ %7817, %for.inc4900.3 ], [ %7819, %for.inc4900.4 ], [ %7821, %for.inc4900.5 ], [ %7823, %for.inc4900.6 ]
  %5250 = trunc i64 %indvars.iv9505 to i32
  %5251 = trunc i64 %indvars.iv9503.lcssa to i32
  %call4898 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %5250, i32 signext %5251, double %.lcssa11810, double %.lcssa11808)
  br label %cleanup5832

for.end4912:                                      ; preds = %for.inc4908
  %puts8263 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8264 = call i32 @puts(i8* getelementptr inbounds ([50 x i8], [50 x i8]* @str.353, i64 0, i64 0))
  %5252 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4925, i64 0, i64 0
  %5253 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4926, i64 0, i64 0
  %5254 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4927, i64 0, i64 0
  %5255 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4925, i64 0, i64 1
  %5256 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4926, i64 0, i64 1
  %5257 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4927, i64 0, i64 1
  %5258 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs4925, i64 0, i64 2
  %5259 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs4926, i64 0, i64 2
  %5260 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes4927, i64 0, i64 2
  %5261 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs4966, i64 0, i64 0
  %5262 = bitcast [8 x i8*]* %.offload_baseptrs4966 to i64*
  %5263 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs4967, i64 0, i64 0
  %5264 = bitcast [8 x i8*]* %.offload_ptrs4967 to i64*
  %5265 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs4966, i64 0, i64 1
  %5266 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs4967, i64 0, i64 1
  %5267 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs4966, i64 0, i64 2
  %5268 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs4967, i64 0, i64 2
  %5269 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs4966, i64 0, i64 3
  %5270 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs4967, i64 0, i64 3
  %5271 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs4966, i64 0, i64 4
  %5272 = bitcast i8** %5271 to i64*
  %5273 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs4967, i64 0, i64 4
  %5274 = bitcast i8** %5273 to i64*
  %5275 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs4966, i64 0, i64 5
  %5276 = bitcast i8** %5275 to i64*
  %5277 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs4967, i64 0, i64 5
  %5278 = bitcast i8** %5277 to i64*
  %5279 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs4966, i64 0, i64 6
  %5280 = bitcast i8** %5279 to i64*
  %5281 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs4967, i64 0, i64 6
  %5282 = bitcast i8** %5281 to i64*
  %5283 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs4966, i64 0, i64 7
  %5284 = bitcast i8** %5283 to i64*
  %5285 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs4967, i64 0, i64 7
  %5286 = bitcast i8** %5285 to i64*
  %5287 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs5015, i64 0, i64 0
  %5288 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs5016, i64 0, i64 0
  %5289 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes5017, i64 0, i64 0
  %5290 = bitcast i8* %arrayidx.i to <2 x double>*
  %5291 = bitcast i8* %arrayidx2.i to <2 x double>*
  %5292 = bitcast i8* %arrayidx5.i to <2 x double>*
  %5293 = bitcast i8* %arrayidx8.i to <2 x double>*
  %5294 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %5295 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %5296 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %5297 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %5298 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %5299 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %5300 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %5301 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %5302 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %5303 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %5304 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %5305 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %5306 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %5307 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %5308 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %5309 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %5310 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %5311 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %5312 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %5313 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %5314 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %5315 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %5316 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %5317 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %5318 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %5319 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %5320 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %5321 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8549.preheader

for.body.i8549.preheader:                         ; preds = %for.end4912, %for.inc5045
  %indvar11059 = phi i64 [ 0, %for.end4912 ], [ %indvar.next11060, %for.inc5045 ]
  %indvars.iv9497 = phi i64 [ 32, %for.end4912 ], [ %indvars.iv.next9498, %for.inc5045 ]
  %5322 = mul nuw nsw i64 %indvar11059, 5000
  br label %vector.body11119

vector.body11119:                                 ; preds = %vector.body11119, %for.body.i8549.preheader
  %index11123 = phi i64 [ 0, %for.body.i8549.preheader ], [ %index.next11124, %vector.body11119 ]
  %vec.ind11141 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8549.preheader ], [ %vec.ind.next11154, %vector.body11119 ]
  %step.add11142 = add <2 x i32> %vec.ind11141, <i32 2, i32 2>
  %step.add11143 = add <2 x i32> %vec.ind11141, <i32 4, i32 4>
  %step.add11144 = add <2 x i32> %vec.ind11141, <i32 6, i32 6>
  %step.add11145 = add <2 x i32> %vec.ind11141, <i32 8, i32 8>
  %step.add11146 = add <2 x i32> %vec.ind11141, <i32 10, i32 10>
  %step.add11147 = add <2 x i32> %vec.ind11141, <i32 12, i32 12>
  %step.add11148 = add <2 x i32> %vec.ind11141, <i32 14, i32 14>
  %step.add11149 = add <2 x i32> %vec.ind11141, <i32 16, i32 16>
  %step.add11150 = add <2 x i32> %vec.ind11141, <i32 18, i32 18>
  %step.add11151 = add <2 x i32> %vec.ind11141, <i32 20, i32 20>
  %step.add11152 = add <2 x i32> %vec.ind11141, <i32 22, i32 22>
  %5323 = sitofp <2 x i32> %vec.ind11141 to <2 x double>
  %5324 = sitofp <2 x i32> %step.add11142 to <2 x double>
  %5325 = sitofp <2 x i32> %step.add11143 to <2 x double>
  %5326 = sitofp <2 x i32> %step.add11144 to <2 x double>
  %5327 = sitofp <2 x i32> %step.add11145 to <2 x double>
  %5328 = sitofp <2 x i32> %step.add11146 to <2 x double>
  %5329 = sitofp <2 x i32> %step.add11147 to <2 x double>
  %5330 = sitofp <2 x i32> %step.add11148 to <2 x double>
  %5331 = sitofp <2 x i32> %step.add11149 to <2 x double>
  %5332 = sitofp <2 x i32> %step.add11150 to <2 x double>
  %5333 = sitofp <2 x i32> %step.add11151 to <2 x double>
  %5334 = sitofp <2 x i32> %step.add11152 to <2 x double>
  %5335 = getelementptr inbounds double, double* %1795, i64 %index11123
  %5336 = bitcast double* %5335 to <2 x double>*
  store <2 x double> %5323, <2 x double>* %5336, align 8, !tbaa !62
  %5337 = getelementptr inbounds double, double* %5335, i64 2
  %5338 = bitcast double* %5337 to <2 x double>*
  store <2 x double> %5324, <2 x double>* %5338, align 8, !tbaa !62
  %5339 = getelementptr inbounds double, double* %5335, i64 4
  %5340 = bitcast double* %5339 to <2 x double>*
  store <2 x double> %5325, <2 x double>* %5340, align 8, !tbaa !62
  %5341 = getelementptr inbounds double, double* %5335, i64 6
  %5342 = bitcast double* %5341 to <2 x double>*
  store <2 x double> %5326, <2 x double>* %5342, align 8, !tbaa !62
  %5343 = getelementptr inbounds double, double* %5335, i64 8
  %5344 = bitcast double* %5343 to <2 x double>*
  store <2 x double> %5327, <2 x double>* %5344, align 8, !tbaa !62
  %5345 = getelementptr inbounds double, double* %5335, i64 10
  %5346 = bitcast double* %5345 to <2 x double>*
  store <2 x double> %5328, <2 x double>* %5346, align 8, !tbaa !62
  %5347 = getelementptr inbounds double, double* %5335, i64 12
  %5348 = bitcast double* %5347 to <2 x double>*
  store <2 x double> %5329, <2 x double>* %5348, align 8, !tbaa !62
  %5349 = getelementptr inbounds double, double* %5335, i64 14
  %5350 = bitcast double* %5349 to <2 x double>*
  store <2 x double> %5330, <2 x double>* %5350, align 8, !tbaa !62
  %5351 = getelementptr inbounds double, double* %5335, i64 16
  %5352 = bitcast double* %5351 to <2 x double>*
  store <2 x double> %5331, <2 x double>* %5352, align 8, !tbaa !62
  %5353 = getelementptr inbounds double, double* %5335, i64 18
  %5354 = bitcast double* %5353 to <2 x double>*
  store <2 x double> %5332, <2 x double>* %5354, align 8, !tbaa !62
  %5355 = getelementptr inbounds double, double* %5335, i64 20
  %5356 = bitcast double* %5355 to <2 x double>*
  store <2 x double> %5333, <2 x double>* %5356, align 8, !tbaa !62
  %5357 = getelementptr inbounds double, double* %5335, i64 22
  %5358 = bitcast double* %5357 to <2 x double>*
  store <2 x double> %5334, <2 x double>* %5358, align 8, !tbaa !62
  %5359 = getelementptr inbounds double, double* %1794, i64 %index11123
  %5360 = bitcast double* %5359 to <2 x double>*
  store <2 x double> %5323, <2 x double>* %5360, align 8, !tbaa !62
  %5361 = getelementptr inbounds double, double* %5359, i64 2
  %5362 = bitcast double* %5361 to <2 x double>*
  store <2 x double> %5324, <2 x double>* %5362, align 8, !tbaa !62
  %5363 = getelementptr inbounds double, double* %5359, i64 4
  %5364 = bitcast double* %5363 to <2 x double>*
  store <2 x double> %5325, <2 x double>* %5364, align 8, !tbaa !62
  %5365 = getelementptr inbounds double, double* %5359, i64 6
  %5366 = bitcast double* %5365 to <2 x double>*
  store <2 x double> %5326, <2 x double>* %5366, align 8, !tbaa !62
  %5367 = getelementptr inbounds double, double* %5359, i64 8
  %5368 = bitcast double* %5367 to <2 x double>*
  store <2 x double> %5327, <2 x double>* %5368, align 8, !tbaa !62
  %5369 = getelementptr inbounds double, double* %5359, i64 10
  %5370 = bitcast double* %5369 to <2 x double>*
  store <2 x double> %5328, <2 x double>* %5370, align 8, !tbaa !62
  %5371 = getelementptr inbounds double, double* %5359, i64 12
  %5372 = bitcast double* %5371 to <2 x double>*
  store <2 x double> %5329, <2 x double>* %5372, align 8, !tbaa !62
  %5373 = getelementptr inbounds double, double* %5359, i64 14
  %5374 = bitcast double* %5373 to <2 x double>*
  store <2 x double> %5330, <2 x double>* %5374, align 8, !tbaa !62
  %5375 = getelementptr inbounds double, double* %5359, i64 16
  %5376 = bitcast double* %5375 to <2 x double>*
  store <2 x double> %5331, <2 x double>* %5376, align 8, !tbaa !62
  %5377 = getelementptr inbounds double, double* %5359, i64 18
  %5378 = bitcast double* %5377 to <2 x double>*
  store <2 x double> %5332, <2 x double>* %5378, align 8, !tbaa !62
  %5379 = getelementptr inbounds double, double* %5359, i64 20
  %5380 = bitcast double* %5379 to <2 x double>*
  store <2 x double> %5333, <2 x double>* %5380, align 8, !tbaa !62
  %5381 = getelementptr inbounds double, double* %5359, i64 22
  %5382 = bitcast double* %5381 to <2 x double>*
  store <2 x double> %5334, <2 x double>* %5382, align 8, !tbaa !62
  %5383 = shl <2 x i32> %vec.ind11141, <i32 1, i32 1>
  %5384 = shl <2 x i32> %step.add11142, <i32 1, i32 1>
  %5385 = shl <2 x i32> %step.add11143, <i32 1, i32 1>
  %5386 = shl <2 x i32> %step.add11144, <i32 1, i32 1>
  %5387 = shl <2 x i32> %step.add11145, <i32 1, i32 1>
  %5388 = shl <2 x i32> %step.add11146, <i32 1, i32 1>
  %5389 = shl <2 x i32> %step.add11147, <i32 1, i32 1>
  %5390 = shl <2 x i32> %step.add11148, <i32 1, i32 1>
  %5391 = shl <2 x i32> %step.add11149, <i32 1, i32 1>
  %5392 = shl <2 x i32> %step.add11150, <i32 1, i32 1>
  %5393 = shl <2 x i32> %step.add11151, <i32 1, i32 1>
  %5394 = shl <2 x i32> %step.add11152, <i32 1, i32 1>
  %5395 = sitofp <2 x i32> %5383 to <2 x double>
  %5396 = sitofp <2 x i32> %5384 to <2 x double>
  %5397 = sitofp <2 x i32> %5385 to <2 x double>
  %5398 = sitofp <2 x i32> %5386 to <2 x double>
  %5399 = sitofp <2 x i32> %5387 to <2 x double>
  %5400 = sitofp <2 x i32> %5388 to <2 x double>
  %5401 = sitofp <2 x i32> %5389 to <2 x double>
  %5402 = sitofp <2 x i32> %5390 to <2 x double>
  %5403 = sitofp <2 x i32> %5391 to <2 x double>
  %5404 = sitofp <2 x i32> %5392 to <2 x double>
  %5405 = sitofp <2 x i32> %5393 to <2 x double>
  %5406 = sitofp <2 x i32> %5394 to <2 x double>
  %5407 = getelementptr inbounds double, double* %1796, i64 %index11123
  %5408 = bitcast double* %5407 to <2 x double>*
  store <2 x double> %5395, <2 x double>* %5408, align 8, !tbaa !62
  %5409 = getelementptr inbounds double, double* %5407, i64 2
  %5410 = bitcast double* %5409 to <2 x double>*
  store <2 x double> %5396, <2 x double>* %5410, align 8, !tbaa !62
  %5411 = getelementptr inbounds double, double* %5407, i64 4
  %5412 = bitcast double* %5411 to <2 x double>*
  store <2 x double> %5397, <2 x double>* %5412, align 8, !tbaa !62
  %5413 = getelementptr inbounds double, double* %5407, i64 6
  %5414 = bitcast double* %5413 to <2 x double>*
  store <2 x double> %5398, <2 x double>* %5414, align 8, !tbaa !62
  %5415 = getelementptr inbounds double, double* %5407, i64 8
  %5416 = bitcast double* %5415 to <2 x double>*
  store <2 x double> %5399, <2 x double>* %5416, align 8, !tbaa !62
  %5417 = getelementptr inbounds double, double* %5407, i64 10
  %5418 = bitcast double* %5417 to <2 x double>*
  store <2 x double> %5400, <2 x double>* %5418, align 8, !tbaa !62
  %5419 = getelementptr inbounds double, double* %5407, i64 12
  %5420 = bitcast double* %5419 to <2 x double>*
  store <2 x double> %5401, <2 x double>* %5420, align 8, !tbaa !62
  %5421 = getelementptr inbounds double, double* %5407, i64 14
  %5422 = bitcast double* %5421 to <2 x double>*
  store <2 x double> %5402, <2 x double>* %5422, align 8, !tbaa !62
  %5423 = getelementptr inbounds double, double* %5407, i64 16
  %5424 = bitcast double* %5423 to <2 x double>*
  store <2 x double> %5403, <2 x double>* %5424, align 8, !tbaa !62
  %5425 = getelementptr inbounds double, double* %5407, i64 18
  %5426 = bitcast double* %5425 to <2 x double>*
  store <2 x double> %5404, <2 x double>* %5426, align 8, !tbaa !62
  %5427 = getelementptr inbounds double, double* %5407, i64 20
  %5428 = bitcast double* %5427 to <2 x double>*
  store <2 x double> %5405, <2 x double>* %5428, align 8, !tbaa !62
  %5429 = getelementptr inbounds double, double* %5407, i64 22
  %5430 = bitcast double* %5429 to <2 x double>*
  store <2 x double> %5406, <2 x double>* %5430, align 8, !tbaa !62
  %5431 = add <2 x i32> %vec.ind11141, <i32 -3, i32 -3>
  %5432 = add <2 x i32> %vec.ind11141, <i32 -1, i32 -1>
  %5433 = add <2 x i32> %vec.ind11141, <i32 1, i32 1>
  %5434 = add <2 x i32> %vec.ind11141, <i32 3, i32 3>
  %5435 = add <2 x i32> %vec.ind11141, <i32 5, i32 5>
  %5436 = add <2 x i32> %vec.ind11141, <i32 7, i32 7>
  %5437 = add <2 x i32> %vec.ind11141, <i32 9, i32 9>
  %5438 = add <2 x i32> %vec.ind11141, <i32 11, i32 11>
  %5439 = add <2 x i32> %vec.ind11141, <i32 13, i32 13>
  %5440 = add <2 x i32> %vec.ind11141, <i32 15, i32 15>
  %5441 = add <2 x i32> %vec.ind11141, <i32 17, i32 17>
  %5442 = add <2 x i32> %vec.ind11141, <i32 19, i32 19>
  %5443 = sitofp <2 x i32> %5431 to <2 x double>
  %5444 = sitofp <2 x i32> %5432 to <2 x double>
  %5445 = sitofp <2 x i32> %5433 to <2 x double>
  %5446 = sitofp <2 x i32> %5434 to <2 x double>
  %5447 = sitofp <2 x i32> %5435 to <2 x double>
  %5448 = sitofp <2 x i32> %5436 to <2 x double>
  %5449 = sitofp <2 x i32> %5437 to <2 x double>
  %5450 = sitofp <2 x i32> %5438 to <2 x double>
  %5451 = sitofp <2 x i32> %5439 to <2 x double>
  %5452 = sitofp <2 x i32> %5440 to <2 x double>
  %5453 = sitofp <2 x i32> %5441 to <2 x double>
  %5454 = sitofp <2 x i32> %5442 to <2 x double>
  %5455 = getelementptr inbounds double, double* %1797, i64 %index11123
  %5456 = bitcast double* %5455 to <2 x double>*
  store <2 x double> %5443, <2 x double>* %5456, align 8, !tbaa !62
  %5457 = getelementptr inbounds double, double* %5455, i64 2
  %5458 = bitcast double* %5457 to <2 x double>*
  store <2 x double> %5444, <2 x double>* %5458, align 8, !tbaa !62
  %5459 = getelementptr inbounds double, double* %5455, i64 4
  %5460 = bitcast double* %5459 to <2 x double>*
  store <2 x double> %5445, <2 x double>* %5460, align 8, !tbaa !62
  %5461 = getelementptr inbounds double, double* %5455, i64 6
  %5462 = bitcast double* %5461 to <2 x double>*
  store <2 x double> %5446, <2 x double>* %5462, align 8, !tbaa !62
  %5463 = getelementptr inbounds double, double* %5455, i64 8
  %5464 = bitcast double* %5463 to <2 x double>*
  store <2 x double> %5447, <2 x double>* %5464, align 8, !tbaa !62
  %5465 = getelementptr inbounds double, double* %5455, i64 10
  %5466 = bitcast double* %5465 to <2 x double>*
  store <2 x double> %5448, <2 x double>* %5466, align 8, !tbaa !62
  %5467 = getelementptr inbounds double, double* %5455, i64 12
  %5468 = bitcast double* %5467 to <2 x double>*
  store <2 x double> %5449, <2 x double>* %5468, align 8, !tbaa !62
  %5469 = getelementptr inbounds double, double* %5455, i64 14
  %5470 = bitcast double* %5469 to <2 x double>*
  store <2 x double> %5450, <2 x double>* %5470, align 8, !tbaa !62
  %5471 = getelementptr inbounds double, double* %5455, i64 16
  %5472 = bitcast double* %5471 to <2 x double>*
  store <2 x double> %5451, <2 x double>* %5472, align 8, !tbaa !62
  %5473 = getelementptr inbounds double, double* %5455, i64 18
  %5474 = bitcast double* %5473 to <2 x double>*
  store <2 x double> %5452, <2 x double>* %5474, align 8, !tbaa !62
  %5475 = getelementptr inbounds double, double* %5455, i64 20
  %5476 = bitcast double* %5475 to <2 x double>*
  store <2 x double> %5453, <2 x double>* %5476, align 8, !tbaa !62
  %5477 = getelementptr inbounds double, double* %5455, i64 22
  %5478 = bitcast double* %5477 to <2 x double>*
  store <2 x double> %5454, <2 x double>* %5478, align 8, !tbaa !62
  %index.next11124 = add nuw nsw i64 %index11123, 24
  %vec.ind.next11154 = add <2 x i32> %vec.ind11141, <i32 24, i32 24>
  %5479 = icmp eq i64 %index.next11124, 24984
  br i1 %5479, label %for.body.i8549, label %vector.body11119, !llvm.loop !110

for.body.i8549:                                   ; preds = %vector.body11119
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %5290, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %5291, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %5292, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %5293, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %5294, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %5295, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %5296, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %5297, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %5298, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %5299, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %5300, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %5301, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %5302, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %5303, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %5304, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %5305, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %5306, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %5307, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %5308, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %5309, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %5310, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %5311, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %5312, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %5313, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %5314, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %5315, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %5316, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %5317, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %5318, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %5319, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %5320, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %5321, align 8, !tbaa !62
  %5480 = shl nuw nsw i64 %indvars.iv9497, 3
  store i8* %call3733, i8** %5252, align 8
  store i8* %call3733, i8** %5253, align 8
  store i64 %5480, i64* %5254, align 8
  store i8* %call3735, i8** %5255, align 8
  store i8* %call3735, i8** %5256, align 8
  store i64 %5480, i64* %5257, align 8
  store i8* %call3736, i8** %5258, align 8
  store i8* %call3736, i8** %5259, align 8
  store i64 %5480, i64* %5260, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %5252, i8** nonnull %5253, i64* nonnull %5254, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  %cmp49418895 = icmp ult i64 %indvars.iv9497, 128
  br label %for.cond4935.preheader

for.cond4987.preheader:                           ; preds = %for.cond.cleanup4938
  %5481 = add nuw nsw i64 %5322, 32
  %cmp49888906 = icmp sgt i32 %t4921.2.lcssa.lcssa, 0
  br i1 %cmp49888906, label %for.cond4993.preheader.preheader43, label %for.cond.cleanup4990

for.cond4993.preheader.preheader43:               ; preds = %for.cond4987.preheader
  %n.mod.vf11063 = urem i64 %5481, 24
  %n.vec11064 = sub nuw nsw i64 %5481, %n.mod.vf11063
  %cmp.n11068 = icmp eq i64 %n.mod.vf11063, 0
  br label %for.cond4993.preheader

for.cond4935.preheader:                           ; preds = %for.body.i8549, %for.cond.cleanup4938
  %tms4928.08904 = phi i32 [ 1, %for.body.i8549 ], [ %mul4983, %for.cond.cleanup4938 ]
  %t4921.08903 = phi i32 [ 0, %for.body.i8549 ], [ %t4921.2.lcssa.lcssa, %for.cond.cleanup4938 ]
  %.capture_expr..casted4962.sroa.0.0.insert.ext = zext i32 %tms4928.08904 to i64
  br i1 %cmp49418895, label %for.cond.cleanup4938, label %for.cond4940.preheader

for.cond4940.preheader:                           ; preds = %for.cond4935.preheader, %for.cond.cleanup4943
  %ths4934.08900 = phi i32 [ %mul4979, %for.cond.cleanup4943 ], [ 32, %for.cond4935.preheader ]
  %t4921.18899 = phi i32 [ %inc4951, %for.cond.cleanup4943 ], [ %t4921.08903, %for.cond4935.preheader ]
  %.capture_expr..casted4964.sroa.0.0.insert.ext = zext i32 %ths4934.08900 to i64
  br label %for.cond4946.preheader

for.cond.cleanup4938:                             ; preds = %for.cond.cleanup4943, %for.cond4935.preheader
  %t4921.2.lcssa.lcssa = phi i32 [ %t4921.08903, %for.cond4935.preheader ], [ %inc4951, %for.cond.cleanup4943 ]
  %mul4983 = shl nsw i32 %tms4928.08904, 1
  %cmp4930 = icmp ult i32 %mul4983, 257
  br i1 %cmp4930, label %for.cond4935.preheader, label %for.cond4987.preheader

for.cond4946.preheader:                           ; preds = %for.cond4940.preheader, %for.cond.cleanup4949
  %dssch.08897 = phi i32 [ %mul4975, %for.cond.cleanup4949 ], [ 128, %for.cond4940.preheader ]
  %t4921.28896 = phi i32 [ %inc4951, %for.cond.cleanup4949 ], [ %t4921.18899, %for.cond4940.preheader ]
  %.capture_expr..casted4958.sroa.0.0.insert.ext = zext i32 %dssch.08897 to i64
  br label %for.body4950

for.cond.cleanup4943:                             ; preds = %for.cond.cleanup4949
  %mul4979 = shl nsw i32 %ths4934.08900, 1
  %cmp4936 = icmp ult i32 %mul4979, 1025
  br i1 %cmp4936, label %for.cond4940.preheader, label %for.cond.cleanup4938

for.cond.cleanup4949:                             ; preds = %omp_offload.cont4969
  %mul4975 = mul nsw i32 %dssch.08897, 1200
  %5482 = zext i32 %mul4975 to i64
  %cmp4941 = icmp ult i64 %indvars.iv9497, %5482
  br i1 %cmp4941, label %for.cond.cleanup4943, label %for.cond4946.preheader

for.body4950:                                     ; preds = %for.cond4946.preheader, %omp_offload.cont4969
  %sch4945.08893 = phi i32 [ 100, %for.cond4946.preheader ], [ %mul4971, %omp_offload.cont4969 ]
  %t4921.38892 = phi i32 [ %t4921.28896, %for.cond4946.preheader ], [ %inc4951, %omp_offload.cont4969 ]
  %inc4951 = add nsw i32 %t4921.38892, 1
  %.capture_expr..casted4960.sroa.0.0.insert.ext = zext i32 %sch4945.08893 to i64
  store i64 %indvars.iv9497, i64* %5262, align 8
  store i64 %indvars.iv9497, i64* %5264, align 8
  store i8* %call3733, i8** %5265, align 8
  store i8* %call3733, i8** %5266, align 8
  store i8* %call3735, i8** %5267, align 8
  store i8* %call3735, i8** %5268, align 8
  store i8* %call3736, i8** %5269, align 8
  store i8* %call3736, i8** %5270, align 8
  store i64 %.capture_expr..casted4958.sroa.0.0.insert.ext, i64* %5272, align 8
  store i64 %.capture_expr..casted4958.sroa.0.0.insert.ext, i64* %5274, align 8
  store i64 %.capture_expr..casted4960.sroa.0.0.insert.ext, i64* %5276, align 8
  store i64 %.capture_expr..casted4960.sroa.0.0.insert.ext, i64* %5278, align 8
  store i64 %.capture_expr..casted4962.sroa.0.0.insert.ext, i64* %5280, align 8
  store i64 %.capture_expr..casted4962.sroa.0.0.insert.ext, i64* %5282, align 8
  store i64 %.capture_expr..casted4964.sroa.0.0.insert.ext, i64* %5284, align 8
  store i64 %.capture_expr..casted4964.sroa.0.0.insert.ext, i64* %5286, align 8
  %5483 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1391.region_id, i32 8, i8** nonnull %5261, i8** nonnull %5263, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.259, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.260, i64 0, i64 0), i32 %tms4928.08904, i32 %ths4934.08900) #6
  %5484 = icmp eq i32 %5483, 0
  br i1 %5484, label %omp_offload.cont4969, label %omp_offload.failed4968

omp_offload.failed4968:                           ; preds = %for.body4950
  %5485 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %5486 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %5485, i32 %tms4928.08904, i32 %ths4934.08900) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 6, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64, i64)* @.omp_outlined..229 to void (i32*, i32*, ...)*), i64 %indvars.iv9497, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted4958.sroa.0.0.insert.ext, i64 %.capture_expr..casted4960.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont4969

omp_offload.cont4969:                             ; preds = %for.body4950, %omp_offload.failed4968
  %mul4971 = mul nsw i32 %sch4945.08893, 3000
  %5487 = zext i32 %mul4971 to i64
  %cmp4947 = icmp ult i64 %indvars.iv9497, %5487
  br i1 %cmp4947, label %for.cond.cleanup4949, label %for.body4950

for.cond4993.preheader:                           ; preds = %for.cond4993.preheader.preheader43, %for.cond.cleanup4996
  %times4986.08907 = phi i32 [ %inc5011, %for.cond.cleanup4996 ], [ 0, %for.cond4993.preheader.preheader43 ]
  br label %vector.body11056

vector.body11056:                                 ; preds = %for.cond4993.preheader, %vector.body11056
  %index11065 = phi i64 [ %index.next11066, %vector.body11056 ], [ 0, %for.cond4993.preheader ]
  %5488 = getelementptr inbounds double, double* %1796, i64 %index11065
  %5489 = bitcast double* %5488 to <2 x double>*
  %wide.load11083 = load <2 x double>, <2 x double>* %5489, align 8, !tbaa !62
  %5490 = getelementptr inbounds double, double* %5488, i64 2
  %5491 = bitcast double* %5490 to <2 x double>*
  %wide.load11084 = load <2 x double>, <2 x double>* %5491, align 8, !tbaa !62
  %5492 = getelementptr inbounds double, double* %5488, i64 4
  %5493 = bitcast double* %5492 to <2 x double>*
  %wide.load11085 = load <2 x double>, <2 x double>* %5493, align 8, !tbaa !62
  %5494 = getelementptr inbounds double, double* %5488, i64 6
  %5495 = bitcast double* %5494 to <2 x double>*
  %wide.load11086 = load <2 x double>, <2 x double>* %5495, align 8, !tbaa !62
  %5496 = getelementptr inbounds double, double* %5488, i64 8
  %5497 = bitcast double* %5496 to <2 x double>*
  %wide.load11087 = load <2 x double>, <2 x double>* %5497, align 8, !tbaa !62
  %5498 = getelementptr inbounds double, double* %5488, i64 10
  %5499 = bitcast double* %5498 to <2 x double>*
  %wide.load11088 = load <2 x double>, <2 x double>* %5499, align 8, !tbaa !62
  %5500 = getelementptr inbounds double, double* %5488, i64 12
  %5501 = bitcast double* %5500 to <2 x double>*
  %wide.load11089 = load <2 x double>, <2 x double>* %5501, align 8, !tbaa !62
  %5502 = getelementptr inbounds double, double* %5488, i64 14
  %5503 = bitcast double* %5502 to <2 x double>*
  %wide.load11090 = load <2 x double>, <2 x double>* %5503, align 8, !tbaa !62
  %5504 = getelementptr inbounds double, double* %5488, i64 16
  %5505 = bitcast double* %5504 to <2 x double>*
  %wide.load11091 = load <2 x double>, <2 x double>* %5505, align 8, !tbaa !62
  %5506 = getelementptr inbounds double, double* %5488, i64 18
  %5507 = bitcast double* %5506 to <2 x double>*
  %wide.load11092 = load <2 x double>, <2 x double>* %5507, align 8, !tbaa !62
  %5508 = getelementptr inbounds double, double* %5488, i64 20
  %5509 = bitcast double* %5508 to <2 x double>*
  %wide.load11093 = load <2 x double>, <2 x double>* %5509, align 8, !tbaa !62
  %5510 = getelementptr inbounds double, double* %5488, i64 22
  %5511 = bitcast double* %5510 to <2 x double>*
  %wide.load11094 = load <2 x double>, <2 x double>* %5511, align 8, !tbaa !62
  %5512 = getelementptr inbounds double, double* %1797, i64 %index11065
  %5513 = bitcast double* %5512 to <2 x double>*
  %wide.load11095 = load <2 x double>, <2 x double>* %5513, align 8, !tbaa !62
  %5514 = getelementptr inbounds double, double* %5512, i64 2
  %5515 = bitcast double* %5514 to <2 x double>*
  %wide.load11096 = load <2 x double>, <2 x double>* %5515, align 8, !tbaa !62
  %5516 = getelementptr inbounds double, double* %5512, i64 4
  %5517 = bitcast double* %5516 to <2 x double>*
  %wide.load11097 = load <2 x double>, <2 x double>* %5517, align 8, !tbaa !62
  %5518 = getelementptr inbounds double, double* %5512, i64 6
  %5519 = bitcast double* %5518 to <2 x double>*
  %wide.load11098 = load <2 x double>, <2 x double>* %5519, align 8, !tbaa !62
  %5520 = getelementptr inbounds double, double* %5512, i64 8
  %5521 = bitcast double* %5520 to <2 x double>*
  %wide.load11099 = load <2 x double>, <2 x double>* %5521, align 8, !tbaa !62
  %5522 = getelementptr inbounds double, double* %5512, i64 10
  %5523 = bitcast double* %5522 to <2 x double>*
  %wide.load11100 = load <2 x double>, <2 x double>* %5523, align 8, !tbaa !62
  %5524 = getelementptr inbounds double, double* %5512, i64 12
  %5525 = bitcast double* %5524 to <2 x double>*
  %wide.load11101 = load <2 x double>, <2 x double>* %5525, align 8, !tbaa !62
  %5526 = getelementptr inbounds double, double* %5512, i64 14
  %5527 = bitcast double* %5526 to <2 x double>*
  %wide.load11102 = load <2 x double>, <2 x double>* %5527, align 8, !tbaa !62
  %5528 = getelementptr inbounds double, double* %5512, i64 16
  %5529 = bitcast double* %5528 to <2 x double>*
  %wide.load11103 = load <2 x double>, <2 x double>* %5529, align 8, !tbaa !62
  %5530 = getelementptr inbounds double, double* %5512, i64 18
  %5531 = bitcast double* %5530 to <2 x double>*
  %wide.load11104 = load <2 x double>, <2 x double>* %5531, align 8, !tbaa !62
  %5532 = getelementptr inbounds double, double* %5512, i64 20
  %5533 = bitcast double* %5532 to <2 x double>*
  %wide.load11105 = load <2 x double>, <2 x double>* %5533, align 8, !tbaa !62
  %5534 = getelementptr inbounds double, double* %5512, i64 22
  %5535 = bitcast double* %5534 to <2 x double>*
  %wide.load11106 = load <2 x double>, <2 x double>* %5535, align 8, !tbaa !62
  %5536 = fadd <2 x double> %wide.load11083, %wide.load11095
  %5537 = fadd <2 x double> %wide.load11084, %wide.load11096
  %5538 = fadd <2 x double> %wide.load11085, %wide.load11097
  %5539 = fadd <2 x double> %wide.load11086, %wide.load11098
  %5540 = fadd <2 x double> %wide.load11087, %wide.load11099
  %5541 = fadd <2 x double> %wide.load11088, %wide.load11100
  %5542 = fadd <2 x double> %wide.load11089, %wide.load11101
  %5543 = fadd <2 x double> %wide.load11090, %wide.load11102
  %5544 = fadd <2 x double> %wide.load11091, %wide.load11103
  %5545 = fadd <2 x double> %wide.load11092, %wide.load11104
  %5546 = fadd <2 x double> %wide.load11093, %wide.load11105
  %5547 = fadd <2 x double> %wide.load11094, %wide.load11106
  %5548 = getelementptr inbounds double, double* %1795, i64 %index11065
  %5549 = bitcast double* %5548 to <2 x double>*
  %wide.load11107 = load <2 x double>, <2 x double>* %5549, align 8, !tbaa !62
  %5550 = getelementptr inbounds double, double* %5548, i64 2
  %5551 = bitcast double* %5550 to <2 x double>*
  %wide.load11108 = load <2 x double>, <2 x double>* %5551, align 8, !tbaa !62
  %5552 = getelementptr inbounds double, double* %5548, i64 4
  %5553 = bitcast double* %5552 to <2 x double>*
  %wide.load11109 = load <2 x double>, <2 x double>* %5553, align 8, !tbaa !62
  %5554 = getelementptr inbounds double, double* %5548, i64 6
  %5555 = bitcast double* %5554 to <2 x double>*
  %wide.load11110 = load <2 x double>, <2 x double>* %5555, align 8, !tbaa !62
  %5556 = getelementptr inbounds double, double* %5548, i64 8
  %5557 = bitcast double* %5556 to <2 x double>*
  %wide.load11111 = load <2 x double>, <2 x double>* %5557, align 8, !tbaa !62
  %5558 = getelementptr inbounds double, double* %5548, i64 10
  %5559 = bitcast double* %5558 to <2 x double>*
  %wide.load11112 = load <2 x double>, <2 x double>* %5559, align 8, !tbaa !62
  %5560 = getelementptr inbounds double, double* %5548, i64 12
  %5561 = bitcast double* %5560 to <2 x double>*
  %wide.load11113 = load <2 x double>, <2 x double>* %5561, align 8, !tbaa !62
  %5562 = getelementptr inbounds double, double* %5548, i64 14
  %5563 = bitcast double* %5562 to <2 x double>*
  %wide.load11114 = load <2 x double>, <2 x double>* %5563, align 8, !tbaa !62
  %5564 = getelementptr inbounds double, double* %5548, i64 16
  %5565 = bitcast double* %5564 to <2 x double>*
  %wide.load11115 = load <2 x double>, <2 x double>* %5565, align 8, !tbaa !62
  %5566 = getelementptr inbounds double, double* %5548, i64 18
  %5567 = bitcast double* %5566 to <2 x double>*
  %wide.load11116 = load <2 x double>, <2 x double>* %5567, align 8, !tbaa !62
  %5568 = getelementptr inbounds double, double* %5548, i64 20
  %5569 = bitcast double* %5568 to <2 x double>*
  %wide.load11117 = load <2 x double>, <2 x double>* %5569, align 8, !tbaa !62
  %5570 = getelementptr inbounds double, double* %5548, i64 22
  %5571 = bitcast double* %5570 to <2 x double>*
  %wide.load11118 = load <2 x double>, <2 x double>* %5571, align 8, !tbaa !62
  %5572 = fadd <2 x double> %5536, %wide.load11107
  %5573 = fadd <2 x double> %5537, %wide.load11108
  %5574 = fadd <2 x double> %5538, %wide.load11109
  %5575 = fadd <2 x double> %5539, %wide.load11110
  %5576 = fadd <2 x double> %5540, %wide.load11111
  %5577 = fadd <2 x double> %5541, %wide.load11112
  %5578 = fadd <2 x double> %5542, %wide.load11113
  %5579 = fadd <2 x double> %5543, %wide.load11114
  %5580 = fadd <2 x double> %5544, %wide.load11115
  %5581 = fadd <2 x double> %5545, %wide.load11116
  %5582 = fadd <2 x double> %5546, %wide.load11117
  %5583 = fadd <2 x double> %5547, %wide.load11118
  store <2 x double> %5572, <2 x double>* %5549, align 8, !tbaa !62
  store <2 x double> %5573, <2 x double>* %5551, align 8, !tbaa !62
  store <2 x double> %5574, <2 x double>* %5553, align 8, !tbaa !62
  store <2 x double> %5575, <2 x double>* %5555, align 8, !tbaa !62
  store <2 x double> %5576, <2 x double>* %5557, align 8, !tbaa !62
  store <2 x double> %5577, <2 x double>* %5559, align 8, !tbaa !62
  store <2 x double> %5578, <2 x double>* %5561, align 8, !tbaa !62
  store <2 x double> %5579, <2 x double>* %5563, align 8, !tbaa !62
  store <2 x double> %5580, <2 x double>* %5565, align 8, !tbaa !62
  store <2 x double> %5581, <2 x double>* %5567, align 8, !tbaa !62
  store <2 x double> %5582, <2 x double>* %5569, align 8, !tbaa !62
  store <2 x double> %5583, <2 x double>* %5571, align 8, !tbaa !62
  %index.next11066 = add nuw nsw i64 %index11065, 24
  %5584 = icmp eq i64 %index.next11066, %n.vec11064
  br i1 %5584, label %middle.block11057, label %vector.body11056, !llvm.loop !111

middle.block11057:                                ; preds = %vector.body11056
  br i1 %cmp.n11068, label %for.cond.cleanup4996, label %for.body4997

for.cond.cleanup4990:                             ; preds = %for.cond.cleanup4996, %for.cond4987.preheader
  store i8* %call3733, i8** %5287, align 8
  store i8* %call3733, i8** %5288, align 8
  store i64 %5480, i64* %5289, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %5287, i8** nonnull %5288, i64* nonnull %5289, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body5023

for.cond.cleanup4996:                             ; preds = %for.body4997, %middle.block11057
  %inc5011 = add nuw nsw i32 %times4986.08907, 1
  %exitcond9494 = icmp eq i32 %inc5011, %t4921.2.lcssa.lcssa
  br i1 %exitcond9494, label %for.cond.cleanup4990, label %for.cond4993.preheader

for.body4997:                                     ; preds = %middle.block11057, %for.body4997
  %indvars.iv9491 = phi i64 [ %indvars.iv.next9492, %for.body4997 ], [ %n.vec11064, %middle.block11057 ]
  %arrayidx4999 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9491
  %5585 = load double, double* %arrayidx4999, align 8, !tbaa !62
  %arrayidx5001 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9491
  %5586 = load double, double* %arrayidx5001, align 8, !tbaa !62
  %add5002 = fadd double %5585, %5586
  %arrayidx5004 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9491
  %5587 = load double, double* %arrayidx5004, align 8, !tbaa !62
  %add5005 = fadd double %5587, %add5002
  store double %add5005, double* %arrayidx5004, align 8, !tbaa !62
  %indvars.iv.next9492 = add nuw nsw i64 %indvars.iv9491, 1
  %exitcond9493 = icmp eq i64 %indvars.iv.next9492, %indvars.iv9497
  br i1 %exitcond9493, label %for.cond.cleanup4996, label %for.body4997, !llvm.loop !112

for.body5023:                                     ; preds = %for.inc5037.7, %for.cond.cleanup4990
  %indvars.iv9495 = phi i64 [ 0, %for.cond.cleanup4990 ], [ %indvars.iv.next9496.7, %for.inc5037.7 ]
  %arrayidx5025 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9495
  %5588 = load double, double* %arrayidx5025, align 8, !tbaa !62
  %arrayidx5027 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9495
  %5589 = load double, double* %arrayidx5027, align 8, !tbaa !62
  %cmp5028 = fcmp une double %5588, %5589
  br i1 %cmp5028, label %cleanup5047, label %for.inc5037

for.inc5037:                                      ; preds = %for.body5023
  %indvars.iv.next9496 = or i64 %indvars.iv9495, 1
  %arrayidx5025.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9496
  %5590 = load double, double* %arrayidx5025.1, align 8, !tbaa !62
  %arrayidx5027.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9496
  %5591 = load double, double* %arrayidx5027.1, align 8, !tbaa !62
  %cmp5028.1 = fcmp une double %5590, %5591
  br i1 %cmp5028.1, label %cleanup5047, label %for.inc5037.1

for.inc5045:                                      ; preds = %for.inc5037.7
  %indvars.iv.next9498 = add nuw nsw i64 %indvars.iv9497, 5000
  %cmp4917 = icmp ult i64 %indvars.iv.next9498, 25000
  %indvar.next11060 = add nuw nsw i64 %indvar11059, 1
  br i1 %cmp4917, label %for.body.i8549.preheader, label %for.end5049

cleanup5047:                                      ; preds = %for.inc5037.6, %for.inc5037.5, %for.inc5037.4, %for.inc5037.3, %for.inc5037.2, %for.inc5037.1, %for.inc5037, %for.body5023
  %indvars.iv9495.lcssa = phi i64 [ %indvars.iv9495, %for.body5023 ], [ %indvars.iv.next9496, %for.inc5037 ], [ %indvars.iv.next9496.1, %for.inc5037.1 ], [ %indvars.iv.next9496.2, %for.inc5037.2 ], [ %indvars.iv.next9496.3, %for.inc5037.3 ], [ %indvars.iv.next9496.4, %for.inc5037.4 ], [ %indvars.iv.next9496.5, %for.inc5037.5 ], [ %indvars.iv.next9496.6, %for.inc5037.6 ]
  %.lcssa11804 = phi double [ %5588, %for.body5023 ], [ %5590, %for.inc5037 ], [ %7800, %for.inc5037.1 ], [ %7802, %for.inc5037.2 ], [ %7804, %for.inc5037.3 ], [ %7806, %for.inc5037.4 ], [ %7808, %for.inc5037.5 ], [ %7810, %for.inc5037.6 ]
  %.lcssa11802 = phi double [ %5589, %for.body5023 ], [ %5591, %for.inc5037 ], [ %7801, %for.inc5037.1 ], [ %7803, %for.inc5037.2 ], [ %7805, %for.inc5037.3 ], [ %7807, %for.inc5037.4 ], [ %7809, %for.inc5037.5 ], [ %7811, %for.inc5037.6 ]
  %5592 = trunc i64 %indvars.iv9497 to i32
  %5593 = trunc i64 %indvars.iv9495.lcssa to i32
  %call5035 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %5592, i32 signext %5593, double %.lcssa11804, double %.lcssa11802)
  br label %cleanup5832

for.end5049:                                      ; preds = %for.inc5045
  %puts8265 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8266 = call i32 @puts(i8* getelementptr inbounds ([54 x i8], [54 x i8]* @str.355, i64 0, i64 0))
  %5594 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5062, i64 0, i64 0
  %5595 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5063, i64 0, i64 0
  %5596 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5064, i64 0, i64 0
  %5597 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5062, i64 0, i64 1
  %5598 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5063, i64 0, i64 1
  %5599 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5064, i64 0, i64 1
  %5600 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5062, i64 0, i64 2
  %5601 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5063, i64 0, i64 2
  %5602 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5064, i64 0, i64 2
  %5603 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5095, i64 0, i64 0
  %5604 = bitcast [7 x i8*]* %.offload_baseptrs5095 to i64*
  %5605 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5096, i64 0, i64 0
  %5606 = bitcast [7 x i8*]* %.offload_ptrs5096 to i64*
  %5607 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5095, i64 0, i64 1
  %5608 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5096, i64 0, i64 1
  %5609 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5095, i64 0, i64 2
  %5610 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5096, i64 0, i64 2
  %5611 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5095, i64 0, i64 3
  %5612 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5096, i64 0, i64 3
  %5613 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5095, i64 0, i64 4
  %5614 = bitcast i8** %5613 to i64*
  %5615 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5096, i64 0, i64 4
  %5616 = bitcast i8** %5615 to i64*
  %5617 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5095, i64 0, i64 5
  %5618 = bitcast i8** %5617 to i64*
  %5619 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5096, i64 0, i64 5
  %5620 = bitcast i8** %5619 to i64*
  %5621 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5095, i64 0, i64 6
  %5622 = bitcast i8** %5621 to i64*
  %5623 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5096, i64 0, i64 6
  %5624 = bitcast i8** %5623 to i64*
  %5625 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs5140, i64 0, i64 0
  %5626 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs5141, i64 0, i64 0
  %5627 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes5142, i64 0, i64 0
  %5628 = bitcast i8* %arrayidx.i to <2 x double>*
  %5629 = bitcast i8* %arrayidx2.i to <2 x double>*
  %5630 = bitcast i8* %arrayidx5.i to <2 x double>*
  %5631 = bitcast i8* %arrayidx8.i to <2 x double>*
  %5632 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %5633 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %5634 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %5635 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %5636 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %5637 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %5638 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %5639 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %5640 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %5641 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %5642 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %5643 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %5644 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %5645 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %5646 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %5647 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %5648 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %5649 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %5650 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %5651 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %5652 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %5653 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %5654 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %5655 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %5656 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %5657 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %5658 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %5659 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8564.preheader

for.body.i8564.preheader:                         ; preds = %for.end5049, %for.inc5170
  %indvar11158 = phi i64 [ 0, %for.end5049 ], [ %indvar.next11159, %for.inc5170 ]
  %indvars.iv9489 = phi i64 [ 32, %for.end5049 ], [ %indvars.iv.next9490, %for.inc5170 ]
  %5660 = mul nuw nsw i64 %indvar11158, 5000
  br label %vector.body11218

vector.body11218:                                 ; preds = %vector.body11218, %for.body.i8564.preheader
  %index11222 = phi i64 [ 0, %for.body.i8564.preheader ], [ %index.next11223, %vector.body11218 ]
  %vec.ind11240 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8564.preheader ], [ %vec.ind.next11253, %vector.body11218 ]
  %step.add11241 = add <2 x i32> %vec.ind11240, <i32 2, i32 2>
  %step.add11242 = add <2 x i32> %vec.ind11240, <i32 4, i32 4>
  %step.add11243 = add <2 x i32> %vec.ind11240, <i32 6, i32 6>
  %step.add11244 = add <2 x i32> %vec.ind11240, <i32 8, i32 8>
  %step.add11245 = add <2 x i32> %vec.ind11240, <i32 10, i32 10>
  %step.add11246 = add <2 x i32> %vec.ind11240, <i32 12, i32 12>
  %step.add11247 = add <2 x i32> %vec.ind11240, <i32 14, i32 14>
  %step.add11248 = add <2 x i32> %vec.ind11240, <i32 16, i32 16>
  %step.add11249 = add <2 x i32> %vec.ind11240, <i32 18, i32 18>
  %step.add11250 = add <2 x i32> %vec.ind11240, <i32 20, i32 20>
  %step.add11251 = add <2 x i32> %vec.ind11240, <i32 22, i32 22>
  %5661 = sitofp <2 x i32> %vec.ind11240 to <2 x double>
  %5662 = sitofp <2 x i32> %step.add11241 to <2 x double>
  %5663 = sitofp <2 x i32> %step.add11242 to <2 x double>
  %5664 = sitofp <2 x i32> %step.add11243 to <2 x double>
  %5665 = sitofp <2 x i32> %step.add11244 to <2 x double>
  %5666 = sitofp <2 x i32> %step.add11245 to <2 x double>
  %5667 = sitofp <2 x i32> %step.add11246 to <2 x double>
  %5668 = sitofp <2 x i32> %step.add11247 to <2 x double>
  %5669 = sitofp <2 x i32> %step.add11248 to <2 x double>
  %5670 = sitofp <2 x i32> %step.add11249 to <2 x double>
  %5671 = sitofp <2 x i32> %step.add11250 to <2 x double>
  %5672 = sitofp <2 x i32> %step.add11251 to <2 x double>
  %5673 = getelementptr inbounds double, double* %1795, i64 %index11222
  %5674 = bitcast double* %5673 to <2 x double>*
  store <2 x double> %5661, <2 x double>* %5674, align 8, !tbaa !62
  %5675 = getelementptr inbounds double, double* %5673, i64 2
  %5676 = bitcast double* %5675 to <2 x double>*
  store <2 x double> %5662, <2 x double>* %5676, align 8, !tbaa !62
  %5677 = getelementptr inbounds double, double* %5673, i64 4
  %5678 = bitcast double* %5677 to <2 x double>*
  store <2 x double> %5663, <2 x double>* %5678, align 8, !tbaa !62
  %5679 = getelementptr inbounds double, double* %5673, i64 6
  %5680 = bitcast double* %5679 to <2 x double>*
  store <2 x double> %5664, <2 x double>* %5680, align 8, !tbaa !62
  %5681 = getelementptr inbounds double, double* %5673, i64 8
  %5682 = bitcast double* %5681 to <2 x double>*
  store <2 x double> %5665, <2 x double>* %5682, align 8, !tbaa !62
  %5683 = getelementptr inbounds double, double* %5673, i64 10
  %5684 = bitcast double* %5683 to <2 x double>*
  store <2 x double> %5666, <2 x double>* %5684, align 8, !tbaa !62
  %5685 = getelementptr inbounds double, double* %5673, i64 12
  %5686 = bitcast double* %5685 to <2 x double>*
  store <2 x double> %5667, <2 x double>* %5686, align 8, !tbaa !62
  %5687 = getelementptr inbounds double, double* %5673, i64 14
  %5688 = bitcast double* %5687 to <2 x double>*
  store <2 x double> %5668, <2 x double>* %5688, align 8, !tbaa !62
  %5689 = getelementptr inbounds double, double* %5673, i64 16
  %5690 = bitcast double* %5689 to <2 x double>*
  store <2 x double> %5669, <2 x double>* %5690, align 8, !tbaa !62
  %5691 = getelementptr inbounds double, double* %5673, i64 18
  %5692 = bitcast double* %5691 to <2 x double>*
  store <2 x double> %5670, <2 x double>* %5692, align 8, !tbaa !62
  %5693 = getelementptr inbounds double, double* %5673, i64 20
  %5694 = bitcast double* %5693 to <2 x double>*
  store <2 x double> %5671, <2 x double>* %5694, align 8, !tbaa !62
  %5695 = getelementptr inbounds double, double* %5673, i64 22
  %5696 = bitcast double* %5695 to <2 x double>*
  store <2 x double> %5672, <2 x double>* %5696, align 8, !tbaa !62
  %5697 = getelementptr inbounds double, double* %1794, i64 %index11222
  %5698 = bitcast double* %5697 to <2 x double>*
  store <2 x double> %5661, <2 x double>* %5698, align 8, !tbaa !62
  %5699 = getelementptr inbounds double, double* %5697, i64 2
  %5700 = bitcast double* %5699 to <2 x double>*
  store <2 x double> %5662, <2 x double>* %5700, align 8, !tbaa !62
  %5701 = getelementptr inbounds double, double* %5697, i64 4
  %5702 = bitcast double* %5701 to <2 x double>*
  store <2 x double> %5663, <2 x double>* %5702, align 8, !tbaa !62
  %5703 = getelementptr inbounds double, double* %5697, i64 6
  %5704 = bitcast double* %5703 to <2 x double>*
  store <2 x double> %5664, <2 x double>* %5704, align 8, !tbaa !62
  %5705 = getelementptr inbounds double, double* %5697, i64 8
  %5706 = bitcast double* %5705 to <2 x double>*
  store <2 x double> %5665, <2 x double>* %5706, align 8, !tbaa !62
  %5707 = getelementptr inbounds double, double* %5697, i64 10
  %5708 = bitcast double* %5707 to <2 x double>*
  store <2 x double> %5666, <2 x double>* %5708, align 8, !tbaa !62
  %5709 = getelementptr inbounds double, double* %5697, i64 12
  %5710 = bitcast double* %5709 to <2 x double>*
  store <2 x double> %5667, <2 x double>* %5710, align 8, !tbaa !62
  %5711 = getelementptr inbounds double, double* %5697, i64 14
  %5712 = bitcast double* %5711 to <2 x double>*
  store <2 x double> %5668, <2 x double>* %5712, align 8, !tbaa !62
  %5713 = getelementptr inbounds double, double* %5697, i64 16
  %5714 = bitcast double* %5713 to <2 x double>*
  store <2 x double> %5669, <2 x double>* %5714, align 8, !tbaa !62
  %5715 = getelementptr inbounds double, double* %5697, i64 18
  %5716 = bitcast double* %5715 to <2 x double>*
  store <2 x double> %5670, <2 x double>* %5716, align 8, !tbaa !62
  %5717 = getelementptr inbounds double, double* %5697, i64 20
  %5718 = bitcast double* %5717 to <2 x double>*
  store <2 x double> %5671, <2 x double>* %5718, align 8, !tbaa !62
  %5719 = getelementptr inbounds double, double* %5697, i64 22
  %5720 = bitcast double* %5719 to <2 x double>*
  store <2 x double> %5672, <2 x double>* %5720, align 8, !tbaa !62
  %5721 = shl <2 x i32> %vec.ind11240, <i32 1, i32 1>
  %5722 = shl <2 x i32> %step.add11241, <i32 1, i32 1>
  %5723 = shl <2 x i32> %step.add11242, <i32 1, i32 1>
  %5724 = shl <2 x i32> %step.add11243, <i32 1, i32 1>
  %5725 = shl <2 x i32> %step.add11244, <i32 1, i32 1>
  %5726 = shl <2 x i32> %step.add11245, <i32 1, i32 1>
  %5727 = shl <2 x i32> %step.add11246, <i32 1, i32 1>
  %5728 = shl <2 x i32> %step.add11247, <i32 1, i32 1>
  %5729 = shl <2 x i32> %step.add11248, <i32 1, i32 1>
  %5730 = shl <2 x i32> %step.add11249, <i32 1, i32 1>
  %5731 = shl <2 x i32> %step.add11250, <i32 1, i32 1>
  %5732 = shl <2 x i32> %step.add11251, <i32 1, i32 1>
  %5733 = sitofp <2 x i32> %5721 to <2 x double>
  %5734 = sitofp <2 x i32> %5722 to <2 x double>
  %5735 = sitofp <2 x i32> %5723 to <2 x double>
  %5736 = sitofp <2 x i32> %5724 to <2 x double>
  %5737 = sitofp <2 x i32> %5725 to <2 x double>
  %5738 = sitofp <2 x i32> %5726 to <2 x double>
  %5739 = sitofp <2 x i32> %5727 to <2 x double>
  %5740 = sitofp <2 x i32> %5728 to <2 x double>
  %5741 = sitofp <2 x i32> %5729 to <2 x double>
  %5742 = sitofp <2 x i32> %5730 to <2 x double>
  %5743 = sitofp <2 x i32> %5731 to <2 x double>
  %5744 = sitofp <2 x i32> %5732 to <2 x double>
  %5745 = getelementptr inbounds double, double* %1796, i64 %index11222
  %5746 = bitcast double* %5745 to <2 x double>*
  store <2 x double> %5733, <2 x double>* %5746, align 8, !tbaa !62
  %5747 = getelementptr inbounds double, double* %5745, i64 2
  %5748 = bitcast double* %5747 to <2 x double>*
  store <2 x double> %5734, <2 x double>* %5748, align 8, !tbaa !62
  %5749 = getelementptr inbounds double, double* %5745, i64 4
  %5750 = bitcast double* %5749 to <2 x double>*
  store <2 x double> %5735, <2 x double>* %5750, align 8, !tbaa !62
  %5751 = getelementptr inbounds double, double* %5745, i64 6
  %5752 = bitcast double* %5751 to <2 x double>*
  store <2 x double> %5736, <2 x double>* %5752, align 8, !tbaa !62
  %5753 = getelementptr inbounds double, double* %5745, i64 8
  %5754 = bitcast double* %5753 to <2 x double>*
  store <2 x double> %5737, <2 x double>* %5754, align 8, !tbaa !62
  %5755 = getelementptr inbounds double, double* %5745, i64 10
  %5756 = bitcast double* %5755 to <2 x double>*
  store <2 x double> %5738, <2 x double>* %5756, align 8, !tbaa !62
  %5757 = getelementptr inbounds double, double* %5745, i64 12
  %5758 = bitcast double* %5757 to <2 x double>*
  store <2 x double> %5739, <2 x double>* %5758, align 8, !tbaa !62
  %5759 = getelementptr inbounds double, double* %5745, i64 14
  %5760 = bitcast double* %5759 to <2 x double>*
  store <2 x double> %5740, <2 x double>* %5760, align 8, !tbaa !62
  %5761 = getelementptr inbounds double, double* %5745, i64 16
  %5762 = bitcast double* %5761 to <2 x double>*
  store <2 x double> %5741, <2 x double>* %5762, align 8, !tbaa !62
  %5763 = getelementptr inbounds double, double* %5745, i64 18
  %5764 = bitcast double* %5763 to <2 x double>*
  store <2 x double> %5742, <2 x double>* %5764, align 8, !tbaa !62
  %5765 = getelementptr inbounds double, double* %5745, i64 20
  %5766 = bitcast double* %5765 to <2 x double>*
  store <2 x double> %5743, <2 x double>* %5766, align 8, !tbaa !62
  %5767 = getelementptr inbounds double, double* %5745, i64 22
  %5768 = bitcast double* %5767 to <2 x double>*
  store <2 x double> %5744, <2 x double>* %5768, align 8, !tbaa !62
  %5769 = add <2 x i32> %vec.ind11240, <i32 -3, i32 -3>
  %5770 = add <2 x i32> %vec.ind11240, <i32 -1, i32 -1>
  %5771 = add <2 x i32> %vec.ind11240, <i32 1, i32 1>
  %5772 = add <2 x i32> %vec.ind11240, <i32 3, i32 3>
  %5773 = add <2 x i32> %vec.ind11240, <i32 5, i32 5>
  %5774 = add <2 x i32> %vec.ind11240, <i32 7, i32 7>
  %5775 = add <2 x i32> %vec.ind11240, <i32 9, i32 9>
  %5776 = add <2 x i32> %vec.ind11240, <i32 11, i32 11>
  %5777 = add <2 x i32> %vec.ind11240, <i32 13, i32 13>
  %5778 = add <2 x i32> %vec.ind11240, <i32 15, i32 15>
  %5779 = add <2 x i32> %vec.ind11240, <i32 17, i32 17>
  %5780 = add <2 x i32> %vec.ind11240, <i32 19, i32 19>
  %5781 = sitofp <2 x i32> %5769 to <2 x double>
  %5782 = sitofp <2 x i32> %5770 to <2 x double>
  %5783 = sitofp <2 x i32> %5771 to <2 x double>
  %5784 = sitofp <2 x i32> %5772 to <2 x double>
  %5785 = sitofp <2 x i32> %5773 to <2 x double>
  %5786 = sitofp <2 x i32> %5774 to <2 x double>
  %5787 = sitofp <2 x i32> %5775 to <2 x double>
  %5788 = sitofp <2 x i32> %5776 to <2 x double>
  %5789 = sitofp <2 x i32> %5777 to <2 x double>
  %5790 = sitofp <2 x i32> %5778 to <2 x double>
  %5791 = sitofp <2 x i32> %5779 to <2 x double>
  %5792 = sitofp <2 x i32> %5780 to <2 x double>
  %5793 = getelementptr inbounds double, double* %1797, i64 %index11222
  %5794 = bitcast double* %5793 to <2 x double>*
  store <2 x double> %5781, <2 x double>* %5794, align 8, !tbaa !62
  %5795 = getelementptr inbounds double, double* %5793, i64 2
  %5796 = bitcast double* %5795 to <2 x double>*
  store <2 x double> %5782, <2 x double>* %5796, align 8, !tbaa !62
  %5797 = getelementptr inbounds double, double* %5793, i64 4
  %5798 = bitcast double* %5797 to <2 x double>*
  store <2 x double> %5783, <2 x double>* %5798, align 8, !tbaa !62
  %5799 = getelementptr inbounds double, double* %5793, i64 6
  %5800 = bitcast double* %5799 to <2 x double>*
  store <2 x double> %5784, <2 x double>* %5800, align 8, !tbaa !62
  %5801 = getelementptr inbounds double, double* %5793, i64 8
  %5802 = bitcast double* %5801 to <2 x double>*
  store <2 x double> %5785, <2 x double>* %5802, align 8, !tbaa !62
  %5803 = getelementptr inbounds double, double* %5793, i64 10
  %5804 = bitcast double* %5803 to <2 x double>*
  store <2 x double> %5786, <2 x double>* %5804, align 8, !tbaa !62
  %5805 = getelementptr inbounds double, double* %5793, i64 12
  %5806 = bitcast double* %5805 to <2 x double>*
  store <2 x double> %5787, <2 x double>* %5806, align 8, !tbaa !62
  %5807 = getelementptr inbounds double, double* %5793, i64 14
  %5808 = bitcast double* %5807 to <2 x double>*
  store <2 x double> %5788, <2 x double>* %5808, align 8, !tbaa !62
  %5809 = getelementptr inbounds double, double* %5793, i64 16
  %5810 = bitcast double* %5809 to <2 x double>*
  store <2 x double> %5789, <2 x double>* %5810, align 8, !tbaa !62
  %5811 = getelementptr inbounds double, double* %5793, i64 18
  %5812 = bitcast double* %5811 to <2 x double>*
  store <2 x double> %5790, <2 x double>* %5812, align 8, !tbaa !62
  %5813 = getelementptr inbounds double, double* %5793, i64 20
  %5814 = bitcast double* %5813 to <2 x double>*
  store <2 x double> %5791, <2 x double>* %5814, align 8, !tbaa !62
  %5815 = getelementptr inbounds double, double* %5793, i64 22
  %5816 = bitcast double* %5815 to <2 x double>*
  store <2 x double> %5792, <2 x double>* %5816, align 8, !tbaa !62
  %index.next11223 = add nuw nsw i64 %index11222, 24
  %vec.ind.next11253 = add <2 x i32> %vec.ind11240, <i32 24, i32 24>
  %5817 = icmp eq i64 %index.next11223, 24984
  br i1 %5817, label %for.body.i8564, label %vector.body11218, !llvm.loop !113

for.body.i8564:                                   ; preds = %vector.body11218
  %5818 = add nuw nsw i64 %5660, 32
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %5628, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %5629, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %5630, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %5631, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %5632, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %5633, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %5634, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %5635, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %5636, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %5637, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %5638, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %5639, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %5640, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %5641, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %5642, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %5643, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %5644, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %5645, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %5646, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %5647, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %5648, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %5649, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %5650, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %5651, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %5652, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %5653, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %5654, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %5655, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %5656, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %5657, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %5658, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %5659, align 8, !tbaa !62
  %5819 = shl nuw nsw i64 %indvars.iv9489, 3
  store i8* %call3733, i8** %5594, align 8
  store i8* %call3733, i8** %5595, align 8
  store i64 %5819, i64* %5596, align 8
  store i8* %call3735, i8** %5597, align 8
  store i8* %call3735, i8** %5598, align 8
  store i64 %5819, i64* %5599, align 8
  store i8* %call3736, i8** %5600, align 8
  store i8* %call3736, i8** %5601, align 8
  store i64 %5819, i64* %5602, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %5594, i8** nonnull %5595, i64* nonnull %5596, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  %cmp50798876 = icmp ult i64 %indvars.iv9489, 128
  br i1 %cmp50798876, label %for.cond.cleanup5115, label %for.cond5072.preheader

for.cond5112.preheader:                           ; preds = %for.cond.cleanup5075
  %cmp51138887 = icmp sgt i32 %t5058.28877, -1
  br i1 %cmp51138887, label %for.cond5118.preheader.preheader39, label %for.cond.cleanup5115

for.cond5118.preheader.preheader39:               ; preds = %for.cond5112.preheader
  %n.mod.vf11162 = urem i64 %5818, 24
  %n.vec11163 = sub nuw nsw i64 %5818, %n.mod.vf11162
  %cmp.n11167 = icmp eq i64 %n.mod.vf11162, 0
  br label %for.cond5118.preheader

for.cond5072.preheader:                           ; preds = %for.body.i8564, %for.cond.cleanup5075
  %tms5065.08884 = phi i32 [ %mul5108, %for.cond.cleanup5075 ], [ 1, %for.body.i8564 ]
  %t5058.08883 = phi i32 [ %inc5083, %for.cond.cleanup5075 ], [ 0, %for.body.i8564 ]
  %.capture_expr..casted5091.sroa.0.0.insert.ext = zext i32 %tms5065.08884 to i64
  br label %for.cond5078.preheader

for.cond5078.preheader:                           ; preds = %for.cond5072.preheader, %for.cond.cleanup5081
  %ths5071.08881 = phi i32 [ %mul5104, %for.cond.cleanup5081 ], [ 32, %for.cond5072.preheader ]
  %t5058.18880 = phi i32 [ %inc5083, %for.cond.cleanup5081 ], [ %t5058.08883, %for.cond5072.preheader ]
  %.capture_expr..casted5093.sroa.0.0.insert.ext = zext i32 %ths5071.08881 to i64
  br label %for.body5082

for.cond.cleanup5075:                             ; preds = %for.cond.cleanup5081
  %mul5108 = shl nsw i32 %tms5065.08884, 1
  %cmp5067 = icmp ult i32 %mul5108, 257
  br i1 %cmp5067, label %for.cond5072.preheader, label %for.cond5112.preheader

for.cond.cleanup5081:                             ; preds = %omp_offload.cont5098
  %mul5104 = shl nsw i32 %ths5071.08881, 1
  %cmp5073 = icmp ult i32 %mul5104, 1025
  br i1 %cmp5073, label %for.cond5078.preheader, label %for.cond.cleanup5075

for.body5082:                                     ; preds = %for.cond5078.preheader, %omp_offload.cont5098
  %sch5077.08878 = phi i32 [ 128, %for.cond5078.preheader ], [ %mul5100, %omp_offload.cont5098 ]
  %t5058.28877 = phi i32 [ %t5058.18880, %for.cond5078.preheader ], [ %inc5083, %omp_offload.cont5098 ]
  %inc5083 = add nsw i32 %t5058.28877, 1
  %.capture_expr..casted5089.sroa.0.0.insert.ext = zext i32 %sch5077.08878 to i64
  store i64 %indvars.iv9489, i64* %5604, align 8
  store i64 %indvars.iv9489, i64* %5606, align 8
  store i8* %call3733, i8** %5607, align 8
  store i8* %call3733, i8** %5608, align 8
  store i8* %call3735, i8** %5609, align 8
  store i8* %call3735, i8** %5610, align 8
  store i8* %call3736, i8** %5611, align 8
  store i8* %call3736, i8** %5612, align 8
  store i64 %.capture_expr..casted5089.sroa.0.0.insert.ext, i64* %5614, align 8
  store i64 %.capture_expr..casted5089.sroa.0.0.insert.ext, i64* %5616, align 8
  store i64 %.capture_expr..casted5091.sroa.0.0.insert.ext, i64* %5618, align 8
  store i64 %.capture_expr..casted5091.sroa.0.0.insert.ext, i64* %5620, align 8
  store i64 %.capture_expr..casted5093.sroa.0.0.insert.ext, i64* %5622, align 8
  store i64 %.capture_expr..casted5093.sroa.0.0.insert.ext, i64* %5624, align 8
  %5820 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1428.region_id, i32 7, i8** nonnull %5603, i8** nonnull %5605, i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_sizes.273, i64 0, i64 0), i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_maptypes.274, i64 0, i64 0), i32 %tms5065.08884, i32 %ths5071.08881) #6
  %5821 = icmp eq i32 %5820, 0
  br i1 %5821, label %omp_offload.cont5098, label %omp_offload.failed5097

omp_offload.failed5097:                           ; preds = %for.body5082
  %5822 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %5823 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %5822, i32 %tms5065.08884, i32 %ths5071.08881) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 5, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64)* @.omp_outlined..236 to void (i32*, i32*, ...)*), i64 %indvars.iv9489, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted5089.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont5098

omp_offload.cont5098:                             ; preds = %for.body5082, %omp_offload.failed5097
  %mul5100 = mul nsw i32 %sch5077.08878, 3000
  %5824 = zext i32 %mul5100 to i64
  %cmp5079 = icmp ult i64 %indvars.iv9489, %5824
  br i1 %cmp5079, label %for.cond.cleanup5081, label %for.body5082

for.cond5118.preheader:                           ; preds = %for.cond5118.preheader.preheader39, %for.cond.cleanup5121
  %times5111.08888 = phi i32 [ %inc5136, %for.cond.cleanup5121 ], [ 0, %for.cond5118.preheader.preheader39 ]
  br label %vector.body11155

vector.body11155:                                 ; preds = %for.cond5118.preheader, %vector.body11155
  %index11164 = phi i64 [ %index.next11165, %vector.body11155 ], [ 0, %for.cond5118.preheader ]
  %5825 = getelementptr inbounds double, double* %1796, i64 %index11164
  %5826 = bitcast double* %5825 to <2 x double>*
  %wide.load11182 = load <2 x double>, <2 x double>* %5826, align 8, !tbaa !62
  %5827 = getelementptr inbounds double, double* %5825, i64 2
  %5828 = bitcast double* %5827 to <2 x double>*
  %wide.load11183 = load <2 x double>, <2 x double>* %5828, align 8, !tbaa !62
  %5829 = getelementptr inbounds double, double* %5825, i64 4
  %5830 = bitcast double* %5829 to <2 x double>*
  %wide.load11184 = load <2 x double>, <2 x double>* %5830, align 8, !tbaa !62
  %5831 = getelementptr inbounds double, double* %5825, i64 6
  %5832 = bitcast double* %5831 to <2 x double>*
  %wide.load11185 = load <2 x double>, <2 x double>* %5832, align 8, !tbaa !62
  %5833 = getelementptr inbounds double, double* %5825, i64 8
  %5834 = bitcast double* %5833 to <2 x double>*
  %wide.load11186 = load <2 x double>, <2 x double>* %5834, align 8, !tbaa !62
  %5835 = getelementptr inbounds double, double* %5825, i64 10
  %5836 = bitcast double* %5835 to <2 x double>*
  %wide.load11187 = load <2 x double>, <2 x double>* %5836, align 8, !tbaa !62
  %5837 = getelementptr inbounds double, double* %5825, i64 12
  %5838 = bitcast double* %5837 to <2 x double>*
  %wide.load11188 = load <2 x double>, <2 x double>* %5838, align 8, !tbaa !62
  %5839 = getelementptr inbounds double, double* %5825, i64 14
  %5840 = bitcast double* %5839 to <2 x double>*
  %wide.load11189 = load <2 x double>, <2 x double>* %5840, align 8, !tbaa !62
  %5841 = getelementptr inbounds double, double* %5825, i64 16
  %5842 = bitcast double* %5841 to <2 x double>*
  %wide.load11190 = load <2 x double>, <2 x double>* %5842, align 8, !tbaa !62
  %5843 = getelementptr inbounds double, double* %5825, i64 18
  %5844 = bitcast double* %5843 to <2 x double>*
  %wide.load11191 = load <2 x double>, <2 x double>* %5844, align 8, !tbaa !62
  %5845 = getelementptr inbounds double, double* %5825, i64 20
  %5846 = bitcast double* %5845 to <2 x double>*
  %wide.load11192 = load <2 x double>, <2 x double>* %5846, align 8, !tbaa !62
  %5847 = getelementptr inbounds double, double* %5825, i64 22
  %5848 = bitcast double* %5847 to <2 x double>*
  %wide.load11193 = load <2 x double>, <2 x double>* %5848, align 8, !tbaa !62
  %5849 = getelementptr inbounds double, double* %1797, i64 %index11164
  %5850 = bitcast double* %5849 to <2 x double>*
  %wide.load11194 = load <2 x double>, <2 x double>* %5850, align 8, !tbaa !62
  %5851 = getelementptr inbounds double, double* %5849, i64 2
  %5852 = bitcast double* %5851 to <2 x double>*
  %wide.load11195 = load <2 x double>, <2 x double>* %5852, align 8, !tbaa !62
  %5853 = getelementptr inbounds double, double* %5849, i64 4
  %5854 = bitcast double* %5853 to <2 x double>*
  %wide.load11196 = load <2 x double>, <2 x double>* %5854, align 8, !tbaa !62
  %5855 = getelementptr inbounds double, double* %5849, i64 6
  %5856 = bitcast double* %5855 to <2 x double>*
  %wide.load11197 = load <2 x double>, <2 x double>* %5856, align 8, !tbaa !62
  %5857 = getelementptr inbounds double, double* %5849, i64 8
  %5858 = bitcast double* %5857 to <2 x double>*
  %wide.load11198 = load <2 x double>, <2 x double>* %5858, align 8, !tbaa !62
  %5859 = getelementptr inbounds double, double* %5849, i64 10
  %5860 = bitcast double* %5859 to <2 x double>*
  %wide.load11199 = load <2 x double>, <2 x double>* %5860, align 8, !tbaa !62
  %5861 = getelementptr inbounds double, double* %5849, i64 12
  %5862 = bitcast double* %5861 to <2 x double>*
  %wide.load11200 = load <2 x double>, <2 x double>* %5862, align 8, !tbaa !62
  %5863 = getelementptr inbounds double, double* %5849, i64 14
  %5864 = bitcast double* %5863 to <2 x double>*
  %wide.load11201 = load <2 x double>, <2 x double>* %5864, align 8, !tbaa !62
  %5865 = getelementptr inbounds double, double* %5849, i64 16
  %5866 = bitcast double* %5865 to <2 x double>*
  %wide.load11202 = load <2 x double>, <2 x double>* %5866, align 8, !tbaa !62
  %5867 = getelementptr inbounds double, double* %5849, i64 18
  %5868 = bitcast double* %5867 to <2 x double>*
  %wide.load11203 = load <2 x double>, <2 x double>* %5868, align 8, !tbaa !62
  %5869 = getelementptr inbounds double, double* %5849, i64 20
  %5870 = bitcast double* %5869 to <2 x double>*
  %wide.load11204 = load <2 x double>, <2 x double>* %5870, align 8, !tbaa !62
  %5871 = getelementptr inbounds double, double* %5849, i64 22
  %5872 = bitcast double* %5871 to <2 x double>*
  %wide.load11205 = load <2 x double>, <2 x double>* %5872, align 8, !tbaa !62
  %5873 = fadd <2 x double> %wide.load11182, %wide.load11194
  %5874 = fadd <2 x double> %wide.load11183, %wide.load11195
  %5875 = fadd <2 x double> %wide.load11184, %wide.load11196
  %5876 = fadd <2 x double> %wide.load11185, %wide.load11197
  %5877 = fadd <2 x double> %wide.load11186, %wide.load11198
  %5878 = fadd <2 x double> %wide.load11187, %wide.load11199
  %5879 = fadd <2 x double> %wide.load11188, %wide.load11200
  %5880 = fadd <2 x double> %wide.load11189, %wide.load11201
  %5881 = fadd <2 x double> %wide.load11190, %wide.load11202
  %5882 = fadd <2 x double> %wide.load11191, %wide.load11203
  %5883 = fadd <2 x double> %wide.load11192, %wide.load11204
  %5884 = fadd <2 x double> %wide.load11193, %wide.load11205
  %5885 = getelementptr inbounds double, double* %1795, i64 %index11164
  %5886 = bitcast double* %5885 to <2 x double>*
  %wide.load11206 = load <2 x double>, <2 x double>* %5886, align 8, !tbaa !62
  %5887 = getelementptr inbounds double, double* %5885, i64 2
  %5888 = bitcast double* %5887 to <2 x double>*
  %wide.load11207 = load <2 x double>, <2 x double>* %5888, align 8, !tbaa !62
  %5889 = getelementptr inbounds double, double* %5885, i64 4
  %5890 = bitcast double* %5889 to <2 x double>*
  %wide.load11208 = load <2 x double>, <2 x double>* %5890, align 8, !tbaa !62
  %5891 = getelementptr inbounds double, double* %5885, i64 6
  %5892 = bitcast double* %5891 to <2 x double>*
  %wide.load11209 = load <2 x double>, <2 x double>* %5892, align 8, !tbaa !62
  %5893 = getelementptr inbounds double, double* %5885, i64 8
  %5894 = bitcast double* %5893 to <2 x double>*
  %wide.load11210 = load <2 x double>, <2 x double>* %5894, align 8, !tbaa !62
  %5895 = getelementptr inbounds double, double* %5885, i64 10
  %5896 = bitcast double* %5895 to <2 x double>*
  %wide.load11211 = load <2 x double>, <2 x double>* %5896, align 8, !tbaa !62
  %5897 = getelementptr inbounds double, double* %5885, i64 12
  %5898 = bitcast double* %5897 to <2 x double>*
  %wide.load11212 = load <2 x double>, <2 x double>* %5898, align 8, !tbaa !62
  %5899 = getelementptr inbounds double, double* %5885, i64 14
  %5900 = bitcast double* %5899 to <2 x double>*
  %wide.load11213 = load <2 x double>, <2 x double>* %5900, align 8, !tbaa !62
  %5901 = getelementptr inbounds double, double* %5885, i64 16
  %5902 = bitcast double* %5901 to <2 x double>*
  %wide.load11214 = load <2 x double>, <2 x double>* %5902, align 8, !tbaa !62
  %5903 = getelementptr inbounds double, double* %5885, i64 18
  %5904 = bitcast double* %5903 to <2 x double>*
  %wide.load11215 = load <2 x double>, <2 x double>* %5904, align 8, !tbaa !62
  %5905 = getelementptr inbounds double, double* %5885, i64 20
  %5906 = bitcast double* %5905 to <2 x double>*
  %wide.load11216 = load <2 x double>, <2 x double>* %5906, align 8, !tbaa !62
  %5907 = getelementptr inbounds double, double* %5885, i64 22
  %5908 = bitcast double* %5907 to <2 x double>*
  %wide.load11217 = load <2 x double>, <2 x double>* %5908, align 8, !tbaa !62
  %5909 = fadd <2 x double> %5873, %wide.load11206
  %5910 = fadd <2 x double> %5874, %wide.load11207
  %5911 = fadd <2 x double> %5875, %wide.load11208
  %5912 = fadd <2 x double> %5876, %wide.load11209
  %5913 = fadd <2 x double> %5877, %wide.load11210
  %5914 = fadd <2 x double> %5878, %wide.load11211
  %5915 = fadd <2 x double> %5879, %wide.load11212
  %5916 = fadd <2 x double> %5880, %wide.load11213
  %5917 = fadd <2 x double> %5881, %wide.load11214
  %5918 = fadd <2 x double> %5882, %wide.load11215
  %5919 = fadd <2 x double> %5883, %wide.load11216
  %5920 = fadd <2 x double> %5884, %wide.load11217
  store <2 x double> %5909, <2 x double>* %5886, align 8, !tbaa !62
  store <2 x double> %5910, <2 x double>* %5888, align 8, !tbaa !62
  store <2 x double> %5911, <2 x double>* %5890, align 8, !tbaa !62
  store <2 x double> %5912, <2 x double>* %5892, align 8, !tbaa !62
  store <2 x double> %5913, <2 x double>* %5894, align 8, !tbaa !62
  store <2 x double> %5914, <2 x double>* %5896, align 8, !tbaa !62
  store <2 x double> %5915, <2 x double>* %5898, align 8, !tbaa !62
  store <2 x double> %5916, <2 x double>* %5900, align 8, !tbaa !62
  store <2 x double> %5917, <2 x double>* %5902, align 8, !tbaa !62
  store <2 x double> %5918, <2 x double>* %5904, align 8, !tbaa !62
  store <2 x double> %5919, <2 x double>* %5906, align 8, !tbaa !62
  store <2 x double> %5920, <2 x double>* %5908, align 8, !tbaa !62
  %index.next11165 = add nuw nsw i64 %index11164, 24
  %5921 = icmp eq i64 %index.next11165, %n.vec11163
  br i1 %5921, label %middle.block11156, label %vector.body11155, !llvm.loop !114

middle.block11156:                                ; preds = %vector.body11155
  br i1 %cmp.n11167, label %for.cond.cleanup5121, label %for.body5122

for.cond.cleanup5115:                             ; preds = %for.cond.cleanup5121, %for.body.i8564, %for.cond5112.preheader
  store i8* %call3733, i8** %5625, align 8
  store i8* %call3733, i8** %5626, align 8
  store i64 %5819, i64* %5627, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %5625, i8** nonnull %5626, i64* nonnull %5627, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body5148

for.cond.cleanup5121:                             ; preds = %for.body5122, %middle.block11156
  %inc5136 = add nuw nsw i32 %times5111.08888, 1
  %exitcond9486 = icmp eq i32 %inc5136, %inc5083
  br i1 %exitcond9486, label %for.cond.cleanup5115, label %for.cond5118.preheader

for.body5122:                                     ; preds = %middle.block11156, %for.body5122
  %indvars.iv9483 = phi i64 [ %indvars.iv.next9484, %for.body5122 ], [ %n.vec11163, %middle.block11156 ]
  %arrayidx5124 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9483
  %5922 = load double, double* %arrayidx5124, align 8, !tbaa !62
  %arrayidx5126 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9483
  %5923 = load double, double* %arrayidx5126, align 8, !tbaa !62
  %add5127 = fadd double %5922, %5923
  %arrayidx5129 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9483
  %5924 = load double, double* %arrayidx5129, align 8, !tbaa !62
  %add5130 = fadd double %5924, %add5127
  store double %add5130, double* %arrayidx5129, align 8, !tbaa !62
  %indvars.iv.next9484 = add nuw nsw i64 %indvars.iv9483, 1
  %exitcond9485 = icmp eq i64 %indvars.iv.next9484, %indvars.iv9489
  br i1 %exitcond9485, label %for.cond.cleanup5121, label %for.body5122, !llvm.loop !115

for.body5148:                                     ; preds = %for.inc5162.7, %for.cond.cleanup5115
  %indvars.iv9487 = phi i64 [ 0, %for.cond.cleanup5115 ], [ %indvars.iv.next9488.7, %for.inc5162.7 ]
  %arrayidx5150 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9487
  %5925 = load double, double* %arrayidx5150, align 8, !tbaa !62
  %arrayidx5152 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9487
  %5926 = load double, double* %arrayidx5152, align 8, !tbaa !62
  %cmp5153 = fcmp une double %5925, %5926
  br i1 %cmp5153, label %cleanup5172, label %for.inc5162

for.inc5162:                                      ; preds = %for.body5148
  %indvars.iv.next9488 = or i64 %indvars.iv9487, 1
  %arrayidx5150.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9488
  %5927 = load double, double* %arrayidx5150.1, align 8, !tbaa !62
  %arrayidx5152.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9488
  %5928 = load double, double* %arrayidx5152.1, align 8, !tbaa !62
  %cmp5153.1 = fcmp une double %5927, %5928
  br i1 %cmp5153.1, label %cleanup5172, label %for.inc5162.1

for.inc5170:                                      ; preds = %for.inc5162.7
  %indvars.iv.next9490 = add nuw nsw i64 %indvars.iv9489, 5000
  %cmp5054 = icmp ult i64 %indvars.iv.next9490, 25000
  %indvar.next11159 = add nuw nsw i64 %indvar11158, 1
  br i1 %cmp5054, label %for.body.i8564.preheader, label %for.end5174

cleanup5172:                                      ; preds = %for.inc5162.6, %for.inc5162.5, %for.inc5162.4, %for.inc5162.3, %for.inc5162.2, %for.inc5162.1, %for.inc5162, %for.body5148
  %indvars.iv9487.lcssa = phi i64 [ %indvars.iv9487, %for.body5148 ], [ %indvars.iv.next9488, %for.inc5162 ], [ %indvars.iv.next9488.1, %for.inc5162.1 ], [ %indvars.iv.next9488.2, %for.inc5162.2 ], [ %indvars.iv.next9488.3, %for.inc5162.3 ], [ %indvars.iv.next9488.4, %for.inc5162.4 ], [ %indvars.iv.next9488.5, %for.inc5162.5 ], [ %indvars.iv.next9488.6, %for.inc5162.6 ]
  %.lcssa11798 = phi double [ %5925, %for.body5148 ], [ %5927, %for.inc5162 ], [ %7788, %for.inc5162.1 ], [ %7790, %for.inc5162.2 ], [ %7792, %for.inc5162.3 ], [ %7794, %for.inc5162.4 ], [ %7796, %for.inc5162.5 ], [ %7798, %for.inc5162.6 ]
  %.lcssa11796 = phi double [ %5926, %for.body5148 ], [ %5928, %for.inc5162 ], [ %7789, %for.inc5162.1 ], [ %7791, %for.inc5162.2 ], [ %7793, %for.inc5162.3 ], [ %7795, %for.inc5162.4 ], [ %7797, %for.inc5162.5 ], [ %7799, %for.inc5162.6 ]
  %5929 = trunc i64 %indvars.iv9489 to i32
  %5930 = trunc i64 %indvars.iv9487.lcssa to i32
  %call5160 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %5929, i32 signext %5930, double %.lcssa11798, double %.lcssa11796)
  br label %cleanup5832

for.end5174:                                      ; preds = %for.inc5170
  %puts8267 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8268 = call i32 @puts(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @str.357, i64 0, i64 0))
  %5931 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5187, i64 0, i64 0
  %5932 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5188, i64 0, i64 0
  %5933 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5189, i64 0, i64 0
  %5934 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5187, i64 0, i64 1
  %5935 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5188, i64 0, i64 1
  %5936 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5189, i64 0, i64 1
  %5937 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5187, i64 0, i64 2
  %5938 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5188, i64 0, i64 2
  %5939 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5189, i64 0, i64 2
  %5940 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5229, i64 0, i64 0
  %5941 = bitcast [8 x i8*]* %.offload_baseptrs5229 to i64*
  %5942 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5230, i64 0, i64 0
  %5943 = bitcast [8 x i8*]* %.offload_ptrs5230 to i64*
  %5944 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5229, i64 0, i64 1
  %5945 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5230, i64 0, i64 1
  %5946 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5229, i64 0, i64 2
  %5947 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5230, i64 0, i64 2
  %5948 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5229, i64 0, i64 3
  %5949 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5230, i64 0, i64 3
  %5950 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5229, i64 0, i64 4
  %5951 = bitcast i8** %5950 to i64*
  %5952 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5230, i64 0, i64 4
  %5953 = bitcast i8** %5952 to i64*
  %5954 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5229, i64 0, i64 5
  %5955 = bitcast i8** %5954 to i64*
  %5956 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5230, i64 0, i64 5
  %5957 = bitcast i8** %5956 to i64*
  %5958 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5229, i64 0, i64 6
  %5959 = bitcast i8** %5958 to i64*
  %5960 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5230, i64 0, i64 6
  %5961 = bitcast i8** %5960 to i64*
  %5962 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5229, i64 0, i64 7
  %5963 = bitcast i8** %5962 to i64*
  %5964 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5230, i64 0, i64 7
  %5965 = bitcast i8** %5964 to i64*
  %5966 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs5278, i64 0, i64 0
  %5967 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs5279, i64 0, i64 0
  %5968 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes5280, i64 0, i64 0
  %5969 = bitcast i8* %arrayidx.i to <2 x double>*
  %5970 = bitcast i8* %arrayidx2.i to <2 x double>*
  %5971 = bitcast i8* %arrayidx5.i to <2 x double>*
  %5972 = bitcast i8* %arrayidx8.i to <2 x double>*
  %5973 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %5974 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %5975 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %5976 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %5977 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %5978 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %5979 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %5980 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %5981 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %5982 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %5983 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %5984 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %5985 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %5986 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %5987 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %5988 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %5989 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %5990 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %5991 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %5992 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %5993 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %5994 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %5995 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %5996 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %5997 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %5998 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %5999 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %6000 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8579.preheader

for.body.i8579.preheader:                         ; preds = %for.end5174, %for.inc5308
  %indvar11257 = phi i64 [ 0, %for.end5174 ], [ %indvar.next11258, %for.inc5308 ]
  %indvars.iv9481 = phi i64 [ 32, %for.end5174 ], [ %indvars.iv.next9482, %for.inc5308 ]
  %6001 = mul nuw nsw i64 %indvar11257, 5000
  br label %vector.body11317

vector.body11317:                                 ; preds = %vector.body11317, %for.body.i8579.preheader
  %index11321 = phi i64 [ 0, %for.body.i8579.preheader ], [ %index.next11322, %vector.body11317 ]
  %vec.ind11339 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8579.preheader ], [ %vec.ind.next11352, %vector.body11317 ]
  %step.add11340 = add <2 x i32> %vec.ind11339, <i32 2, i32 2>
  %step.add11341 = add <2 x i32> %vec.ind11339, <i32 4, i32 4>
  %step.add11342 = add <2 x i32> %vec.ind11339, <i32 6, i32 6>
  %step.add11343 = add <2 x i32> %vec.ind11339, <i32 8, i32 8>
  %step.add11344 = add <2 x i32> %vec.ind11339, <i32 10, i32 10>
  %step.add11345 = add <2 x i32> %vec.ind11339, <i32 12, i32 12>
  %step.add11346 = add <2 x i32> %vec.ind11339, <i32 14, i32 14>
  %step.add11347 = add <2 x i32> %vec.ind11339, <i32 16, i32 16>
  %step.add11348 = add <2 x i32> %vec.ind11339, <i32 18, i32 18>
  %step.add11349 = add <2 x i32> %vec.ind11339, <i32 20, i32 20>
  %step.add11350 = add <2 x i32> %vec.ind11339, <i32 22, i32 22>
  %6002 = sitofp <2 x i32> %vec.ind11339 to <2 x double>
  %6003 = sitofp <2 x i32> %step.add11340 to <2 x double>
  %6004 = sitofp <2 x i32> %step.add11341 to <2 x double>
  %6005 = sitofp <2 x i32> %step.add11342 to <2 x double>
  %6006 = sitofp <2 x i32> %step.add11343 to <2 x double>
  %6007 = sitofp <2 x i32> %step.add11344 to <2 x double>
  %6008 = sitofp <2 x i32> %step.add11345 to <2 x double>
  %6009 = sitofp <2 x i32> %step.add11346 to <2 x double>
  %6010 = sitofp <2 x i32> %step.add11347 to <2 x double>
  %6011 = sitofp <2 x i32> %step.add11348 to <2 x double>
  %6012 = sitofp <2 x i32> %step.add11349 to <2 x double>
  %6013 = sitofp <2 x i32> %step.add11350 to <2 x double>
  %6014 = getelementptr inbounds double, double* %1795, i64 %index11321
  %6015 = bitcast double* %6014 to <2 x double>*
  store <2 x double> %6002, <2 x double>* %6015, align 8, !tbaa !62
  %6016 = getelementptr inbounds double, double* %6014, i64 2
  %6017 = bitcast double* %6016 to <2 x double>*
  store <2 x double> %6003, <2 x double>* %6017, align 8, !tbaa !62
  %6018 = getelementptr inbounds double, double* %6014, i64 4
  %6019 = bitcast double* %6018 to <2 x double>*
  store <2 x double> %6004, <2 x double>* %6019, align 8, !tbaa !62
  %6020 = getelementptr inbounds double, double* %6014, i64 6
  %6021 = bitcast double* %6020 to <2 x double>*
  store <2 x double> %6005, <2 x double>* %6021, align 8, !tbaa !62
  %6022 = getelementptr inbounds double, double* %6014, i64 8
  %6023 = bitcast double* %6022 to <2 x double>*
  store <2 x double> %6006, <2 x double>* %6023, align 8, !tbaa !62
  %6024 = getelementptr inbounds double, double* %6014, i64 10
  %6025 = bitcast double* %6024 to <2 x double>*
  store <2 x double> %6007, <2 x double>* %6025, align 8, !tbaa !62
  %6026 = getelementptr inbounds double, double* %6014, i64 12
  %6027 = bitcast double* %6026 to <2 x double>*
  store <2 x double> %6008, <2 x double>* %6027, align 8, !tbaa !62
  %6028 = getelementptr inbounds double, double* %6014, i64 14
  %6029 = bitcast double* %6028 to <2 x double>*
  store <2 x double> %6009, <2 x double>* %6029, align 8, !tbaa !62
  %6030 = getelementptr inbounds double, double* %6014, i64 16
  %6031 = bitcast double* %6030 to <2 x double>*
  store <2 x double> %6010, <2 x double>* %6031, align 8, !tbaa !62
  %6032 = getelementptr inbounds double, double* %6014, i64 18
  %6033 = bitcast double* %6032 to <2 x double>*
  store <2 x double> %6011, <2 x double>* %6033, align 8, !tbaa !62
  %6034 = getelementptr inbounds double, double* %6014, i64 20
  %6035 = bitcast double* %6034 to <2 x double>*
  store <2 x double> %6012, <2 x double>* %6035, align 8, !tbaa !62
  %6036 = getelementptr inbounds double, double* %6014, i64 22
  %6037 = bitcast double* %6036 to <2 x double>*
  store <2 x double> %6013, <2 x double>* %6037, align 8, !tbaa !62
  %6038 = getelementptr inbounds double, double* %1794, i64 %index11321
  %6039 = bitcast double* %6038 to <2 x double>*
  store <2 x double> %6002, <2 x double>* %6039, align 8, !tbaa !62
  %6040 = getelementptr inbounds double, double* %6038, i64 2
  %6041 = bitcast double* %6040 to <2 x double>*
  store <2 x double> %6003, <2 x double>* %6041, align 8, !tbaa !62
  %6042 = getelementptr inbounds double, double* %6038, i64 4
  %6043 = bitcast double* %6042 to <2 x double>*
  store <2 x double> %6004, <2 x double>* %6043, align 8, !tbaa !62
  %6044 = getelementptr inbounds double, double* %6038, i64 6
  %6045 = bitcast double* %6044 to <2 x double>*
  store <2 x double> %6005, <2 x double>* %6045, align 8, !tbaa !62
  %6046 = getelementptr inbounds double, double* %6038, i64 8
  %6047 = bitcast double* %6046 to <2 x double>*
  store <2 x double> %6006, <2 x double>* %6047, align 8, !tbaa !62
  %6048 = getelementptr inbounds double, double* %6038, i64 10
  %6049 = bitcast double* %6048 to <2 x double>*
  store <2 x double> %6007, <2 x double>* %6049, align 8, !tbaa !62
  %6050 = getelementptr inbounds double, double* %6038, i64 12
  %6051 = bitcast double* %6050 to <2 x double>*
  store <2 x double> %6008, <2 x double>* %6051, align 8, !tbaa !62
  %6052 = getelementptr inbounds double, double* %6038, i64 14
  %6053 = bitcast double* %6052 to <2 x double>*
  store <2 x double> %6009, <2 x double>* %6053, align 8, !tbaa !62
  %6054 = getelementptr inbounds double, double* %6038, i64 16
  %6055 = bitcast double* %6054 to <2 x double>*
  store <2 x double> %6010, <2 x double>* %6055, align 8, !tbaa !62
  %6056 = getelementptr inbounds double, double* %6038, i64 18
  %6057 = bitcast double* %6056 to <2 x double>*
  store <2 x double> %6011, <2 x double>* %6057, align 8, !tbaa !62
  %6058 = getelementptr inbounds double, double* %6038, i64 20
  %6059 = bitcast double* %6058 to <2 x double>*
  store <2 x double> %6012, <2 x double>* %6059, align 8, !tbaa !62
  %6060 = getelementptr inbounds double, double* %6038, i64 22
  %6061 = bitcast double* %6060 to <2 x double>*
  store <2 x double> %6013, <2 x double>* %6061, align 8, !tbaa !62
  %6062 = shl <2 x i32> %vec.ind11339, <i32 1, i32 1>
  %6063 = shl <2 x i32> %step.add11340, <i32 1, i32 1>
  %6064 = shl <2 x i32> %step.add11341, <i32 1, i32 1>
  %6065 = shl <2 x i32> %step.add11342, <i32 1, i32 1>
  %6066 = shl <2 x i32> %step.add11343, <i32 1, i32 1>
  %6067 = shl <2 x i32> %step.add11344, <i32 1, i32 1>
  %6068 = shl <2 x i32> %step.add11345, <i32 1, i32 1>
  %6069 = shl <2 x i32> %step.add11346, <i32 1, i32 1>
  %6070 = shl <2 x i32> %step.add11347, <i32 1, i32 1>
  %6071 = shl <2 x i32> %step.add11348, <i32 1, i32 1>
  %6072 = shl <2 x i32> %step.add11349, <i32 1, i32 1>
  %6073 = shl <2 x i32> %step.add11350, <i32 1, i32 1>
  %6074 = sitofp <2 x i32> %6062 to <2 x double>
  %6075 = sitofp <2 x i32> %6063 to <2 x double>
  %6076 = sitofp <2 x i32> %6064 to <2 x double>
  %6077 = sitofp <2 x i32> %6065 to <2 x double>
  %6078 = sitofp <2 x i32> %6066 to <2 x double>
  %6079 = sitofp <2 x i32> %6067 to <2 x double>
  %6080 = sitofp <2 x i32> %6068 to <2 x double>
  %6081 = sitofp <2 x i32> %6069 to <2 x double>
  %6082 = sitofp <2 x i32> %6070 to <2 x double>
  %6083 = sitofp <2 x i32> %6071 to <2 x double>
  %6084 = sitofp <2 x i32> %6072 to <2 x double>
  %6085 = sitofp <2 x i32> %6073 to <2 x double>
  %6086 = getelementptr inbounds double, double* %1796, i64 %index11321
  %6087 = bitcast double* %6086 to <2 x double>*
  store <2 x double> %6074, <2 x double>* %6087, align 8, !tbaa !62
  %6088 = getelementptr inbounds double, double* %6086, i64 2
  %6089 = bitcast double* %6088 to <2 x double>*
  store <2 x double> %6075, <2 x double>* %6089, align 8, !tbaa !62
  %6090 = getelementptr inbounds double, double* %6086, i64 4
  %6091 = bitcast double* %6090 to <2 x double>*
  store <2 x double> %6076, <2 x double>* %6091, align 8, !tbaa !62
  %6092 = getelementptr inbounds double, double* %6086, i64 6
  %6093 = bitcast double* %6092 to <2 x double>*
  store <2 x double> %6077, <2 x double>* %6093, align 8, !tbaa !62
  %6094 = getelementptr inbounds double, double* %6086, i64 8
  %6095 = bitcast double* %6094 to <2 x double>*
  store <2 x double> %6078, <2 x double>* %6095, align 8, !tbaa !62
  %6096 = getelementptr inbounds double, double* %6086, i64 10
  %6097 = bitcast double* %6096 to <2 x double>*
  store <2 x double> %6079, <2 x double>* %6097, align 8, !tbaa !62
  %6098 = getelementptr inbounds double, double* %6086, i64 12
  %6099 = bitcast double* %6098 to <2 x double>*
  store <2 x double> %6080, <2 x double>* %6099, align 8, !tbaa !62
  %6100 = getelementptr inbounds double, double* %6086, i64 14
  %6101 = bitcast double* %6100 to <2 x double>*
  store <2 x double> %6081, <2 x double>* %6101, align 8, !tbaa !62
  %6102 = getelementptr inbounds double, double* %6086, i64 16
  %6103 = bitcast double* %6102 to <2 x double>*
  store <2 x double> %6082, <2 x double>* %6103, align 8, !tbaa !62
  %6104 = getelementptr inbounds double, double* %6086, i64 18
  %6105 = bitcast double* %6104 to <2 x double>*
  store <2 x double> %6083, <2 x double>* %6105, align 8, !tbaa !62
  %6106 = getelementptr inbounds double, double* %6086, i64 20
  %6107 = bitcast double* %6106 to <2 x double>*
  store <2 x double> %6084, <2 x double>* %6107, align 8, !tbaa !62
  %6108 = getelementptr inbounds double, double* %6086, i64 22
  %6109 = bitcast double* %6108 to <2 x double>*
  store <2 x double> %6085, <2 x double>* %6109, align 8, !tbaa !62
  %6110 = add <2 x i32> %vec.ind11339, <i32 -3, i32 -3>
  %6111 = add <2 x i32> %vec.ind11339, <i32 -1, i32 -1>
  %6112 = add <2 x i32> %vec.ind11339, <i32 1, i32 1>
  %6113 = add <2 x i32> %vec.ind11339, <i32 3, i32 3>
  %6114 = add <2 x i32> %vec.ind11339, <i32 5, i32 5>
  %6115 = add <2 x i32> %vec.ind11339, <i32 7, i32 7>
  %6116 = add <2 x i32> %vec.ind11339, <i32 9, i32 9>
  %6117 = add <2 x i32> %vec.ind11339, <i32 11, i32 11>
  %6118 = add <2 x i32> %vec.ind11339, <i32 13, i32 13>
  %6119 = add <2 x i32> %vec.ind11339, <i32 15, i32 15>
  %6120 = add <2 x i32> %vec.ind11339, <i32 17, i32 17>
  %6121 = add <2 x i32> %vec.ind11339, <i32 19, i32 19>
  %6122 = sitofp <2 x i32> %6110 to <2 x double>
  %6123 = sitofp <2 x i32> %6111 to <2 x double>
  %6124 = sitofp <2 x i32> %6112 to <2 x double>
  %6125 = sitofp <2 x i32> %6113 to <2 x double>
  %6126 = sitofp <2 x i32> %6114 to <2 x double>
  %6127 = sitofp <2 x i32> %6115 to <2 x double>
  %6128 = sitofp <2 x i32> %6116 to <2 x double>
  %6129 = sitofp <2 x i32> %6117 to <2 x double>
  %6130 = sitofp <2 x i32> %6118 to <2 x double>
  %6131 = sitofp <2 x i32> %6119 to <2 x double>
  %6132 = sitofp <2 x i32> %6120 to <2 x double>
  %6133 = sitofp <2 x i32> %6121 to <2 x double>
  %6134 = getelementptr inbounds double, double* %1797, i64 %index11321
  %6135 = bitcast double* %6134 to <2 x double>*
  store <2 x double> %6122, <2 x double>* %6135, align 8, !tbaa !62
  %6136 = getelementptr inbounds double, double* %6134, i64 2
  %6137 = bitcast double* %6136 to <2 x double>*
  store <2 x double> %6123, <2 x double>* %6137, align 8, !tbaa !62
  %6138 = getelementptr inbounds double, double* %6134, i64 4
  %6139 = bitcast double* %6138 to <2 x double>*
  store <2 x double> %6124, <2 x double>* %6139, align 8, !tbaa !62
  %6140 = getelementptr inbounds double, double* %6134, i64 6
  %6141 = bitcast double* %6140 to <2 x double>*
  store <2 x double> %6125, <2 x double>* %6141, align 8, !tbaa !62
  %6142 = getelementptr inbounds double, double* %6134, i64 8
  %6143 = bitcast double* %6142 to <2 x double>*
  store <2 x double> %6126, <2 x double>* %6143, align 8, !tbaa !62
  %6144 = getelementptr inbounds double, double* %6134, i64 10
  %6145 = bitcast double* %6144 to <2 x double>*
  store <2 x double> %6127, <2 x double>* %6145, align 8, !tbaa !62
  %6146 = getelementptr inbounds double, double* %6134, i64 12
  %6147 = bitcast double* %6146 to <2 x double>*
  store <2 x double> %6128, <2 x double>* %6147, align 8, !tbaa !62
  %6148 = getelementptr inbounds double, double* %6134, i64 14
  %6149 = bitcast double* %6148 to <2 x double>*
  store <2 x double> %6129, <2 x double>* %6149, align 8, !tbaa !62
  %6150 = getelementptr inbounds double, double* %6134, i64 16
  %6151 = bitcast double* %6150 to <2 x double>*
  store <2 x double> %6130, <2 x double>* %6151, align 8, !tbaa !62
  %6152 = getelementptr inbounds double, double* %6134, i64 18
  %6153 = bitcast double* %6152 to <2 x double>*
  store <2 x double> %6131, <2 x double>* %6153, align 8, !tbaa !62
  %6154 = getelementptr inbounds double, double* %6134, i64 20
  %6155 = bitcast double* %6154 to <2 x double>*
  store <2 x double> %6132, <2 x double>* %6155, align 8, !tbaa !62
  %6156 = getelementptr inbounds double, double* %6134, i64 22
  %6157 = bitcast double* %6156 to <2 x double>*
  store <2 x double> %6133, <2 x double>* %6157, align 8, !tbaa !62
  %index.next11322 = add nuw nsw i64 %index11321, 24
  %vec.ind.next11352 = add <2 x i32> %vec.ind11339, <i32 24, i32 24>
  %6158 = icmp eq i64 %index.next11322, 24984
  br i1 %6158, label %for.body.i8579, label %vector.body11317, !llvm.loop !116

for.body.i8579:                                   ; preds = %vector.body11317
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %5969, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %5970, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %5971, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %5972, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %5973, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %5974, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %5975, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %5976, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %5977, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %5978, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %5979, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %5980, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %5981, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %5982, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %5983, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %5984, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %5985, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %5986, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %5987, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %5988, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %5989, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %5990, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %5991, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %5992, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %5993, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %5994, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %5995, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %5996, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %5997, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %5998, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %5999, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %6000, align 8, !tbaa !62
  %6159 = shl nuw nsw i64 %indvars.iv9481, 3
  store i8* %call3733, i8** %5931, align 8
  store i8* %call3733, i8** %5932, align 8
  store i64 %6159, i64* %5933, align 8
  store i8* %call3735, i8** %5934, align 8
  store i8* %call3735, i8** %5935, align 8
  store i64 %6159, i64* %5936, align 8
  store i8* %call3736, i8** %5937, align 8
  store i8* %call3736, i8** %5938, align 8
  store i64 %6159, i64* %5939, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %5931, i8** nonnull %5932, i64* nonnull %5933, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  %cmp52048861 = icmp ult i64 %indvars.iv9481, 128
  %cmp52108857 = icmp ult i64 %indvars.iv9481, 1000
  br label %for.cond5197.preheader

for.cond5250.preheader:                           ; preds = %for.cond.cleanup5200
  %6160 = add nuw nsw i64 %6001, 32
  %cmp52518872 = icmp sgt i32 %t5183.2.lcssa.lcssa, 0
  br i1 %cmp52518872, label %for.cond5256.preheader.preheader37, label %for.cond.cleanup5253

for.cond5256.preheader.preheader37:               ; preds = %for.cond5250.preheader
  %n.mod.vf11261 = urem i64 %6160, 24
  %n.vec11262 = sub nuw nsw i64 %6160, %n.mod.vf11261
  %cmp.n11266 = icmp eq i64 %n.mod.vf11261, 0
  br label %for.cond5256.preheader

for.cond5197.preheader:                           ; preds = %for.body.i8579, %for.cond.cleanup5200
  %tms5190.08870 = phi i32 [ 1, %for.body.i8579 ], [ %mul5246, %for.cond.cleanup5200 ]
  %t5183.08869 = phi i32 [ 0, %for.body.i8579 ], [ %t5183.2.lcssa.lcssa, %for.cond.cleanup5200 ]
  %.capture_expr..casted5225.sroa.0.0.insert.ext = zext i32 %tms5190.08870 to i64
  br i1 %cmp52048861, label %for.cond.cleanup5200, label %for.cond5203.preheader

for.cond5203.preheader:                           ; preds = %for.cond5197.preheader, %for.cond.cleanup5206
  %ths5196.08866 = phi i32 [ %mul5242, %for.cond.cleanup5206 ], [ 32, %for.cond5197.preheader ]
  %t5183.18865 = phi i32 [ %t5183.2.lcssa, %for.cond.cleanup5206 ], [ %t5183.08869, %for.cond5197.preheader ]
  %.capture_expr..casted5227.sroa.0.0.insert.ext = zext i32 %ths5196.08866 to i64
  br i1 %cmp52108857, label %for.cond5209.preheader.us, label %for.cond5209.preheader

for.cond5209.preheader.us:                        ; preds = %for.cond5203.preheader, %for.cond5209.preheader.us
  %dssch5202.08863.us = phi i32 [ %mul5238.us, %for.cond5209.preheader.us ], [ 128, %for.cond5203.preheader ]
  %mul5238.us = mul nsw i32 %dssch5202.08863.us, 3000
  %6161 = zext i32 %mul5238.us to i64
  %cmp5204.us = icmp ult i64 %indvars.iv9481, %6161
  br i1 %cmp5204.us, label %for.cond.cleanup5206, label %for.cond5209.preheader.us

for.cond.cleanup5200:                             ; preds = %for.cond.cleanup5206, %for.cond5197.preheader
  %t5183.2.lcssa.lcssa = phi i32 [ %t5183.08869, %for.cond5197.preheader ], [ %t5183.2.lcssa, %for.cond.cleanup5206 ]
  %mul5246 = shl nsw i32 %tms5190.08870, 1
  %cmp5192 = icmp ult i32 %mul5246, 257
  br i1 %cmp5192, label %for.cond5197.preheader, label %for.cond5250.preheader

for.cond5209.preheader:                           ; preds = %for.cond5203.preheader, %for.cond.cleanup5212
  %dssch5202.08863 = phi i32 [ %mul5238, %for.cond.cleanup5212 ], [ 128, %for.cond5203.preheader ]
  %t5183.28862 = phi i32 [ %inc5214, %for.cond.cleanup5212 ], [ %t5183.18865, %for.cond5203.preheader ]
  %.capture_expr..casted5221.sroa.0.0.insert.ext = zext i32 %dssch5202.08863 to i64
  br label %for.body5213

for.cond.cleanup5206:                             ; preds = %for.cond.cleanup5212, %for.cond5209.preheader.us
  %t5183.2.lcssa = phi i32 [ %t5183.18865, %for.cond5209.preheader.us ], [ %inc5214, %for.cond.cleanup5212 ]
  %mul5242 = shl nsw i32 %ths5196.08866, 1
  %cmp5198 = icmp ult i32 %mul5242, 1025
  br i1 %cmp5198, label %for.cond5203.preheader, label %for.cond.cleanup5200

for.cond.cleanup5212:                             ; preds = %omp_offload.cont5232
  %mul5238 = mul nsw i32 %dssch5202.08863, 3000
  %6162 = zext i32 %mul5238 to i64
  %cmp5204 = icmp ult i64 %indvars.iv9481, %6162
  br i1 %cmp5204, label %for.cond.cleanup5206, label %for.cond5209.preheader

for.body5213:                                     ; preds = %for.cond5209.preheader, %omp_offload.cont5232
  %sch5208.08859 = phi i32 [ 1000, %for.cond5209.preheader ], [ %mul5234, %omp_offload.cont5232 ]
  %t5183.38858 = phi i32 [ %t5183.28862, %for.cond5209.preheader ], [ %inc5214, %omp_offload.cont5232 ]
  %inc5214 = add nsw i32 %t5183.38858, 1
  %.capture_expr..casted5223.sroa.0.0.insert.ext = zext i32 %sch5208.08859 to i64
  store i64 %indvars.iv9481, i64* %5941, align 8
  store i64 %indvars.iv9481, i64* %5943, align 8
  store i8* %call3733, i8** %5944, align 8
  store i8* %call3733, i8** %5945, align 8
  store i8* %call3735, i8** %5946, align 8
  store i8* %call3735, i8** %5947, align 8
  store i8* %call3736, i8** %5948, align 8
  store i8* %call3736, i8** %5949, align 8
  store i64 %.capture_expr..casted5221.sroa.0.0.insert.ext, i64* %5951, align 8
  store i64 %.capture_expr..casted5221.sroa.0.0.insert.ext, i64* %5953, align 8
  store i64 %.capture_expr..casted5223.sroa.0.0.insert.ext, i64* %5955, align 8
  store i64 %.capture_expr..casted5223.sroa.0.0.insert.ext, i64* %5957, align 8
  store i64 %.capture_expr..casted5225.sroa.0.0.insert.ext, i64* %5959, align 8
  store i64 %.capture_expr..casted5225.sroa.0.0.insert.ext, i64* %5961, align 8
  store i64 %.capture_expr..casted5227.sroa.0.0.insert.ext, i64* %5963, align 8
  store i64 %.capture_expr..casted5227.sroa.0.0.insert.ext, i64* %5965, align 8
  %6163 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1465.region_id, i32 8, i8** nonnull %5940, i8** nonnull %5942, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.259, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.260, i64 0, i64 0), i32 %tms5190.08870, i32 %ths5196.08866) #6
  %6164 = icmp eq i32 %6163, 0
  br i1 %6164, label %omp_offload.cont5232, label %omp_offload.failed5231

omp_offload.failed5231:                           ; preds = %for.body5213
  %6165 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %6166 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %6165, i32 %tms5190.08870, i32 %ths5196.08866) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 6, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64, i64)* @.omp_outlined..243 to void (i32*, i32*, ...)*), i64 %indvars.iv9481, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted5221.sroa.0.0.insert.ext, i64 %.capture_expr..casted5223.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont5232

omp_offload.cont5232:                             ; preds = %for.body5213, %omp_offload.failed5231
  %mul5234 = mul nsw i32 %sch5208.08859, 3000
  %6167 = zext i32 %mul5234 to i64
  %cmp5210 = icmp ult i64 %indvars.iv9481, %6167
  br i1 %cmp5210, label %for.cond.cleanup5212, label %for.body5213

for.cond5256.preheader:                           ; preds = %for.cond5256.preheader.preheader37, %for.cond.cleanup5259
  %times5249.08873 = phi i32 [ %inc5274, %for.cond.cleanup5259 ], [ 0, %for.cond5256.preheader.preheader37 ]
  br label %vector.body11254

vector.body11254:                                 ; preds = %for.cond5256.preheader, %vector.body11254
  %index11263 = phi i64 [ %index.next11264, %vector.body11254 ], [ 0, %for.cond5256.preheader ]
  %6168 = getelementptr inbounds double, double* %1796, i64 %index11263
  %6169 = bitcast double* %6168 to <2 x double>*
  %wide.load11281 = load <2 x double>, <2 x double>* %6169, align 8, !tbaa !62
  %6170 = getelementptr inbounds double, double* %6168, i64 2
  %6171 = bitcast double* %6170 to <2 x double>*
  %wide.load11282 = load <2 x double>, <2 x double>* %6171, align 8, !tbaa !62
  %6172 = getelementptr inbounds double, double* %6168, i64 4
  %6173 = bitcast double* %6172 to <2 x double>*
  %wide.load11283 = load <2 x double>, <2 x double>* %6173, align 8, !tbaa !62
  %6174 = getelementptr inbounds double, double* %6168, i64 6
  %6175 = bitcast double* %6174 to <2 x double>*
  %wide.load11284 = load <2 x double>, <2 x double>* %6175, align 8, !tbaa !62
  %6176 = getelementptr inbounds double, double* %6168, i64 8
  %6177 = bitcast double* %6176 to <2 x double>*
  %wide.load11285 = load <2 x double>, <2 x double>* %6177, align 8, !tbaa !62
  %6178 = getelementptr inbounds double, double* %6168, i64 10
  %6179 = bitcast double* %6178 to <2 x double>*
  %wide.load11286 = load <2 x double>, <2 x double>* %6179, align 8, !tbaa !62
  %6180 = getelementptr inbounds double, double* %6168, i64 12
  %6181 = bitcast double* %6180 to <2 x double>*
  %wide.load11287 = load <2 x double>, <2 x double>* %6181, align 8, !tbaa !62
  %6182 = getelementptr inbounds double, double* %6168, i64 14
  %6183 = bitcast double* %6182 to <2 x double>*
  %wide.load11288 = load <2 x double>, <2 x double>* %6183, align 8, !tbaa !62
  %6184 = getelementptr inbounds double, double* %6168, i64 16
  %6185 = bitcast double* %6184 to <2 x double>*
  %wide.load11289 = load <2 x double>, <2 x double>* %6185, align 8, !tbaa !62
  %6186 = getelementptr inbounds double, double* %6168, i64 18
  %6187 = bitcast double* %6186 to <2 x double>*
  %wide.load11290 = load <2 x double>, <2 x double>* %6187, align 8, !tbaa !62
  %6188 = getelementptr inbounds double, double* %6168, i64 20
  %6189 = bitcast double* %6188 to <2 x double>*
  %wide.load11291 = load <2 x double>, <2 x double>* %6189, align 8, !tbaa !62
  %6190 = getelementptr inbounds double, double* %6168, i64 22
  %6191 = bitcast double* %6190 to <2 x double>*
  %wide.load11292 = load <2 x double>, <2 x double>* %6191, align 8, !tbaa !62
  %6192 = getelementptr inbounds double, double* %1797, i64 %index11263
  %6193 = bitcast double* %6192 to <2 x double>*
  %wide.load11293 = load <2 x double>, <2 x double>* %6193, align 8, !tbaa !62
  %6194 = getelementptr inbounds double, double* %6192, i64 2
  %6195 = bitcast double* %6194 to <2 x double>*
  %wide.load11294 = load <2 x double>, <2 x double>* %6195, align 8, !tbaa !62
  %6196 = getelementptr inbounds double, double* %6192, i64 4
  %6197 = bitcast double* %6196 to <2 x double>*
  %wide.load11295 = load <2 x double>, <2 x double>* %6197, align 8, !tbaa !62
  %6198 = getelementptr inbounds double, double* %6192, i64 6
  %6199 = bitcast double* %6198 to <2 x double>*
  %wide.load11296 = load <2 x double>, <2 x double>* %6199, align 8, !tbaa !62
  %6200 = getelementptr inbounds double, double* %6192, i64 8
  %6201 = bitcast double* %6200 to <2 x double>*
  %wide.load11297 = load <2 x double>, <2 x double>* %6201, align 8, !tbaa !62
  %6202 = getelementptr inbounds double, double* %6192, i64 10
  %6203 = bitcast double* %6202 to <2 x double>*
  %wide.load11298 = load <2 x double>, <2 x double>* %6203, align 8, !tbaa !62
  %6204 = getelementptr inbounds double, double* %6192, i64 12
  %6205 = bitcast double* %6204 to <2 x double>*
  %wide.load11299 = load <2 x double>, <2 x double>* %6205, align 8, !tbaa !62
  %6206 = getelementptr inbounds double, double* %6192, i64 14
  %6207 = bitcast double* %6206 to <2 x double>*
  %wide.load11300 = load <2 x double>, <2 x double>* %6207, align 8, !tbaa !62
  %6208 = getelementptr inbounds double, double* %6192, i64 16
  %6209 = bitcast double* %6208 to <2 x double>*
  %wide.load11301 = load <2 x double>, <2 x double>* %6209, align 8, !tbaa !62
  %6210 = getelementptr inbounds double, double* %6192, i64 18
  %6211 = bitcast double* %6210 to <2 x double>*
  %wide.load11302 = load <2 x double>, <2 x double>* %6211, align 8, !tbaa !62
  %6212 = getelementptr inbounds double, double* %6192, i64 20
  %6213 = bitcast double* %6212 to <2 x double>*
  %wide.load11303 = load <2 x double>, <2 x double>* %6213, align 8, !tbaa !62
  %6214 = getelementptr inbounds double, double* %6192, i64 22
  %6215 = bitcast double* %6214 to <2 x double>*
  %wide.load11304 = load <2 x double>, <2 x double>* %6215, align 8, !tbaa !62
  %6216 = fadd <2 x double> %wide.load11281, %wide.load11293
  %6217 = fadd <2 x double> %wide.load11282, %wide.load11294
  %6218 = fadd <2 x double> %wide.load11283, %wide.load11295
  %6219 = fadd <2 x double> %wide.load11284, %wide.load11296
  %6220 = fadd <2 x double> %wide.load11285, %wide.load11297
  %6221 = fadd <2 x double> %wide.load11286, %wide.load11298
  %6222 = fadd <2 x double> %wide.load11287, %wide.load11299
  %6223 = fadd <2 x double> %wide.load11288, %wide.load11300
  %6224 = fadd <2 x double> %wide.load11289, %wide.load11301
  %6225 = fadd <2 x double> %wide.load11290, %wide.load11302
  %6226 = fadd <2 x double> %wide.load11291, %wide.load11303
  %6227 = fadd <2 x double> %wide.load11292, %wide.load11304
  %6228 = getelementptr inbounds double, double* %1795, i64 %index11263
  %6229 = bitcast double* %6228 to <2 x double>*
  %wide.load11305 = load <2 x double>, <2 x double>* %6229, align 8, !tbaa !62
  %6230 = getelementptr inbounds double, double* %6228, i64 2
  %6231 = bitcast double* %6230 to <2 x double>*
  %wide.load11306 = load <2 x double>, <2 x double>* %6231, align 8, !tbaa !62
  %6232 = getelementptr inbounds double, double* %6228, i64 4
  %6233 = bitcast double* %6232 to <2 x double>*
  %wide.load11307 = load <2 x double>, <2 x double>* %6233, align 8, !tbaa !62
  %6234 = getelementptr inbounds double, double* %6228, i64 6
  %6235 = bitcast double* %6234 to <2 x double>*
  %wide.load11308 = load <2 x double>, <2 x double>* %6235, align 8, !tbaa !62
  %6236 = getelementptr inbounds double, double* %6228, i64 8
  %6237 = bitcast double* %6236 to <2 x double>*
  %wide.load11309 = load <2 x double>, <2 x double>* %6237, align 8, !tbaa !62
  %6238 = getelementptr inbounds double, double* %6228, i64 10
  %6239 = bitcast double* %6238 to <2 x double>*
  %wide.load11310 = load <2 x double>, <2 x double>* %6239, align 8, !tbaa !62
  %6240 = getelementptr inbounds double, double* %6228, i64 12
  %6241 = bitcast double* %6240 to <2 x double>*
  %wide.load11311 = load <2 x double>, <2 x double>* %6241, align 8, !tbaa !62
  %6242 = getelementptr inbounds double, double* %6228, i64 14
  %6243 = bitcast double* %6242 to <2 x double>*
  %wide.load11312 = load <2 x double>, <2 x double>* %6243, align 8, !tbaa !62
  %6244 = getelementptr inbounds double, double* %6228, i64 16
  %6245 = bitcast double* %6244 to <2 x double>*
  %wide.load11313 = load <2 x double>, <2 x double>* %6245, align 8, !tbaa !62
  %6246 = getelementptr inbounds double, double* %6228, i64 18
  %6247 = bitcast double* %6246 to <2 x double>*
  %wide.load11314 = load <2 x double>, <2 x double>* %6247, align 8, !tbaa !62
  %6248 = getelementptr inbounds double, double* %6228, i64 20
  %6249 = bitcast double* %6248 to <2 x double>*
  %wide.load11315 = load <2 x double>, <2 x double>* %6249, align 8, !tbaa !62
  %6250 = getelementptr inbounds double, double* %6228, i64 22
  %6251 = bitcast double* %6250 to <2 x double>*
  %wide.load11316 = load <2 x double>, <2 x double>* %6251, align 8, !tbaa !62
  %6252 = fadd <2 x double> %6216, %wide.load11305
  %6253 = fadd <2 x double> %6217, %wide.load11306
  %6254 = fadd <2 x double> %6218, %wide.load11307
  %6255 = fadd <2 x double> %6219, %wide.load11308
  %6256 = fadd <2 x double> %6220, %wide.load11309
  %6257 = fadd <2 x double> %6221, %wide.load11310
  %6258 = fadd <2 x double> %6222, %wide.load11311
  %6259 = fadd <2 x double> %6223, %wide.load11312
  %6260 = fadd <2 x double> %6224, %wide.load11313
  %6261 = fadd <2 x double> %6225, %wide.load11314
  %6262 = fadd <2 x double> %6226, %wide.load11315
  %6263 = fadd <2 x double> %6227, %wide.load11316
  store <2 x double> %6252, <2 x double>* %6229, align 8, !tbaa !62
  store <2 x double> %6253, <2 x double>* %6231, align 8, !tbaa !62
  store <2 x double> %6254, <2 x double>* %6233, align 8, !tbaa !62
  store <2 x double> %6255, <2 x double>* %6235, align 8, !tbaa !62
  store <2 x double> %6256, <2 x double>* %6237, align 8, !tbaa !62
  store <2 x double> %6257, <2 x double>* %6239, align 8, !tbaa !62
  store <2 x double> %6258, <2 x double>* %6241, align 8, !tbaa !62
  store <2 x double> %6259, <2 x double>* %6243, align 8, !tbaa !62
  store <2 x double> %6260, <2 x double>* %6245, align 8, !tbaa !62
  store <2 x double> %6261, <2 x double>* %6247, align 8, !tbaa !62
  store <2 x double> %6262, <2 x double>* %6249, align 8, !tbaa !62
  store <2 x double> %6263, <2 x double>* %6251, align 8, !tbaa !62
  %index.next11264 = add nuw nsw i64 %index11263, 24
  %6264 = icmp eq i64 %index.next11264, %n.vec11262
  br i1 %6264, label %middle.block11255, label %vector.body11254, !llvm.loop !117

middle.block11255:                                ; preds = %vector.body11254
  br i1 %cmp.n11266, label %for.cond.cleanup5259, label %for.body5260

for.cond.cleanup5253:                             ; preds = %for.cond.cleanup5259, %for.cond5250.preheader
  store i8* %call3733, i8** %5966, align 8
  store i8* %call3733, i8** %5967, align 8
  store i64 %6159, i64* %5968, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %5966, i8** nonnull %5967, i64* nonnull %5968, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body5286

for.cond.cleanup5259:                             ; preds = %for.body5260, %middle.block11255
  %inc5274 = add nuw nsw i32 %times5249.08873, 1
  %exitcond9478 = icmp eq i32 %inc5274, %t5183.2.lcssa.lcssa
  br i1 %exitcond9478, label %for.cond.cleanup5253, label %for.cond5256.preheader

for.body5260:                                     ; preds = %middle.block11255, %for.body5260
  %indvars.iv9475 = phi i64 [ %indvars.iv.next9476, %for.body5260 ], [ %n.vec11262, %middle.block11255 ]
  %arrayidx5262 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9475
  %6265 = load double, double* %arrayidx5262, align 8, !tbaa !62
  %arrayidx5264 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9475
  %6266 = load double, double* %arrayidx5264, align 8, !tbaa !62
  %add5265 = fadd double %6265, %6266
  %arrayidx5267 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9475
  %6267 = load double, double* %arrayidx5267, align 8, !tbaa !62
  %add5268 = fadd double %6267, %add5265
  store double %add5268, double* %arrayidx5267, align 8, !tbaa !62
  %indvars.iv.next9476 = add nuw nsw i64 %indvars.iv9475, 1
  %exitcond9477 = icmp eq i64 %indvars.iv.next9476, %indvars.iv9481
  br i1 %exitcond9477, label %for.cond.cleanup5259, label %for.body5260, !llvm.loop !118

for.body5286:                                     ; preds = %for.inc5300.7, %for.cond.cleanup5253
  %indvars.iv9479 = phi i64 [ 0, %for.cond.cleanup5253 ], [ %indvars.iv.next9480.7, %for.inc5300.7 ]
  %arrayidx5288 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9479
  %6268 = load double, double* %arrayidx5288, align 8, !tbaa !62
  %arrayidx5290 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9479
  %6269 = load double, double* %arrayidx5290, align 8, !tbaa !62
  %cmp5291 = fcmp une double %6268, %6269
  br i1 %cmp5291, label %cleanup5310, label %for.inc5300

for.inc5300:                                      ; preds = %for.body5286
  %indvars.iv.next9480 = or i64 %indvars.iv9479, 1
  %arrayidx5288.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9480
  %6270 = load double, double* %arrayidx5288.1, align 8, !tbaa !62
  %arrayidx5290.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9480
  %6271 = load double, double* %arrayidx5290.1, align 8, !tbaa !62
  %cmp5291.1 = fcmp une double %6270, %6271
  br i1 %cmp5291.1, label %cleanup5310, label %for.inc5300.1

for.inc5308:                                      ; preds = %for.inc5300.7
  %indvars.iv.next9482 = add nuw nsw i64 %indvars.iv9481, 5000
  %cmp5179 = icmp ult i64 %indvars.iv.next9482, 25000
  %indvar.next11258 = add nuw nsw i64 %indvar11257, 1
  br i1 %cmp5179, label %for.body.i8579.preheader, label %for.end5312

cleanup5310:                                      ; preds = %for.inc5300.6, %for.inc5300.5, %for.inc5300.4, %for.inc5300.3, %for.inc5300.2, %for.inc5300.1, %for.inc5300, %for.body5286
  %indvars.iv9479.lcssa = phi i64 [ %indvars.iv9479, %for.body5286 ], [ %indvars.iv.next9480, %for.inc5300 ], [ %indvars.iv.next9480.1, %for.inc5300.1 ], [ %indvars.iv.next9480.2, %for.inc5300.2 ], [ %indvars.iv.next9480.3, %for.inc5300.3 ], [ %indvars.iv.next9480.4, %for.inc5300.4 ], [ %indvars.iv.next9480.5, %for.inc5300.5 ], [ %indvars.iv.next9480.6, %for.inc5300.6 ]
  %.lcssa11792 = phi double [ %6268, %for.body5286 ], [ %6270, %for.inc5300 ], [ %7776, %for.inc5300.1 ], [ %7778, %for.inc5300.2 ], [ %7780, %for.inc5300.3 ], [ %7782, %for.inc5300.4 ], [ %7784, %for.inc5300.5 ], [ %7786, %for.inc5300.6 ]
  %.lcssa11790 = phi double [ %6269, %for.body5286 ], [ %6271, %for.inc5300 ], [ %7777, %for.inc5300.1 ], [ %7779, %for.inc5300.2 ], [ %7781, %for.inc5300.3 ], [ %7783, %for.inc5300.4 ], [ %7785, %for.inc5300.5 ], [ %7787, %for.inc5300.6 ]
  %6272 = trunc i64 %indvars.iv9481 to i32
  %6273 = trunc i64 %indvars.iv9479.lcssa to i32
  %call5298 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %6272, i32 signext %6273, double %.lcssa11792, double %.lcssa11790)
  br label %cleanup5832

for.end5312:                                      ; preds = %for.inc5308
  %puts8269 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8270 = call i32 @puts(i8* getelementptr inbounds ([53 x i8], [53 x i8]* @str.359, i64 0, i64 0))
  %6274 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5325, i64 0, i64 0
  %6275 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5326, i64 0, i64 0
  %6276 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5327, i64 0, i64 0
  %6277 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5325, i64 0, i64 1
  %6278 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5326, i64 0, i64 1
  %6279 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5327, i64 0, i64 1
  %6280 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5325, i64 0, i64 2
  %6281 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5326, i64 0, i64 2
  %6282 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5327, i64 0, i64 2
  %6283 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5358, i64 0, i64 0
  %6284 = bitcast [7 x i8*]* %.offload_baseptrs5358 to i64*
  %6285 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5359, i64 0, i64 0
  %6286 = bitcast [7 x i8*]* %.offload_ptrs5359 to i64*
  %6287 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5358, i64 0, i64 1
  %6288 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5359, i64 0, i64 1
  %6289 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5358, i64 0, i64 2
  %6290 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5359, i64 0, i64 2
  %6291 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5358, i64 0, i64 3
  %6292 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5359, i64 0, i64 3
  %6293 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5358, i64 0, i64 4
  %6294 = bitcast i8** %6293 to i64*
  %6295 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5359, i64 0, i64 4
  %6296 = bitcast i8** %6295 to i64*
  %6297 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5358, i64 0, i64 5
  %6298 = bitcast i8** %6297 to i64*
  %6299 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5359, i64 0, i64 5
  %6300 = bitcast i8** %6299 to i64*
  %6301 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5358, i64 0, i64 6
  %6302 = bitcast i8** %6301 to i64*
  %6303 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5359, i64 0, i64 6
  %6304 = bitcast i8** %6303 to i64*
  %6305 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs5403, i64 0, i64 0
  %6306 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs5404, i64 0, i64 0
  %6307 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes5405, i64 0, i64 0
  %6308 = bitcast i8* %arrayidx.i to <2 x double>*
  %6309 = bitcast i8* %arrayidx2.i to <2 x double>*
  %6310 = bitcast i8* %arrayidx5.i to <2 x double>*
  %6311 = bitcast i8* %arrayidx8.i to <2 x double>*
  %6312 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %6313 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %6314 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %6315 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %6316 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %6317 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %6318 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %6319 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %6320 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %6321 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %6322 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %6323 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %6324 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %6325 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %6326 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %6327 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %6328 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %6329 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %6330 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %6331 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %6332 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %6333 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %6334 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %6335 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %6336 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %6337 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %6338 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %6339 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8595.preheader

for.body.i8595.preheader:                         ; preds = %for.end5312, %for.inc5433
  %indvar11356 = phi i64 [ 0, %for.end5312 ], [ %indvar.next11357, %for.inc5433 ]
  %indvars.iv9473 = phi i64 [ 32, %for.end5312 ], [ %indvars.iv.next9474, %for.inc5433 ]
  %6340 = mul nuw nsw i64 %indvar11356, 5000
  br label %vector.body11416

vector.body11416:                                 ; preds = %vector.body11416, %for.body.i8595.preheader
  %index11420 = phi i64 [ 0, %for.body.i8595.preheader ], [ %index.next11421, %vector.body11416 ]
  %vec.ind11438 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8595.preheader ], [ %vec.ind.next11451, %vector.body11416 ]
  %step.add11439 = add <2 x i32> %vec.ind11438, <i32 2, i32 2>
  %step.add11440 = add <2 x i32> %vec.ind11438, <i32 4, i32 4>
  %step.add11441 = add <2 x i32> %vec.ind11438, <i32 6, i32 6>
  %step.add11442 = add <2 x i32> %vec.ind11438, <i32 8, i32 8>
  %step.add11443 = add <2 x i32> %vec.ind11438, <i32 10, i32 10>
  %step.add11444 = add <2 x i32> %vec.ind11438, <i32 12, i32 12>
  %step.add11445 = add <2 x i32> %vec.ind11438, <i32 14, i32 14>
  %step.add11446 = add <2 x i32> %vec.ind11438, <i32 16, i32 16>
  %step.add11447 = add <2 x i32> %vec.ind11438, <i32 18, i32 18>
  %step.add11448 = add <2 x i32> %vec.ind11438, <i32 20, i32 20>
  %step.add11449 = add <2 x i32> %vec.ind11438, <i32 22, i32 22>
  %6341 = sitofp <2 x i32> %vec.ind11438 to <2 x double>
  %6342 = sitofp <2 x i32> %step.add11439 to <2 x double>
  %6343 = sitofp <2 x i32> %step.add11440 to <2 x double>
  %6344 = sitofp <2 x i32> %step.add11441 to <2 x double>
  %6345 = sitofp <2 x i32> %step.add11442 to <2 x double>
  %6346 = sitofp <2 x i32> %step.add11443 to <2 x double>
  %6347 = sitofp <2 x i32> %step.add11444 to <2 x double>
  %6348 = sitofp <2 x i32> %step.add11445 to <2 x double>
  %6349 = sitofp <2 x i32> %step.add11446 to <2 x double>
  %6350 = sitofp <2 x i32> %step.add11447 to <2 x double>
  %6351 = sitofp <2 x i32> %step.add11448 to <2 x double>
  %6352 = sitofp <2 x i32> %step.add11449 to <2 x double>
  %6353 = getelementptr inbounds double, double* %1795, i64 %index11420
  %6354 = bitcast double* %6353 to <2 x double>*
  store <2 x double> %6341, <2 x double>* %6354, align 8, !tbaa !62
  %6355 = getelementptr inbounds double, double* %6353, i64 2
  %6356 = bitcast double* %6355 to <2 x double>*
  store <2 x double> %6342, <2 x double>* %6356, align 8, !tbaa !62
  %6357 = getelementptr inbounds double, double* %6353, i64 4
  %6358 = bitcast double* %6357 to <2 x double>*
  store <2 x double> %6343, <2 x double>* %6358, align 8, !tbaa !62
  %6359 = getelementptr inbounds double, double* %6353, i64 6
  %6360 = bitcast double* %6359 to <2 x double>*
  store <2 x double> %6344, <2 x double>* %6360, align 8, !tbaa !62
  %6361 = getelementptr inbounds double, double* %6353, i64 8
  %6362 = bitcast double* %6361 to <2 x double>*
  store <2 x double> %6345, <2 x double>* %6362, align 8, !tbaa !62
  %6363 = getelementptr inbounds double, double* %6353, i64 10
  %6364 = bitcast double* %6363 to <2 x double>*
  store <2 x double> %6346, <2 x double>* %6364, align 8, !tbaa !62
  %6365 = getelementptr inbounds double, double* %6353, i64 12
  %6366 = bitcast double* %6365 to <2 x double>*
  store <2 x double> %6347, <2 x double>* %6366, align 8, !tbaa !62
  %6367 = getelementptr inbounds double, double* %6353, i64 14
  %6368 = bitcast double* %6367 to <2 x double>*
  store <2 x double> %6348, <2 x double>* %6368, align 8, !tbaa !62
  %6369 = getelementptr inbounds double, double* %6353, i64 16
  %6370 = bitcast double* %6369 to <2 x double>*
  store <2 x double> %6349, <2 x double>* %6370, align 8, !tbaa !62
  %6371 = getelementptr inbounds double, double* %6353, i64 18
  %6372 = bitcast double* %6371 to <2 x double>*
  store <2 x double> %6350, <2 x double>* %6372, align 8, !tbaa !62
  %6373 = getelementptr inbounds double, double* %6353, i64 20
  %6374 = bitcast double* %6373 to <2 x double>*
  store <2 x double> %6351, <2 x double>* %6374, align 8, !tbaa !62
  %6375 = getelementptr inbounds double, double* %6353, i64 22
  %6376 = bitcast double* %6375 to <2 x double>*
  store <2 x double> %6352, <2 x double>* %6376, align 8, !tbaa !62
  %6377 = getelementptr inbounds double, double* %1794, i64 %index11420
  %6378 = bitcast double* %6377 to <2 x double>*
  store <2 x double> %6341, <2 x double>* %6378, align 8, !tbaa !62
  %6379 = getelementptr inbounds double, double* %6377, i64 2
  %6380 = bitcast double* %6379 to <2 x double>*
  store <2 x double> %6342, <2 x double>* %6380, align 8, !tbaa !62
  %6381 = getelementptr inbounds double, double* %6377, i64 4
  %6382 = bitcast double* %6381 to <2 x double>*
  store <2 x double> %6343, <2 x double>* %6382, align 8, !tbaa !62
  %6383 = getelementptr inbounds double, double* %6377, i64 6
  %6384 = bitcast double* %6383 to <2 x double>*
  store <2 x double> %6344, <2 x double>* %6384, align 8, !tbaa !62
  %6385 = getelementptr inbounds double, double* %6377, i64 8
  %6386 = bitcast double* %6385 to <2 x double>*
  store <2 x double> %6345, <2 x double>* %6386, align 8, !tbaa !62
  %6387 = getelementptr inbounds double, double* %6377, i64 10
  %6388 = bitcast double* %6387 to <2 x double>*
  store <2 x double> %6346, <2 x double>* %6388, align 8, !tbaa !62
  %6389 = getelementptr inbounds double, double* %6377, i64 12
  %6390 = bitcast double* %6389 to <2 x double>*
  store <2 x double> %6347, <2 x double>* %6390, align 8, !tbaa !62
  %6391 = getelementptr inbounds double, double* %6377, i64 14
  %6392 = bitcast double* %6391 to <2 x double>*
  store <2 x double> %6348, <2 x double>* %6392, align 8, !tbaa !62
  %6393 = getelementptr inbounds double, double* %6377, i64 16
  %6394 = bitcast double* %6393 to <2 x double>*
  store <2 x double> %6349, <2 x double>* %6394, align 8, !tbaa !62
  %6395 = getelementptr inbounds double, double* %6377, i64 18
  %6396 = bitcast double* %6395 to <2 x double>*
  store <2 x double> %6350, <2 x double>* %6396, align 8, !tbaa !62
  %6397 = getelementptr inbounds double, double* %6377, i64 20
  %6398 = bitcast double* %6397 to <2 x double>*
  store <2 x double> %6351, <2 x double>* %6398, align 8, !tbaa !62
  %6399 = getelementptr inbounds double, double* %6377, i64 22
  %6400 = bitcast double* %6399 to <2 x double>*
  store <2 x double> %6352, <2 x double>* %6400, align 8, !tbaa !62
  %6401 = shl <2 x i32> %vec.ind11438, <i32 1, i32 1>
  %6402 = shl <2 x i32> %step.add11439, <i32 1, i32 1>
  %6403 = shl <2 x i32> %step.add11440, <i32 1, i32 1>
  %6404 = shl <2 x i32> %step.add11441, <i32 1, i32 1>
  %6405 = shl <2 x i32> %step.add11442, <i32 1, i32 1>
  %6406 = shl <2 x i32> %step.add11443, <i32 1, i32 1>
  %6407 = shl <2 x i32> %step.add11444, <i32 1, i32 1>
  %6408 = shl <2 x i32> %step.add11445, <i32 1, i32 1>
  %6409 = shl <2 x i32> %step.add11446, <i32 1, i32 1>
  %6410 = shl <2 x i32> %step.add11447, <i32 1, i32 1>
  %6411 = shl <2 x i32> %step.add11448, <i32 1, i32 1>
  %6412 = shl <2 x i32> %step.add11449, <i32 1, i32 1>
  %6413 = sitofp <2 x i32> %6401 to <2 x double>
  %6414 = sitofp <2 x i32> %6402 to <2 x double>
  %6415 = sitofp <2 x i32> %6403 to <2 x double>
  %6416 = sitofp <2 x i32> %6404 to <2 x double>
  %6417 = sitofp <2 x i32> %6405 to <2 x double>
  %6418 = sitofp <2 x i32> %6406 to <2 x double>
  %6419 = sitofp <2 x i32> %6407 to <2 x double>
  %6420 = sitofp <2 x i32> %6408 to <2 x double>
  %6421 = sitofp <2 x i32> %6409 to <2 x double>
  %6422 = sitofp <2 x i32> %6410 to <2 x double>
  %6423 = sitofp <2 x i32> %6411 to <2 x double>
  %6424 = sitofp <2 x i32> %6412 to <2 x double>
  %6425 = getelementptr inbounds double, double* %1796, i64 %index11420
  %6426 = bitcast double* %6425 to <2 x double>*
  store <2 x double> %6413, <2 x double>* %6426, align 8, !tbaa !62
  %6427 = getelementptr inbounds double, double* %6425, i64 2
  %6428 = bitcast double* %6427 to <2 x double>*
  store <2 x double> %6414, <2 x double>* %6428, align 8, !tbaa !62
  %6429 = getelementptr inbounds double, double* %6425, i64 4
  %6430 = bitcast double* %6429 to <2 x double>*
  store <2 x double> %6415, <2 x double>* %6430, align 8, !tbaa !62
  %6431 = getelementptr inbounds double, double* %6425, i64 6
  %6432 = bitcast double* %6431 to <2 x double>*
  store <2 x double> %6416, <2 x double>* %6432, align 8, !tbaa !62
  %6433 = getelementptr inbounds double, double* %6425, i64 8
  %6434 = bitcast double* %6433 to <2 x double>*
  store <2 x double> %6417, <2 x double>* %6434, align 8, !tbaa !62
  %6435 = getelementptr inbounds double, double* %6425, i64 10
  %6436 = bitcast double* %6435 to <2 x double>*
  store <2 x double> %6418, <2 x double>* %6436, align 8, !tbaa !62
  %6437 = getelementptr inbounds double, double* %6425, i64 12
  %6438 = bitcast double* %6437 to <2 x double>*
  store <2 x double> %6419, <2 x double>* %6438, align 8, !tbaa !62
  %6439 = getelementptr inbounds double, double* %6425, i64 14
  %6440 = bitcast double* %6439 to <2 x double>*
  store <2 x double> %6420, <2 x double>* %6440, align 8, !tbaa !62
  %6441 = getelementptr inbounds double, double* %6425, i64 16
  %6442 = bitcast double* %6441 to <2 x double>*
  store <2 x double> %6421, <2 x double>* %6442, align 8, !tbaa !62
  %6443 = getelementptr inbounds double, double* %6425, i64 18
  %6444 = bitcast double* %6443 to <2 x double>*
  store <2 x double> %6422, <2 x double>* %6444, align 8, !tbaa !62
  %6445 = getelementptr inbounds double, double* %6425, i64 20
  %6446 = bitcast double* %6445 to <2 x double>*
  store <2 x double> %6423, <2 x double>* %6446, align 8, !tbaa !62
  %6447 = getelementptr inbounds double, double* %6425, i64 22
  %6448 = bitcast double* %6447 to <2 x double>*
  store <2 x double> %6424, <2 x double>* %6448, align 8, !tbaa !62
  %6449 = add <2 x i32> %vec.ind11438, <i32 -3, i32 -3>
  %6450 = add <2 x i32> %vec.ind11438, <i32 -1, i32 -1>
  %6451 = add <2 x i32> %vec.ind11438, <i32 1, i32 1>
  %6452 = add <2 x i32> %vec.ind11438, <i32 3, i32 3>
  %6453 = add <2 x i32> %vec.ind11438, <i32 5, i32 5>
  %6454 = add <2 x i32> %vec.ind11438, <i32 7, i32 7>
  %6455 = add <2 x i32> %vec.ind11438, <i32 9, i32 9>
  %6456 = add <2 x i32> %vec.ind11438, <i32 11, i32 11>
  %6457 = add <2 x i32> %vec.ind11438, <i32 13, i32 13>
  %6458 = add <2 x i32> %vec.ind11438, <i32 15, i32 15>
  %6459 = add <2 x i32> %vec.ind11438, <i32 17, i32 17>
  %6460 = add <2 x i32> %vec.ind11438, <i32 19, i32 19>
  %6461 = sitofp <2 x i32> %6449 to <2 x double>
  %6462 = sitofp <2 x i32> %6450 to <2 x double>
  %6463 = sitofp <2 x i32> %6451 to <2 x double>
  %6464 = sitofp <2 x i32> %6452 to <2 x double>
  %6465 = sitofp <2 x i32> %6453 to <2 x double>
  %6466 = sitofp <2 x i32> %6454 to <2 x double>
  %6467 = sitofp <2 x i32> %6455 to <2 x double>
  %6468 = sitofp <2 x i32> %6456 to <2 x double>
  %6469 = sitofp <2 x i32> %6457 to <2 x double>
  %6470 = sitofp <2 x i32> %6458 to <2 x double>
  %6471 = sitofp <2 x i32> %6459 to <2 x double>
  %6472 = sitofp <2 x i32> %6460 to <2 x double>
  %6473 = getelementptr inbounds double, double* %1797, i64 %index11420
  %6474 = bitcast double* %6473 to <2 x double>*
  store <2 x double> %6461, <2 x double>* %6474, align 8, !tbaa !62
  %6475 = getelementptr inbounds double, double* %6473, i64 2
  %6476 = bitcast double* %6475 to <2 x double>*
  store <2 x double> %6462, <2 x double>* %6476, align 8, !tbaa !62
  %6477 = getelementptr inbounds double, double* %6473, i64 4
  %6478 = bitcast double* %6477 to <2 x double>*
  store <2 x double> %6463, <2 x double>* %6478, align 8, !tbaa !62
  %6479 = getelementptr inbounds double, double* %6473, i64 6
  %6480 = bitcast double* %6479 to <2 x double>*
  store <2 x double> %6464, <2 x double>* %6480, align 8, !tbaa !62
  %6481 = getelementptr inbounds double, double* %6473, i64 8
  %6482 = bitcast double* %6481 to <2 x double>*
  store <2 x double> %6465, <2 x double>* %6482, align 8, !tbaa !62
  %6483 = getelementptr inbounds double, double* %6473, i64 10
  %6484 = bitcast double* %6483 to <2 x double>*
  store <2 x double> %6466, <2 x double>* %6484, align 8, !tbaa !62
  %6485 = getelementptr inbounds double, double* %6473, i64 12
  %6486 = bitcast double* %6485 to <2 x double>*
  store <2 x double> %6467, <2 x double>* %6486, align 8, !tbaa !62
  %6487 = getelementptr inbounds double, double* %6473, i64 14
  %6488 = bitcast double* %6487 to <2 x double>*
  store <2 x double> %6468, <2 x double>* %6488, align 8, !tbaa !62
  %6489 = getelementptr inbounds double, double* %6473, i64 16
  %6490 = bitcast double* %6489 to <2 x double>*
  store <2 x double> %6469, <2 x double>* %6490, align 8, !tbaa !62
  %6491 = getelementptr inbounds double, double* %6473, i64 18
  %6492 = bitcast double* %6491 to <2 x double>*
  store <2 x double> %6470, <2 x double>* %6492, align 8, !tbaa !62
  %6493 = getelementptr inbounds double, double* %6473, i64 20
  %6494 = bitcast double* %6493 to <2 x double>*
  store <2 x double> %6471, <2 x double>* %6494, align 8, !tbaa !62
  %6495 = getelementptr inbounds double, double* %6473, i64 22
  %6496 = bitcast double* %6495 to <2 x double>*
  store <2 x double> %6472, <2 x double>* %6496, align 8, !tbaa !62
  %index.next11421 = add nuw nsw i64 %index11420, 24
  %vec.ind.next11451 = add <2 x i32> %vec.ind11438, <i32 24, i32 24>
  %6497 = icmp eq i64 %index.next11421, 24984
  br i1 %6497, label %for.body.i8595, label %vector.body11416, !llvm.loop !119

for.body.i8595:                                   ; preds = %vector.body11416
  %6498 = add nuw nsw i64 %6340, 32
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %6308, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %6309, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %6310, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %6311, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %6312, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %6313, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %6314, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %6315, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %6316, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %6317, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %6318, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %6319, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %6320, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %6321, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %6322, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %6323, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %6324, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %6325, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %6326, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %6327, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %6328, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %6329, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %6330, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %6331, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %6332, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %6333, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %6334, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %6335, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %6336, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %6337, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %6338, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %6339, align 8, !tbaa !62
  %6499 = shl nuw nsw i64 %indvars.iv9473, 3
  store i8* %call3733, i8** %6274, align 8
  store i8* %call3733, i8** %6275, align 8
  store i64 %6499, i64* %6276, align 8
  store i8* %call3735, i8** %6277, align 8
  store i8* %call3735, i8** %6278, align 8
  store i64 %6499, i64* %6279, align 8
  store i8* %call3736, i8** %6280, align 8
  store i8* %call3736, i8** %6281, align 8
  store i64 %6499, i64* %6282, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %6274, i8** nonnull %6275, i64* nonnull %6276, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  %cmp53428842 = icmp ult i64 %indvars.iv9473, 1000
  br i1 %cmp53428842, label %for.cond.cleanup5378, label %for.cond5335.preheader

for.cond5375.preheader:                           ; preds = %for.cond.cleanup5338
  %cmp53768853 = icmp sgt i32 %t5321.28843, -1
  br i1 %cmp53768853, label %for.cond5381.preheader.preheader32, label %for.cond.cleanup5378

for.cond5381.preheader.preheader32:               ; preds = %for.cond5375.preheader
  %n.mod.vf11360 = urem i64 %6498, 24
  %n.vec11361 = sub nuw nsw i64 %6498, %n.mod.vf11360
  %cmp.n11365 = icmp eq i64 %n.mod.vf11360, 0
  br label %for.cond5381.preheader

for.cond5335.preheader:                           ; preds = %for.body.i8595, %for.cond.cleanup5338
  %tms5328.08850 = phi i32 [ %mul5371, %for.cond.cleanup5338 ], [ 1, %for.body.i8595 ]
  %t5321.08849 = phi i32 [ %inc5346, %for.cond.cleanup5338 ], [ 0, %for.body.i8595 ]
  %.capture_expr..casted5354.sroa.0.0.insert.ext = zext i32 %tms5328.08850 to i64
  br label %for.cond5341.preheader

for.cond5341.preheader:                           ; preds = %for.cond5335.preheader, %for.cond.cleanup5344
  %ths5334.08847 = phi i32 [ %mul5367, %for.cond.cleanup5344 ], [ 32, %for.cond5335.preheader ]
  %t5321.18846 = phi i32 [ %inc5346, %for.cond.cleanup5344 ], [ %t5321.08849, %for.cond5335.preheader ]
  %.capture_expr..casted5356.sroa.0.0.insert.ext = zext i32 %ths5334.08847 to i64
  br label %for.body5345

for.cond.cleanup5338:                             ; preds = %for.cond.cleanup5344
  %mul5371 = shl nsw i32 %tms5328.08850, 1
  %cmp5330 = icmp ult i32 %mul5371, 257
  br i1 %cmp5330, label %for.cond5335.preheader, label %for.cond5375.preheader

for.cond.cleanup5344:                             ; preds = %omp_offload.cont5361
  %mul5367 = shl nsw i32 %ths5334.08847, 1
  %cmp5336 = icmp ult i32 %mul5367, 1025
  br i1 %cmp5336, label %for.cond5341.preheader, label %for.cond.cleanup5338

for.body5345:                                     ; preds = %for.cond5341.preheader, %omp_offload.cont5361
  %sch5340.08844 = phi i32 [ 1000, %for.cond5341.preheader ], [ %mul5363, %omp_offload.cont5361 ]
  %t5321.28843 = phi i32 [ %t5321.18846, %for.cond5341.preheader ], [ %inc5346, %omp_offload.cont5361 ]
  %inc5346 = add nsw i32 %t5321.28843, 1
  %.capture_expr..casted5352.sroa.0.0.insert.ext = zext i32 %sch5340.08844 to i64
  store i64 %indvars.iv9473, i64* %6284, align 8
  store i64 %indvars.iv9473, i64* %6286, align 8
  store i8* %call3733, i8** %6287, align 8
  store i8* %call3733, i8** %6288, align 8
  store i8* %call3735, i8** %6289, align 8
  store i8* %call3735, i8** %6290, align 8
  store i8* %call3736, i8** %6291, align 8
  store i8* %call3736, i8** %6292, align 8
  store i64 %.capture_expr..casted5352.sroa.0.0.insert.ext, i64* %6294, align 8
  store i64 %.capture_expr..casted5352.sroa.0.0.insert.ext, i64* %6296, align 8
  store i64 %.capture_expr..casted5354.sroa.0.0.insert.ext, i64* %6298, align 8
  store i64 %.capture_expr..casted5354.sroa.0.0.insert.ext, i64* %6300, align 8
  store i64 %.capture_expr..casted5356.sroa.0.0.insert.ext, i64* %6302, align 8
  store i64 %.capture_expr..casted5356.sroa.0.0.insert.ext, i64* %6304, align 8
  %6500 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1502.region_id, i32 7, i8** nonnull %6283, i8** nonnull %6285, i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_sizes.273, i64 0, i64 0), i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_maptypes.274, i64 0, i64 0), i32 %tms5328.08850, i32 %ths5334.08847) #6
  %6501 = icmp eq i32 %6500, 0
  br i1 %6501, label %omp_offload.cont5361, label %omp_offload.failed5360

omp_offload.failed5360:                           ; preds = %for.body5345
  %6502 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %6503 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %6502, i32 %tms5328.08850, i32 %ths5334.08847) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 5, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64)* @.omp_outlined..250 to void (i32*, i32*, ...)*), i64 %indvars.iv9473, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted5352.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont5361

omp_offload.cont5361:                             ; preds = %for.body5345, %omp_offload.failed5360
  %mul5363 = mul nsw i32 %sch5340.08844, 3000
  %6504 = zext i32 %mul5363 to i64
  %cmp5342 = icmp ult i64 %indvars.iv9473, %6504
  br i1 %cmp5342, label %for.cond.cleanup5344, label %for.body5345

for.cond5381.preheader:                           ; preds = %for.cond5381.preheader.preheader32, %for.cond.cleanup5384
  %times5374.08854 = phi i32 [ %inc5399, %for.cond.cleanup5384 ], [ 0, %for.cond5381.preheader.preheader32 ]
  br label %vector.body11353

vector.body11353:                                 ; preds = %for.cond5381.preheader, %vector.body11353
  %index11362 = phi i64 [ %index.next11363, %vector.body11353 ], [ 0, %for.cond5381.preheader ]
  %6505 = getelementptr inbounds double, double* %1796, i64 %index11362
  %6506 = bitcast double* %6505 to <2 x double>*
  %wide.load11380 = load <2 x double>, <2 x double>* %6506, align 8, !tbaa !62
  %6507 = getelementptr inbounds double, double* %6505, i64 2
  %6508 = bitcast double* %6507 to <2 x double>*
  %wide.load11381 = load <2 x double>, <2 x double>* %6508, align 8, !tbaa !62
  %6509 = getelementptr inbounds double, double* %6505, i64 4
  %6510 = bitcast double* %6509 to <2 x double>*
  %wide.load11382 = load <2 x double>, <2 x double>* %6510, align 8, !tbaa !62
  %6511 = getelementptr inbounds double, double* %6505, i64 6
  %6512 = bitcast double* %6511 to <2 x double>*
  %wide.load11383 = load <2 x double>, <2 x double>* %6512, align 8, !tbaa !62
  %6513 = getelementptr inbounds double, double* %6505, i64 8
  %6514 = bitcast double* %6513 to <2 x double>*
  %wide.load11384 = load <2 x double>, <2 x double>* %6514, align 8, !tbaa !62
  %6515 = getelementptr inbounds double, double* %6505, i64 10
  %6516 = bitcast double* %6515 to <2 x double>*
  %wide.load11385 = load <2 x double>, <2 x double>* %6516, align 8, !tbaa !62
  %6517 = getelementptr inbounds double, double* %6505, i64 12
  %6518 = bitcast double* %6517 to <2 x double>*
  %wide.load11386 = load <2 x double>, <2 x double>* %6518, align 8, !tbaa !62
  %6519 = getelementptr inbounds double, double* %6505, i64 14
  %6520 = bitcast double* %6519 to <2 x double>*
  %wide.load11387 = load <2 x double>, <2 x double>* %6520, align 8, !tbaa !62
  %6521 = getelementptr inbounds double, double* %6505, i64 16
  %6522 = bitcast double* %6521 to <2 x double>*
  %wide.load11388 = load <2 x double>, <2 x double>* %6522, align 8, !tbaa !62
  %6523 = getelementptr inbounds double, double* %6505, i64 18
  %6524 = bitcast double* %6523 to <2 x double>*
  %wide.load11389 = load <2 x double>, <2 x double>* %6524, align 8, !tbaa !62
  %6525 = getelementptr inbounds double, double* %6505, i64 20
  %6526 = bitcast double* %6525 to <2 x double>*
  %wide.load11390 = load <2 x double>, <2 x double>* %6526, align 8, !tbaa !62
  %6527 = getelementptr inbounds double, double* %6505, i64 22
  %6528 = bitcast double* %6527 to <2 x double>*
  %wide.load11391 = load <2 x double>, <2 x double>* %6528, align 8, !tbaa !62
  %6529 = getelementptr inbounds double, double* %1797, i64 %index11362
  %6530 = bitcast double* %6529 to <2 x double>*
  %wide.load11392 = load <2 x double>, <2 x double>* %6530, align 8, !tbaa !62
  %6531 = getelementptr inbounds double, double* %6529, i64 2
  %6532 = bitcast double* %6531 to <2 x double>*
  %wide.load11393 = load <2 x double>, <2 x double>* %6532, align 8, !tbaa !62
  %6533 = getelementptr inbounds double, double* %6529, i64 4
  %6534 = bitcast double* %6533 to <2 x double>*
  %wide.load11394 = load <2 x double>, <2 x double>* %6534, align 8, !tbaa !62
  %6535 = getelementptr inbounds double, double* %6529, i64 6
  %6536 = bitcast double* %6535 to <2 x double>*
  %wide.load11395 = load <2 x double>, <2 x double>* %6536, align 8, !tbaa !62
  %6537 = getelementptr inbounds double, double* %6529, i64 8
  %6538 = bitcast double* %6537 to <2 x double>*
  %wide.load11396 = load <2 x double>, <2 x double>* %6538, align 8, !tbaa !62
  %6539 = getelementptr inbounds double, double* %6529, i64 10
  %6540 = bitcast double* %6539 to <2 x double>*
  %wide.load11397 = load <2 x double>, <2 x double>* %6540, align 8, !tbaa !62
  %6541 = getelementptr inbounds double, double* %6529, i64 12
  %6542 = bitcast double* %6541 to <2 x double>*
  %wide.load11398 = load <2 x double>, <2 x double>* %6542, align 8, !tbaa !62
  %6543 = getelementptr inbounds double, double* %6529, i64 14
  %6544 = bitcast double* %6543 to <2 x double>*
  %wide.load11399 = load <2 x double>, <2 x double>* %6544, align 8, !tbaa !62
  %6545 = getelementptr inbounds double, double* %6529, i64 16
  %6546 = bitcast double* %6545 to <2 x double>*
  %wide.load11400 = load <2 x double>, <2 x double>* %6546, align 8, !tbaa !62
  %6547 = getelementptr inbounds double, double* %6529, i64 18
  %6548 = bitcast double* %6547 to <2 x double>*
  %wide.load11401 = load <2 x double>, <2 x double>* %6548, align 8, !tbaa !62
  %6549 = getelementptr inbounds double, double* %6529, i64 20
  %6550 = bitcast double* %6549 to <2 x double>*
  %wide.load11402 = load <2 x double>, <2 x double>* %6550, align 8, !tbaa !62
  %6551 = getelementptr inbounds double, double* %6529, i64 22
  %6552 = bitcast double* %6551 to <2 x double>*
  %wide.load11403 = load <2 x double>, <2 x double>* %6552, align 8, !tbaa !62
  %6553 = fadd <2 x double> %wide.load11380, %wide.load11392
  %6554 = fadd <2 x double> %wide.load11381, %wide.load11393
  %6555 = fadd <2 x double> %wide.load11382, %wide.load11394
  %6556 = fadd <2 x double> %wide.load11383, %wide.load11395
  %6557 = fadd <2 x double> %wide.load11384, %wide.load11396
  %6558 = fadd <2 x double> %wide.load11385, %wide.load11397
  %6559 = fadd <2 x double> %wide.load11386, %wide.load11398
  %6560 = fadd <2 x double> %wide.load11387, %wide.load11399
  %6561 = fadd <2 x double> %wide.load11388, %wide.load11400
  %6562 = fadd <2 x double> %wide.load11389, %wide.load11401
  %6563 = fadd <2 x double> %wide.load11390, %wide.load11402
  %6564 = fadd <2 x double> %wide.load11391, %wide.load11403
  %6565 = getelementptr inbounds double, double* %1795, i64 %index11362
  %6566 = bitcast double* %6565 to <2 x double>*
  %wide.load11404 = load <2 x double>, <2 x double>* %6566, align 8, !tbaa !62
  %6567 = getelementptr inbounds double, double* %6565, i64 2
  %6568 = bitcast double* %6567 to <2 x double>*
  %wide.load11405 = load <2 x double>, <2 x double>* %6568, align 8, !tbaa !62
  %6569 = getelementptr inbounds double, double* %6565, i64 4
  %6570 = bitcast double* %6569 to <2 x double>*
  %wide.load11406 = load <2 x double>, <2 x double>* %6570, align 8, !tbaa !62
  %6571 = getelementptr inbounds double, double* %6565, i64 6
  %6572 = bitcast double* %6571 to <2 x double>*
  %wide.load11407 = load <2 x double>, <2 x double>* %6572, align 8, !tbaa !62
  %6573 = getelementptr inbounds double, double* %6565, i64 8
  %6574 = bitcast double* %6573 to <2 x double>*
  %wide.load11408 = load <2 x double>, <2 x double>* %6574, align 8, !tbaa !62
  %6575 = getelementptr inbounds double, double* %6565, i64 10
  %6576 = bitcast double* %6575 to <2 x double>*
  %wide.load11409 = load <2 x double>, <2 x double>* %6576, align 8, !tbaa !62
  %6577 = getelementptr inbounds double, double* %6565, i64 12
  %6578 = bitcast double* %6577 to <2 x double>*
  %wide.load11410 = load <2 x double>, <2 x double>* %6578, align 8, !tbaa !62
  %6579 = getelementptr inbounds double, double* %6565, i64 14
  %6580 = bitcast double* %6579 to <2 x double>*
  %wide.load11411 = load <2 x double>, <2 x double>* %6580, align 8, !tbaa !62
  %6581 = getelementptr inbounds double, double* %6565, i64 16
  %6582 = bitcast double* %6581 to <2 x double>*
  %wide.load11412 = load <2 x double>, <2 x double>* %6582, align 8, !tbaa !62
  %6583 = getelementptr inbounds double, double* %6565, i64 18
  %6584 = bitcast double* %6583 to <2 x double>*
  %wide.load11413 = load <2 x double>, <2 x double>* %6584, align 8, !tbaa !62
  %6585 = getelementptr inbounds double, double* %6565, i64 20
  %6586 = bitcast double* %6585 to <2 x double>*
  %wide.load11414 = load <2 x double>, <2 x double>* %6586, align 8, !tbaa !62
  %6587 = getelementptr inbounds double, double* %6565, i64 22
  %6588 = bitcast double* %6587 to <2 x double>*
  %wide.load11415 = load <2 x double>, <2 x double>* %6588, align 8, !tbaa !62
  %6589 = fadd <2 x double> %6553, %wide.load11404
  %6590 = fadd <2 x double> %6554, %wide.load11405
  %6591 = fadd <2 x double> %6555, %wide.load11406
  %6592 = fadd <2 x double> %6556, %wide.load11407
  %6593 = fadd <2 x double> %6557, %wide.load11408
  %6594 = fadd <2 x double> %6558, %wide.load11409
  %6595 = fadd <2 x double> %6559, %wide.load11410
  %6596 = fadd <2 x double> %6560, %wide.load11411
  %6597 = fadd <2 x double> %6561, %wide.load11412
  %6598 = fadd <2 x double> %6562, %wide.load11413
  %6599 = fadd <2 x double> %6563, %wide.load11414
  %6600 = fadd <2 x double> %6564, %wide.load11415
  store <2 x double> %6589, <2 x double>* %6566, align 8, !tbaa !62
  store <2 x double> %6590, <2 x double>* %6568, align 8, !tbaa !62
  store <2 x double> %6591, <2 x double>* %6570, align 8, !tbaa !62
  store <2 x double> %6592, <2 x double>* %6572, align 8, !tbaa !62
  store <2 x double> %6593, <2 x double>* %6574, align 8, !tbaa !62
  store <2 x double> %6594, <2 x double>* %6576, align 8, !tbaa !62
  store <2 x double> %6595, <2 x double>* %6578, align 8, !tbaa !62
  store <2 x double> %6596, <2 x double>* %6580, align 8, !tbaa !62
  store <2 x double> %6597, <2 x double>* %6582, align 8, !tbaa !62
  store <2 x double> %6598, <2 x double>* %6584, align 8, !tbaa !62
  store <2 x double> %6599, <2 x double>* %6586, align 8, !tbaa !62
  store <2 x double> %6600, <2 x double>* %6588, align 8, !tbaa !62
  %index.next11363 = add nuw nsw i64 %index11362, 24
  %6601 = icmp eq i64 %index.next11363, %n.vec11361
  br i1 %6601, label %middle.block11354, label %vector.body11353, !llvm.loop !120

middle.block11354:                                ; preds = %vector.body11353
  br i1 %cmp.n11365, label %for.cond.cleanup5384, label %for.body5385

for.cond.cleanup5378:                             ; preds = %for.cond.cleanup5384, %for.body.i8595, %for.cond5375.preheader
  store i8* %call3733, i8** %6305, align 8
  store i8* %call3733, i8** %6306, align 8
  store i64 %6499, i64* %6307, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %6305, i8** nonnull %6306, i64* nonnull %6307, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body5411

for.cond.cleanup5384:                             ; preds = %for.body5385, %middle.block11354
  %inc5399 = add nuw nsw i32 %times5374.08854, 1
  %exitcond9470 = icmp eq i32 %inc5399, %inc5346
  br i1 %exitcond9470, label %for.cond.cleanup5378, label %for.cond5381.preheader

for.body5385:                                     ; preds = %middle.block11354, %for.body5385
  %indvars.iv9467 = phi i64 [ %indvars.iv.next9468, %for.body5385 ], [ %n.vec11361, %middle.block11354 ]
  %arrayidx5387 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9467
  %6602 = load double, double* %arrayidx5387, align 8, !tbaa !62
  %arrayidx5389 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9467
  %6603 = load double, double* %arrayidx5389, align 8, !tbaa !62
  %add5390 = fadd double %6602, %6603
  %arrayidx5392 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9467
  %6604 = load double, double* %arrayidx5392, align 8, !tbaa !62
  %add5393 = fadd double %6604, %add5390
  store double %add5393, double* %arrayidx5392, align 8, !tbaa !62
  %indvars.iv.next9468 = add nuw nsw i64 %indvars.iv9467, 1
  %exitcond9469 = icmp eq i64 %indvars.iv.next9468, %indvars.iv9473
  br i1 %exitcond9469, label %for.cond.cleanup5384, label %for.body5385, !llvm.loop !121

for.body5411:                                     ; preds = %for.inc5425.7, %for.cond.cleanup5378
  %indvars.iv9471 = phi i64 [ 0, %for.cond.cleanup5378 ], [ %indvars.iv.next9472.7, %for.inc5425.7 ]
  %arrayidx5413 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9471
  %6605 = load double, double* %arrayidx5413, align 8, !tbaa !62
  %arrayidx5415 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9471
  %6606 = load double, double* %arrayidx5415, align 8, !tbaa !62
  %cmp5416 = fcmp une double %6605, %6606
  br i1 %cmp5416, label %cleanup5435, label %for.inc5425

for.inc5425:                                      ; preds = %for.body5411
  %indvars.iv.next9472 = or i64 %indvars.iv9471, 1
  %arrayidx5413.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9472
  %6607 = load double, double* %arrayidx5413.1, align 8, !tbaa !62
  %arrayidx5415.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9472
  %6608 = load double, double* %arrayidx5415.1, align 8, !tbaa !62
  %cmp5416.1 = fcmp une double %6607, %6608
  br i1 %cmp5416.1, label %cleanup5435, label %for.inc5425.1

for.inc5433:                                      ; preds = %for.inc5425.7
  %indvars.iv.next9474 = add nuw nsw i64 %indvars.iv9473, 5000
  %cmp5317 = icmp ult i64 %indvars.iv.next9474, 25000
  %indvar.next11357 = add nuw nsw i64 %indvar11356, 1
  br i1 %cmp5317, label %for.body.i8595.preheader, label %for.end5437

cleanup5435:                                      ; preds = %for.inc5425.6, %for.inc5425.5, %for.inc5425.4, %for.inc5425.3, %for.inc5425.2, %for.inc5425.1, %for.inc5425, %for.body5411
  %indvars.iv9471.lcssa = phi i64 [ %indvars.iv9471, %for.body5411 ], [ %indvars.iv.next9472, %for.inc5425 ], [ %indvars.iv.next9472.1, %for.inc5425.1 ], [ %indvars.iv.next9472.2, %for.inc5425.2 ], [ %indvars.iv.next9472.3, %for.inc5425.3 ], [ %indvars.iv.next9472.4, %for.inc5425.4 ], [ %indvars.iv.next9472.5, %for.inc5425.5 ], [ %indvars.iv.next9472.6, %for.inc5425.6 ]
  %.lcssa11786 = phi double [ %6605, %for.body5411 ], [ %6607, %for.inc5425 ], [ %7764, %for.inc5425.1 ], [ %7766, %for.inc5425.2 ], [ %7768, %for.inc5425.3 ], [ %7770, %for.inc5425.4 ], [ %7772, %for.inc5425.5 ], [ %7774, %for.inc5425.6 ]
  %.lcssa11784 = phi double [ %6606, %for.body5411 ], [ %6608, %for.inc5425 ], [ %7765, %for.inc5425.1 ], [ %7767, %for.inc5425.2 ], [ %7769, %for.inc5425.3 ], [ %7771, %for.inc5425.4 ], [ %7773, %for.inc5425.5 ], [ %7775, %for.inc5425.6 ]
  %6609 = trunc i64 %indvars.iv9473 to i32
  %6610 = trunc i64 %indvars.iv9471.lcssa to i32
  %call5423 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %6609, i32 signext %6610, double %.lcssa11786, double %.lcssa11784)
  br label %cleanup5832

for.end5437:                                      ; preds = %for.inc5433
  %puts8271 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8272 = call i32 @puts(i8* getelementptr inbounds ([50 x i8], [50 x i8]* @str.361, i64 0, i64 0))
  %6611 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5450, i64 0, i64 0
  %6612 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5451, i64 0, i64 0
  %6613 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5452, i64 0, i64 0
  %6614 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5450, i64 0, i64 1
  %6615 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5451, i64 0, i64 1
  %6616 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5452, i64 0, i64 1
  %6617 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5450, i64 0, i64 2
  %6618 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5451, i64 0, i64 2
  %6619 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5452, i64 0, i64 2
  %6620 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5492, i64 0, i64 0
  %6621 = bitcast [8 x i8*]* %.offload_baseptrs5492 to i64*
  %6622 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5493, i64 0, i64 0
  %6623 = bitcast [8 x i8*]* %.offload_ptrs5493 to i64*
  %6624 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5492, i64 0, i64 1
  %6625 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5493, i64 0, i64 1
  %6626 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5492, i64 0, i64 2
  %6627 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5493, i64 0, i64 2
  %6628 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5492, i64 0, i64 3
  %6629 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5493, i64 0, i64 3
  %6630 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5492, i64 0, i64 4
  %6631 = bitcast i8** %6630 to i64*
  %6632 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5493, i64 0, i64 4
  %6633 = bitcast i8** %6632 to i64*
  %6634 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5492, i64 0, i64 5
  %6635 = bitcast i8** %6634 to i64*
  %6636 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5493, i64 0, i64 5
  %6637 = bitcast i8** %6636 to i64*
  %6638 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5492, i64 0, i64 6
  %6639 = bitcast i8** %6638 to i64*
  %6640 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5493, i64 0, i64 6
  %6641 = bitcast i8** %6640 to i64*
  %6642 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_baseptrs5492, i64 0, i64 7
  %6643 = bitcast i8** %6642 to i64*
  %6644 = getelementptr inbounds [8 x i8*], [8 x i8*]* %.offload_ptrs5493, i64 0, i64 7
  %6645 = bitcast i8** %6644 to i64*
  %6646 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs5541, i64 0, i64 0
  %6647 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs5542, i64 0, i64 0
  %6648 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes5543, i64 0, i64 0
  %6649 = bitcast i8* %arrayidx.i to <2 x double>*
  %6650 = bitcast i8* %arrayidx2.i to <2 x double>*
  %6651 = bitcast i8* %arrayidx5.i to <2 x double>*
  %6652 = bitcast i8* %arrayidx8.i to <2 x double>*
  %6653 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %6654 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %6655 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %6656 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %6657 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %6658 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %6659 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %6660 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %6661 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %6662 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %6663 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %6664 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %6665 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %6666 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %6667 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %6668 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %6669 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %6670 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %6671 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %6672 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %6673 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %6674 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %6675 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %6676 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %6677 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %6678 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %6679 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %6680 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8610.preheader

for.body.i8610.preheader:                         ; preds = %for.end5437, %for.inc5571
  %indvar11455 = phi i64 [ 0, %for.end5437 ], [ %indvar.next11456, %for.inc5571 ]
  %indvars.iv9465 = phi i64 [ 32, %for.end5437 ], [ %indvars.iv.next9466, %for.inc5571 ]
  %6681 = mul nuw nsw i64 %indvar11455, 5000
  br label %vector.body11515

vector.body11515:                                 ; preds = %vector.body11515, %for.body.i8610.preheader
  %index11519 = phi i64 [ 0, %for.body.i8610.preheader ], [ %index.next11520, %vector.body11515 ]
  %vec.ind11537 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8610.preheader ], [ %vec.ind.next11550, %vector.body11515 ]
  %step.add11538 = add <2 x i32> %vec.ind11537, <i32 2, i32 2>
  %step.add11539 = add <2 x i32> %vec.ind11537, <i32 4, i32 4>
  %step.add11540 = add <2 x i32> %vec.ind11537, <i32 6, i32 6>
  %step.add11541 = add <2 x i32> %vec.ind11537, <i32 8, i32 8>
  %step.add11542 = add <2 x i32> %vec.ind11537, <i32 10, i32 10>
  %step.add11543 = add <2 x i32> %vec.ind11537, <i32 12, i32 12>
  %step.add11544 = add <2 x i32> %vec.ind11537, <i32 14, i32 14>
  %step.add11545 = add <2 x i32> %vec.ind11537, <i32 16, i32 16>
  %step.add11546 = add <2 x i32> %vec.ind11537, <i32 18, i32 18>
  %step.add11547 = add <2 x i32> %vec.ind11537, <i32 20, i32 20>
  %step.add11548 = add <2 x i32> %vec.ind11537, <i32 22, i32 22>
  %6682 = sitofp <2 x i32> %vec.ind11537 to <2 x double>
  %6683 = sitofp <2 x i32> %step.add11538 to <2 x double>
  %6684 = sitofp <2 x i32> %step.add11539 to <2 x double>
  %6685 = sitofp <2 x i32> %step.add11540 to <2 x double>
  %6686 = sitofp <2 x i32> %step.add11541 to <2 x double>
  %6687 = sitofp <2 x i32> %step.add11542 to <2 x double>
  %6688 = sitofp <2 x i32> %step.add11543 to <2 x double>
  %6689 = sitofp <2 x i32> %step.add11544 to <2 x double>
  %6690 = sitofp <2 x i32> %step.add11545 to <2 x double>
  %6691 = sitofp <2 x i32> %step.add11546 to <2 x double>
  %6692 = sitofp <2 x i32> %step.add11547 to <2 x double>
  %6693 = sitofp <2 x i32> %step.add11548 to <2 x double>
  %6694 = getelementptr inbounds double, double* %1795, i64 %index11519
  %6695 = bitcast double* %6694 to <2 x double>*
  store <2 x double> %6682, <2 x double>* %6695, align 8, !tbaa !62
  %6696 = getelementptr inbounds double, double* %6694, i64 2
  %6697 = bitcast double* %6696 to <2 x double>*
  store <2 x double> %6683, <2 x double>* %6697, align 8, !tbaa !62
  %6698 = getelementptr inbounds double, double* %6694, i64 4
  %6699 = bitcast double* %6698 to <2 x double>*
  store <2 x double> %6684, <2 x double>* %6699, align 8, !tbaa !62
  %6700 = getelementptr inbounds double, double* %6694, i64 6
  %6701 = bitcast double* %6700 to <2 x double>*
  store <2 x double> %6685, <2 x double>* %6701, align 8, !tbaa !62
  %6702 = getelementptr inbounds double, double* %6694, i64 8
  %6703 = bitcast double* %6702 to <2 x double>*
  store <2 x double> %6686, <2 x double>* %6703, align 8, !tbaa !62
  %6704 = getelementptr inbounds double, double* %6694, i64 10
  %6705 = bitcast double* %6704 to <2 x double>*
  store <2 x double> %6687, <2 x double>* %6705, align 8, !tbaa !62
  %6706 = getelementptr inbounds double, double* %6694, i64 12
  %6707 = bitcast double* %6706 to <2 x double>*
  store <2 x double> %6688, <2 x double>* %6707, align 8, !tbaa !62
  %6708 = getelementptr inbounds double, double* %6694, i64 14
  %6709 = bitcast double* %6708 to <2 x double>*
  store <2 x double> %6689, <2 x double>* %6709, align 8, !tbaa !62
  %6710 = getelementptr inbounds double, double* %6694, i64 16
  %6711 = bitcast double* %6710 to <2 x double>*
  store <2 x double> %6690, <2 x double>* %6711, align 8, !tbaa !62
  %6712 = getelementptr inbounds double, double* %6694, i64 18
  %6713 = bitcast double* %6712 to <2 x double>*
  store <2 x double> %6691, <2 x double>* %6713, align 8, !tbaa !62
  %6714 = getelementptr inbounds double, double* %6694, i64 20
  %6715 = bitcast double* %6714 to <2 x double>*
  store <2 x double> %6692, <2 x double>* %6715, align 8, !tbaa !62
  %6716 = getelementptr inbounds double, double* %6694, i64 22
  %6717 = bitcast double* %6716 to <2 x double>*
  store <2 x double> %6693, <2 x double>* %6717, align 8, !tbaa !62
  %6718 = getelementptr inbounds double, double* %1794, i64 %index11519
  %6719 = bitcast double* %6718 to <2 x double>*
  store <2 x double> %6682, <2 x double>* %6719, align 8, !tbaa !62
  %6720 = getelementptr inbounds double, double* %6718, i64 2
  %6721 = bitcast double* %6720 to <2 x double>*
  store <2 x double> %6683, <2 x double>* %6721, align 8, !tbaa !62
  %6722 = getelementptr inbounds double, double* %6718, i64 4
  %6723 = bitcast double* %6722 to <2 x double>*
  store <2 x double> %6684, <2 x double>* %6723, align 8, !tbaa !62
  %6724 = getelementptr inbounds double, double* %6718, i64 6
  %6725 = bitcast double* %6724 to <2 x double>*
  store <2 x double> %6685, <2 x double>* %6725, align 8, !tbaa !62
  %6726 = getelementptr inbounds double, double* %6718, i64 8
  %6727 = bitcast double* %6726 to <2 x double>*
  store <2 x double> %6686, <2 x double>* %6727, align 8, !tbaa !62
  %6728 = getelementptr inbounds double, double* %6718, i64 10
  %6729 = bitcast double* %6728 to <2 x double>*
  store <2 x double> %6687, <2 x double>* %6729, align 8, !tbaa !62
  %6730 = getelementptr inbounds double, double* %6718, i64 12
  %6731 = bitcast double* %6730 to <2 x double>*
  store <2 x double> %6688, <2 x double>* %6731, align 8, !tbaa !62
  %6732 = getelementptr inbounds double, double* %6718, i64 14
  %6733 = bitcast double* %6732 to <2 x double>*
  store <2 x double> %6689, <2 x double>* %6733, align 8, !tbaa !62
  %6734 = getelementptr inbounds double, double* %6718, i64 16
  %6735 = bitcast double* %6734 to <2 x double>*
  store <2 x double> %6690, <2 x double>* %6735, align 8, !tbaa !62
  %6736 = getelementptr inbounds double, double* %6718, i64 18
  %6737 = bitcast double* %6736 to <2 x double>*
  store <2 x double> %6691, <2 x double>* %6737, align 8, !tbaa !62
  %6738 = getelementptr inbounds double, double* %6718, i64 20
  %6739 = bitcast double* %6738 to <2 x double>*
  store <2 x double> %6692, <2 x double>* %6739, align 8, !tbaa !62
  %6740 = getelementptr inbounds double, double* %6718, i64 22
  %6741 = bitcast double* %6740 to <2 x double>*
  store <2 x double> %6693, <2 x double>* %6741, align 8, !tbaa !62
  %6742 = shl <2 x i32> %vec.ind11537, <i32 1, i32 1>
  %6743 = shl <2 x i32> %step.add11538, <i32 1, i32 1>
  %6744 = shl <2 x i32> %step.add11539, <i32 1, i32 1>
  %6745 = shl <2 x i32> %step.add11540, <i32 1, i32 1>
  %6746 = shl <2 x i32> %step.add11541, <i32 1, i32 1>
  %6747 = shl <2 x i32> %step.add11542, <i32 1, i32 1>
  %6748 = shl <2 x i32> %step.add11543, <i32 1, i32 1>
  %6749 = shl <2 x i32> %step.add11544, <i32 1, i32 1>
  %6750 = shl <2 x i32> %step.add11545, <i32 1, i32 1>
  %6751 = shl <2 x i32> %step.add11546, <i32 1, i32 1>
  %6752 = shl <2 x i32> %step.add11547, <i32 1, i32 1>
  %6753 = shl <2 x i32> %step.add11548, <i32 1, i32 1>
  %6754 = sitofp <2 x i32> %6742 to <2 x double>
  %6755 = sitofp <2 x i32> %6743 to <2 x double>
  %6756 = sitofp <2 x i32> %6744 to <2 x double>
  %6757 = sitofp <2 x i32> %6745 to <2 x double>
  %6758 = sitofp <2 x i32> %6746 to <2 x double>
  %6759 = sitofp <2 x i32> %6747 to <2 x double>
  %6760 = sitofp <2 x i32> %6748 to <2 x double>
  %6761 = sitofp <2 x i32> %6749 to <2 x double>
  %6762 = sitofp <2 x i32> %6750 to <2 x double>
  %6763 = sitofp <2 x i32> %6751 to <2 x double>
  %6764 = sitofp <2 x i32> %6752 to <2 x double>
  %6765 = sitofp <2 x i32> %6753 to <2 x double>
  %6766 = getelementptr inbounds double, double* %1796, i64 %index11519
  %6767 = bitcast double* %6766 to <2 x double>*
  store <2 x double> %6754, <2 x double>* %6767, align 8, !tbaa !62
  %6768 = getelementptr inbounds double, double* %6766, i64 2
  %6769 = bitcast double* %6768 to <2 x double>*
  store <2 x double> %6755, <2 x double>* %6769, align 8, !tbaa !62
  %6770 = getelementptr inbounds double, double* %6766, i64 4
  %6771 = bitcast double* %6770 to <2 x double>*
  store <2 x double> %6756, <2 x double>* %6771, align 8, !tbaa !62
  %6772 = getelementptr inbounds double, double* %6766, i64 6
  %6773 = bitcast double* %6772 to <2 x double>*
  store <2 x double> %6757, <2 x double>* %6773, align 8, !tbaa !62
  %6774 = getelementptr inbounds double, double* %6766, i64 8
  %6775 = bitcast double* %6774 to <2 x double>*
  store <2 x double> %6758, <2 x double>* %6775, align 8, !tbaa !62
  %6776 = getelementptr inbounds double, double* %6766, i64 10
  %6777 = bitcast double* %6776 to <2 x double>*
  store <2 x double> %6759, <2 x double>* %6777, align 8, !tbaa !62
  %6778 = getelementptr inbounds double, double* %6766, i64 12
  %6779 = bitcast double* %6778 to <2 x double>*
  store <2 x double> %6760, <2 x double>* %6779, align 8, !tbaa !62
  %6780 = getelementptr inbounds double, double* %6766, i64 14
  %6781 = bitcast double* %6780 to <2 x double>*
  store <2 x double> %6761, <2 x double>* %6781, align 8, !tbaa !62
  %6782 = getelementptr inbounds double, double* %6766, i64 16
  %6783 = bitcast double* %6782 to <2 x double>*
  store <2 x double> %6762, <2 x double>* %6783, align 8, !tbaa !62
  %6784 = getelementptr inbounds double, double* %6766, i64 18
  %6785 = bitcast double* %6784 to <2 x double>*
  store <2 x double> %6763, <2 x double>* %6785, align 8, !tbaa !62
  %6786 = getelementptr inbounds double, double* %6766, i64 20
  %6787 = bitcast double* %6786 to <2 x double>*
  store <2 x double> %6764, <2 x double>* %6787, align 8, !tbaa !62
  %6788 = getelementptr inbounds double, double* %6766, i64 22
  %6789 = bitcast double* %6788 to <2 x double>*
  store <2 x double> %6765, <2 x double>* %6789, align 8, !tbaa !62
  %6790 = add <2 x i32> %vec.ind11537, <i32 -3, i32 -3>
  %6791 = add <2 x i32> %vec.ind11537, <i32 -1, i32 -1>
  %6792 = add <2 x i32> %vec.ind11537, <i32 1, i32 1>
  %6793 = add <2 x i32> %vec.ind11537, <i32 3, i32 3>
  %6794 = add <2 x i32> %vec.ind11537, <i32 5, i32 5>
  %6795 = add <2 x i32> %vec.ind11537, <i32 7, i32 7>
  %6796 = add <2 x i32> %vec.ind11537, <i32 9, i32 9>
  %6797 = add <2 x i32> %vec.ind11537, <i32 11, i32 11>
  %6798 = add <2 x i32> %vec.ind11537, <i32 13, i32 13>
  %6799 = add <2 x i32> %vec.ind11537, <i32 15, i32 15>
  %6800 = add <2 x i32> %vec.ind11537, <i32 17, i32 17>
  %6801 = add <2 x i32> %vec.ind11537, <i32 19, i32 19>
  %6802 = sitofp <2 x i32> %6790 to <2 x double>
  %6803 = sitofp <2 x i32> %6791 to <2 x double>
  %6804 = sitofp <2 x i32> %6792 to <2 x double>
  %6805 = sitofp <2 x i32> %6793 to <2 x double>
  %6806 = sitofp <2 x i32> %6794 to <2 x double>
  %6807 = sitofp <2 x i32> %6795 to <2 x double>
  %6808 = sitofp <2 x i32> %6796 to <2 x double>
  %6809 = sitofp <2 x i32> %6797 to <2 x double>
  %6810 = sitofp <2 x i32> %6798 to <2 x double>
  %6811 = sitofp <2 x i32> %6799 to <2 x double>
  %6812 = sitofp <2 x i32> %6800 to <2 x double>
  %6813 = sitofp <2 x i32> %6801 to <2 x double>
  %6814 = getelementptr inbounds double, double* %1797, i64 %index11519
  %6815 = bitcast double* %6814 to <2 x double>*
  store <2 x double> %6802, <2 x double>* %6815, align 8, !tbaa !62
  %6816 = getelementptr inbounds double, double* %6814, i64 2
  %6817 = bitcast double* %6816 to <2 x double>*
  store <2 x double> %6803, <2 x double>* %6817, align 8, !tbaa !62
  %6818 = getelementptr inbounds double, double* %6814, i64 4
  %6819 = bitcast double* %6818 to <2 x double>*
  store <2 x double> %6804, <2 x double>* %6819, align 8, !tbaa !62
  %6820 = getelementptr inbounds double, double* %6814, i64 6
  %6821 = bitcast double* %6820 to <2 x double>*
  store <2 x double> %6805, <2 x double>* %6821, align 8, !tbaa !62
  %6822 = getelementptr inbounds double, double* %6814, i64 8
  %6823 = bitcast double* %6822 to <2 x double>*
  store <2 x double> %6806, <2 x double>* %6823, align 8, !tbaa !62
  %6824 = getelementptr inbounds double, double* %6814, i64 10
  %6825 = bitcast double* %6824 to <2 x double>*
  store <2 x double> %6807, <2 x double>* %6825, align 8, !tbaa !62
  %6826 = getelementptr inbounds double, double* %6814, i64 12
  %6827 = bitcast double* %6826 to <2 x double>*
  store <2 x double> %6808, <2 x double>* %6827, align 8, !tbaa !62
  %6828 = getelementptr inbounds double, double* %6814, i64 14
  %6829 = bitcast double* %6828 to <2 x double>*
  store <2 x double> %6809, <2 x double>* %6829, align 8, !tbaa !62
  %6830 = getelementptr inbounds double, double* %6814, i64 16
  %6831 = bitcast double* %6830 to <2 x double>*
  store <2 x double> %6810, <2 x double>* %6831, align 8, !tbaa !62
  %6832 = getelementptr inbounds double, double* %6814, i64 18
  %6833 = bitcast double* %6832 to <2 x double>*
  store <2 x double> %6811, <2 x double>* %6833, align 8, !tbaa !62
  %6834 = getelementptr inbounds double, double* %6814, i64 20
  %6835 = bitcast double* %6834 to <2 x double>*
  store <2 x double> %6812, <2 x double>* %6835, align 8, !tbaa !62
  %6836 = getelementptr inbounds double, double* %6814, i64 22
  %6837 = bitcast double* %6836 to <2 x double>*
  store <2 x double> %6813, <2 x double>* %6837, align 8, !tbaa !62
  %index.next11520 = add nuw nsw i64 %index11519, 24
  %vec.ind.next11550 = add <2 x i32> %vec.ind11537, <i32 24, i32 24>
  %6838 = icmp eq i64 %index.next11520, 24984
  br i1 %6838, label %for.body.i8610, label %vector.body11515, !llvm.loop !122

for.body.i8610:                                   ; preds = %vector.body11515
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %6649, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %6650, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %6651, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %6652, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %6653, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %6654, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %6655, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %6656, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %6657, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %6658, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %6659, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %6660, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %6661, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %6662, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %6663, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %6664, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %6665, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %6666, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %6667, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %6668, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %6669, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %6670, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %6671, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %6672, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %6673, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %6674, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %6675, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %6676, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %6677, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %6678, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %6679, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %6680, align 8, !tbaa !62
  %6839 = shl nuw nsw i64 %indvars.iv9465, 3
  store i8* %call3733, i8** %6611, align 8
  store i8* %call3733, i8** %6612, align 8
  store i64 %6839, i64* %6613, align 8
  store i8* %call3735, i8** %6614, align 8
  store i8* %call3735, i8** %6615, align 8
  store i64 %6839, i64* %6616, align 8
  store i8* %call3736, i8** %6617, align 8
  store i8* %call3736, i8** %6618, align 8
  store i64 %6839, i64* %6619, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %6611, i8** nonnull %6612, i64* nonnull %6613, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  %cmp54678827 = icmp ult i64 %indvars.iv9465, 1000
  br label %for.cond5460.preheader

for.cond5513.preheader:                           ; preds = %for.cond.cleanup5463
  %6840 = add nuw nsw i64 %6681, 32
  %cmp55148838 = icmp sgt i32 %t5446.2.lcssa.lcssa, 0
  br i1 %cmp55148838, label %for.cond5519.preheader.preheader30, label %for.cond.cleanup5516

for.cond5519.preheader.preheader30:               ; preds = %for.cond5513.preheader
  %n.mod.vf11459 = urem i64 %6840, 24
  %n.vec11460 = sub nuw nsw i64 %6840, %n.mod.vf11459
  %cmp.n11464 = icmp eq i64 %n.mod.vf11459, 0
  br label %for.cond5519.preheader

for.cond5460.preheader:                           ; preds = %for.body.i8610, %for.cond.cleanup5463
  %tms5453.08836 = phi i32 [ 1, %for.body.i8610 ], [ %mul5509, %for.cond.cleanup5463 ]
  %t5446.08835 = phi i32 [ 0, %for.body.i8610 ], [ %t5446.2.lcssa.lcssa, %for.cond.cleanup5463 ]
  %.capture_expr..casted5488.sroa.0.0.insert.ext = zext i32 %tms5453.08836 to i64
  br i1 %cmp54678827, label %for.cond.cleanup5463, label %for.cond5466.preheader

for.cond5466.preheader:                           ; preds = %for.cond5460.preheader, %for.cond.cleanup5469
  %ths5459.08832 = phi i32 [ %mul5505, %for.cond.cleanup5469 ], [ 32, %for.cond5460.preheader ]
  %t5446.18831 = phi i32 [ %inc5477, %for.cond.cleanup5469 ], [ %t5446.08835, %for.cond5460.preheader ]
  %.capture_expr..casted5490.sroa.0.0.insert.ext = zext i32 %ths5459.08832 to i64
  br label %for.cond5472.preheader

for.cond.cleanup5463:                             ; preds = %for.cond.cleanup5469, %for.cond5460.preheader
  %t5446.2.lcssa.lcssa = phi i32 [ %t5446.08835, %for.cond5460.preheader ], [ %inc5477, %for.cond.cleanup5469 ]
  %mul5509 = shl nsw i32 %tms5453.08836, 1
  %cmp5455 = icmp ult i32 %mul5509, 257
  br i1 %cmp5455, label %for.cond5460.preheader, label %for.cond5513.preheader

for.cond5472.preheader:                           ; preds = %for.cond5466.preheader, %for.cond.cleanup5475
  %dssch5465.08829 = phi i32 [ 1000, %for.cond5466.preheader ], [ %mul5501, %for.cond.cleanup5475 ]
  %t5446.28828 = phi i32 [ %t5446.18831, %for.cond5466.preheader ], [ %inc5477, %for.cond.cleanup5475 ]
  %.capture_expr..casted5484.sroa.0.0.insert.ext = zext i32 %dssch5465.08829 to i64
  br label %for.body5476

for.cond.cleanup5469:                             ; preds = %for.cond.cleanup5475
  %mul5505 = shl nsw i32 %ths5459.08832, 1
  %cmp5461 = icmp ult i32 %mul5505, 1025
  br i1 %cmp5461, label %for.cond5466.preheader, label %for.cond.cleanup5463

for.cond.cleanup5475:                             ; preds = %omp_offload.cont5495
  %mul5501 = mul nsw i32 %dssch5465.08829, 3000
  %6841 = zext i32 %mul5501 to i64
  %cmp5467 = icmp ult i64 %indvars.iv9465, %6841
  br i1 %cmp5467, label %for.cond.cleanup5469, label %for.cond5472.preheader

for.body5476:                                     ; preds = %for.cond5472.preheader, %omp_offload.cont5495
  %sch5471.08825 = phi i32 [ 1000, %for.cond5472.preheader ], [ %mul5497, %omp_offload.cont5495 ]
  %t5446.38824 = phi i32 [ %t5446.28828, %for.cond5472.preheader ], [ %inc5477, %omp_offload.cont5495 ]
  %inc5477 = add nsw i32 %t5446.38824, 1
  %.capture_expr..casted5486.sroa.0.0.insert.ext = zext i32 %sch5471.08825 to i64
  store i64 %indvars.iv9465, i64* %6621, align 8
  store i64 %indvars.iv9465, i64* %6623, align 8
  store i8* %call3733, i8** %6624, align 8
  store i8* %call3733, i8** %6625, align 8
  store i8* %call3735, i8** %6626, align 8
  store i8* %call3735, i8** %6627, align 8
  store i8* %call3736, i8** %6628, align 8
  store i8* %call3736, i8** %6629, align 8
  store i64 %.capture_expr..casted5484.sroa.0.0.insert.ext, i64* %6631, align 8
  store i64 %.capture_expr..casted5484.sroa.0.0.insert.ext, i64* %6633, align 8
  store i64 %.capture_expr..casted5486.sroa.0.0.insert.ext, i64* %6635, align 8
  store i64 %.capture_expr..casted5486.sroa.0.0.insert.ext, i64* %6637, align 8
  store i64 %.capture_expr..casted5488.sroa.0.0.insert.ext, i64* %6639, align 8
  store i64 %.capture_expr..casted5488.sroa.0.0.insert.ext, i64* %6641, align 8
  store i64 %.capture_expr..casted5490.sroa.0.0.insert.ext, i64* %6643, align 8
  store i64 %.capture_expr..casted5490.sroa.0.0.insert.ext, i64* %6645, align 8
  %6842 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1539.region_id, i32 8, i8** nonnull %6620, i8** nonnull %6622, i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_sizes.259, i64 0, i64 0), i64* getelementptr inbounds ([8 x i64], [8 x i64]* @.offload_maptypes.260, i64 0, i64 0), i32 %tms5453.08836, i32 %ths5459.08832) #6
  %6843 = icmp eq i32 %6842, 0
  br i1 %6843, label %omp_offload.cont5495, label %omp_offload.failed5494

omp_offload.failed5494:                           ; preds = %for.body5476
  %6844 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %6845 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %6844, i32 %tms5453.08836, i32 %ths5459.08832) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 6, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64, i64)* @.omp_outlined..257 to void (i32*, i32*, ...)*), i64 %indvars.iv9465, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted5484.sroa.0.0.insert.ext, i64 %.capture_expr..casted5486.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont5495

omp_offload.cont5495:                             ; preds = %for.body5476, %omp_offload.failed5494
  %mul5497 = mul nsw i32 %sch5471.08825, 3000
  %6846 = zext i32 %mul5497 to i64
  %cmp5473 = icmp ult i64 %indvars.iv9465, %6846
  br i1 %cmp5473, label %for.cond.cleanup5475, label %for.body5476

for.cond5519.preheader:                           ; preds = %for.cond5519.preheader.preheader30, %for.cond.cleanup5522
  %times5512.08839 = phi i32 [ %inc5537, %for.cond.cleanup5522 ], [ 0, %for.cond5519.preheader.preheader30 ]
  br label %vector.body11452

vector.body11452:                                 ; preds = %for.cond5519.preheader, %vector.body11452
  %index11461 = phi i64 [ %index.next11462, %vector.body11452 ], [ 0, %for.cond5519.preheader ]
  %6847 = getelementptr inbounds double, double* %1796, i64 %index11461
  %6848 = bitcast double* %6847 to <2 x double>*
  %wide.load11479 = load <2 x double>, <2 x double>* %6848, align 8, !tbaa !62
  %6849 = getelementptr inbounds double, double* %6847, i64 2
  %6850 = bitcast double* %6849 to <2 x double>*
  %wide.load11480 = load <2 x double>, <2 x double>* %6850, align 8, !tbaa !62
  %6851 = getelementptr inbounds double, double* %6847, i64 4
  %6852 = bitcast double* %6851 to <2 x double>*
  %wide.load11481 = load <2 x double>, <2 x double>* %6852, align 8, !tbaa !62
  %6853 = getelementptr inbounds double, double* %6847, i64 6
  %6854 = bitcast double* %6853 to <2 x double>*
  %wide.load11482 = load <2 x double>, <2 x double>* %6854, align 8, !tbaa !62
  %6855 = getelementptr inbounds double, double* %6847, i64 8
  %6856 = bitcast double* %6855 to <2 x double>*
  %wide.load11483 = load <2 x double>, <2 x double>* %6856, align 8, !tbaa !62
  %6857 = getelementptr inbounds double, double* %6847, i64 10
  %6858 = bitcast double* %6857 to <2 x double>*
  %wide.load11484 = load <2 x double>, <2 x double>* %6858, align 8, !tbaa !62
  %6859 = getelementptr inbounds double, double* %6847, i64 12
  %6860 = bitcast double* %6859 to <2 x double>*
  %wide.load11485 = load <2 x double>, <2 x double>* %6860, align 8, !tbaa !62
  %6861 = getelementptr inbounds double, double* %6847, i64 14
  %6862 = bitcast double* %6861 to <2 x double>*
  %wide.load11486 = load <2 x double>, <2 x double>* %6862, align 8, !tbaa !62
  %6863 = getelementptr inbounds double, double* %6847, i64 16
  %6864 = bitcast double* %6863 to <2 x double>*
  %wide.load11487 = load <2 x double>, <2 x double>* %6864, align 8, !tbaa !62
  %6865 = getelementptr inbounds double, double* %6847, i64 18
  %6866 = bitcast double* %6865 to <2 x double>*
  %wide.load11488 = load <2 x double>, <2 x double>* %6866, align 8, !tbaa !62
  %6867 = getelementptr inbounds double, double* %6847, i64 20
  %6868 = bitcast double* %6867 to <2 x double>*
  %wide.load11489 = load <2 x double>, <2 x double>* %6868, align 8, !tbaa !62
  %6869 = getelementptr inbounds double, double* %6847, i64 22
  %6870 = bitcast double* %6869 to <2 x double>*
  %wide.load11490 = load <2 x double>, <2 x double>* %6870, align 8, !tbaa !62
  %6871 = getelementptr inbounds double, double* %1797, i64 %index11461
  %6872 = bitcast double* %6871 to <2 x double>*
  %wide.load11491 = load <2 x double>, <2 x double>* %6872, align 8, !tbaa !62
  %6873 = getelementptr inbounds double, double* %6871, i64 2
  %6874 = bitcast double* %6873 to <2 x double>*
  %wide.load11492 = load <2 x double>, <2 x double>* %6874, align 8, !tbaa !62
  %6875 = getelementptr inbounds double, double* %6871, i64 4
  %6876 = bitcast double* %6875 to <2 x double>*
  %wide.load11493 = load <2 x double>, <2 x double>* %6876, align 8, !tbaa !62
  %6877 = getelementptr inbounds double, double* %6871, i64 6
  %6878 = bitcast double* %6877 to <2 x double>*
  %wide.load11494 = load <2 x double>, <2 x double>* %6878, align 8, !tbaa !62
  %6879 = getelementptr inbounds double, double* %6871, i64 8
  %6880 = bitcast double* %6879 to <2 x double>*
  %wide.load11495 = load <2 x double>, <2 x double>* %6880, align 8, !tbaa !62
  %6881 = getelementptr inbounds double, double* %6871, i64 10
  %6882 = bitcast double* %6881 to <2 x double>*
  %wide.load11496 = load <2 x double>, <2 x double>* %6882, align 8, !tbaa !62
  %6883 = getelementptr inbounds double, double* %6871, i64 12
  %6884 = bitcast double* %6883 to <2 x double>*
  %wide.load11497 = load <2 x double>, <2 x double>* %6884, align 8, !tbaa !62
  %6885 = getelementptr inbounds double, double* %6871, i64 14
  %6886 = bitcast double* %6885 to <2 x double>*
  %wide.load11498 = load <2 x double>, <2 x double>* %6886, align 8, !tbaa !62
  %6887 = getelementptr inbounds double, double* %6871, i64 16
  %6888 = bitcast double* %6887 to <2 x double>*
  %wide.load11499 = load <2 x double>, <2 x double>* %6888, align 8, !tbaa !62
  %6889 = getelementptr inbounds double, double* %6871, i64 18
  %6890 = bitcast double* %6889 to <2 x double>*
  %wide.load11500 = load <2 x double>, <2 x double>* %6890, align 8, !tbaa !62
  %6891 = getelementptr inbounds double, double* %6871, i64 20
  %6892 = bitcast double* %6891 to <2 x double>*
  %wide.load11501 = load <2 x double>, <2 x double>* %6892, align 8, !tbaa !62
  %6893 = getelementptr inbounds double, double* %6871, i64 22
  %6894 = bitcast double* %6893 to <2 x double>*
  %wide.load11502 = load <2 x double>, <2 x double>* %6894, align 8, !tbaa !62
  %6895 = fadd <2 x double> %wide.load11479, %wide.load11491
  %6896 = fadd <2 x double> %wide.load11480, %wide.load11492
  %6897 = fadd <2 x double> %wide.load11481, %wide.load11493
  %6898 = fadd <2 x double> %wide.load11482, %wide.load11494
  %6899 = fadd <2 x double> %wide.load11483, %wide.load11495
  %6900 = fadd <2 x double> %wide.load11484, %wide.load11496
  %6901 = fadd <2 x double> %wide.load11485, %wide.load11497
  %6902 = fadd <2 x double> %wide.load11486, %wide.load11498
  %6903 = fadd <2 x double> %wide.load11487, %wide.load11499
  %6904 = fadd <2 x double> %wide.load11488, %wide.load11500
  %6905 = fadd <2 x double> %wide.load11489, %wide.load11501
  %6906 = fadd <2 x double> %wide.load11490, %wide.load11502
  %6907 = getelementptr inbounds double, double* %1795, i64 %index11461
  %6908 = bitcast double* %6907 to <2 x double>*
  %wide.load11503 = load <2 x double>, <2 x double>* %6908, align 8, !tbaa !62
  %6909 = getelementptr inbounds double, double* %6907, i64 2
  %6910 = bitcast double* %6909 to <2 x double>*
  %wide.load11504 = load <2 x double>, <2 x double>* %6910, align 8, !tbaa !62
  %6911 = getelementptr inbounds double, double* %6907, i64 4
  %6912 = bitcast double* %6911 to <2 x double>*
  %wide.load11505 = load <2 x double>, <2 x double>* %6912, align 8, !tbaa !62
  %6913 = getelementptr inbounds double, double* %6907, i64 6
  %6914 = bitcast double* %6913 to <2 x double>*
  %wide.load11506 = load <2 x double>, <2 x double>* %6914, align 8, !tbaa !62
  %6915 = getelementptr inbounds double, double* %6907, i64 8
  %6916 = bitcast double* %6915 to <2 x double>*
  %wide.load11507 = load <2 x double>, <2 x double>* %6916, align 8, !tbaa !62
  %6917 = getelementptr inbounds double, double* %6907, i64 10
  %6918 = bitcast double* %6917 to <2 x double>*
  %wide.load11508 = load <2 x double>, <2 x double>* %6918, align 8, !tbaa !62
  %6919 = getelementptr inbounds double, double* %6907, i64 12
  %6920 = bitcast double* %6919 to <2 x double>*
  %wide.load11509 = load <2 x double>, <2 x double>* %6920, align 8, !tbaa !62
  %6921 = getelementptr inbounds double, double* %6907, i64 14
  %6922 = bitcast double* %6921 to <2 x double>*
  %wide.load11510 = load <2 x double>, <2 x double>* %6922, align 8, !tbaa !62
  %6923 = getelementptr inbounds double, double* %6907, i64 16
  %6924 = bitcast double* %6923 to <2 x double>*
  %wide.load11511 = load <2 x double>, <2 x double>* %6924, align 8, !tbaa !62
  %6925 = getelementptr inbounds double, double* %6907, i64 18
  %6926 = bitcast double* %6925 to <2 x double>*
  %wide.load11512 = load <2 x double>, <2 x double>* %6926, align 8, !tbaa !62
  %6927 = getelementptr inbounds double, double* %6907, i64 20
  %6928 = bitcast double* %6927 to <2 x double>*
  %wide.load11513 = load <2 x double>, <2 x double>* %6928, align 8, !tbaa !62
  %6929 = getelementptr inbounds double, double* %6907, i64 22
  %6930 = bitcast double* %6929 to <2 x double>*
  %wide.load11514 = load <2 x double>, <2 x double>* %6930, align 8, !tbaa !62
  %6931 = fadd <2 x double> %6895, %wide.load11503
  %6932 = fadd <2 x double> %6896, %wide.load11504
  %6933 = fadd <2 x double> %6897, %wide.load11505
  %6934 = fadd <2 x double> %6898, %wide.load11506
  %6935 = fadd <2 x double> %6899, %wide.load11507
  %6936 = fadd <2 x double> %6900, %wide.load11508
  %6937 = fadd <2 x double> %6901, %wide.load11509
  %6938 = fadd <2 x double> %6902, %wide.load11510
  %6939 = fadd <2 x double> %6903, %wide.load11511
  %6940 = fadd <2 x double> %6904, %wide.load11512
  %6941 = fadd <2 x double> %6905, %wide.load11513
  %6942 = fadd <2 x double> %6906, %wide.load11514
  store <2 x double> %6931, <2 x double>* %6908, align 8, !tbaa !62
  store <2 x double> %6932, <2 x double>* %6910, align 8, !tbaa !62
  store <2 x double> %6933, <2 x double>* %6912, align 8, !tbaa !62
  store <2 x double> %6934, <2 x double>* %6914, align 8, !tbaa !62
  store <2 x double> %6935, <2 x double>* %6916, align 8, !tbaa !62
  store <2 x double> %6936, <2 x double>* %6918, align 8, !tbaa !62
  store <2 x double> %6937, <2 x double>* %6920, align 8, !tbaa !62
  store <2 x double> %6938, <2 x double>* %6922, align 8, !tbaa !62
  store <2 x double> %6939, <2 x double>* %6924, align 8, !tbaa !62
  store <2 x double> %6940, <2 x double>* %6926, align 8, !tbaa !62
  store <2 x double> %6941, <2 x double>* %6928, align 8, !tbaa !62
  store <2 x double> %6942, <2 x double>* %6930, align 8, !tbaa !62
  %index.next11462 = add nuw nsw i64 %index11461, 24
  %6943 = icmp eq i64 %index.next11462, %n.vec11460
  br i1 %6943, label %middle.block11453, label %vector.body11452, !llvm.loop !123

middle.block11453:                                ; preds = %vector.body11452
  br i1 %cmp.n11464, label %for.cond.cleanup5522, label %for.body5523

for.cond.cleanup5516:                             ; preds = %for.cond.cleanup5522, %for.cond5513.preheader
  store i8* %call3733, i8** %6646, align 8
  store i8* %call3733, i8** %6647, align 8
  store i64 %6839, i64* %6648, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %6646, i8** nonnull %6647, i64* nonnull %6648, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body5549

for.cond.cleanup5522:                             ; preds = %for.body5523, %middle.block11453
  %inc5537 = add nuw nsw i32 %times5512.08839, 1
  %exitcond9462 = icmp eq i32 %inc5537, %t5446.2.lcssa.lcssa
  br i1 %exitcond9462, label %for.cond.cleanup5516, label %for.cond5519.preheader

for.body5523:                                     ; preds = %middle.block11453, %for.body5523
  %indvars.iv9459 = phi i64 [ %indvars.iv.next9460, %for.body5523 ], [ %n.vec11460, %middle.block11453 ]
  %arrayidx5525 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9459
  %6944 = load double, double* %arrayidx5525, align 8, !tbaa !62
  %arrayidx5527 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9459
  %6945 = load double, double* %arrayidx5527, align 8, !tbaa !62
  %add5528 = fadd double %6944, %6945
  %arrayidx5530 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9459
  %6946 = load double, double* %arrayidx5530, align 8, !tbaa !62
  %add5531 = fadd double %6946, %add5528
  store double %add5531, double* %arrayidx5530, align 8, !tbaa !62
  %indvars.iv.next9460 = add nuw nsw i64 %indvars.iv9459, 1
  %exitcond9461 = icmp eq i64 %indvars.iv.next9460, %indvars.iv9465
  br i1 %exitcond9461, label %for.cond.cleanup5522, label %for.body5523, !llvm.loop !124

for.body5549:                                     ; preds = %for.inc5563.7, %for.cond.cleanup5516
  %indvars.iv9463 = phi i64 [ 0, %for.cond.cleanup5516 ], [ %indvars.iv.next9464.7, %for.inc5563.7 ]
  %arrayidx5551 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9463
  %6947 = load double, double* %arrayidx5551, align 8, !tbaa !62
  %arrayidx5553 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9463
  %6948 = load double, double* %arrayidx5553, align 8, !tbaa !62
  %cmp5554 = fcmp une double %6947, %6948
  br i1 %cmp5554, label %cleanup5573, label %for.inc5563

for.inc5563:                                      ; preds = %for.body5549
  %indvars.iv.next9464 = or i64 %indvars.iv9463, 1
  %arrayidx5551.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9464
  %6949 = load double, double* %arrayidx5551.1, align 8, !tbaa !62
  %arrayidx5553.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9464
  %6950 = load double, double* %arrayidx5553.1, align 8, !tbaa !62
  %cmp5554.1 = fcmp une double %6949, %6950
  br i1 %cmp5554.1, label %cleanup5573, label %for.inc5563.1

for.inc5571:                                      ; preds = %for.inc5563.7
  %indvars.iv.next9466 = add nuw nsw i64 %indvars.iv9465, 5000
  %cmp5442 = icmp ult i64 %indvars.iv.next9466, 25000
  %indvar.next11456 = add nuw nsw i64 %indvar11455, 1
  br i1 %cmp5442, label %for.body.i8610.preheader, label %for.end5575

cleanup5573:                                      ; preds = %for.inc5563.6, %for.inc5563.5, %for.inc5563.4, %for.inc5563.3, %for.inc5563.2, %for.inc5563.1, %for.inc5563, %for.body5549
  %indvars.iv9463.lcssa = phi i64 [ %indvars.iv9463, %for.body5549 ], [ %indvars.iv.next9464, %for.inc5563 ], [ %indvars.iv.next9464.1, %for.inc5563.1 ], [ %indvars.iv.next9464.2, %for.inc5563.2 ], [ %indvars.iv.next9464.3, %for.inc5563.3 ], [ %indvars.iv.next9464.4, %for.inc5563.4 ], [ %indvars.iv.next9464.5, %for.inc5563.5 ], [ %indvars.iv.next9464.6, %for.inc5563.6 ]
  %.lcssa11780 = phi double [ %6947, %for.body5549 ], [ %6949, %for.inc5563 ], [ %7752, %for.inc5563.1 ], [ %7754, %for.inc5563.2 ], [ %7756, %for.inc5563.3 ], [ %7758, %for.inc5563.4 ], [ %7760, %for.inc5563.5 ], [ %7762, %for.inc5563.6 ]
  %.lcssa11778 = phi double [ %6948, %for.body5549 ], [ %6950, %for.inc5563 ], [ %7753, %for.inc5563.1 ], [ %7755, %for.inc5563.2 ], [ %7757, %for.inc5563.3 ], [ %7759, %for.inc5563.4 ], [ %7761, %for.inc5563.5 ], [ %7763, %for.inc5563.6 ]
  %6951 = trunc i64 %indvars.iv9465 to i32
  %6952 = trunc i64 %indvars.iv9463.lcssa to i32
  %call5561 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %6951, i32 signext %6952, double %.lcssa11780, double %.lcssa11778)
  br label %cleanup5832

for.end5575:                                      ; preds = %for.inc5571
  %puts8273 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8274 = call i32 @puts(i8* getelementptr inbounds ([42 x i8], [42 x i8]* @str.363, i64 0, i64 0))
  %6953 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5588, i64 0, i64 0
  %6954 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5589, i64 0, i64 0
  %6955 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5590, i64 0, i64 0
  %6956 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5588, i64 0, i64 1
  %6957 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5589, i64 0, i64 1
  %6958 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5590, i64 0, i64 1
  %6959 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5588, i64 0, i64 2
  %6960 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5589, i64 0, i64 2
  %6961 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5590, i64 0, i64 2
  %6962 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5621, i64 0, i64 0
  %6963 = bitcast [7 x i8*]* %.offload_baseptrs5621 to i64*
  %6964 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5622, i64 0, i64 0
  %6965 = bitcast [7 x i8*]* %.offload_ptrs5622 to i64*
  %6966 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5621, i64 0, i64 1
  %6967 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5622, i64 0, i64 1
  %6968 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5621, i64 0, i64 2
  %6969 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5622, i64 0, i64 2
  %6970 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5621, i64 0, i64 3
  %6971 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5622, i64 0, i64 3
  %6972 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5621, i64 0, i64 4
  %6973 = bitcast i8** %6972 to i64*
  %6974 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5622, i64 0, i64 4
  %6975 = bitcast i8** %6974 to i64*
  %6976 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5621, i64 0, i64 5
  %6977 = bitcast i8** %6976 to i64*
  %6978 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5622, i64 0, i64 5
  %6979 = bitcast i8** %6978 to i64*
  %6980 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5621, i64 0, i64 6
  %6981 = bitcast i8** %6980 to i64*
  %6982 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5622, i64 0, i64 6
  %6983 = bitcast i8** %6982 to i64*
  %6984 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs5666, i64 0, i64 0
  %6985 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs5667, i64 0, i64 0
  %6986 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes5668, i64 0, i64 0
  %6987 = bitcast i8* %arrayidx.i to <2 x double>*
  %6988 = bitcast i8* %arrayidx2.i to <2 x double>*
  %6989 = bitcast i8* %arrayidx5.i to <2 x double>*
  %6990 = bitcast i8* %arrayidx8.i to <2 x double>*
  %6991 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %6992 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %6993 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %6994 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %6995 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %6996 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %6997 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %6998 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %6999 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %7000 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %7001 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %7002 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %7003 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %7004 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %7005 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %7006 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %7007 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %7008 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %7009 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %7010 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %7011 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %7012 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %7013 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %7014 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %7015 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %7016 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %7017 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %7018 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8626.preheader

for.body.i8626.preheader:                         ; preds = %for.end5575, %for.inc5696
  %indvar11554 = phi i64 [ 0, %for.end5575 ], [ %indvar.next11555, %for.inc5696 ]
  %indvars.iv9457 = phi i64 [ 32, %for.end5575 ], [ %indvars.iv.next9458, %for.inc5696 ]
  %7019 = mul nuw nsw i64 %indvar11554, 5000
  br label %vector.body11614

vector.body11614:                                 ; preds = %vector.body11614, %for.body.i8626.preheader
  %index11618 = phi i64 [ 0, %for.body.i8626.preheader ], [ %index.next11619, %vector.body11614 ]
  %vec.ind11636 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8626.preheader ], [ %vec.ind.next11649, %vector.body11614 ]
  %step.add11637 = add <2 x i32> %vec.ind11636, <i32 2, i32 2>
  %step.add11638 = add <2 x i32> %vec.ind11636, <i32 4, i32 4>
  %step.add11639 = add <2 x i32> %vec.ind11636, <i32 6, i32 6>
  %step.add11640 = add <2 x i32> %vec.ind11636, <i32 8, i32 8>
  %step.add11641 = add <2 x i32> %vec.ind11636, <i32 10, i32 10>
  %step.add11642 = add <2 x i32> %vec.ind11636, <i32 12, i32 12>
  %step.add11643 = add <2 x i32> %vec.ind11636, <i32 14, i32 14>
  %step.add11644 = add <2 x i32> %vec.ind11636, <i32 16, i32 16>
  %step.add11645 = add <2 x i32> %vec.ind11636, <i32 18, i32 18>
  %step.add11646 = add <2 x i32> %vec.ind11636, <i32 20, i32 20>
  %step.add11647 = add <2 x i32> %vec.ind11636, <i32 22, i32 22>
  %7020 = sitofp <2 x i32> %vec.ind11636 to <2 x double>
  %7021 = sitofp <2 x i32> %step.add11637 to <2 x double>
  %7022 = sitofp <2 x i32> %step.add11638 to <2 x double>
  %7023 = sitofp <2 x i32> %step.add11639 to <2 x double>
  %7024 = sitofp <2 x i32> %step.add11640 to <2 x double>
  %7025 = sitofp <2 x i32> %step.add11641 to <2 x double>
  %7026 = sitofp <2 x i32> %step.add11642 to <2 x double>
  %7027 = sitofp <2 x i32> %step.add11643 to <2 x double>
  %7028 = sitofp <2 x i32> %step.add11644 to <2 x double>
  %7029 = sitofp <2 x i32> %step.add11645 to <2 x double>
  %7030 = sitofp <2 x i32> %step.add11646 to <2 x double>
  %7031 = sitofp <2 x i32> %step.add11647 to <2 x double>
  %7032 = getelementptr inbounds double, double* %1795, i64 %index11618
  %7033 = bitcast double* %7032 to <2 x double>*
  store <2 x double> %7020, <2 x double>* %7033, align 8, !tbaa !62
  %7034 = getelementptr inbounds double, double* %7032, i64 2
  %7035 = bitcast double* %7034 to <2 x double>*
  store <2 x double> %7021, <2 x double>* %7035, align 8, !tbaa !62
  %7036 = getelementptr inbounds double, double* %7032, i64 4
  %7037 = bitcast double* %7036 to <2 x double>*
  store <2 x double> %7022, <2 x double>* %7037, align 8, !tbaa !62
  %7038 = getelementptr inbounds double, double* %7032, i64 6
  %7039 = bitcast double* %7038 to <2 x double>*
  store <2 x double> %7023, <2 x double>* %7039, align 8, !tbaa !62
  %7040 = getelementptr inbounds double, double* %7032, i64 8
  %7041 = bitcast double* %7040 to <2 x double>*
  store <2 x double> %7024, <2 x double>* %7041, align 8, !tbaa !62
  %7042 = getelementptr inbounds double, double* %7032, i64 10
  %7043 = bitcast double* %7042 to <2 x double>*
  store <2 x double> %7025, <2 x double>* %7043, align 8, !tbaa !62
  %7044 = getelementptr inbounds double, double* %7032, i64 12
  %7045 = bitcast double* %7044 to <2 x double>*
  store <2 x double> %7026, <2 x double>* %7045, align 8, !tbaa !62
  %7046 = getelementptr inbounds double, double* %7032, i64 14
  %7047 = bitcast double* %7046 to <2 x double>*
  store <2 x double> %7027, <2 x double>* %7047, align 8, !tbaa !62
  %7048 = getelementptr inbounds double, double* %7032, i64 16
  %7049 = bitcast double* %7048 to <2 x double>*
  store <2 x double> %7028, <2 x double>* %7049, align 8, !tbaa !62
  %7050 = getelementptr inbounds double, double* %7032, i64 18
  %7051 = bitcast double* %7050 to <2 x double>*
  store <2 x double> %7029, <2 x double>* %7051, align 8, !tbaa !62
  %7052 = getelementptr inbounds double, double* %7032, i64 20
  %7053 = bitcast double* %7052 to <2 x double>*
  store <2 x double> %7030, <2 x double>* %7053, align 8, !tbaa !62
  %7054 = getelementptr inbounds double, double* %7032, i64 22
  %7055 = bitcast double* %7054 to <2 x double>*
  store <2 x double> %7031, <2 x double>* %7055, align 8, !tbaa !62
  %7056 = getelementptr inbounds double, double* %1794, i64 %index11618
  %7057 = bitcast double* %7056 to <2 x double>*
  store <2 x double> %7020, <2 x double>* %7057, align 8, !tbaa !62
  %7058 = getelementptr inbounds double, double* %7056, i64 2
  %7059 = bitcast double* %7058 to <2 x double>*
  store <2 x double> %7021, <2 x double>* %7059, align 8, !tbaa !62
  %7060 = getelementptr inbounds double, double* %7056, i64 4
  %7061 = bitcast double* %7060 to <2 x double>*
  store <2 x double> %7022, <2 x double>* %7061, align 8, !tbaa !62
  %7062 = getelementptr inbounds double, double* %7056, i64 6
  %7063 = bitcast double* %7062 to <2 x double>*
  store <2 x double> %7023, <2 x double>* %7063, align 8, !tbaa !62
  %7064 = getelementptr inbounds double, double* %7056, i64 8
  %7065 = bitcast double* %7064 to <2 x double>*
  store <2 x double> %7024, <2 x double>* %7065, align 8, !tbaa !62
  %7066 = getelementptr inbounds double, double* %7056, i64 10
  %7067 = bitcast double* %7066 to <2 x double>*
  store <2 x double> %7025, <2 x double>* %7067, align 8, !tbaa !62
  %7068 = getelementptr inbounds double, double* %7056, i64 12
  %7069 = bitcast double* %7068 to <2 x double>*
  store <2 x double> %7026, <2 x double>* %7069, align 8, !tbaa !62
  %7070 = getelementptr inbounds double, double* %7056, i64 14
  %7071 = bitcast double* %7070 to <2 x double>*
  store <2 x double> %7027, <2 x double>* %7071, align 8, !tbaa !62
  %7072 = getelementptr inbounds double, double* %7056, i64 16
  %7073 = bitcast double* %7072 to <2 x double>*
  store <2 x double> %7028, <2 x double>* %7073, align 8, !tbaa !62
  %7074 = getelementptr inbounds double, double* %7056, i64 18
  %7075 = bitcast double* %7074 to <2 x double>*
  store <2 x double> %7029, <2 x double>* %7075, align 8, !tbaa !62
  %7076 = getelementptr inbounds double, double* %7056, i64 20
  %7077 = bitcast double* %7076 to <2 x double>*
  store <2 x double> %7030, <2 x double>* %7077, align 8, !tbaa !62
  %7078 = getelementptr inbounds double, double* %7056, i64 22
  %7079 = bitcast double* %7078 to <2 x double>*
  store <2 x double> %7031, <2 x double>* %7079, align 8, !tbaa !62
  %7080 = shl <2 x i32> %vec.ind11636, <i32 1, i32 1>
  %7081 = shl <2 x i32> %step.add11637, <i32 1, i32 1>
  %7082 = shl <2 x i32> %step.add11638, <i32 1, i32 1>
  %7083 = shl <2 x i32> %step.add11639, <i32 1, i32 1>
  %7084 = shl <2 x i32> %step.add11640, <i32 1, i32 1>
  %7085 = shl <2 x i32> %step.add11641, <i32 1, i32 1>
  %7086 = shl <2 x i32> %step.add11642, <i32 1, i32 1>
  %7087 = shl <2 x i32> %step.add11643, <i32 1, i32 1>
  %7088 = shl <2 x i32> %step.add11644, <i32 1, i32 1>
  %7089 = shl <2 x i32> %step.add11645, <i32 1, i32 1>
  %7090 = shl <2 x i32> %step.add11646, <i32 1, i32 1>
  %7091 = shl <2 x i32> %step.add11647, <i32 1, i32 1>
  %7092 = sitofp <2 x i32> %7080 to <2 x double>
  %7093 = sitofp <2 x i32> %7081 to <2 x double>
  %7094 = sitofp <2 x i32> %7082 to <2 x double>
  %7095 = sitofp <2 x i32> %7083 to <2 x double>
  %7096 = sitofp <2 x i32> %7084 to <2 x double>
  %7097 = sitofp <2 x i32> %7085 to <2 x double>
  %7098 = sitofp <2 x i32> %7086 to <2 x double>
  %7099 = sitofp <2 x i32> %7087 to <2 x double>
  %7100 = sitofp <2 x i32> %7088 to <2 x double>
  %7101 = sitofp <2 x i32> %7089 to <2 x double>
  %7102 = sitofp <2 x i32> %7090 to <2 x double>
  %7103 = sitofp <2 x i32> %7091 to <2 x double>
  %7104 = getelementptr inbounds double, double* %1796, i64 %index11618
  %7105 = bitcast double* %7104 to <2 x double>*
  store <2 x double> %7092, <2 x double>* %7105, align 8, !tbaa !62
  %7106 = getelementptr inbounds double, double* %7104, i64 2
  %7107 = bitcast double* %7106 to <2 x double>*
  store <2 x double> %7093, <2 x double>* %7107, align 8, !tbaa !62
  %7108 = getelementptr inbounds double, double* %7104, i64 4
  %7109 = bitcast double* %7108 to <2 x double>*
  store <2 x double> %7094, <2 x double>* %7109, align 8, !tbaa !62
  %7110 = getelementptr inbounds double, double* %7104, i64 6
  %7111 = bitcast double* %7110 to <2 x double>*
  store <2 x double> %7095, <2 x double>* %7111, align 8, !tbaa !62
  %7112 = getelementptr inbounds double, double* %7104, i64 8
  %7113 = bitcast double* %7112 to <2 x double>*
  store <2 x double> %7096, <2 x double>* %7113, align 8, !tbaa !62
  %7114 = getelementptr inbounds double, double* %7104, i64 10
  %7115 = bitcast double* %7114 to <2 x double>*
  store <2 x double> %7097, <2 x double>* %7115, align 8, !tbaa !62
  %7116 = getelementptr inbounds double, double* %7104, i64 12
  %7117 = bitcast double* %7116 to <2 x double>*
  store <2 x double> %7098, <2 x double>* %7117, align 8, !tbaa !62
  %7118 = getelementptr inbounds double, double* %7104, i64 14
  %7119 = bitcast double* %7118 to <2 x double>*
  store <2 x double> %7099, <2 x double>* %7119, align 8, !tbaa !62
  %7120 = getelementptr inbounds double, double* %7104, i64 16
  %7121 = bitcast double* %7120 to <2 x double>*
  store <2 x double> %7100, <2 x double>* %7121, align 8, !tbaa !62
  %7122 = getelementptr inbounds double, double* %7104, i64 18
  %7123 = bitcast double* %7122 to <2 x double>*
  store <2 x double> %7101, <2 x double>* %7123, align 8, !tbaa !62
  %7124 = getelementptr inbounds double, double* %7104, i64 20
  %7125 = bitcast double* %7124 to <2 x double>*
  store <2 x double> %7102, <2 x double>* %7125, align 8, !tbaa !62
  %7126 = getelementptr inbounds double, double* %7104, i64 22
  %7127 = bitcast double* %7126 to <2 x double>*
  store <2 x double> %7103, <2 x double>* %7127, align 8, !tbaa !62
  %7128 = add <2 x i32> %vec.ind11636, <i32 -3, i32 -3>
  %7129 = add <2 x i32> %vec.ind11636, <i32 -1, i32 -1>
  %7130 = add <2 x i32> %vec.ind11636, <i32 1, i32 1>
  %7131 = add <2 x i32> %vec.ind11636, <i32 3, i32 3>
  %7132 = add <2 x i32> %vec.ind11636, <i32 5, i32 5>
  %7133 = add <2 x i32> %vec.ind11636, <i32 7, i32 7>
  %7134 = add <2 x i32> %vec.ind11636, <i32 9, i32 9>
  %7135 = add <2 x i32> %vec.ind11636, <i32 11, i32 11>
  %7136 = add <2 x i32> %vec.ind11636, <i32 13, i32 13>
  %7137 = add <2 x i32> %vec.ind11636, <i32 15, i32 15>
  %7138 = add <2 x i32> %vec.ind11636, <i32 17, i32 17>
  %7139 = add <2 x i32> %vec.ind11636, <i32 19, i32 19>
  %7140 = sitofp <2 x i32> %7128 to <2 x double>
  %7141 = sitofp <2 x i32> %7129 to <2 x double>
  %7142 = sitofp <2 x i32> %7130 to <2 x double>
  %7143 = sitofp <2 x i32> %7131 to <2 x double>
  %7144 = sitofp <2 x i32> %7132 to <2 x double>
  %7145 = sitofp <2 x i32> %7133 to <2 x double>
  %7146 = sitofp <2 x i32> %7134 to <2 x double>
  %7147 = sitofp <2 x i32> %7135 to <2 x double>
  %7148 = sitofp <2 x i32> %7136 to <2 x double>
  %7149 = sitofp <2 x i32> %7137 to <2 x double>
  %7150 = sitofp <2 x i32> %7138 to <2 x double>
  %7151 = sitofp <2 x i32> %7139 to <2 x double>
  %7152 = getelementptr inbounds double, double* %1797, i64 %index11618
  %7153 = bitcast double* %7152 to <2 x double>*
  store <2 x double> %7140, <2 x double>* %7153, align 8, !tbaa !62
  %7154 = getelementptr inbounds double, double* %7152, i64 2
  %7155 = bitcast double* %7154 to <2 x double>*
  store <2 x double> %7141, <2 x double>* %7155, align 8, !tbaa !62
  %7156 = getelementptr inbounds double, double* %7152, i64 4
  %7157 = bitcast double* %7156 to <2 x double>*
  store <2 x double> %7142, <2 x double>* %7157, align 8, !tbaa !62
  %7158 = getelementptr inbounds double, double* %7152, i64 6
  %7159 = bitcast double* %7158 to <2 x double>*
  store <2 x double> %7143, <2 x double>* %7159, align 8, !tbaa !62
  %7160 = getelementptr inbounds double, double* %7152, i64 8
  %7161 = bitcast double* %7160 to <2 x double>*
  store <2 x double> %7144, <2 x double>* %7161, align 8, !tbaa !62
  %7162 = getelementptr inbounds double, double* %7152, i64 10
  %7163 = bitcast double* %7162 to <2 x double>*
  store <2 x double> %7145, <2 x double>* %7163, align 8, !tbaa !62
  %7164 = getelementptr inbounds double, double* %7152, i64 12
  %7165 = bitcast double* %7164 to <2 x double>*
  store <2 x double> %7146, <2 x double>* %7165, align 8, !tbaa !62
  %7166 = getelementptr inbounds double, double* %7152, i64 14
  %7167 = bitcast double* %7166 to <2 x double>*
  store <2 x double> %7147, <2 x double>* %7167, align 8, !tbaa !62
  %7168 = getelementptr inbounds double, double* %7152, i64 16
  %7169 = bitcast double* %7168 to <2 x double>*
  store <2 x double> %7148, <2 x double>* %7169, align 8, !tbaa !62
  %7170 = getelementptr inbounds double, double* %7152, i64 18
  %7171 = bitcast double* %7170 to <2 x double>*
  store <2 x double> %7149, <2 x double>* %7171, align 8, !tbaa !62
  %7172 = getelementptr inbounds double, double* %7152, i64 20
  %7173 = bitcast double* %7172 to <2 x double>*
  store <2 x double> %7150, <2 x double>* %7173, align 8, !tbaa !62
  %7174 = getelementptr inbounds double, double* %7152, i64 22
  %7175 = bitcast double* %7174 to <2 x double>*
  store <2 x double> %7151, <2 x double>* %7175, align 8, !tbaa !62
  %index.next11619 = add nuw nsw i64 %index11618, 24
  %vec.ind.next11649 = add <2 x i32> %vec.ind11636, <i32 24, i32 24>
  %7176 = icmp eq i64 %index.next11619, 24984
  br i1 %7176, label %for.body.i8626, label %vector.body11614, !llvm.loop !125

for.body.i8626:                                   ; preds = %vector.body11614
  %7177 = add nuw nsw i64 %7019, 32
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %6987, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %6988, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %6989, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %6990, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %6991, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %6992, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %6993, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %6994, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %6995, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %6996, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %6997, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %6998, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %6999, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %7000, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %7001, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %7002, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %7003, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %7004, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %7005, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %7006, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %7007, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %7008, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %7009, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %7010, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %7011, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %7012, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %7013, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %7014, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %7015, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %7016, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %7017, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %7018, align 8, !tbaa !62
  %7178 = shl nuw nsw i64 %indvars.iv9457, 3
  store i8* %call3733, i8** %6953, align 8
  store i8* %call3733, i8** %6954, align 8
  store i64 %7178, i64* %6955, align 8
  store i8* %call3735, i8** %6956, align 8
  store i8* %call3735, i8** %6957, align 8
  store i64 %7178, i64* %6958, align 8
  store i8* %call3736, i8** %6959, align 8
  store i8* %call3736, i8** %6960, align 8
  store i64 %7178, i64* %6961, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %6953, i8** nonnull %6954, i64* nonnull %6955, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  %cmp56058808 = icmp ult i64 %indvars.iv9457, 1000
  br i1 %cmp56058808, label %for.cond.cleanup5641, label %for.cond5598.preheader

for.cond5638.preheader:                           ; preds = %for.cond.cleanup5601
  %cmp56398819 = icmp sgt i32 %t5584.28809, -1
  br i1 %cmp56398819, label %for.cond5644.preheader.preheader26, label %for.cond.cleanup5641

for.cond5644.preheader.preheader26:               ; preds = %for.cond5638.preheader
  %n.mod.vf11558 = urem i64 %7177, 24
  %n.vec11559 = sub nuw nsw i64 %7177, %n.mod.vf11558
  %cmp.n11563 = icmp eq i64 %n.mod.vf11558, 0
  br label %for.cond5644.preheader

for.cond5598.preheader:                           ; preds = %for.body.i8626, %for.cond.cleanup5601
  %tms5591.08816 = phi i32 [ %mul5634, %for.cond.cleanup5601 ], [ 1, %for.body.i8626 ]
  %t5584.08815 = phi i32 [ %inc5609, %for.cond.cleanup5601 ], [ 0, %for.body.i8626 ]
  %.capture_expr..casted5617.sroa.0.0.insert.ext = zext i32 %tms5591.08816 to i64
  br label %for.cond5604.preheader

for.cond5604.preheader:                           ; preds = %for.cond5598.preheader, %for.cond.cleanup5607
  %ths5597.08813 = phi i32 [ %mul5630, %for.cond.cleanup5607 ], [ 32, %for.cond5598.preheader ]
  %t5584.18812 = phi i32 [ %inc5609, %for.cond.cleanup5607 ], [ %t5584.08815, %for.cond5598.preheader ]
  %.capture_expr..casted5619.sroa.0.0.insert.ext = zext i32 %ths5597.08813 to i64
  br label %for.body5608

for.cond.cleanup5601:                             ; preds = %for.cond.cleanup5607
  %mul5634 = shl nsw i32 %tms5591.08816, 1
  %cmp5593 = icmp ult i32 %mul5634, 257
  br i1 %cmp5593, label %for.cond5598.preheader, label %for.cond5638.preheader

for.cond.cleanup5607:                             ; preds = %omp_offload.cont5624
  %mul5630 = shl nsw i32 %ths5597.08813, 1
  %cmp5599 = icmp ult i32 %mul5630, 1025
  br i1 %cmp5599, label %for.cond5604.preheader, label %for.cond.cleanup5601

for.body5608:                                     ; preds = %for.cond5604.preheader, %omp_offload.cont5624
  %sch5603.08810 = phi i32 [ 1000, %for.cond5604.preheader ], [ %mul5626, %omp_offload.cont5624 ]
  %t5584.28809 = phi i32 [ %t5584.18812, %for.cond5604.preheader ], [ %inc5609, %omp_offload.cont5624 ]
  %inc5609 = add nsw i32 %t5584.28809, 1
  %.capture_expr..casted5615.sroa.0.0.insert.ext = zext i32 %sch5603.08810 to i64
  store i64 %indvars.iv9457, i64* %6963, align 8
  store i64 %indvars.iv9457, i64* %6965, align 8
  store i8* %call3733, i8** %6966, align 8
  store i8* %call3733, i8** %6967, align 8
  store i8* %call3735, i8** %6968, align 8
  store i8* %call3735, i8** %6969, align 8
  store i8* %call3736, i8** %6970, align 8
  store i8* %call3736, i8** %6971, align 8
  store i64 %.capture_expr..casted5615.sroa.0.0.insert.ext, i64* %6973, align 8
  store i64 %.capture_expr..casted5615.sroa.0.0.insert.ext, i64* %6975, align 8
  store i64 %.capture_expr..casted5617.sroa.0.0.insert.ext, i64* %6977, align 8
  store i64 %.capture_expr..casted5617.sroa.0.0.insert.ext, i64* %6979, align 8
  store i64 %.capture_expr..casted5619.sroa.0.0.insert.ext, i64* %6981, align 8
  store i64 %.capture_expr..casted5619.sroa.0.0.insert.ext, i64* %6983, align 8
  %7179 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1577.region_id, i32 7, i8** nonnull %6962, i8** nonnull %6964, i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_sizes.273, i64 0, i64 0), i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_maptypes.274, i64 0, i64 0), i32 %tms5591.08816, i32 %ths5597.08813) #6
  %7180 = icmp eq i32 %7179, 0
  br i1 %7180, label %omp_offload.cont5624, label %omp_offload.failed5623

omp_offload.failed5623:                           ; preds = %for.body5608
  %7181 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7182 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7181, i32 %tms5591.08816, i32 %ths5597.08813) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 5, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64)* @.omp_outlined..264 to void (i32*, i32*, ...)*), i64 %indvars.iv9457, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted5615.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont5624

omp_offload.cont5624:                             ; preds = %for.body5608, %omp_offload.failed5623
  %mul5626 = mul nsw i32 %sch5603.08810, 3000
  %7183 = zext i32 %mul5626 to i64
  %cmp5605 = icmp ult i64 %indvars.iv9457, %7183
  br i1 %cmp5605, label %for.cond.cleanup5607, label %for.body5608

for.cond5644.preheader:                           ; preds = %for.cond5644.preheader.preheader26, %for.cond.cleanup5647
  %times5637.08820 = phi i32 [ %inc5662, %for.cond.cleanup5647 ], [ 0, %for.cond5644.preheader.preheader26 ]
  br label %vector.body11551

vector.body11551:                                 ; preds = %for.cond5644.preheader, %vector.body11551
  %index11560 = phi i64 [ %index.next11561, %vector.body11551 ], [ 0, %for.cond5644.preheader ]
  %7184 = getelementptr inbounds double, double* %1796, i64 %index11560
  %7185 = bitcast double* %7184 to <2 x double>*
  %wide.load11578 = load <2 x double>, <2 x double>* %7185, align 8, !tbaa !62
  %7186 = getelementptr inbounds double, double* %7184, i64 2
  %7187 = bitcast double* %7186 to <2 x double>*
  %wide.load11579 = load <2 x double>, <2 x double>* %7187, align 8, !tbaa !62
  %7188 = getelementptr inbounds double, double* %7184, i64 4
  %7189 = bitcast double* %7188 to <2 x double>*
  %wide.load11580 = load <2 x double>, <2 x double>* %7189, align 8, !tbaa !62
  %7190 = getelementptr inbounds double, double* %7184, i64 6
  %7191 = bitcast double* %7190 to <2 x double>*
  %wide.load11581 = load <2 x double>, <2 x double>* %7191, align 8, !tbaa !62
  %7192 = getelementptr inbounds double, double* %7184, i64 8
  %7193 = bitcast double* %7192 to <2 x double>*
  %wide.load11582 = load <2 x double>, <2 x double>* %7193, align 8, !tbaa !62
  %7194 = getelementptr inbounds double, double* %7184, i64 10
  %7195 = bitcast double* %7194 to <2 x double>*
  %wide.load11583 = load <2 x double>, <2 x double>* %7195, align 8, !tbaa !62
  %7196 = getelementptr inbounds double, double* %7184, i64 12
  %7197 = bitcast double* %7196 to <2 x double>*
  %wide.load11584 = load <2 x double>, <2 x double>* %7197, align 8, !tbaa !62
  %7198 = getelementptr inbounds double, double* %7184, i64 14
  %7199 = bitcast double* %7198 to <2 x double>*
  %wide.load11585 = load <2 x double>, <2 x double>* %7199, align 8, !tbaa !62
  %7200 = getelementptr inbounds double, double* %7184, i64 16
  %7201 = bitcast double* %7200 to <2 x double>*
  %wide.load11586 = load <2 x double>, <2 x double>* %7201, align 8, !tbaa !62
  %7202 = getelementptr inbounds double, double* %7184, i64 18
  %7203 = bitcast double* %7202 to <2 x double>*
  %wide.load11587 = load <2 x double>, <2 x double>* %7203, align 8, !tbaa !62
  %7204 = getelementptr inbounds double, double* %7184, i64 20
  %7205 = bitcast double* %7204 to <2 x double>*
  %wide.load11588 = load <2 x double>, <2 x double>* %7205, align 8, !tbaa !62
  %7206 = getelementptr inbounds double, double* %7184, i64 22
  %7207 = bitcast double* %7206 to <2 x double>*
  %wide.load11589 = load <2 x double>, <2 x double>* %7207, align 8, !tbaa !62
  %7208 = getelementptr inbounds double, double* %1797, i64 %index11560
  %7209 = bitcast double* %7208 to <2 x double>*
  %wide.load11590 = load <2 x double>, <2 x double>* %7209, align 8, !tbaa !62
  %7210 = getelementptr inbounds double, double* %7208, i64 2
  %7211 = bitcast double* %7210 to <2 x double>*
  %wide.load11591 = load <2 x double>, <2 x double>* %7211, align 8, !tbaa !62
  %7212 = getelementptr inbounds double, double* %7208, i64 4
  %7213 = bitcast double* %7212 to <2 x double>*
  %wide.load11592 = load <2 x double>, <2 x double>* %7213, align 8, !tbaa !62
  %7214 = getelementptr inbounds double, double* %7208, i64 6
  %7215 = bitcast double* %7214 to <2 x double>*
  %wide.load11593 = load <2 x double>, <2 x double>* %7215, align 8, !tbaa !62
  %7216 = getelementptr inbounds double, double* %7208, i64 8
  %7217 = bitcast double* %7216 to <2 x double>*
  %wide.load11594 = load <2 x double>, <2 x double>* %7217, align 8, !tbaa !62
  %7218 = getelementptr inbounds double, double* %7208, i64 10
  %7219 = bitcast double* %7218 to <2 x double>*
  %wide.load11595 = load <2 x double>, <2 x double>* %7219, align 8, !tbaa !62
  %7220 = getelementptr inbounds double, double* %7208, i64 12
  %7221 = bitcast double* %7220 to <2 x double>*
  %wide.load11596 = load <2 x double>, <2 x double>* %7221, align 8, !tbaa !62
  %7222 = getelementptr inbounds double, double* %7208, i64 14
  %7223 = bitcast double* %7222 to <2 x double>*
  %wide.load11597 = load <2 x double>, <2 x double>* %7223, align 8, !tbaa !62
  %7224 = getelementptr inbounds double, double* %7208, i64 16
  %7225 = bitcast double* %7224 to <2 x double>*
  %wide.load11598 = load <2 x double>, <2 x double>* %7225, align 8, !tbaa !62
  %7226 = getelementptr inbounds double, double* %7208, i64 18
  %7227 = bitcast double* %7226 to <2 x double>*
  %wide.load11599 = load <2 x double>, <2 x double>* %7227, align 8, !tbaa !62
  %7228 = getelementptr inbounds double, double* %7208, i64 20
  %7229 = bitcast double* %7228 to <2 x double>*
  %wide.load11600 = load <2 x double>, <2 x double>* %7229, align 8, !tbaa !62
  %7230 = getelementptr inbounds double, double* %7208, i64 22
  %7231 = bitcast double* %7230 to <2 x double>*
  %wide.load11601 = load <2 x double>, <2 x double>* %7231, align 8, !tbaa !62
  %7232 = fadd <2 x double> %wide.load11578, %wide.load11590
  %7233 = fadd <2 x double> %wide.load11579, %wide.load11591
  %7234 = fadd <2 x double> %wide.load11580, %wide.load11592
  %7235 = fadd <2 x double> %wide.load11581, %wide.load11593
  %7236 = fadd <2 x double> %wide.load11582, %wide.load11594
  %7237 = fadd <2 x double> %wide.load11583, %wide.load11595
  %7238 = fadd <2 x double> %wide.load11584, %wide.load11596
  %7239 = fadd <2 x double> %wide.load11585, %wide.load11597
  %7240 = fadd <2 x double> %wide.load11586, %wide.load11598
  %7241 = fadd <2 x double> %wide.load11587, %wide.load11599
  %7242 = fadd <2 x double> %wide.load11588, %wide.load11600
  %7243 = fadd <2 x double> %wide.load11589, %wide.load11601
  %7244 = getelementptr inbounds double, double* %1795, i64 %index11560
  %7245 = bitcast double* %7244 to <2 x double>*
  %wide.load11602 = load <2 x double>, <2 x double>* %7245, align 8, !tbaa !62
  %7246 = getelementptr inbounds double, double* %7244, i64 2
  %7247 = bitcast double* %7246 to <2 x double>*
  %wide.load11603 = load <2 x double>, <2 x double>* %7247, align 8, !tbaa !62
  %7248 = getelementptr inbounds double, double* %7244, i64 4
  %7249 = bitcast double* %7248 to <2 x double>*
  %wide.load11604 = load <2 x double>, <2 x double>* %7249, align 8, !tbaa !62
  %7250 = getelementptr inbounds double, double* %7244, i64 6
  %7251 = bitcast double* %7250 to <2 x double>*
  %wide.load11605 = load <2 x double>, <2 x double>* %7251, align 8, !tbaa !62
  %7252 = getelementptr inbounds double, double* %7244, i64 8
  %7253 = bitcast double* %7252 to <2 x double>*
  %wide.load11606 = load <2 x double>, <2 x double>* %7253, align 8, !tbaa !62
  %7254 = getelementptr inbounds double, double* %7244, i64 10
  %7255 = bitcast double* %7254 to <2 x double>*
  %wide.load11607 = load <2 x double>, <2 x double>* %7255, align 8, !tbaa !62
  %7256 = getelementptr inbounds double, double* %7244, i64 12
  %7257 = bitcast double* %7256 to <2 x double>*
  %wide.load11608 = load <2 x double>, <2 x double>* %7257, align 8, !tbaa !62
  %7258 = getelementptr inbounds double, double* %7244, i64 14
  %7259 = bitcast double* %7258 to <2 x double>*
  %wide.load11609 = load <2 x double>, <2 x double>* %7259, align 8, !tbaa !62
  %7260 = getelementptr inbounds double, double* %7244, i64 16
  %7261 = bitcast double* %7260 to <2 x double>*
  %wide.load11610 = load <2 x double>, <2 x double>* %7261, align 8, !tbaa !62
  %7262 = getelementptr inbounds double, double* %7244, i64 18
  %7263 = bitcast double* %7262 to <2 x double>*
  %wide.load11611 = load <2 x double>, <2 x double>* %7263, align 8, !tbaa !62
  %7264 = getelementptr inbounds double, double* %7244, i64 20
  %7265 = bitcast double* %7264 to <2 x double>*
  %wide.load11612 = load <2 x double>, <2 x double>* %7265, align 8, !tbaa !62
  %7266 = getelementptr inbounds double, double* %7244, i64 22
  %7267 = bitcast double* %7266 to <2 x double>*
  %wide.load11613 = load <2 x double>, <2 x double>* %7267, align 8, !tbaa !62
  %7268 = fadd <2 x double> %7232, %wide.load11602
  %7269 = fadd <2 x double> %7233, %wide.load11603
  %7270 = fadd <2 x double> %7234, %wide.load11604
  %7271 = fadd <2 x double> %7235, %wide.load11605
  %7272 = fadd <2 x double> %7236, %wide.load11606
  %7273 = fadd <2 x double> %7237, %wide.load11607
  %7274 = fadd <2 x double> %7238, %wide.load11608
  %7275 = fadd <2 x double> %7239, %wide.load11609
  %7276 = fadd <2 x double> %7240, %wide.load11610
  %7277 = fadd <2 x double> %7241, %wide.load11611
  %7278 = fadd <2 x double> %7242, %wide.load11612
  %7279 = fadd <2 x double> %7243, %wide.load11613
  store <2 x double> %7268, <2 x double>* %7245, align 8, !tbaa !62
  store <2 x double> %7269, <2 x double>* %7247, align 8, !tbaa !62
  store <2 x double> %7270, <2 x double>* %7249, align 8, !tbaa !62
  store <2 x double> %7271, <2 x double>* %7251, align 8, !tbaa !62
  store <2 x double> %7272, <2 x double>* %7253, align 8, !tbaa !62
  store <2 x double> %7273, <2 x double>* %7255, align 8, !tbaa !62
  store <2 x double> %7274, <2 x double>* %7257, align 8, !tbaa !62
  store <2 x double> %7275, <2 x double>* %7259, align 8, !tbaa !62
  store <2 x double> %7276, <2 x double>* %7261, align 8, !tbaa !62
  store <2 x double> %7277, <2 x double>* %7263, align 8, !tbaa !62
  store <2 x double> %7278, <2 x double>* %7265, align 8, !tbaa !62
  store <2 x double> %7279, <2 x double>* %7267, align 8, !tbaa !62
  %index.next11561 = add nuw nsw i64 %index11560, 24
  %7280 = icmp eq i64 %index.next11561, %n.vec11559
  br i1 %7280, label %middle.block11552, label %vector.body11551, !llvm.loop !126

middle.block11552:                                ; preds = %vector.body11551
  br i1 %cmp.n11563, label %for.cond.cleanup5647, label %for.body5648

for.cond.cleanup5641:                             ; preds = %for.cond.cleanup5647, %for.body.i8626, %for.cond5638.preheader
  store i8* %call3733, i8** %6984, align 8
  store i8* %call3733, i8** %6985, align 8
  store i64 %7178, i64* %6986, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %6984, i8** nonnull %6985, i64* nonnull %6986, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body5674

for.cond.cleanup5647:                             ; preds = %for.body5648, %middle.block11552
  %inc5662 = add nuw nsw i32 %times5637.08820, 1
  %exitcond9454 = icmp eq i32 %inc5662, %inc5609
  br i1 %exitcond9454, label %for.cond.cleanup5641, label %for.cond5644.preheader

for.body5648:                                     ; preds = %middle.block11552, %for.body5648
  %indvars.iv9451 = phi i64 [ %indvars.iv.next9452, %for.body5648 ], [ %n.vec11559, %middle.block11552 ]
  %arrayidx5650 = getelementptr inbounds double, double* %1796, i64 %indvars.iv9451
  %7281 = load double, double* %arrayidx5650, align 8, !tbaa !62
  %arrayidx5652 = getelementptr inbounds double, double* %1797, i64 %indvars.iv9451
  %7282 = load double, double* %arrayidx5652, align 8, !tbaa !62
  %add5653 = fadd double %7281, %7282
  %arrayidx5655 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9451
  %7283 = load double, double* %arrayidx5655, align 8, !tbaa !62
  %add5656 = fadd double %7283, %add5653
  store double %add5656, double* %arrayidx5655, align 8, !tbaa !62
  %indvars.iv.next9452 = add nuw nsw i64 %indvars.iv9451, 1
  %exitcond9453 = icmp eq i64 %indvars.iv.next9452, %indvars.iv9457
  br i1 %exitcond9453, label %for.cond.cleanup5647, label %for.body5648, !llvm.loop !127

for.body5674:                                     ; preds = %for.inc5688.7, %for.cond.cleanup5641
  %indvars.iv9455 = phi i64 [ 0, %for.cond.cleanup5641 ], [ %indvars.iv.next9456.7, %for.inc5688.7 ]
  %arrayidx5676 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9455
  %7284 = load double, double* %arrayidx5676, align 8, !tbaa !62
  %arrayidx5678 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9455
  %7285 = load double, double* %arrayidx5678, align 8, !tbaa !62
  %cmp5679 = fcmp une double %7284, %7285
  br i1 %cmp5679, label %cleanup5698, label %for.inc5688

for.inc5688:                                      ; preds = %for.body5674
  %indvars.iv.next9456 = or i64 %indvars.iv9455, 1
  %arrayidx5676.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9456
  %7286 = load double, double* %arrayidx5676.1, align 8, !tbaa !62
  %arrayidx5678.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9456
  %7287 = load double, double* %arrayidx5678.1, align 8, !tbaa !62
  %cmp5679.1 = fcmp une double %7286, %7287
  br i1 %cmp5679.1, label %cleanup5698, label %for.inc5688.1

for.inc5696:                                      ; preds = %for.inc5688.7
  %indvars.iv.next9458 = add nuw nsw i64 %indvars.iv9457, 5000
  %cmp5580 = icmp ult i64 %indvars.iv.next9458, 25000
  %indvar.next11555 = add nuw nsw i64 %indvar11554, 1
  br i1 %cmp5580, label %for.body.i8626.preheader, label %for.end5700

cleanup5698:                                      ; preds = %for.inc5688.6, %for.inc5688.5, %for.inc5688.4, %for.inc5688.3, %for.inc5688.2, %for.inc5688.1, %for.inc5688, %for.body5674
  %indvars.iv9455.lcssa = phi i64 [ %indvars.iv9455, %for.body5674 ], [ %indvars.iv.next9456, %for.inc5688 ], [ %indvars.iv.next9456.1, %for.inc5688.1 ], [ %indvars.iv.next9456.2, %for.inc5688.2 ], [ %indvars.iv.next9456.3, %for.inc5688.3 ], [ %indvars.iv.next9456.4, %for.inc5688.4 ], [ %indvars.iv.next9456.5, %for.inc5688.5 ], [ %indvars.iv.next9456.6, %for.inc5688.6 ]
  %.lcssa11774 = phi double [ %7284, %for.body5674 ], [ %7286, %for.inc5688 ], [ %7740, %for.inc5688.1 ], [ %7742, %for.inc5688.2 ], [ %7744, %for.inc5688.3 ], [ %7746, %for.inc5688.4 ], [ %7748, %for.inc5688.5 ], [ %7750, %for.inc5688.6 ]
  %.lcssa11772 = phi double [ %7285, %for.body5674 ], [ %7287, %for.inc5688 ], [ %7741, %for.inc5688.1 ], [ %7743, %for.inc5688.2 ], [ %7745, %for.inc5688.3 ], [ %7747, %for.inc5688.4 ], [ %7749, %for.inc5688.5 ], [ %7751, %for.inc5688.6 ]
  %7288 = trunc i64 %indvars.iv9457 to i32
  %7289 = trunc i64 %indvars.iv9455.lcssa to i32
  %call5686 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %7288, i32 signext %7289, double %.lcssa11774, double %.lcssa11772)
  br label %cleanup5832

for.end5700:                                      ; preds = %for.inc5696
  %puts8275 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %puts8276 = call i32 @puts(i8* getelementptr inbounds ([45 x i8], [45 x i8]* @str.365, i64 0, i64 0))
  %7290 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5713, i64 0, i64 0
  %7291 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5714, i64 0, i64 0
  %7292 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5715, i64 0, i64 0
  %7293 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5713, i64 0, i64 1
  %7294 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5714, i64 0, i64 1
  %7295 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5715, i64 0, i64 1
  %7296 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5713, i64 0, i64 2
  %7297 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5714, i64 0, i64 2
  %7298 = getelementptr inbounds [3 x i64], [3 x i64]* %.offload_sizes5715, i64 0, i64 2
  %7299 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5746, i64 0, i64 0
  %7300 = bitcast [7 x i8*]* %.offload_baseptrs5746 to i64*
  %7301 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5747, i64 0, i64 0
  %7302 = bitcast [7 x i8*]* %.offload_ptrs5747 to i64*
  %7303 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5746, i64 0, i64 1
  %7304 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5747, i64 0, i64 1
  %7305 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5746, i64 0, i64 2
  %7306 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5747, i64 0, i64 2
  %7307 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5746, i64 0, i64 3
  %7308 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5747, i64 0, i64 3
  %7309 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5746, i64 0, i64 4
  %7310 = bitcast i8** %7309 to i64*
  %7311 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5747, i64 0, i64 4
  %7312 = bitcast i8** %7311 to i64*
  %7313 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5746, i64 0, i64 5
  %7314 = bitcast i8** %7313 to i64*
  %7315 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5747, i64 0, i64 5
  %7316 = bitcast i8** %7315 to i64*
  %7317 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_baseptrs5746, i64 0, i64 6
  %7318 = bitcast i8** %7317 to i64*
  %7319 = getelementptr inbounds [7 x i8*], [7 x i8*]* %.offload_ptrs5747, i64 0, i64 6
  %7320 = bitcast i8** %7319 to i64*
  %7321 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_baseptrs5791, i64 0, i64 0
  %7322 = getelementptr inbounds [1 x i8*], [1 x i8*]* %.offload_ptrs5792, i64 0, i64 0
  %7323 = getelementptr inbounds [1 x i64], [1 x i64]* %.offload_sizes5793, i64 0, i64 0
  %7324 = bitcast i8* %arrayidx.i to <2 x double>*
  %7325 = bitcast i8* %arrayidx2.i to <2 x double>*
  %7326 = bitcast i8* %arrayidx5.i to <2 x double>*
  %7327 = bitcast i8* %arrayidx8.i to <2 x double>*
  %7328 = bitcast i8* %arrayidx.i.2 to <2 x double>*
  %7329 = bitcast i8* %arrayidx2.i.2 to <2 x double>*
  %7330 = bitcast i8* %arrayidx5.i.2 to <2 x double>*
  %7331 = bitcast i8* %arrayidx8.i.2 to <2 x double>*
  %7332 = bitcast i8* %arrayidx.i.4 to <2 x double>*
  %7333 = bitcast i8* %arrayidx2.i.4 to <2 x double>*
  %7334 = bitcast i8* %arrayidx5.i.4 to <2 x double>*
  %7335 = bitcast i8* %arrayidx8.i.4 to <2 x double>*
  %7336 = bitcast i8* %arrayidx.i.6 to <2 x double>*
  %7337 = bitcast i8* %arrayidx2.i.6 to <2 x double>*
  %7338 = bitcast i8* %arrayidx5.i.6 to <2 x double>*
  %7339 = bitcast i8* %arrayidx8.i.6 to <2 x double>*
  %7340 = bitcast i8* %arrayidx.i.8 to <2 x double>*
  %7341 = bitcast i8* %arrayidx2.i.8 to <2 x double>*
  %7342 = bitcast i8* %arrayidx5.i.8 to <2 x double>*
  %7343 = bitcast i8* %arrayidx8.i.8 to <2 x double>*
  %7344 = bitcast i8* %arrayidx.i.10 to <2 x double>*
  %7345 = bitcast i8* %arrayidx2.i.10 to <2 x double>*
  %7346 = bitcast i8* %arrayidx5.i.10 to <2 x double>*
  %7347 = bitcast i8* %arrayidx8.i.10 to <2 x double>*
  %7348 = bitcast i8* %arrayidx.i.12 to <2 x double>*
  %7349 = bitcast i8* %arrayidx2.i.12 to <2 x double>*
  %7350 = bitcast i8* %arrayidx5.i.12 to <2 x double>*
  %7351 = bitcast i8* %arrayidx8.i.12 to <2 x double>*
  %7352 = bitcast i8* %arrayidx.i.14 to <2 x double>*
  %7353 = bitcast i8* %arrayidx2.i.14 to <2 x double>*
  %7354 = bitcast i8* %arrayidx5.i.14 to <2 x double>*
  %7355 = bitcast i8* %arrayidx8.i.14 to <2 x double>*
  br label %for.body.i8641.preheader

for.body.i8641.preheader:                         ; preds = %for.end5700, %for.inc5821
  %indvar11653 = phi i64 [ 0, %for.end5700 ], [ %indvar.next11654, %for.inc5821 ]
  %indvars.iv9449 = phi i64 [ 32, %for.end5700 ], [ %indvars.iv.next9450, %for.inc5821 ]
  %7356 = mul nuw nsw i64 %indvar11653, 5000
  br label %vector.body11713

vector.body11713:                                 ; preds = %vector.body11713, %for.body.i8641.preheader
  %index11717 = phi i64 [ 0, %for.body.i8641.preheader ], [ %index.next11718, %vector.body11713 ]
  %vec.ind11735 = phi <2 x i32> [ <i32 0, i32 1>, %for.body.i8641.preheader ], [ %vec.ind.next11748, %vector.body11713 ]
  %step.add11736 = add <2 x i32> %vec.ind11735, <i32 2, i32 2>
  %step.add11737 = add <2 x i32> %vec.ind11735, <i32 4, i32 4>
  %step.add11738 = add <2 x i32> %vec.ind11735, <i32 6, i32 6>
  %step.add11739 = add <2 x i32> %vec.ind11735, <i32 8, i32 8>
  %step.add11740 = add <2 x i32> %vec.ind11735, <i32 10, i32 10>
  %step.add11741 = add <2 x i32> %vec.ind11735, <i32 12, i32 12>
  %step.add11742 = add <2 x i32> %vec.ind11735, <i32 14, i32 14>
  %step.add11743 = add <2 x i32> %vec.ind11735, <i32 16, i32 16>
  %step.add11744 = add <2 x i32> %vec.ind11735, <i32 18, i32 18>
  %step.add11745 = add <2 x i32> %vec.ind11735, <i32 20, i32 20>
  %step.add11746 = add <2 x i32> %vec.ind11735, <i32 22, i32 22>
  %7357 = sitofp <2 x i32> %vec.ind11735 to <2 x double>
  %7358 = sitofp <2 x i32> %step.add11736 to <2 x double>
  %7359 = sitofp <2 x i32> %step.add11737 to <2 x double>
  %7360 = sitofp <2 x i32> %step.add11738 to <2 x double>
  %7361 = sitofp <2 x i32> %step.add11739 to <2 x double>
  %7362 = sitofp <2 x i32> %step.add11740 to <2 x double>
  %7363 = sitofp <2 x i32> %step.add11741 to <2 x double>
  %7364 = sitofp <2 x i32> %step.add11742 to <2 x double>
  %7365 = sitofp <2 x i32> %step.add11743 to <2 x double>
  %7366 = sitofp <2 x i32> %step.add11744 to <2 x double>
  %7367 = sitofp <2 x i32> %step.add11745 to <2 x double>
  %7368 = sitofp <2 x i32> %step.add11746 to <2 x double>
  %7369 = getelementptr inbounds double, double* %1795, i64 %index11717
  %7370 = bitcast double* %7369 to <2 x double>*
  store <2 x double> %7357, <2 x double>* %7370, align 8, !tbaa !62
  %7371 = getelementptr inbounds double, double* %7369, i64 2
  %7372 = bitcast double* %7371 to <2 x double>*
  store <2 x double> %7358, <2 x double>* %7372, align 8, !tbaa !62
  %7373 = getelementptr inbounds double, double* %7369, i64 4
  %7374 = bitcast double* %7373 to <2 x double>*
  store <2 x double> %7359, <2 x double>* %7374, align 8, !tbaa !62
  %7375 = getelementptr inbounds double, double* %7369, i64 6
  %7376 = bitcast double* %7375 to <2 x double>*
  store <2 x double> %7360, <2 x double>* %7376, align 8, !tbaa !62
  %7377 = getelementptr inbounds double, double* %7369, i64 8
  %7378 = bitcast double* %7377 to <2 x double>*
  store <2 x double> %7361, <2 x double>* %7378, align 8, !tbaa !62
  %7379 = getelementptr inbounds double, double* %7369, i64 10
  %7380 = bitcast double* %7379 to <2 x double>*
  store <2 x double> %7362, <2 x double>* %7380, align 8, !tbaa !62
  %7381 = getelementptr inbounds double, double* %7369, i64 12
  %7382 = bitcast double* %7381 to <2 x double>*
  store <2 x double> %7363, <2 x double>* %7382, align 8, !tbaa !62
  %7383 = getelementptr inbounds double, double* %7369, i64 14
  %7384 = bitcast double* %7383 to <2 x double>*
  store <2 x double> %7364, <2 x double>* %7384, align 8, !tbaa !62
  %7385 = getelementptr inbounds double, double* %7369, i64 16
  %7386 = bitcast double* %7385 to <2 x double>*
  store <2 x double> %7365, <2 x double>* %7386, align 8, !tbaa !62
  %7387 = getelementptr inbounds double, double* %7369, i64 18
  %7388 = bitcast double* %7387 to <2 x double>*
  store <2 x double> %7366, <2 x double>* %7388, align 8, !tbaa !62
  %7389 = getelementptr inbounds double, double* %7369, i64 20
  %7390 = bitcast double* %7389 to <2 x double>*
  store <2 x double> %7367, <2 x double>* %7390, align 8, !tbaa !62
  %7391 = getelementptr inbounds double, double* %7369, i64 22
  %7392 = bitcast double* %7391 to <2 x double>*
  store <2 x double> %7368, <2 x double>* %7392, align 8, !tbaa !62
  %7393 = getelementptr inbounds double, double* %1794, i64 %index11717
  %7394 = bitcast double* %7393 to <2 x double>*
  store <2 x double> %7357, <2 x double>* %7394, align 8, !tbaa !62
  %7395 = getelementptr inbounds double, double* %7393, i64 2
  %7396 = bitcast double* %7395 to <2 x double>*
  store <2 x double> %7358, <2 x double>* %7396, align 8, !tbaa !62
  %7397 = getelementptr inbounds double, double* %7393, i64 4
  %7398 = bitcast double* %7397 to <2 x double>*
  store <2 x double> %7359, <2 x double>* %7398, align 8, !tbaa !62
  %7399 = getelementptr inbounds double, double* %7393, i64 6
  %7400 = bitcast double* %7399 to <2 x double>*
  store <2 x double> %7360, <2 x double>* %7400, align 8, !tbaa !62
  %7401 = getelementptr inbounds double, double* %7393, i64 8
  %7402 = bitcast double* %7401 to <2 x double>*
  store <2 x double> %7361, <2 x double>* %7402, align 8, !tbaa !62
  %7403 = getelementptr inbounds double, double* %7393, i64 10
  %7404 = bitcast double* %7403 to <2 x double>*
  store <2 x double> %7362, <2 x double>* %7404, align 8, !tbaa !62
  %7405 = getelementptr inbounds double, double* %7393, i64 12
  %7406 = bitcast double* %7405 to <2 x double>*
  store <2 x double> %7363, <2 x double>* %7406, align 8, !tbaa !62
  %7407 = getelementptr inbounds double, double* %7393, i64 14
  %7408 = bitcast double* %7407 to <2 x double>*
  store <2 x double> %7364, <2 x double>* %7408, align 8, !tbaa !62
  %7409 = getelementptr inbounds double, double* %7393, i64 16
  %7410 = bitcast double* %7409 to <2 x double>*
  store <2 x double> %7365, <2 x double>* %7410, align 8, !tbaa !62
  %7411 = getelementptr inbounds double, double* %7393, i64 18
  %7412 = bitcast double* %7411 to <2 x double>*
  store <2 x double> %7366, <2 x double>* %7412, align 8, !tbaa !62
  %7413 = getelementptr inbounds double, double* %7393, i64 20
  %7414 = bitcast double* %7413 to <2 x double>*
  store <2 x double> %7367, <2 x double>* %7414, align 8, !tbaa !62
  %7415 = getelementptr inbounds double, double* %7393, i64 22
  %7416 = bitcast double* %7415 to <2 x double>*
  store <2 x double> %7368, <2 x double>* %7416, align 8, !tbaa !62
  %7417 = shl <2 x i32> %vec.ind11735, <i32 1, i32 1>
  %7418 = shl <2 x i32> %step.add11736, <i32 1, i32 1>
  %7419 = shl <2 x i32> %step.add11737, <i32 1, i32 1>
  %7420 = shl <2 x i32> %step.add11738, <i32 1, i32 1>
  %7421 = shl <2 x i32> %step.add11739, <i32 1, i32 1>
  %7422 = shl <2 x i32> %step.add11740, <i32 1, i32 1>
  %7423 = shl <2 x i32> %step.add11741, <i32 1, i32 1>
  %7424 = shl <2 x i32> %step.add11742, <i32 1, i32 1>
  %7425 = shl <2 x i32> %step.add11743, <i32 1, i32 1>
  %7426 = shl <2 x i32> %step.add11744, <i32 1, i32 1>
  %7427 = shl <2 x i32> %step.add11745, <i32 1, i32 1>
  %7428 = shl <2 x i32> %step.add11746, <i32 1, i32 1>
  %7429 = sitofp <2 x i32> %7417 to <2 x double>
  %7430 = sitofp <2 x i32> %7418 to <2 x double>
  %7431 = sitofp <2 x i32> %7419 to <2 x double>
  %7432 = sitofp <2 x i32> %7420 to <2 x double>
  %7433 = sitofp <2 x i32> %7421 to <2 x double>
  %7434 = sitofp <2 x i32> %7422 to <2 x double>
  %7435 = sitofp <2 x i32> %7423 to <2 x double>
  %7436 = sitofp <2 x i32> %7424 to <2 x double>
  %7437 = sitofp <2 x i32> %7425 to <2 x double>
  %7438 = sitofp <2 x i32> %7426 to <2 x double>
  %7439 = sitofp <2 x i32> %7427 to <2 x double>
  %7440 = sitofp <2 x i32> %7428 to <2 x double>
  %7441 = getelementptr inbounds double, double* %1796, i64 %index11717
  %7442 = bitcast double* %7441 to <2 x double>*
  store <2 x double> %7429, <2 x double>* %7442, align 8, !tbaa !62
  %7443 = getelementptr inbounds double, double* %7441, i64 2
  %7444 = bitcast double* %7443 to <2 x double>*
  store <2 x double> %7430, <2 x double>* %7444, align 8, !tbaa !62
  %7445 = getelementptr inbounds double, double* %7441, i64 4
  %7446 = bitcast double* %7445 to <2 x double>*
  store <2 x double> %7431, <2 x double>* %7446, align 8, !tbaa !62
  %7447 = getelementptr inbounds double, double* %7441, i64 6
  %7448 = bitcast double* %7447 to <2 x double>*
  store <2 x double> %7432, <2 x double>* %7448, align 8, !tbaa !62
  %7449 = getelementptr inbounds double, double* %7441, i64 8
  %7450 = bitcast double* %7449 to <2 x double>*
  store <2 x double> %7433, <2 x double>* %7450, align 8, !tbaa !62
  %7451 = getelementptr inbounds double, double* %7441, i64 10
  %7452 = bitcast double* %7451 to <2 x double>*
  store <2 x double> %7434, <2 x double>* %7452, align 8, !tbaa !62
  %7453 = getelementptr inbounds double, double* %7441, i64 12
  %7454 = bitcast double* %7453 to <2 x double>*
  store <2 x double> %7435, <2 x double>* %7454, align 8, !tbaa !62
  %7455 = getelementptr inbounds double, double* %7441, i64 14
  %7456 = bitcast double* %7455 to <2 x double>*
  store <2 x double> %7436, <2 x double>* %7456, align 8, !tbaa !62
  %7457 = getelementptr inbounds double, double* %7441, i64 16
  %7458 = bitcast double* %7457 to <2 x double>*
  store <2 x double> %7437, <2 x double>* %7458, align 8, !tbaa !62
  %7459 = getelementptr inbounds double, double* %7441, i64 18
  %7460 = bitcast double* %7459 to <2 x double>*
  store <2 x double> %7438, <2 x double>* %7460, align 8, !tbaa !62
  %7461 = getelementptr inbounds double, double* %7441, i64 20
  %7462 = bitcast double* %7461 to <2 x double>*
  store <2 x double> %7439, <2 x double>* %7462, align 8, !tbaa !62
  %7463 = getelementptr inbounds double, double* %7441, i64 22
  %7464 = bitcast double* %7463 to <2 x double>*
  store <2 x double> %7440, <2 x double>* %7464, align 8, !tbaa !62
  %7465 = add <2 x i32> %vec.ind11735, <i32 -3, i32 -3>
  %7466 = add <2 x i32> %vec.ind11735, <i32 -1, i32 -1>
  %7467 = add <2 x i32> %vec.ind11735, <i32 1, i32 1>
  %7468 = add <2 x i32> %vec.ind11735, <i32 3, i32 3>
  %7469 = add <2 x i32> %vec.ind11735, <i32 5, i32 5>
  %7470 = add <2 x i32> %vec.ind11735, <i32 7, i32 7>
  %7471 = add <2 x i32> %vec.ind11735, <i32 9, i32 9>
  %7472 = add <2 x i32> %vec.ind11735, <i32 11, i32 11>
  %7473 = add <2 x i32> %vec.ind11735, <i32 13, i32 13>
  %7474 = add <2 x i32> %vec.ind11735, <i32 15, i32 15>
  %7475 = add <2 x i32> %vec.ind11735, <i32 17, i32 17>
  %7476 = add <2 x i32> %vec.ind11735, <i32 19, i32 19>
  %7477 = sitofp <2 x i32> %7465 to <2 x double>
  %7478 = sitofp <2 x i32> %7466 to <2 x double>
  %7479 = sitofp <2 x i32> %7467 to <2 x double>
  %7480 = sitofp <2 x i32> %7468 to <2 x double>
  %7481 = sitofp <2 x i32> %7469 to <2 x double>
  %7482 = sitofp <2 x i32> %7470 to <2 x double>
  %7483 = sitofp <2 x i32> %7471 to <2 x double>
  %7484 = sitofp <2 x i32> %7472 to <2 x double>
  %7485 = sitofp <2 x i32> %7473 to <2 x double>
  %7486 = sitofp <2 x i32> %7474 to <2 x double>
  %7487 = sitofp <2 x i32> %7475 to <2 x double>
  %7488 = sitofp <2 x i32> %7476 to <2 x double>
  %7489 = getelementptr inbounds double, double* %1797, i64 %index11717
  %7490 = bitcast double* %7489 to <2 x double>*
  store <2 x double> %7477, <2 x double>* %7490, align 8, !tbaa !62
  %7491 = getelementptr inbounds double, double* %7489, i64 2
  %7492 = bitcast double* %7491 to <2 x double>*
  store <2 x double> %7478, <2 x double>* %7492, align 8, !tbaa !62
  %7493 = getelementptr inbounds double, double* %7489, i64 4
  %7494 = bitcast double* %7493 to <2 x double>*
  store <2 x double> %7479, <2 x double>* %7494, align 8, !tbaa !62
  %7495 = getelementptr inbounds double, double* %7489, i64 6
  %7496 = bitcast double* %7495 to <2 x double>*
  store <2 x double> %7480, <2 x double>* %7496, align 8, !tbaa !62
  %7497 = getelementptr inbounds double, double* %7489, i64 8
  %7498 = bitcast double* %7497 to <2 x double>*
  store <2 x double> %7481, <2 x double>* %7498, align 8, !tbaa !62
  %7499 = getelementptr inbounds double, double* %7489, i64 10
  %7500 = bitcast double* %7499 to <2 x double>*
  store <2 x double> %7482, <2 x double>* %7500, align 8, !tbaa !62
  %7501 = getelementptr inbounds double, double* %7489, i64 12
  %7502 = bitcast double* %7501 to <2 x double>*
  store <2 x double> %7483, <2 x double>* %7502, align 8, !tbaa !62
  %7503 = getelementptr inbounds double, double* %7489, i64 14
  %7504 = bitcast double* %7503 to <2 x double>*
  store <2 x double> %7484, <2 x double>* %7504, align 8, !tbaa !62
  %7505 = getelementptr inbounds double, double* %7489, i64 16
  %7506 = bitcast double* %7505 to <2 x double>*
  store <2 x double> %7485, <2 x double>* %7506, align 8, !tbaa !62
  %7507 = getelementptr inbounds double, double* %7489, i64 18
  %7508 = bitcast double* %7507 to <2 x double>*
  store <2 x double> %7486, <2 x double>* %7508, align 8, !tbaa !62
  %7509 = getelementptr inbounds double, double* %7489, i64 20
  %7510 = bitcast double* %7509 to <2 x double>*
  store <2 x double> %7487, <2 x double>* %7510, align 8, !tbaa !62
  %7511 = getelementptr inbounds double, double* %7489, i64 22
  %7512 = bitcast double* %7511 to <2 x double>*
  store <2 x double> %7488, <2 x double>* %7512, align 8, !tbaa !62
  %index.next11718 = add nuw nsw i64 %index11717, 24
  %vec.ind.next11748 = add <2 x i32> %vec.ind11735, <i32 24, i32 24>
  %7513 = icmp eq i64 %index.next11718, 24984
  br i1 %7513, label %for.body.i8641, label %vector.body11713, !llvm.loop !128

for.body.i8641:                                   ; preds = %vector.body11713
  %7514 = add nuw nsw i64 %7356, 32
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %7324, align 8, !tbaa !62
  store <2 x double> <double 2.498400e+04, double 2.498500e+04>, <2 x double>* %7325, align 8, !tbaa !62
  store <2 x double> <double 4.996800e+04, double 4.997000e+04>, <2 x double>* %7326, align 8, !tbaa !62
  store <2 x double> <double 2.498100e+04, double 2.498200e+04>, <2 x double>* %7327, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %7328, align 8, !tbaa !62
  store <2 x double> <double 2.498600e+04, double 2.498700e+04>, <2 x double>* %7329, align 8, !tbaa !62
  store <2 x double> <double 4.997200e+04, double 4.997400e+04>, <2 x double>* %7330, align 8, !tbaa !62
  store <2 x double> <double 2.498300e+04, double 2.498400e+04>, <2 x double>* %7331, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %7332, align 8, !tbaa !62
  store <2 x double> <double 2.498800e+04, double 2.498900e+04>, <2 x double>* %7333, align 8, !tbaa !62
  store <2 x double> <double 4.997600e+04, double 4.997800e+04>, <2 x double>* %7334, align 8, !tbaa !62
  store <2 x double> <double 2.498500e+04, double 2.498600e+04>, <2 x double>* %7335, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %7336, align 8, !tbaa !62
  store <2 x double> <double 2.499000e+04, double 2.499100e+04>, <2 x double>* %7337, align 8, !tbaa !62
  store <2 x double> <double 4.998000e+04, double 4.998200e+04>, <2 x double>* %7338, align 8, !tbaa !62
  store <2 x double> <double 2.498700e+04, double 2.498800e+04>, <2 x double>* %7339, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %7340, align 8, !tbaa !62
  store <2 x double> <double 2.499200e+04, double 2.499300e+04>, <2 x double>* %7341, align 8, !tbaa !62
  store <2 x double> <double 4.998400e+04, double 4.998600e+04>, <2 x double>* %7342, align 8, !tbaa !62
  store <2 x double> <double 2.498900e+04, double 2.499000e+04>, <2 x double>* %7343, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %7344, align 8, !tbaa !62
  store <2 x double> <double 2.499400e+04, double 2.499500e+04>, <2 x double>* %7345, align 8, !tbaa !62
  store <2 x double> <double 4.998800e+04, double 4.999000e+04>, <2 x double>* %7346, align 8, !tbaa !62
  store <2 x double> <double 2.499100e+04, double 2.499200e+04>, <2 x double>* %7347, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %7348, align 8, !tbaa !62
  store <2 x double> <double 2.499600e+04, double 2.499700e+04>, <2 x double>* %7349, align 8, !tbaa !62
  store <2 x double> <double 4.999200e+04, double 4.999400e+04>, <2 x double>* %7350, align 8, !tbaa !62
  store <2 x double> <double 2.499300e+04, double 2.499400e+04>, <2 x double>* %7351, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %7352, align 8, !tbaa !62
  store <2 x double> <double 2.499800e+04, double 2.499900e+04>, <2 x double>* %7353, align 8, !tbaa !62
  store <2 x double> <double 4.999600e+04, double 4.999800e+04>, <2 x double>* %7354, align 8, !tbaa !62
  store <2 x double> <double 2.499500e+04, double 2.499600e+04>, <2 x double>* %7355, align 8, !tbaa !62
  %7515 = shl nuw nsw i64 %indvars.iv9449, 3
  store i8* %call3733, i8** %7290, align 8
  store i8* %call3733, i8** %7291, align 8
  store i64 %7515, i64* %7292, align 8
  store i8* %call3735, i8** %7293, align 8
  store i8* %call3735, i8** %7294, align 8
  store i64 %7515, i64* %7295, align 8
  store i8* %call3736, i8** %7296, align 8
  store i8* %call3736, i8** %7297, align 8
  store i64 %7515, i64* %7298, align 8
  call void @__tgt_target_data_update(i64 -1, i32 3, i8** nonnull %7290, i8** nonnull %7291, i64* nonnull %7292, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.270, i64 0, i64 0)) #6
  %cmp57308794 = icmp ult i64 %indvars.iv9449, 1000
  br i1 %cmp57308794, label %for.cond.cleanup5766, label %for.cond5723.preheader

for.cond5763.preheader:                           ; preds = %for.cond.cleanup5726
  %cmp57648804 = icmp sgt i32 %t5709.28795, -1
  br i1 %cmp57648804, label %for.cond5769.preheader.preheader23, label %for.cond.cleanup5766

for.cond5769.preheader.preheader23:               ; preds = %for.cond5763.preheader
  %n.mod.vf11657 = urem i64 %7514, 24
  %n.vec11658 = sub nuw nsw i64 %7514, %n.mod.vf11657
  %cmp.n11662 = icmp eq i64 %n.mod.vf11657, 0
  br label %for.cond5769.preheader

for.cond5723.preheader:                           ; preds = %for.body.i8641, %for.cond.cleanup5726
  %tms5716.08801 = phi i32 [ %mul5759, %for.cond.cleanup5726 ], [ 1, %for.body.i8641 ]
  %t5709.08800 = phi i32 [ %inc5734, %for.cond.cleanup5726 ], [ 0, %for.body.i8641 ]
  %.capture_expr..casted5742.sroa.0.0.insert.ext = zext i32 %tms5716.08801 to i64
  br label %for.cond5729.preheader

for.cond5729.preheader:                           ; preds = %for.cond5723.preheader, %for.cond.cleanup5732
  %ths5722.08798 = phi i32 [ %mul5755, %for.cond.cleanup5732 ], [ 32, %for.cond5723.preheader ]
  %t5709.18797 = phi i32 [ %inc5734, %for.cond.cleanup5732 ], [ %t5709.08800, %for.cond5723.preheader ]
  %.capture_expr..casted5744.sroa.0.0.insert.ext = zext i32 %ths5722.08798 to i64
  br label %for.body5733

for.cond.cleanup5726:                             ; preds = %for.cond.cleanup5732
  %mul5759 = shl nsw i32 %tms5716.08801, 1
  %cmp5718 = icmp ult i32 %mul5759, 257
  br i1 %cmp5718, label %for.cond5723.preheader, label %for.cond5763.preheader

for.cond.cleanup5732:                             ; preds = %omp_offload.cont5749
  %mul5755 = shl nsw i32 %ths5722.08798, 1
  %cmp5724 = icmp ult i32 %mul5755, 1025
  br i1 %cmp5724, label %for.cond5729.preheader, label %for.cond.cleanup5726

for.body5733:                                     ; preds = %for.cond5729.preheader, %omp_offload.cont5749
  %sch5728.08796 = phi i32 [ 1000, %for.cond5729.preheader ], [ %mul5751, %omp_offload.cont5749 ]
  %t5709.28795 = phi i32 [ %t5709.18797, %for.cond5729.preheader ], [ %inc5734, %omp_offload.cont5749 ]
  %inc5734 = add nsw i32 %t5709.28795, 1
  %.capture_expr..casted5740.sroa.0.0.insert.ext = zext i32 %sch5728.08796 to i64
  store i64 %indvars.iv9449, i64* %7300, align 8
  store i64 %indvars.iv9449, i64* %7302, align 8
  store i8* %call3733, i8** %7303, align 8
  store i8* %call3733, i8** %7304, align 8
  store i8* %call3735, i8** %7305, align 8
  store i8* %call3735, i8** %7306, align 8
  store i8* %call3736, i8** %7307, align 8
  store i8* %call3736, i8** %7308, align 8
  store i64 %.capture_expr..casted5740.sroa.0.0.insert.ext, i64* %7310, align 8
  store i64 %.capture_expr..casted5740.sroa.0.0.insert.ext, i64* %7312, align 8
  store i64 %.capture_expr..casted5742.sroa.0.0.insert.ext, i64* %7314, align 8
  store i64 %.capture_expr..casted5742.sroa.0.0.insert.ext, i64* %7316, align 8
  store i64 %.capture_expr..casted5744.sroa.0.0.insert.ext, i64* %7318, align 8
  store i64 %.capture_expr..casted5744.sroa.0.0.insert.ext, i64* %7320, align 8
  %7516 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1613.region_id, i32 7, i8** nonnull %7299, i8** nonnull %7301, i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_sizes.273, i64 0, i64 0), i64* getelementptr inbounds ([7 x i64], [7 x i64]* @.offload_maptypes.274, i64 0, i64 0), i32 %tms5716.08801, i32 %ths5722.08798) #6
  %7517 = icmp eq i32 %7516, 0
  br i1 %7517, label %omp_offload.cont5749, label %omp_offload.failed5748

omp_offload.failed5748:                           ; preds = %for.body5733
  %7518 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7519 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7518, i32 %tms5716.08801, i32 %ths5722.08798) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 5, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*, i64)* @.omp_outlined..271 to void (i32*, i32*, ...)*), i64 %indvars.iv9449, i8* %call3733, i8* %call3735, i8* %call3736, i64 %.capture_expr..casted5740.sroa.0.0.insert.ext) #6
  br label %omp_offload.cont5749

omp_offload.cont5749:                             ; preds = %for.body5733, %omp_offload.failed5748
  %mul5751 = mul nsw i32 %sch5728.08796, 3000
  %7520 = zext i32 %mul5751 to i64
  %cmp5730 = icmp ult i64 %indvars.iv9449, %7520
  br i1 %cmp5730, label %for.cond.cleanup5732, label %for.body5733

for.cond5769.preheader:                           ; preds = %for.cond5769.preheader.preheader23, %for.cond.cleanup5772
  %times5762.08805 = phi i32 [ %inc5787, %for.cond.cleanup5772 ], [ 0, %for.cond5769.preheader.preheader23 ]
  br label %vector.body11650

vector.body11650:                                 ; preds = %for.cond5769.preheader, %vector.body11650
  %index11659 = phi i64 [ %index.next11660, %vector.body11650 ], [ 0, %for.cond5769.preheader ]
  %7521 = getelementptr inbounds double, double* %1796, i64 %index11659
  %7522 = bitcast double* %7521 to <2 x double>*
  %wide.load11677 = load <2 x double>, <2 x double>* %7522, align 8, !tbaa !62
  %7523 = getelementptr inbounds double, double* %7521, i64 2
  %7524 = bitcast double* %7523 to <2 x double>*
  %wide.load11678 = load <2 x double>, <2 x double>* %7524, align 8, !tbaa !62
  %7525 = getelementptr inbounds double, double* %7521, i64 4
  %7526 = bitcast double* %7525 to <2 x double>*
  %wide.load11679 = load <2 x double>, <2 x double>* %7526, align 8, !tbaa !62
  %7527 = getelementptr inbounds double, double* %7521, i64 6
  %7528 = bitcast double* %7527 to <2 x double>*
  %wide.load11680 = load <2 x double>, <2 x double>* %7528, align 8, !tbaa !62
  %7529 = getelementptr inbounds double, double* %7521, i64 8
  %7530 = bitcast double* %7529 to <2 x double>*
  %wide.load11681 = load <2 x double>, <2 x double>* %7530, align 8, !tbaa !62
  %7531 = getelementptr inbounds double, double* %7521, i64 10
  %7532 = bitcast double* %7531 to <2 x double>*
  %wide.load11682 = load <2 x double>, <2 x double>* %7532, align 8, !tbaa !62
  %7533 = getelementptr inbounds double, double* %7521, i64 12
  %7534 = bitcast double* %7533 to <2 x double>*
  %wide.load11683 = load <2 x double>, <2 x double>* %7534, align 8, !tbaa !62
  %7535 = getelementptr inbounds double, double* %7521, i64 14
  %7536 = bitcast double* %7535 to <2 x double>*
  %wide.load11684 = load <2 x double>, <2 x double>* %7536, align 8, !tbaa !62
  %7537 = getelementptr inbounds double, double* %7521, i64 16
  %7538 = bitcast double* %7537 to <2 x double>*
  %wide.load11685 = load <2 x double>, <2 x double>* %7538, align 8, !tbaa !62
  %7539 = getelementptr inbounds double, double* %7521, i64 18
  %7540 = bitcast double* %7539 to <2 x double>*
  %wide.load11686 = load <2 x double>, <2 x double>* %7540, align 8, !tbaa !62
  %7541 = getelementptr inbounds double, double* %7521, i64 20
  %7542 = bitcast double* %7541 to <2 x double>*
  %wide.load11687 = load <2 x double>, <2 x double>* %7542, align 8, !tbaa !62
  %7543 = getelementptr inbounds double, double* %7521, i64 22
  %7544 = bitcast double* %7543 to <2 x double>*
  %wide.load11688 = load <2 x double>, <2 x double>* %7544, align 8, !tbaa !62
  %7545 = getelementptr inbounds double, double* %1797, i64 %index11659
  %7546 = bitcast double* %7545 to <2 x double>*
  %wide.load11689 = load <2 x double>, <2 x double>* %7546, align 8, !tbaa !62
  %7547 = getelementptr inbounds double, double* %7545, i64 2
  %7548 = bitcast double* %7547 to <2 x double>*
  %wide.load11690 = load <2 x double>, <2 x double>* %7548, align 8, !tbaa !62
  %7549 = getelementptr inbounds double, double* %7545, i64 4
  %7550 = bitcast double* %7549 to <2 x double>*
  %wide.load11691 = load <2 x double>, <2 x double>* %7550, align 8, !tbaa !62
  %7551 = getelementptr inbounds double, double* %7545, i64 6
  %7552 = bitcast double* %7551 to <2 x double>*
  %wide.load11692 = load <2 x double>, <2 x double>* %7552, align 8, !tbaa !62
  %7553 = getelementptr inbounds double, double* %7545, i64 8
  %7554 = bitcast double* %7553 to <2 x double>*
  %wide.load11693 = load <2 x double>, <2 x double>* %7554, align 8, !tbaa !62
  %7555 = getelementptr inbounds double, double* %7545, i64 10
  %7556 = bitcast double* %7555 to <2 x double>*
  %wide.load11694 = load <2 x double>, <2 x double>* %7556, align 8, !tbaa !62
  %7557 = getelementptr inbounds double, double* %7545, i64 12
  %7558 = bitcast double* %7557 to <2 x double>*
  %wide.load11695 = load <2 x double>, <2 x double>* %7558, align 8, !tbaa !62
  %7559 = getelementptr inbounds double, double* %7545, i64 14
  %7560 = bitcast double* %7559 to <2 x double>*
  %wide.load11696 = load <2 x double>, <2 x double>* %7560, align 8, !tbaa !62
  %7561 = getelementptr inbounds double, double* %7545, i64 16
  %7562 = bitcast double* %7561 to <2 x double>*
  %wide.load11697 = load <2 x double>, <2 x double>* %7562, align 8, !tbaa !62
  %7563 = getelementptr inbounds double, double* %7545, i64 18
  %7564 = bitcast double* %7563 to <2 x double>*
  %wide.load11698 = load <2 x double>, <2 x double>* %7564, align 8, !tbaa !62
  %7565 = getelementptr inbounds double, double* %7545, i64 20
  %7566 = bitcast double* %7565 to <2 x double>*
  %wide.load11699 = load <2 x double>, <2 x double>* %7566, align 8, !tbaa !62
  %7567 = getelementptr inbounds double, double* %7545, i64 22
  %7568 = bitcast double* %7567 to <2 x double>*
  %wide.load11700 = load <2 x double>, <2 x double>* %7568, align 8, !tbaa !62
  %7569 = fadd <2 x double> %wide.load11677, %wide.load11689
  %7570 = fadd <2 x double> %wide.load11678, %wide.load11690
  %7571 = fadd <2 x double> %wide.load11679, %wide.load11691
  %7572 = fadd <2 x double> %wide.load11680, %wide.load11692
  %7573 = fadd <2 x double> %wide.load11681, %wide.load11693
  %7574 = fadd <2 x double> %wide.load11682, %wide.load11694
  %7575 = fadd <2 x double> %wide.load11683, %wide.load11695
  %7576 = fadd <2 x double> %wide.load11684, %wide.load11696
  %7577 = fadd <2 x double> %wide.load11685, %wide.load11697
  %7578 = fadd <2 x double> %wide.load11686, %wide.load11698
  %7579 = fadd <2 x double> %wide.load11687, %wide.load11699
  %7580 = fadd <2 x double> %wide.load11688, %wide.load11700
  %7581 = getelementptr inbounds double, double* %1795, i64 %index11659
  %7582 = bitcast double* %7581 to <2 x double>*
  %wide.load11701 = load <2 x double>, <2 x double>* %7582, align 8, !tbaa !62
  %7583 = getelementptr inbounds double, double* %7581, i64 2
  %7584 = bitcast double* %7583 to <2 x double>*
  %wide.load11702 = load <2 x double>, <2 x double>* %7584, align 8, !tbaa !62
  %7585 = getelementptr inbounds double, double* %7581, i64 4
  %7586 = bitcast double* %7585 to <2 x double>*
  %wide.load11703 = load <2 x double>, <2 x double>* %7586, align 8, !tbaa !62
  %7587 = getelementptr inbounds double, double* %7581, i64 6
  %7588 = bitcast double* %7587 to <2 x double>*
  %wide.load11704 = load <2 x double>, <2 x double>* %7588, align 8, !tbaa !62
  %7589 = getelementptr inbounds double, double* %7581, i64 8
  %7590 = bitcast double* %7589 to <2 x double>*
  %wide.load11705 = load <2 x double>, <2 x double>* %7590, align 8, !tbaa !62
  %7591 = getelementptr inbounds double, double* %7581, i64 10
  %7592 = bitcast double* %7591 to <2 x double>*
  %wide.load11706 = load <2 x double>, <2 x double>* %7592, align 8, !tbaa !62
  %7593 = getelementptr inbounds double, double* %7581, i64 12
  %7594 = bitcast double* %7593 to <2 x double>*
  %wide.load11707 = load <2 x double>, <2 x double>* %7594, align 8, !tbaa !62
  %7595 = getelementptr inbounds double, double* %7581, i64 14
  %7596 = bitcast double* %7595 to <2 x double>*
  %wide.load11708 = load <2 x double>, <2 x double>* %7596, align 8, !tbaa !62
  %7597 = getelementptr inbounds double, double* %7581, i64 16
  %7598 = bitcast double* %7597 to <2 x double>*
  %wide.load11709 = load <2 x double>, <2 x double>* %7598, align 8, !tbaa !62
  %7599 = getelementptr inbounds double, double* %7581, i64 18
  %7600 = bitcast double* %7599 to <2 x double>*
  %wide.load11710 = load <2 x double>, <2 x double>* %7600, align 8, !tbaa !62
  %7601 = getelementptr inbounds double, double* %7581, i64 20
  %7602 = bitcast double* %7601 to <2 x double>*
  %wide.load11711 = load <2 x double>, <2 x double>* %7602, align 8, !tbaa !62
  %7603 = getelementptr inbounds double, double* %7581, i64 22
  %7604 = bitcast double* %7603 to <2 x double>*
  %wide.load11712 = load <2 x double>, <2 x double>* %7604, align 8, !tbaa !62
  %7605 = fadd <2 x double> %7569, %wide.load11701
  %7606 = fadd <2 x double> %7570, %wide.load11702
  %7607 = fadd <2 x double> %7571, %wide.load11703
  %7608 = fadd <2 x double> %7572, %wide.load11704
  %7609 = fadd <2 x double> %7573, %wide.load11705
  %7610 = fadd <2 x double> %7574, %wide.load11706
  %7611 = fadd <2 x double> %7575, %wide.load11707
  %7612 = fadd <2 x double> %7576, %wide.load11708
  %7613 = fadd <2 x double> %7577, %wide.load11709
  %7614 = fadd <2 x double> %7578, %wide.load11710
  %7615 = fadd <2 x double> %7579, %wide.load11711
  %7616 = fadd <2 x double> %7580, %wide.load11712
  store <2 x double> %7605, <2 x double>* %7582, align 8, !tbaa !62
  store <2 x double> %7606, <2 x double>* %7584, align 8, !tbaa !62
  store <2 x double> %7607, <2 x double>* %7586, align 8, !tbaa !62
  store <2 x double> %7608, <2 x double>* %7588, align 8, !tbaa !62
  store <2 x double> %7609, <2 x double>* %7590, align 8, !tbaa !62
  store <2 x double> %7610, <2 x double>* %7592, align 8, !tbaa !62
  store <2 x double> %7611, <2 x double>* %7594, align 8, !tbaa !62
  store <2 x double> %7612, <2 x double>* %7596, align 8, !tbaa !62
  store <2 x double> %7613, <2 x double>* %7598, align 8, !tbaa !62
  store <2 x double> %7614, <2 x double>* %7600, align 8, !tbaa !62
  store <2 x double> %7615, <2 x double>* %7602, align 8, !tbaa !62
  store <2 x double> %7616, <2 x double>* %7604, align 8, !tbaa !62
  %index.next11660 = add nuw nsw i64 %index11659, 24
  %7617 = icmp eq i64 %index.next11660, %n.vec11658
  br i1 %7617, label %middle.block11651, label %vector.body11650, !llvm.loop !129

middle.block11651:                                ; preds = %vector.body11650
  br i1 %cmp.n11662, label %for.cond.cleanup5772, label %for.body5773

for.cond.cleanup5766:                             ; preds = %for.cond.cleanup5772, %for.body.i8641, %for.cond5763.preheader
  store i8* %call3733, i8** %7321, align 8
  store i8* %call3733, i8** %7322, align 8
  store i64 %7515, i64* %7323, align 8
  call void @__tgt_target_data_update(i64 -1, i32 1, i8** nonnull %7321, i8** nonnull %7322, i64* nonnull %7323, i64* getelementptr inbounds ([1 x i64], [1 x i64]* @.offload_maptypes.275, i64 0, i64 0)) #6
  br label %for.body5799

for.cond.cleanup5772:                             ; preds = %for.body5773, %middle.block11651
  %inc5787 = add nuw nsw i32 %times5762.08805, 1
  %exitcond9446 = icmp eq i32 %inc5787, %inc5734
  br i1 %exitcond9446, label %for.cond.cleanup5766, label %for.cond5769.preheader

for.body5773:                                     ; preds = %middle.block11651, %for.body5773
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body5773 ], [ %n.vec11658, %middle.block11651 ]
  %arrayidx5775 = getelementptr inbounds double, double* %1796, i64 %indvars.iv
  %7618 = load double, double* %arrayidx5775, align 8, !tbaa !62
  %arrayidx5777 = getelementptr inbounds double, double* %1797, i64 %indvars.iv
  %7619 = load double, double* %arrayidx5777, align 8, !tbaa !62
  %add5778 = fadd double %7618, %7619
  %arrayidx5780 = getelementptr inbounds double, double* %1795, i64 %indvars.iv
  %7620 = load double, double* %arrayidx5780, align 8, !tbaa !62
  %add5781 = fadd double %7620, %add5778
  store double %add5781, double* %arrayidx5780, align 8, !tbaa !62
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, %indvars.iv9449
  br i1 %exitcond, label %for.cond.cleanup5772, label %for.body5773, !llvm.loop !130

for.body5799:                                     ; preds = %for.inc5813.7, %for.cond.cleanup5766
  %indvars.iv9447 = phi i64 [ 0, %for.cond.cleanup5766 ], [ %indvars.iv.next9448.7, %for.inc5813.7 ]
  %arrayidx5801 = getelementptr inbounds double, double* %1795, i64 %indvars.iv9447
  %7621 = load double, double* %arrayidx5801, align 8, !tbaa !62
  %arrayidx5803 = getelementptr inbounds double, double* %1794, i64 %indvars.iv9447
  %7622 = load double, double* %arrayidx5803, align 8, !tbaa !62
  %cmp5804 = fcmp une double %7621, %7622
  br i1 %cmp5804, label %cleanup5823, label %for.inc5813

for.inc5813:                                      ; preds = %for.body5799
  %indvars.iv.next9448 = or i64 %indvars.iv9447, 1
  %arrayidx5801.1 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9448
  %7623 = load double, double* %arrayidx5801.1, align 8, !tbaa !62
  %arrayidx5803.1 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9448
  %7624 = load double, double* %arrayidx5803.1, align 8, !tbaa !62
  %cmp5804.1 = fcmp une double %7623, %7624
  br i1 %cmp5804.1, label %cleanup5823, label %for.inc5813.1

for.inc5821:                                      ; preds = %for.inc5813.7
  %indvars.iv.next9450 = add nuw nsw i64 %indvars.iv9449, 5000
  %cmp5705 = icmp ult i64 %indvars.iv.next9450, 25000
  %indvar.next11654 = add nuw nsw i64 %indvar11653, 1
  br i1 %cmp5705, label %for.body.i8641.preheader, label %for.end5825

cleanup5823:                                      ; preds = %for.inc5813.6, %for.inc5813.5, %for.inc5813.4, %for.inc5813.3, %for.inc5813.2, %for.inc5813.1, %for.inc5813, %for.body5799
  %indvars.iv9447.lcssa = phi i64 [ %indvars.iv9447, %for.body5799 ], [ %indvars.iv.next9448, %for.inc5813 ], [ %indvars.iv.next9448.1, %for.inc5813.1 ], [ %indvars.iv.next9448.2, %for.inc5813.2 ], [ %indvars.iv.next9448.3, %for.inc5813.3 ], [ %indvars.iv.next9448.4, %for.inc5813.4 ], [ %indvars.iv.next9448.5, %for.inc5813.5 ], [ %indvars.iv.next9448.6, %for.inc5813.6 ]
  %.lcssa11768 = phi double [ %7621, %for.body5799 ], [ %7623, %for.inc5813 ], [ %7728, %for.inc5813.1 ], [ %7730, %for.inc5813.2 ], [ %7732, %for.inc5813.3 ], [ %7734, %for.inc5813.4 ], [ %7736, %for.inc5813.5 ], [ %7738, %for.inc5813.6 ]
  %.lcssa = phi double [ %7622, %for.body5799 ], [ %7624, %for.inc5813 ], [ %7729, %for.inc5813.1 ], [ %7731, %for.inc5813.2 ], [ %7733, %for.inc5813.3 ], [ %7735, %for.inc5813.4 ], [ %7737, %for.inc5813.5 ], [ %7739, %for.inc5813.6 ]
  %7625 = trunc i64 %indvars.iv9449 to i32
  %7626 = trunc i64 %indvars.iv9447.lcssa to i32
  %call5811 = call signext i32 (i8*, ...) @printf(i8* getelementptr inbounds ([51 x i8], [51 x i8]* @.str.163, i64 0, i64 0), i32 signext %7625, i32 signext %7626, double %.lcssa11768, double %.lcssa)
  br label %cleanup5832

for.end5825:                                      ; preds = %for.inc5821
  %puts8277 = call i32 @puts(i8* getelementptr inbounds ([10 x i8], [10 x i8]* @str.438, i64 0, i64 0))
  %7627 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5830, i64 0, i64 0
  store i8* %call3733, i8** %7627, align 8
  %7628 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5831, i64 0, i64 0
  store i8* %call3733, i8** %7628, align 8
  %7629 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5830, i64 0, i64 1
  store i8* %call3735, i8** %7629, align 8
  %7630 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5831, i64 0, i64 1
  store i8* %call3735, i8** %7630, align 8
  %7631 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_baseptrs5830, i64 0, i64 2
  store i8* %call3736, i8** %7631, align 8
  %7632 = getelementptr inbounds [3 x i8*], [3 x i8*]* %.offload_ptrs5831, i64 0, i64 2
  store i8* %call3736, i8** %7632, align 8
  call void @__tgt_target_data_end(i64 -1, i32 3, i8** nonnull %7627, i8** nonnull %7628, i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_sizes.276, i64 0, i64 0), i64* getelementptr inbounds ([3 x i64], [3 x i64]* @.offload_maptypes.277, i64 0, i64 0)) #6
  br label %cleanup5832

cleanup5832:                                      ; preds = %cleanup5823, %cleanup5698, %cleanup5573, %cleanup5435, %cleanup5310, %cleanup5172, %cleanup5047, %cleanup4910, %cleanup4785, %cleanup4660, %cleanup4548, %cleanup4423, %cleanup4311, %cleanup4186, %cleanup4074, %cleanup3950, %cleanup3838, %for.end5825
  %retval.51 = phi i32 [ 0, %for.end5825 ], [ 1, %cleanup5823 ], [ 1, %cleanup5698 ], [ 1, %cleanup5573 ], [ 1, %cleanup5435 ], [ 1, %cleanup5310 ], [ 1, %cleanup5172 ], [ 1, %cleanup5047 ], [ 1, %cleanup4910 ], [ 1, %cleanup4785 ], [ 1, %cleanup4660 ], [ 1, %cleanup4548 ], [ 1, %cleanup4423 ], [ 1, %cleanup4311 ], [ 1, %cleanup4186 ], [ 1, %cleanup4074 ], [ 1, %cleanup3950 ], [ 1, %cleanup3838 ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %5) #6
  call void @llvm.lifetime.end.p0i8(i64 24576, i8* nonnull %4) #6
  call void @llvm.lifetime.end.p0i8(i64 24576, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 24576, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 24576, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 24576, i8* nonnull %0) #6
  ret i32 %retval.51

omp_offload.failed4589.1:                         ; preds = %omp_offload.cont4590
  %7633 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7634 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7633, i32 %tms4566.08939, i32 64) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..208 to void (i32*, i32*, ...)*), i64 %indvars.iv9524, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4590.1

omp_offload.cont4590.1:                           ; preds = %omp_offload.failed4589.1, %omp_offload.cont4590
  store i64 %indvars.iv9524, i64* %4255, align 8
  store i64 %indvars.iv9524, i64* %4257, align 8
  store i8* %call3733, i8** %4258, align 8
  store i8* %call3733, i8** %4259, align 8
  store i8* %call3735, i8** %4260, align 8
  store i8* %call3735, i8** %4261, align 8
  store i8* %call3736, i8** %4262, align 8
  store i8* %call3736, i8** %4263, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4265, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4267, align 8
  store i64 128, i64* %4269, align 8
  store i64 128, i64* %4271, align 8
  %7635 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1283.region_id, i32 6, i8** nonnull %4254, i8** nonnull %4256, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4566.08939, i32 128) #6
  %7636 = icmp eq i32 %7635, 0
  br i1 %7636, label %omp_offload.cont4590.2, label %omp_offload.failed4589.2

omp_offload.failed4589.2:                         ; preds = %omp_offload.cont4590.1
  %7637 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7638 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7637, i32 %tms4566.08939, i32 128) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..208 to void (i32*, i32*, ...)*), i64 %indvars.iv9524, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4590.2

omp_offload.cont4590.2:                           ; preds = %omp_offload.failed4589.2, %omp_offload.cont4590.1
  store i64 %indvars.iv9524, i64* %4255, align 8
  store i64 %indvars.iv9524, i64* %4257, align 8
  store i8* %call3733, i8** %4258, align 8
  store i8* %call3733, i8** %4259, align 8
  store i8* %call3735, i8** %4260, align 8
  store i8* %call3735, i8** %4261, align 8
  store i8* %call3736, i8** %4262, align 8
  store i8* %call3736, i8** %4263, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4265, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4267, align 8
  store i64 256, i64* %4269, align 8
  store i64 256, i64* %4271, align 8
  %7639 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1283.region_id, i32 6, i8** nonnull %4254, i8** nonnull %4256, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4566.08939, i32 256) #6
  %7640 = icmp eq i32 %7639, 0
  br i1 %7640, label %omp_offload.cont4590.3, label %omp_offload.failed4589.3

omp_offload.failed4589.3:                         ; preds = %omp_offload.cont4590.2
  %7641 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7642 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7641, i32 %tms4566.08939, i32 256) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..208 to void (i32*, i32*, ...)*), i64 %indvars.iv9524, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4590.3

omp_offload.cont4590.3:                           ; preds = %omp_offload.failed4589.3, %omp_offload.cont4590.2
  store i64 %indvars.iv9524, i64* %4255, align 8
  store i64 %indvars.iv9524, i64* %4257, align 8
  store i8* %call3733, i8** %4258, align 8
  store i8* %call3733, i8** %4259, align 8
  store i8* %call3735, i8** %4260, align 8
  store i8* %call3735, i8** %4261, align 8
  store i8* %call3736, i8** %4262, align 8
  store i8* %call3736, i8** %4263, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4265, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4267, align 8
  store i64 512, i64* %4269, align 8
  store i64 512, i64* %4271, align 8
  %7643 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1283.region_id, i32 6, i8** nonnull %4254, i8** nonnull %4256, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4566.08939, i32 512) #6
  %7644 = icmp eq i32 %7643, 0
  br i1 %7644, label %omp_offload.cont4590.4, label %omp_offload.failed4589.4

omp_offload.failed4589.4:                         ; preds = %omp_offload.cont4590.3
  %7645 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7646 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7645, i32 %tms4566.08939, i32 512) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..208 to void (i32*, i32*, ...)*), i64 %indvars.iv9524, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4590.4

omp_offload.cont4590.4:                           ; preds = %omp_offload.failed4589.4, %omp_offload.cont4590.3
  store i64 %indvars.iv9524, i64* %4255, align 8
  store i64 %indvars.iv9524, i64* %4257, align 8
  store i8* %call3733, i8** %4258, align 8
  store i8* %call3733, i8** %4259, align 8
  store i8* %call3735, i8** %4260, align 8
  store i8* %call3735, i8** %4261, align 8
  store i8* %call3736, i8** %4262, align 8
  store i8* %call3736, i8** %4263, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4265, align 8
  store i64 %.capture_expr..casted4583.sroa.0.0.insert.ext, i64* %4267, align 8
  store i64 1024, i64* %4269, align 8
  store i64 1024, i64* %4271, align 8
  %7647 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1283.region_id, i32 6, i8** nonnull %4254, i8** nonnull %4256, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4566.08939, i32 1024) #6
  %7648 = icmp eq i32 %7647, 0
  br i1 %7648, label %omp_offload.cont4590.5, label %omp_offload.failed4589.5

omp_offload.failed4589.5:                         ; preds = %omp_offload.cont4590.4
  %7649 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7650 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7649, i32 %tms4566.08939, i32 1024) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..208 to void (i32*, i32*, ...)*), i64 %indvars.iv9524, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4590.5

omp_offload.cont4590.5:                           ; preds = %omp_offload.failed4589.5, %omp_offload.cont4590.4
  %mul4596 = shl nsw i32 %tms4566.08939, 1
  %cmp4568 = icmp ult i32 %mul4596, 257
  br i1 %cmp4568, label %for.cond4573.preheader, label %for.cond4606.preheader.preheader50

for.cond4606.preheader.preheader50:               ; preds = %omp_offload.cont4590.5
  %7651 = add nuw nsw i64 %4307, 32
  %n.mod.vf10766 = urem i64 %7651, 24
  %n.vec10767 = sub nuw nsw i64 %7651, %n.mod.vf10766
  %cmp.n10771 = icmp eq i64 %n.mod.vf10766, 0
  br label %for.cond4606.preheader

omp_offload.failed4352.1:                         ; preds = %omp_offload.cont4353
  %7652 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7653 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7652, i32 %tms4329.08963, i32 64) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..194 to void (i32*, i32*, ...)*), i64 %indvars.iv9543, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4353.1

omp_offload.cont4353.1:                           ; preds = %omp_offload.failed4352.1, %omp_offload.cont4353
  store i64 %indvars.iv9543, i64* %3585, align 8
  store i64 %indvars.iv9543, i64* %3587, align 8
  store i8* %call3733, i8** %3588, align 8
  store i8* %call3733, i8** %3589, align 8
  store i8* %call3735, i8** %3590, align 8
  store i8* %call3735, i8** %3591, align 8
  store i8* %call3736, i8** %3592, align 8
  store i8* %call3736, i8** %3593, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3595, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3597, align 8
  store i64 128, i64* %3599, align 8
  store i64 128, i64* %3601, align 8
  %7654 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1213.region_id, i32 6, i8** nonnull %3584, i8** nonnull %3586, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4329.08963, i32 128) #6
  %7655 = icmp eq i32 %7654, 0
  br i1 %7655, label %omp_offload.cont4353.2, label %omp_offload.failed4352.2

omp_offload.failed4352.2:                         ; preds = %omp_offload.cont4353.1
  %7656 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7657 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7656, i32 %tms4329.08963, i32 128) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..194 to void (i32*, i32*, ...)*), i64 %indvars.iv9543, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4353.2

omp_offload.cont4353.2:                           ; preds = %omp_offload.failed4352.2, %omp_offload.cont4353.1
  store i64 %indvars.iv9543, i64* %3585, align 8
  store i64 %indvars.iv9543, i64* %3587, align 8
  store i8* %call3733, i8** %3588, align 8
  store i8* %call3733, i8** %3589, align 8
  store i8* %call3735, i8** %3590, align 8
  store i8* %call3735, i8** %3591, align 8
  store i8* %call3736, i8** %3592, align 8
  store i8* %call3736, i8** %3593, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3595, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3597, align 8
  store i64 256, i64* %3599, align 8
  store i64 256, i64* %3601, align 8
  %7658 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1213.region_id, i32 6, i8** nonnull %3584, i8** nonnull %3586, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4329.08963, i32 256) #6
  %7659 = icmp eq i32 %7658, 0
  br i1 %7659, label %omp_offload.cont4353.3, label %omp_offload.failed4352.3

omp_offload.failed4352.3:                         ; preds = %omp_offload.cont4353.2
  %7660 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7661 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7660, i32 %tms4329.08963, i32 256) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..194 to void (i32*, i32*, ...)*), i64 %indvars.iv9543, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4353.3

omp_offload.cont4353.3:                           ; preds = %omp_offload.failed4352.3, %omp_offload.cont4353.2
  store i64 %indvars.iv9543, i64* %3585, align 8
  store i64 %indvars.iv9543, i64* %3587, align 8
  store i8* %call3733, i8** %3588, align 8
  store i8* %call3733, i8** %3589, align 8
  store i8* %call3735, i8** %3590, align 8
  store i8* %call3735, i8** %3591, align 8
  store i8* %call3736, i8** %3592, align 8
  store i8* %call3736, i8** %3593, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3595, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3597, align 8
  store i64 512, i64* %3599, align 8
  store i64 512, i64* %3601, align 8
  %7662 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1213.region_id, i32 6, i8** nonnull %3584, i8** nonnull %3586, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4329.08963, i32 512) #6
  %7663 = icmp eq i32 %7662, 0
  br i1 %7663, label %omp_offload.cont4353.4, label %omp_offload.failed4352.4

omp_offload.failed4352.4:                         ; preds = %omp_offload.cont4353.3
  %7664 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7665 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7664, i32 %tms4329.08963, i32 512) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..194 to void (i32*, i32*, ...)*), i64 %indvars.iv9543, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4353.4

omp_offload.cont4353.4:                           ; preds = %omp_offload.failed4352.4, %omp_offload.cont4353.3
  store i64 %indvars.iv9543, i64* %3585, align 8
  store i64 %indvars.iv9543, i64* %3587, align 8
  store i8* %call3733, i8** %3588, align 8
  store i8* %call3733, i8** %3589, align 8
  store i8* %call3735, i8** %3590, align 8
  store i8* %call3735, i8** %3591, align 8
  store i8* %call3736, i8** %3592, align 8
  store i8* %call3736, i8** %3593, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3595, align 8
  store i64 %.capture_expr..casted4346.sroa.0.0.insert.ext, i64* %3597, align 8
  store i64 1024, i64* %3599, align 8
  store i64 1024, i64* %3601, align 8
  %7666 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1213.region_id, i32 6, i8** nonnull %3584, i8** nonnull %3586, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4329.08963, i32 1024) #6
  %7667 = icmp eq i32 %7666, 0
  br i1 %7667, label %omp_offload.cont4353.5, label %omp_offload.failed4352.5

omp_offload.failed4352.5:                         ; preds = %omp_offload.cont4353.4
  %7668 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7669 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7668, i32 %tms4329.08963, i32 1024) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..194 to void (i32*, i32*, ...)*), i64 %indvars.iv9543, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4353.5

omp_offload.cont4353.5:                           ; preds = %omp_offload.failed4352.5, %omp_offload.cont4353.4
  %mul4359 = shl nsw i32 %tms4329.08963, 1
  %cmp4331 = icmp ult i32 %mul4359, 257
  br i1 %cmp4331, label %for.cond4336.preheader, label %for.cond4369.preheader.preheader55

for.cond4369.preheader.preheader55:               ; preds = %omp_offload.cont4353.5
  %7670 = add nuw nsw i64 %3637, 32
  %n.mod.vf10568 = urem i64 %7670, 24
  %n.vec10569 = sub nuw nsw i64 %7670, %n.mod.vf10568
  %cmp.n10573 = icmp eq i64 %n.mod.vf10568, 0
  br label %for.cond4369.preheader

omp_offload.failed4115.1:                         ; preds = %omp_offload.cont4116
  %7671 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7672 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7671, i32 %tms4092.08983, i32 64) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..180 to void (i32*, i32*, ...)*), i64 %indvars.iv9562, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4116.1

omp_offload.cont4116.1:                           ; preds = %omp_offload.failed4115.1, %omp_offload.cont4116
  store i64 %indvars.iv9562, i64* %2915, align 8
  store i64 %indvars.iv9562, i64* %2917, align 8
  store i8* %call3733, i8** %2918, align 8
  store i8* %call3733, i8** %2919, align 8
  store i8* %call3735, i8** %2920, align 8
  store i8* %call3735, i8** %2921, align 8
  store i8* %call3736, i8** %2922, align 8
  store i8* %call3736, i8** %2923, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2925, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2927, align 8
  store i64 128, i64* %2929, align 8
  store i64 128, i64* %2931, align 8
  %7673 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1140.region_id, i32 6, i8** nonnull %2914, i8** nonnull %2916, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4092.08983, i32 128) #6
  %7674 = icmp eq i32 %7673, 0
  br i1 %7674, label %omp_offload.cont4116.2, label %omp_offload.failed4115.2

omp_offload.failed4115.2:                         ; preds = %omp_offload.cont4116.1
  %7675 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7676 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7675, i32 %tms4092.08983, i32 128) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..180 to void (i32*, i32*, ...)*), i64 %indvars.iv9562, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4116.2

omp_offload.cont4116.2:                           ; preds = %omp_offload.failed4115.2, %omp_offload.cont4116.1
  store i64 %indvars.iv9562, i64* %2915, align 8
  store i64 %indvars.iv9562, i64* %2917, align 8
  store i8* %call3733, i8** %2918, align 8
  store i8* %call3733, i8** %2919, align 8
  store i8* %call3735, i8** %2920, align 8
  store i8* %call3735, i8** %2921, align 8
  store i8* %call3736, i8** %2922, align 8
  store i8* %call3736, i8** %2923, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2925, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2927, align 8
  store i64 256, i64* %2929, align 8
  store i64 256, i64* %2931, align 8
  %7677 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1140.region_id, i32 6, i8** nonnull %2914, i8** nonnull %2916, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4092.08983, i32 256) #6
  %7678 = icmp eq i32 %7677, 0
  br i1 %7678, label %omp_offload.cont4116.3, label %omp_offload.failed4115.3

omp_offload.failed4115.3:                         ; preds = %omp_offload.cont4116.2
  %7679 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7680 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7679, i32 %tms4092.08983, i32 256) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..180 to void (i32*, i32*, ...)*), i64 %indvars.iv9562, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4116.3

omp_offload.cont4116.3:                           ; preds = %omp_offload.failed4115.3, %omp_offload.cont4116.2
  store i64 %indvars.iv9562, i64* %2915, align 8
  store i64 %indvars.iv9562, i64* %2917, align 8
  store i8* %call3733, i8** %2918, align 8
  store i8* %call3733, i8** %2919, align 8
  store i8* %call3735, i8** %2920, align 8
  store i8* %call3735, i8** %2921, align 8
  store i8* %call3736, i8** %2922, align 8
  store i8* %call3736, i8** %2923, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2925, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2927, align 8
  store i64 512, i64* %2929, align 8
  store i64 512, i64* %2931, align 8
  %7681 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1140.region_id, i32 6, i8** nonnull %2914, i8** nonnull %2916, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4092.08983, i32 512) #6
  %7682 = icmp eq i32 %7681, 0
  br i1 %7682, label %omp_offload.cont4116.4, label %omp_offload.failed4115.4

omp_offload.failed4115.4:                         ; preds = %omp_offload.cont4116.3
  %7683 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7684 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7683, i32 %tms4092.08983, i32 512) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..180 to void (i32*, i32*, ...)*), i64 %indvars.iv9562, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4116.4

omp_offload.cont4116.4:                           ; preds = %omp_offload.failed4115.4, %omp_offload.cont4116.3
  store i64 %indvars.iv9562, i64* %2915, align 8
  store i64 %indvars.iv9562, i64* %2917, align 8
  store i8* %call3733, i8** %2918, align 8
  store i8* %call3733, i8** %2919, align 8
  store i8* %call3735, i8** %2920, align 8
  store i8* %call3735, i8** %2921, align 8
  store i8* %call3736, i8** %2922, align 8
  store i8* %call3736, i8** %2923, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2925, align 8
  store i64 %.capture_expr..casted4109.sroa.0.0.insert.ext, i64* %2927, align 8
  store i64 1024, i64* %2929, align 8
  store i64 1024, i64* %2931, align 8
  %7685 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1140.region_id, i32 6, i8** nonnull %2914, i8** nonnull %2916, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms4092.08983, i32 1024) #6
  %7686 = icmp eq i32 %7685, 0
  br i1 %7686, label %omp_offload.cont4116.5, label %omp_offload.failed4115.5

omp_offload.failed4115.5:                         ; preds = %omp_offload.cont4116.4
  %7687 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7688 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7687, i32 %tms4092.08983, i32 1024) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..180 to void (i32*, i32*, ...)*), i64 %indvars.iv9562, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont4116.5

omp_offload.cont4116.5:                           ; preds = %omp_offload.failed4115.5, %omp_offload.cont4116.4
  %mul4122 = shl nsw i32 %tms4092.08983, 1
  %cmp4094 = icmp ult i32 %mul4122, 257
  br i1 %cmp4094, label %for.cond4099.preheader, label %for.cond4132.preheader.preheader59

for.cond4132.preheader.preheader59:               ; preds = %omp_offload.cont4116.5
  %7689 = add nuw nsw i64 %2967, 32
  %n.mod.vf10370 = urem i64 %7689, 24
  %n.vec10371 = sub nuw nsw i64 %7689, %n.mod.vf10370
  %cmp.n10375 = icmp eq i64 %n.mod.vf10370, 0
  br label %for.cond4132.preheader

omp_offload.failed3879.1:                         ; preds = %omp_offload.cont3880
  %7690 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7691 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7690, i32 %tms3856.09003, i32 64) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..166 to void (i32*, i32*, ...)*), i64 %indvars.iv9581, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3880.1

omp_offload.cont3880.1:                           ; preds = %omp_offload.failed3879.1, %omp_offload.cont3880
  store i64 %indvars.iv9581, i64* %2245, align 8
  store i64 %indvars.iv9581, i64* %2247, align 8
  store i8* %call3733, i8** %2248, align 8
  store i8* %call3733, i8** %2249, align 8
  store i8* %call3735, i8** %2250, align 8
  store i8* %call3735, i8** %2251, align 8
  store i8* %call3736, i8** %2252, align 8
  store i8* %call3736, i8** %2253, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2255, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2257, align 8
  store i64 128, i64* %2259, align 8
  store i64 128, i64* %2261, align 8
  %7692 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1070.region_id, i32 6, i8** nonnull %2244, i8** nonnull %2246, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3856.09003, i32 128) #6
  %7693 = icmp eq i32 %7692, 0
  br i1 %7693, label %omp_offload.cont3880.2, label %omp_offload.failed3879.2

omp_offload.failed3879.2:                         ; preds = %omp_offload.cont3880.1
  %7694 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7695 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7694, i32 %tms3856.09003, i32 128) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..166 to void (i32*, i32*, ...)*), i64 %indvars.iv9581, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3880.2

omp_offload.cont3880.2:                           ; preds = %omp_offload.failed3879.2, %omp_offload.cont3880.1
  store i64 %indvars.iv9581, i64* %2245, align 8
  store i64 %indvars.iv9581, i64* %2247, align 8
  store i8* %call3733, i8** %2248, align 8
  store i8* %call3733, i8** %2249, align 8
  store i8* %call3735, i8** %2250, align 8
  store i8* %call3735, i8** %2251, align 8
  store i8* %call3736, i8** %2252, align 8
  store i8* %call3736, i8** %2253, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2255, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2257, align 8
  store i64 256, i64* %2259, align 8
  store i64 256, i64* %2261, align 8
  %7696 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1070.region_id, i32 6, i8** nonnull %2244, i8** nonnull %2246, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3856.09003, i32 256) #6
  %7697 = icmp eq i32 %7696, 0
  br i1 %7697, label %omp_offload.cont3880.3, label %omp_offload.failed3879.3

omp_offload.failed3879.3:                         ; preds = %omp_offload.cont3880.2
  %7698 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7699 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7698, i32 %tms3856.09003, i32 256) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..166 to void (i32*, i32*, ...)*), i64 %indvars.iv9581, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3880.3

omp_offload.cont3880.3:                           ; preds = %omp_offload.failed3879.3, %omp_offload.cont3880.2
  store i64 %indvars.iv9581, i64* %2245, align 8
  store i64 %indvars.iv9581, i64* %2247, align 8
  store i8* %call3733, i8** %2248, align 8
  store i8* %call3733, i8** %2249, align 8
  store i8* %call3735, i8** %2250, align 8
  store i8* %call3735, i8** %2251, align 8
  store i8* %call3736, i8** %2252, align 8
  store i8* %call3736, i8** %2253, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2255, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2257, align 8
  store i64 512, i64* %2259, align 8
  store i64 512, i64* %2261, align 8
  %7700 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1070.region_id, i32 6, i8** nonnull %2244, i8** nonnull %2246, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3856.09003, i32 512) #6
  %7701 = icmp eq i32 %7700, 0
  br i1 %7701, label %omp_offload.cont3880.4, label %omp_offload.failed3879.4

omp_offload.failed3879.4:                         ; preds = %omp_offload.cont3880.3
  %7702 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7703 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7702, i32 %tms3856.09003, i32 512) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..166 to void (i32*, i32*, ...)*), i64 %indvars.iv9581, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3880.4

omp_offload.cont3880.4:                           ; preds = %omp_offload.failed3879.4, %omp_offload.cont3880.3
  store i64 %indvars.iv9581, i64* %2245, align 8
  store i64 %indvars.iv9581, i64* %2247, align 8
  store i8* %call3733, i8** %2248, align 8
  store i8* %call3733, i8** %2249, align 8
  store i8* %call3735, i8** %2250, align 8
  store i8* %call3735, i8** %2251, align 8
  store i8* %call3736, i8** %2252, align 8
  store i8* %call3736, i8** %2253, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2255, align 8
  store i64 %.capture_expr..casted3873.sroa.0.0.insert.ext, i64* %2257, align 8
  store i64 1024, i64* %2259, align 8
  store i64 1024, i64* %2261, align 8
  %7704 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1070.region_id, i32 6, i8** nonnull %2244, i8** nonnull %2246, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3856.09003, i32 1024) #6
  %7705 = icmp eq i32 %7704, 0
  br i1 %7705, label %omp_offload.cont3880.5, label %omp_offload.failed3879.5

omp_offload.failed3879.5:                         ; preds = %omp_offload.cont3880.4
  %7706 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7707 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7706, i32 %tms3856.09003, i32 1024) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..166 to void (i32*, i32*, ...)*), i64 %indvars.iv9581, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3880.5

omp_offload.cont3880.5:                           ; preds = %omp_offload.failed3879.5, %omp_offload.cont3880.4
  %mul3886 = shl nsw i32 %tms3856.09003, 1
  %cmp3858 = icmp ult i32 %mul3886, 257
  br i1 %cmp3858, label %for.cond3863.preheader, label %for.cond3896.preheader.preheader63

for.cond3896.preheader.preheader63:               ; preds = %omp_offload.cont3880.5
  %7708 = add nuw nsw i64 %2297, 32
  %n.mod.vf10172 = urem i64 %7708, 24
  %n.vec10173 = sub nuw nsw i64 %7708, %n.mod.vf10172
  %cmp.n10177 = icmp eq i64 %n.mod.vf10172, 0
  br label %for.cond3896.preheader

omp_offload.failed3775.1:                         ; preds = %omp_offload.cont3776
  %7709 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7710 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7709, i32 %tms3754.09012, i32 64) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..158 to void (i32*, i32*, ...)*), i64 %indvars.iv9592, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3776.1

omp_offload.cont3776.1:                           ; preds = %omp_offload.failed3775.1, %omp_offload.cont3776
  store i64 %indvars.iv9592, i64* %1814, align 8
  store i64 %indvars.iv9592, i64* %1816, align 8
  store i8* %call3733, i8** %1817, align 8
  store i8* %call3733, i8** %1818, align 8
  store i8* %call3735, i8** %1819, align 8
  store i8* %call3735, i8** %1820, align 8
  store i8* %call3736, i8** %1821, align 8
  store i8* %call3736, i8** %1822, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1824, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1826, align 8
  store i64 128, i64* %1828, align 8
  store i64 128, i64* %1830, align 8
  %7711 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1036.region_id, i32 6, i8** nonnull %1813, i8** nonnull %1815, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3754.09012, i32 128) #6
  %7712 = icmp eq i32 %7711, 0
  br i1 %7712, label %omp_offload.cont3776.2, label %omp_offload.failed3775.2

omp_offload.failed3775.2:                         ; preds = %omp_offload.cont3776.1
  %7713 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7714 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7713, i32 %tms3754.09012, i32 128) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..158 to void (i32*, i32*, ...)*), i64 %indvars.iv9592, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3776.2

omp_offload.cont3776.2:                           ; preds = %omp_offload.failed3775.2, %omp_offload.cont3776.1
  store i64 %indvars.iv9592, i64* %1814, align 8
  store i64 %indvars.iv9592, i64* %1816, align 8
  store i8* %call3733, i8** %1817, align 8
  store i8* %call3733, i8** %1818, align 8
  store i8* %call3735, i8** %1819, align 8
  store i8* %call3735, i8** %1820, align 8
  store i8* %call3736, i8** %1821, align 8
  store i8* %call3736, i8** %1822, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1824, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1826, align 8
  store i64 256, i64* %1828, align 8
  store i64 256, i64* %1830, align 8
  %7715 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1036.region_id, i32 6, i8** nonnull %1813, i8** nonnull %1815, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3754.09012, i32 256) #6
  %7716 = icmp eq i32 %7715, 0
  br i1 %7716, label %omp_offload.cont3776.3, label %omp_offload.failed3775.3

omp_offload.failed3775.3:                         ; preds = %omp_offload.cont3776.2
  %7717 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7718 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7717, i32 %tms3754.09012, i32 256) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..158 to void (i32*, i32*, ...)*), i64 %indvars.iv9592, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3776.3

omp_offload.cont3776.3:                           ; preds = %omp_offload.failed3775.3, %omp_offload.cont3776.2
  store i64 %indvars.iv9592, i64* %1814, align 8
  store i64 %indvars.iv9592, i64* %1816, align 8
  store i8* %call3733, i8** %1817, align 8
  store i8* %call3733, i8** %1818, align 8
  store i8* %call3735, i8** %1819, align 8
  store i8* %call3735, i8** %1820, align 8
  store i8* %call3736, i8** %1821, align 8
  store i8* %call3736, i8** %1822, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1824, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1826, align 8
  store i64 512, i64* %1828, align 8
  store i64 512, i64* %1830, align 8
  %7719 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1036.region_id, i32 6, i8** nonnull %1813, i8** nonnull %1815, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3754.09012, i32 512) #6
  %7720 = icmp eq i32 %7719, 0
  br i1 %7720, label %omp_offload.cont3776.4, label %omp_offload.failed3775.4

omp_offload.failed3775.4:                         ; preds = %omp_offload.cont3776.3
  %7721 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7722 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7721, i32 %tms3754.09012, i32 512) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..158 to void (i32*, i32*, ...)*), i64 %indvars.iv9592, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3776.4

omp_offload.cont3776.4:                           ; preds = %omp_offload.failed3775.4, %omp_offload.cont3776.3
  store i64 %indvars.iv9592, i64* %1814, align 8
  store i64 %indvars.iv9592, i64* %1816, align 8
  store i8* %call3733, i8** %1817, align 8
  store i8* %call3733, i8** %1818, align 8
  store i8* %call3735, i8** %1819, align 8
  store i8* %call3735, i8** %1820, align 8
  store i8* %call3736, i8** %1821, align 8
  store i8* %call3736, i8** %1822, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1824, align 8
  store i64 %.capture_expr..casted3769.sroa.0.0.insert.ext, i64* %1826, align 8
  store i64 1024, i64* %1828, align 8
  store i64 1024, i64* %1830, align 8
  %7723 = call i32 @__tgt_target_teams(i64 -1, i8* nonnull @.__omp_offloading_802_14c18b2_main_l1036.region_id, i32 6, i8** nonnull %1813, i8** nonnull %1815, i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_sizes.210, i64 0, i64 0), i64* getelementptr inbounds ([6 x i64], [6 x i64]* @.offload_maptypes.211, i64 0, i64 0), i32 %tms3754.09012, i32 1024) #6
  %7724 = icmp eq i32 %7723, 0
  br i1 %7724, label %omp_offload.cont3776.5, label %omp_offload.failed3775.5

omp_offload.failed3775.5:                         ; preds = %omp_offload.cont3776.4
  %7725 = call i32 @__kmpc_global_thread_num(%struct.ident_t* nonnull @2) #6
  %7726 = call i32 @__kmpc_push_num_teams(%struct.ident_t* nonnull @2, i32 %7725, i32 %tms3754.09012, i32 1024) #6
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_teams(%struct.ident_t* nonnull @2, i32 4, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, double*, double*, double*)* @.omp_outlined..158 to void (i32*, i32*, ...)*), i64 %indvars.iv9592, i8* %call3733, i8* %call3735, i8* %call3736) #6
  br label %omp_offload.cont3776.5

omp_offload.cont3776.5:                           ; preds = %omp_offload.failed3775.5, %omp_offload.cont3776.4
  %mul3781 = shl nsw i32 %tms3754.09012, 1
  %cmp3756 = icmp ult i32 %mul3781, 257
  br i1 %cmp3756, label %for.cond3760.preheader, label %for.cond3789.preheader.preheader65

for.cond3789.preheader.preheader65:               ; preds = %omp_offload.cont3776.5
  %7727 = add nuw nsw i64 %1964, 32
  %n.mod.vf = urem i64 %7727, 24
  %n.vec = sub nuw nsw i64 %7727, %n.mod.vf
  %cmp.n10082 = icmp eq i64 %n.mod.vf, 0
  br label %for.cond3789.preheader

for.inc5813.1:                                    ; preds = %for.inc5813
  %indvars.iv.next9448.1 = or i64 %indvars.iv9447, 2
  %arrayidx5801.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9448.1
  %7728 = load double, double* %arrayidx5801.2, align 8, !tbaa !62
  %arrayidx5803.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9448.1
  %7729 = load double, double* %arrayidx5803.2, align 8, !tbaa !62
  %cmp5804.2 = fcmp une double %7728, %7729
  br i1 %cmp5804.2, label %cleanup5823, label %for.inc5813.2

for.inc5813.2:                                    ; preds = %for.inc5813.1
  %indvars.iv.next9448.2 = or i64 %indvars.iv9447, 3
  %arrayidx5801.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9448.2
  %7730 = load double, double* %arrayidx5801.3, align 8, !tbaa !62
  %arrayidx5803.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9448.2
  %7731 = load double, double* %arrayidx5803.3, align 8, !tbaa !62
  %cmp5804.3 = fcmp une double %7730, %7731
  br i1 %cmp5804.3, label %cleanup5823, label %for.inc5813.3

for.inc5813.3:                                    ; preds = %for.inc5813.2
  %indvars.iv.next9448.3 = or i64 %indvars.iv9447, 4
  %arrayidx5801.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9448.3
  %7732 = load double, double* %arrayidx5801.4, align 8, !tbaa !62
  %arrayidx5803.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9448.3
  %7733 = load double, double* %arrayidx5803.4, align 8, !tbaa !62
  %cmp5804.4 = fcmp une double %7732, %7733
  br i1 %cmp5804.4, label %cleanup5823, label %for.inc5813.4

for.inc5813.4:                                    ; preds = %for.inc5813.3
  %indvars.iv.next9448.4 = or i64 %indvars.iv9447, 5
  %arrayidx5801.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9448.4
  %7734 = load double, double* %arrayidx5801.5, align 8, !tbaa !62
  %arrayidx5803.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9448.4
  %7735 = load double, double* %arrayidx5803.5, align 8, !tbaa !62
  %cmp5804.5 = fcmp une double %7734, %7735
  br i1 %cmp5804.5, label %cleanup5823, label %for.inc5813.5

for.inc5813.5:                                    ; preds = %for.inc5813.4
  %indvars.iv.next9448.5 = or i64 %indvars.iv9447, 6
  %arrayidx5801.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9448.5
  %7736 = load double, double* %arrayidx5801.6, align 8, !tbaa !62
  %arrayidx5803.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9448.5
  %7737 = load double, double* %arrayidx5803.6, align 8, !tbaa !62
  %cmp5804.6 = fcmp une double %7736, %7737
  br i1 %cmp5804.6, label %cleanup5823, label %for.inc5813.6

for.inc5813.6:                                    ; preds = %for.inc5813.5
  %indvars.iv.next9448.6 = or i64 %indvars.iv9447, 7
  %arrayidx5801.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9448.6
  %7738 = load double, double* %arrayidx5801.7, align 8, !tbaa !62
  %arrayidx5803.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9448.6
  %7739 = load double, double* %arrayidx5803.7, align 8, !tbaa !62
  %cmp5804.7 = fcmp une double %7738, %7739
  br i1 %cmp5804.7, label %cleanup5823, label %for.inc5813.7

for.inc5813.7:                                    ; preds = %for.inc5813.6
  %indvars.iv.next9448.7 = add nuw nsw i64 %indvars.iv9447, 8
  %cmp5796.7 = icmp ult i64 %indvars.iv.next9448.7, %indvars.iv9449
  br i1 %cmp5796.7, label %for.body5799, label %for.inc5821

for.inc5688.1:                                    ; preds = %for.inc5688
  %indvars.iv.next9456.1 = or i64 %indvars.iv9455, 2
  %arrayidx5676.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9456.1
  %7740 = load double, double* %arrayidx5676.2, align 8, !tbaa !62
  %arrayidx5678.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9456.1
  %7741 = load double, double* %arrayidx5678.2, align 8, !tbaa !62
  %cmp5679.2 = fcmp une double %7740, %7741
  br i1 %cmp5679.2, label %cleanup5698, label %for.inc5688.2

for.inc5688.2:                                    ; preds = %for.inc5688.1
  %indvars.iv.next9456.2 = or i64 %indvars.iv9455, 3
  %arrayidx5676.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9456.2
  %7742 = load double, double* %arrayidx5676.3, align 8, !tbaa !62
  %arrayidx5678.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9456.2
  %7743 = load double, double* %arrayidx5678.3, align 8, !tbaa !62
  %cmp5679.3 = fcmp une double %7742, %7743
  br i1 %cmp5679.3, label %cleanup5698, label %for.inc5688.3

for.inc5688.3:                                    ; preds = %for.inc5688.2
  %indvars.iv.next9456.3 = or i64 %indvars.iv9455, 4
  %arrayidx5676.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9456.3
  %7744 = load double, double* %arrayidx5676.4, align 8, !tbaa !62
  %arrayidx5678.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9456.3
  %7745 = load double, double* %arrayidx5678.4, align 8, !tbaa !62
  %cmp5679.4 = fcmp une double %7744, %7745
  br i1 %cmp5679.4, label %cleanup5698, label %for.inc5688.4

for.inc5688.4:                                    ; preds = %for.inc5688.3
  %indvars.iv.next9456.4 = or i64 %indvars.iv9455, 5
  %arrayidx5676.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9456.4
  %7746 = load double, double* %arrayidx5676.5, align 8, !tbaa !62
  %arrayidx5678.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9456.4
  %7747 = load double, double* %arrayidx5678.5, align 8, !tbaa !62
  %cmp5679.5 = fcmp une double %7746, %7747
  br i1 %cmp5679.5, label %cleanup5698, label %for.inc5688.5

for.inc5688.5:                                    ; preds = %for.inc5688.4
  %indvars.iv.next9456.5 = or i64 %indvars.iv9455, 6
  %arrayidx5676.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9456.5
  %7748 = load double, double* %arrayidx5676.6, align 8, !tbaa !62
  %arrayidx5678.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9456.5
  %7749 = load double, double* %arrayidx5678.6, align 8, !tbaa !62
  %cmp5679.6 = fcmp une double %7748, %7749
  br i1 %cmp5679.6, label %cleanup5698, label %for.inc5688.6

for.inc5688.6:                                    ; preds = %for.inc5688.5
  %indvars.iv.next9456.6 = or i64 %indvars.iv9455, 7
  %arrayidx5676.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9456.6
  %7750 = load double, double* %arrayidx5676.7, align 8, !tbaa !62
  %arrayidx5678.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9456.6
  %7751 = load double, double* %arrayidx5678.7, align 8, !tbaa !62
  %cmp5679.7 = fcmp une double %7750, %7751
  br i1 %cmp5679.7, label %cleanup5698, label %for.inc5688.7

for.inc5688.7:                                    ; preds = %for.inc5688.6
  %indvars.iv.next9456.7 = add nuw nsw i64 %indvars.iv9455, 8
  %cmp5671.7 = icmp ult i64 %indvars.iv.next9456.7, %indvars.iv9457
  br i1 %cmp5671.7, label %for.body5674, label %for.inc5696

for.inc5563.1:                                    ; preds = %for.inc5563
  %indvars.iv.next9464.1 = or i64 %indvars.iv9463, 2
  %arrayidx5551.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9464.1
  %7752 = load double, double* %arrayidx5551.2, align 8, !tbaa !62
  %arrayidx5553.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9464.1
  %7753 = load double, double* %arrayidx5553.2, align 8, !tbaa !62
  %cmp5554.2 = fcmp une double %7752, %7753
  br i1 %cmp5554.2, label %cleanup5573, label %for.inc5563.2

for.inc5563.2:                                    ; preds = %for.inc5563.1
  %indvars.iv.next9464.2 = or i64 %indvars.iv9463, 3
  %arrayidx5551.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9464.2
  %7754 = load double, double* %arrayidx5551.3, align 8, !tbaa !62
  %arrayidx5553.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9464.2
  %7755 = load double, double* %arrayidx5553.3, align 8, !tbaa !62
  %cmp5554.3 = fcmp une double %7754, %7755
  br i1 %cmp5554.3, label %cleanup5573, label %for.inc5563.3

for.inc5563.3:                                    ; preds = %for.inc5563.2
  %indvars.iv.next9464.3 = or i64 %indvars.iv9463, 4
  %arrayidx5551.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9464.3
  %7756 = load double, double* %arrayidx5551.4, align 8, !tbaa !62
  %arrayidx5553.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9464.3
  %7757 = load double, double* %arrayidx5553.4, align 8, !tbaa !62
  %cmp5554.4 = fcmp une double %7756, %7757
  br i1 %cmp5554.4, label %cleanup5573, label %for.inc5563.4

for.inc5563.4:                                    ; preds = %for.inc5563.3
  %indvars.iv.next9464.4 = or i64 %indvars.iv9463, 5
  %arrayidx5551.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9464.4
  %7758 = load double, double* %arrayidx5551.5, align 8, !tbaa !62
  %arrayidx5553.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9464.4
  %7759 = load double, double* %arrayidx5553.5, align 8, !tbaa !62
  %cmp5554.5 = fcmp une double %7758, %7759
  br i1 %cmp5554.5, label %cleanup5573, label %for.inc5563.5

for.inc5563.5:                                    ; preds = %for.inc5563.4
  %indvars.iv.next9464.5 = or i64 %indvars.iv9463, 6
  %arrayidx5551.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9464.5
  %7760 = load double, double* %arrayidx5551.6, align 8, !tbaa !62
  %arrayidx5553.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9464.5
  %7761 = load double, double* %arrayidx5553.6, align 8, !tbaa !62
  %cmp5554.6 = fcmp une double %7760, %7761
  br i1 %cmp5554.6, label %cleanup5573, label %for.inc5563.6

for.inc5563.6:                                    ; preds = %for.inc5563.5
  %indvars.iv.next9464.6 = or i64 %indvars.iv9463, 7
  %arrayidx5551.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9464.6
  %7762 = load double, double* %arrayidx5551.7, align 8, !tbaa !62
  %arrayidx5553.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9464.6
  %7763 = load double, double* %arrayidx5553.7, align 8, !tbaa !62
  %cmp5554.7 = fcmp une double %7762, %7763
  br i1 %cmp5554.7, label %cleanup5573, label %for.inc5563.7

for.inc5563.7:                                    ; preds = %for.inc5563.6
  %indvars.iv.next9464.7 = add nuw nsw i64 %indvars.iv9463, 8
  %cmp5546.7 = icmp ult i64 %indvars.iv.next9464.7, %indvars.iv9465
  br i1 %cmp5546.7, label %for.body5549, label %for.inc5571

for.inc5425.1:                                    ; preds = %for.inc5425
  %indvars.iv.next9472.1 = or i64 %indvars.iv9471, 2
  %arrayidx5413.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9472.1
  %7764 = load double, double* %arrayidx5413.2, align 8, !tbaa !62
  %arrayidx5415.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9472.1
  %7765 = load double, double* %arrayidx5415.2, align 8, !tbaa !62
  %cmp5416.2 = fcmp une double %7764, %7765
  br i1 %cmp5416.2, label %cleanup5435, label %for.inc5425.2

for.inc5425.2:                                    ; preds = %for.inc5425.1
  %indvars.iv.next9472.2 = or i64 %indvars.iv9471, 3
  %arrayidx5413.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9472.2
  %7766 = load double, double* %arrayidx5413.3, align 8, !tbaa !62
  %arrayidx5415.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9472.2
  %7767 = load double, double* %arrayidx5415.3, align 8, !tbaa !62
  %cmp5416.3 = fcmp une double %7766, %7767
  br i1 %cmp5416.3, label %cleanup5435, label %for.inc5425.3

for.inc5425.3:                                    ; preds = %for.inc5425.2
  %indvars.iv.next9472.3 = or i64 %indvars.iv9471, 4
  %arrayidx5413.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9472.3
  %7768 = load double, double* %arrayidx5413.4, align 8, !tbaa !62
  %arrayidx5415.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9472.3
  %7769 = load double, double* %arrayidx5415.4, align 8, !tbaa !62
  %cmp5416.4 = fcmp une double %7768, %7769
  br i1 %cmp5416.4, label %cleanup5435, label %for.inc5425.4

for.inc5425.4:                                    ; preds = %for.inc5425.3
  %indvars.iv.next9472.4 = or i64 %indvars.iv9471, 5
  %arrayidx5413.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9472.4
  %7770 = load double, double* %arrayidx5413.5, align 8, !tbaa !62
  %arrayidx5415.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9472.4
  %7771 = load double, double* %arrayidx5415.5, align 8, !tbaa !62
  %cmp5416.5 = fcmp une double %7770, %7771
  br i1 %cmp5416.5, label %cleanup5435, label %for.inc5425.5

for.inc5425.5:                                    ; preds = %for.inc5425.4
  %indvars.iv.next9472.5 = or i64 %indvars.iv9471, 6
  %arrayidx5413.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9472.5
  %7772 = load double, double* %arrayidx5413.6, align 8, !tbaa !62
  %arrayidx5415.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9472.5
  %7773 = load double, double* %arrayidx5415.6, align 8, !tbaa !62
  %cmp5416.6 = fcmp une double %7772, %7773
  br i1 %cmp5416.6, label %cleanup5435, label %for.inc5425.6

for.inc5425.6:                                    ; preds = %for.inc5425.5
  %indvars.iv.next9472.6 = or i64 %indvars.iv9471, 7
  %arrayidx5413.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9472.6
  %7774 = load double, double* %arrayidx5413.7, align 8, !tbaa !62
  %arrayidx5415.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9472.6
  %7775 = load double, double* %arrayidx5415.7, align 8, !tbaa !62
  %cmp5416.7 = fcmp une double %7774, %7775
  br i1 %cmp5416.7, label %cleanup5435, label %for.inc5425.7

for.inc5425.7:                                    ; preds = %for.inc5425.6
  %indvars.iv.next9472.7 = add nuw nsw i64 %indvars.iv9471, 8
  %cmp5408.7 = icmp ult i64 %indvars.iv.next9472.7, %indvars.iv9473
  br i1 %cmp5408.7, label %for.body5411, label %for.inc5433

for.inc5300.1:                                    ; preds = %for.inc5300
  %indvars.iv.next9480.1 = or i64 %indvars.iv9479, 2
  %arrayidx5288.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9480.1
  %7776 = load double, double* %arrayidx5288.2, align 8, !tbaa !62
  %arrayidx5290.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9480.1
  %7777 = load double, double* %arrayidx5290.2, align 8, !tbaa !62
  %cmp5291.2 = fcmp une double %7776, %7777
  br i1 %cmp5291.2, label %cleanup5310, label %for.inc5300.2

for.inc5300.2:                                    ; preds = %for.inc5300.1
  %indvars.iv.next9480.2 = or i64 %indvars.iv9479, 3
  %arrayidx5288.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9480.2
  %7778 = load double, double* %arrayidx5288.3, align 8, !tbaa !62
  %arrayidx5290.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9480.2
  %7779 = load double, double* %arrayidx5290.3, align 8, !tbaa !62
  %cmp5291.3 = fcmp une double %7778, %7779
  br i1 %cmp5291.3, label %cleanup5310, label %for.inc5300.3

for.inc5300.3:                                    ; preds = %for.inc5300.2
  %indvars.iv.next9480.3 = or i64 %indvars.iv9479, 4
  %arrayidx5288.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9480.3
  %7780 = load double, double* %arrayidx5288.4, align 8, !tbaa !62
  %arrayidx5290.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9480.3
  %7781 = load double, double* %arrayidx5290.4, align 8, !tbaa !62
  %cmp5291.4 = fcmp une double %7780, %7781
  br i1 %cmp5291.4, label %cleanup5310, label %for.inc5300.4

for.inc5300.4:                                    ; preds = %for.inc5300.3
  %indvars.iv.next9480.4 = or i64 %indvars.iv9479, 5
  %arrayidx5288.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9480.4
  %7782 = load double, double* %arrayidx5288.5, align 8, !tbaa !62
  %arrayidx5290.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9480.4
  %7783 = load double, double* %arrayidx5290.5, align 8, !tbaa !62
  %cmp5291.5 = fcmp une double %7782, %7783
  br i1 %cmp5291.5, label %cleanup5310, label %for.inc5300.5

for.inc5300.5:                                    ; preds = %for.inc5300.4
  %indvars.iv.next9480.5 = or i64 %indvars.iv9479, 6
  %arrayidx5288.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9480.5
  %7784 = load double, double* %arrayidx5288.6, align 8, !tbaa !62
  %arrayidx5290.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9480.5
  %7785 = load double, double* %arrayidx5290.6, align 8, !tbaa !62
  %cmp5291.6 = fcmp une double %7784, %7785
  br i1 %cmp5291.6, label %cleanup5310, label %for.inc5300.6

for.inc5300.6:                                    ; preds = %for.inc5300.5
  %indvars.iv.next9480.6 = or i64 %indvars.iv9479, 7
  %arrayidx5288.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9480.6
  %7786 = load double, double* %arrayidx5288.7, align 8, !tbaa !62
  %arrayidx5290.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9480.6
  %7787 = load double, double* %arrayidx5290.7, align 8, !tbaa !62
  %cmp5291.7 = fcmp une double %7786, %7787
  br i1 %cmp5291.7, label %cleanup5310, label %for.inc5300.7

for.inc5300.7:                                    ; preds = %for.inc5300.6
  %indvars.iv.next9480.7 = add nuw nsw i64 %indvars.iv9479, 8
  %cmp5283.7 = icmp ult i64 %indvars.iv.next9480.7, %indvars.iv9481
  br i1 %cmp5283.7, label %for.body5286, label %for.inc5308

for.inc5162.1:                                    ; preds = %for.inc5162
  %indvars.iv.next9488.1 = or i64 %indvars.iv9487, 2
  %arrayidx5150.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9488.1
  %7788 = load double, double* %arrayidx5150.2, align 8, !tbaa !62
  %arrayidx5152.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9488.1
  %7789 = load double, double* %arrayidx5152.2, align 8, !tbaa !62
  %cmp5153.2 = fcmp une double %7788, %7789
  br i1 %cmp5153.2, label %cleanup5172, label %for.inc5162.2

for.inc5162.2:                                    ; preds = %for.inc5162.1
  %indvars.iv.next9488.2 = or i64 %indvars.iv9487, 3
  %arrayidx5150.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9488.2
  %7790 = load double, double* %arrayidx5150.3, align 8, !tbaa !62
  %arrayidx5152.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9488.2
  %7791 = load double, double* %arrayidx5152.3, align 8, !tbaa !62
  %cmp5153.3 = fcmp une double %7790, %7791
  br i1 %cmp5153.3, label %cleanup5172, label %for.inc5162.3

for.inc5162.3:                                    ; preds = %for.inc5162.2
  %indvars.iv.next9488.3 = or i64 %indvars.iv9487, 4
  %arrayidx5150.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9488.3
  %7792 = load double, double* %arrayidx5150.4, align 8, !tbaa !62
  %arrayidx5152.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9488.3
  %7793 = load double, double* %arrayidx5152.4, align 8, !tbaa !62
  %cmp5153.4 = fcmp une double %7792, %7793
  br i1 %cmp5153.4, label %cleanup5172, label %for.inc5162.4

for.inc5162.4:                                    ; preds = %for.inc5162.3
  %indvars.iv.next9488.4 = or i64 %indvars.iv9487, 5
  %arrayidx5150.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9488.4
  %7794 = load double, double* %arrayidx5150.5, align 8, !tbaa !62
  %arrayidx5152.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9488.4
  %7795 = load double, double* %arrayidx5152.5, align 8, !tbaa !62
  %cmp5153.5 = fcmp une double %7794, %7795
  br i1 %cmp5153.5, label %cleanup5172, label %for.inc5162.5

for.inc5162.5:                                    ; preds = %for.inc5162.4
  %indvars.iv.next9488.5 = or i64 %indvars.iv9487, 6
  %arrayidx5150.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9488.5
  %7796 = load double, double* %arrayidx5150.6, align 8, !tbaa !62
  %arrayidx5152.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9488.5
  %7797 = load double, double* %arrayidx5152.6, align 8, !tbaa !62
  %cmp5153.6 = fcmp une double %7796, %7797
  br i1 %cmp5153.6, label %cleanup5172, label %for.inc5162.6

for.inc5162.6:                                    ; preds = %for.inc5162.5
  %indvars.iv.next9488.6 = or i64 %indvars.iv9487, 7
  %arrayidx5150.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9488.6
  %7798 = load double, double* %arrayidx5150.7, align 8, !tbaa !62
  %arrayidx5152.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9488.6
  %7799 = load double, double* %arrayidx5152.7, align 8, !tbaa !62
  %cmp5153.7 = fcmp une double %7798, %7799
  br i1 %cmp5153.7, label %cleanup5172, label %for.inc5162.7

for.inc5162.7:                                    ; preds = %for.inc5162.6
  %indvars.iv.next9488.7 = add nuw nsw i64 %indvars.iv9487, 8
  %cmp5145.7 = icmp ult i64 %indvars.iv.next9488.7, %indvars.iv9489
  br i1 %cmp5145.7, label %for.body5148, label %for.inc5170

for.inc5037.1:                                    ; preds = %for.inc5037
  %indvars.iv.next9496.1 = or i64 %indvars.iv9495, 2
  %arrayidx5025.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9496.1
  %7800 = load double, double* %arrayidx5025.2, align 8, !tbaa !62
  %arrayidx5027.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9496.1
  %7801 = load double, double* %arrayidx5027.2, align 8, !tbaa !62
  %cmp5028.2 = fcmp une double %7800, %7801
  br i1 %cmp5028.2, label %cleanup5047, label %for.inc5037.2

for.inc5037.2:                                    ; preds = %for.inc5037.1
  %indvars.iv.next9496.2 = or i64 %indvars.iv9495, 3
  %arrayidx5025.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9496.2
  %7802 = load double, double* %arrayidx5025.3, align 8, !tbaa !62
  %arrayidx5027.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9496.2
  %7803 = load double, double* %arrayidx5027.3, align 8, !tbaa !62
  %cmp5028.3 = fcmp une double %7802, %7803
  br i1 %cmp5028.3, label %cleanup5047, label %for.inc5037.3

for.inc5037.3:                                    ; preds = %for.inc5037.2
  %indvars.iv.next9496.3 = or i64 %indvars.iv9495, 4
  %arrayidx5025.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9496.3
  %7804 = load double, double* %arrayidx5025.4, align 8, !tbaa !62
  %arrayidx5027.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9496.3
  %7805 = load double, double* %arrayidx5027.4, align 8, !tbaa !62
  %cmp5028.4 = fcmp une double %7804, %7805
  br i1 %cmp5028.4, label %cleanup5047, label %for.inc5037.4

for.inc5037.4:                                    ; preds = %for.inc5037.3
  %indvars.iv.next9496.4 = or i64 %indvars.iv9495, 5
  %arrayidx5025.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9496.4
  %7806 = load double, double* %arrayidx5025.5, align 8, !tbaa !62
  %arrayidx5027.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9496.4
  %7807 = load double, double* %arrayidx5027.5, align 8, !tbaa !62
  %cmp5028.5 = fcmp une double %7806, %7807
  br i1 %cmp5028.5, label %cleanup5047, label %for.inc5037.5

for.inc5037.5:                                    ; preds = %for.inc5037.4
  %indvars.iv.next9496.5 = or i64 %indvars.iv9495, 6
  %arrayidx5025.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9496.5
  %7808 = load double, double* %arrayidx5025.6, align 8, !tbaa !62
  %arrayidx5027.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9496.5
  %7809 = load double, double* %arrayidx5027.6, align 8, !tbaa !62
  %cmp5028.6 = fcmp une double %7808, %7809
  br i1 %cmp5028.6, label %cleanup5047, label %for.inc5037.6

for.inc5037.6:                                    ; preds = %for.inc5037.5
  %indvars.iv.next9496.6 = or i64 %indvars.iv9495, 7
  %arrayidx5025.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9496.6
  %7810 = load double, double* %arrayidx5025.7, align 8, !tbaa !62
  %arrayidx5027.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9496.6
  %7811 = load double, double* %arrayidx5027.7, align 8, !tbaa !62
  %cmp5028.7 = fcmp une double %7810, %7811
  br i1 %cmp5028.7, label %cleanup5047, label %for.inc5037.7

for.inc5037.7:                                    ; preds = %for.inc5037.6
  %indvars.iv.next9496.7 = add nuw nsw i64 %indvars.iv9495, 8
  %cmp5020.7 = icmp ult i64 %indvars.iv.next9496.7, %indvars.iv9497
  br i1 %cmp5020.7, label %for.body5023, label %for.inc5045

for.inc4900.1:                                    ; preds = %for.inc4900
  %indvars.iv.next9504.1 = or i64 %indvars.iv9503, 2
  %arrayidx4888.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9504.1
  %7812 = load double, double* %arrayidx4888.2, align 8, !tbaa !62
  %arrayidx4890.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9504.1
  %7813 = load double, double* %arrayidx4890.2, align 8, !tbaa !62
  %cmp4891.2 = fcmp une double %7812, %7813
  br i1 %cmp4891.2, label %cleanup4910, label %for.inc4900.2

for.inc4900.2:                                    ; preds = %for.inc4900.1
  %indvars.iv.next9504.2 = or i64 %indvars.iv9503, 3
  %arrayidx4888.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9504.2
  %7814 = load double, double* %arrayidx4888.3, align 8, !tbaa !62
  %arrayidx4890.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9504.2
  %7815 = load double, double* %arrayidx4890.3, align 8, !tbaa !62
  %cmp4891.3 = fcmp une double %7814, %7815
  br i1 %cmp4891.3, label %cleanup4910, label %for.inc4900.3

for.inc4900.3:                                    ; preds = %for.inc4900.2
  %indvars.iv.next9504.3 = or i64 %indvars.iv9503, 4
  %arrayidx4888.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9504.3
  %7816 = load double, double* %arrayidx4888.4, align 8, !tbaa !62
  %arrayidx4890.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9504.3
  %7817 = load double, double* %arrayidx4890.4, align 8, !tbaa !62
  %cmp4891.4 = fcmp une double %7816, %7817
  br i1 %cmp4891.4, label %cleanup4910, label %for.inc4900.4

for.inc4900.4:                                    ; preds = %for.inc4900.3
  %indvars.iv.next9504.4 = or i64 %indvars.iv9503, 5
  %arrayidx4888.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9504.4
  %7818 = load double, double* %arrayidx4888.5, align 8, !tbaa !62
  %arrayidx4890.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9504.4
  %7819 = load double, double* %arrayidx4890.5, align 8, !tbaa !62
  %cmp4891.5 = fcmp une double %7818, %7819
  br i1 %cmp4891.5, label %cleanup4910, label %for.inc4900.5

for.inc4900.5:                                    ; preds = %for.inc4900.4
  %indvars.iv.next9504.5 = or i64 %indvars.iv9503, 6
  %arrayidx4888.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9504.5
  %7820 = load double, double* %arrayidx4888.6, align 8, !tbaa !62
  %arrayidx4890.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9504.5
  %7821 = load double, double* %arrayidx4890.6, align 8, !tbaa !62
  %cmp4891.6 = fcmp une double %7820, %7821
  br i1 %cmp4891.6, label %cleanup4910, label %for.inc4900.6

for.inc4900.6:                                    ; preds = %for.inc4900.5
  %indvars.iv.next9504.6 = or i64 %indvars.iv9503, 7
  %arrayidx4888.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9504.6
  %7822 = load double, double* %arrayidx4888.7, align 8, !tbaa !62
  %arrayidx4890.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9504.6
  %7823 = load double, double* %arrayidx4890.7, align 8, !tbaa !62
  %cmp4891.7 = fcmp une double %7822, %7823
  br i1 %cmp4891.7, label %cleanup4910, label %for.inc4900.7

for.inc4900.7:                                    ; preds = %for.inc4900.6
  %indvars.iv.next9504.7 = add nuw nsw i64 %indvars.iv9503, 8
  %cmp4883.7 = icmp ult i64 %indvars.iv.next9504.7, %indvars.iv9505
  br i1 %cmp4883.7, label %for.body4886, label %for.inc4908

for.inc4775.1:                                    ; preds = %for.inc4775
  %indvars.iv.next9512.1 = or i64 %indvars.iv9511, 2
  %arrayidx4763.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9512.1
  %7824 = load double, double* %arrayidx4763.2, align 8, !tbaa !62
  %arrayidx4765.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9512.1
  %7825 = load double, double* %arrayidx4765.2, align 8, !tbaa !62
  %cmp4766.2 = fcmp une double %7824, %7825
  br i1 %cmp4766.2, label %cleanup4785, label %for.inc4775.2

for.inc4775.2:                                    ; preds = %for.inc4775.1
  %indvars.iv.next9512.2 = or i64 %indvars.iv9511, 3
  %arrayidx4763.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9512.2
  %7826 = load double, double* %arrayidx4763.3, align 8, !tbaa !62
  %arrayidx4765.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9512.2
  %7827 = load double, double* %arrayidx4765.3, align 8, !tbaa !62
  %cmp4766.3 = fcmp une double %7826, %7827
  br i1 %cmp4766.3, label %cleanup4785, label %for.inc4775.3

for.inc4775.3:                                    ; preds = %for.inc4775.2
  %indvars.iv.next9512.3 = or i64 %indvars.iv9511, 4
  %arrayidx4763.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9512.3
  %7828 = load double, double* %arrayidx4763.4, align 8, !tbaa !62
  %arrayidx4765.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9512.3
  %7829 = load double, double* %arrayidx4765.4, align 8, !tbaa !62
  %cmp4766.4 = fcmp une double %7828, %7829
  br i1 %cmp4766.4, label %cleanup4785, label %for.inc4775.4

for.inc4775.4:                                    ; preds = %for.inc4775.3
  %indvars.iv.next9512.4 = or i64 %indvars.iv9511, 5
  %arrayidx4763.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9512.4
  %7830 = load double, double* %arrayidx4763.5, align 8, !tbaa !62
  %arrayidx4765.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9512.4
  %7831 = load double, double* %arrayidx4765.5, align 8, !tbaa !62
  %cmp4766.5 = fcmp une double %7830, %7831
  br i1 %cmp4766.5, label %cleanup4785, label %for.inc4775.5

for.inc4775.5:                                    ; preds = %for.inc4775.4
  %indvars.iv.next9512.5 = or i64 %indvars.iv9511, 6
  %arrayidx4763.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9512.5
  %7832 = load double, double* %arrayidx4763.6, align 8, !tbaa !62
  %arrayidx4765.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9512.5
  %7833 = load double, double* %arrayidx4765.6, align 8, !tbaa !62
  %cmp4766.6 = fcmp une double %7832, %7833
  br i1 %cmp4766.6, label %cleanup4785, label %for.inc4775.6

for.inc4775.6:                                    ; preds = %for.inc4775.5
  %indvars.iv.next9512.6 = or i64 %indvars.iv9511, 7
  %arrayidx4763.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9512.6
  %7834 = load double, double* %arrayidx4763.7, align 8, !tbaa !62
  %arrayidx4765.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9512.6
  %7835 = load double, double* %arrayidx4765.7, align 8, !tbaa !62
  %cmp4766.7 = fcmp une double %7834, %7835
  br i1 %cmp4766.7, label %cleanup4785, label %for.inc4775.7

for.inc4775.7:                                    ; preds = %for.inc4775.6
  %indvars.iv.next9512.7 = add nuw nsw i64 %indvars.iv9511, 8
  %cmp4758.7 = icmp ult i64 %indvars.iv.next9512.7, %indvars.iv9513
  br i1 %cmp4758.7, label %for.body4761, label %for.inc4783

for.inc4650.1:                                    ; preds = %for.inc4650
  %indvars.iv.next9523.1 = or i64 %indvars.iv9522, 2
  %arrayidx4638.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9523.1
  %7836 = load double, double* %arrayidx4638.2, align 8, !tbaa !62
  %arrayidx4640.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9523.1
  %7837 = load double, double* %arrayidx4640.2, align 8, !tbaa !62
  %cmp4641.2 = fcmp une double %7836, %7837
  br i1 %cmp4641.2, label %cleanup4660, label %for.inc4650.2

for.inc4650.2:                                    ; preds = %for.inc4650.1
  %indvars.iv.next9523.2 = or i64 %indvars.iv9522, 3
  %arrayidx4638.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9523.2
  %7838 = load double, double* %arrayidx4638.3, align 8, !tbaa !62
  %arrayidx4640.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9523.2
  %7839 = load double, double* %arrayidx4640.3, align 8, !tbaa !62
  %cmp4641.3 = fcmp une double %7838, %7839
  br i1 %cmp4641.3, label %cleanup4660, label %for.inc4650.3

for.inc4650.3:                                    ; preds = %for.inc4650.2
  %indvars.iv.next9523.3 = or i64 %indvars.iv9522, 4
  %arrayidx4638.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9523.3
  %7840 = load double, double* %arrayidx4638.4, align 8, !tbaa !62
  %arrayidx4640.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9523.3
  %7841 = load double, double* %arrayidx4640.4, align 8, !tbaa !62
  %cmp4641.4 = fcmp une double %7840, %7841
  br i1 %cmp4641.4, label %cleanup4660, label %for.inc4650.4

for.inc4650.4:                                    ; preds = %for.inc4650.3
  %indvars.iv.next9523.4 = or i64 %indvars.iv9522, 5
  %arrayidx4638.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9523.4
  %7842 = load double, double* %arrayidx4638.5, align 8, !tbaa !62
  %arrayidx4640.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9523.4
  %7843 = load double, double* %arrayidx4640.5, align 8, !tbaa !62
  %cmp4641.5 = fcmp une double %7842, %7843
  br i1 %cmp4641.5, label %cleanup4660, label %for.inc4650.5

for.inc4650.5:                                    ; preds = %for.inc4650.4
  %indvars.iv.next9523.5 = or i64 %indvars.iv9522, 6
  %arrayidx4638.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9523.5
  %7844 = load double, double* %arrayidx4638.6, align 8, !tbaa !62
  %arrayidx4640.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9523.5
  %7845 = load double, double* %arrayidx4640.6, align 8, !tbaa !62
  %cmp4641.6 = fcmp une double %7844, %7845
  br i1 %cmp4641.6, label %cleanup4660, label %for.inc4650.6

for.inc4650.6:                                    ; preds = %for.inc4650.5
  %indvars.iv.next9523.6 = or i64 %indvars.iv9522, 7
  %arrayidx4638.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9523.6
  %7846 = load double, double* %arrayidx4638.7, align 8, !tbaa !62
  %arrayidx4640.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9523.6
  %7847 = load double, double* %arrayidx4640.7, align 8, !tbaa !62
  %cmp4641.7 = fcmp une double %7846, %7847
  br i1 %cmp4641.7, label %cleanup4660, label %for.inc4650.7

for.inc4650.7:                                    ; preds = %for.inc4650.6
  %indvars.iv.next9523.7 = add nuw nsw i64 %indvars.iv9522, 8
  %cmp4633.7 = icmp ult i64 %indvars.iv.next9523.7, %indvars.iv9524
  br i1 %cmp4633.7, label %for.body4636, label %for.inc4658

for.inc4538.1:                                    ; preds = %for.inc4538
  %indvars.iv.next9531.1 = or i64 %indvars.iv9530, 2
  %arrayidx4526.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9531.1
  %7848 = load double, double* %arrayidx4526.2, align 8, !tbaa !62
  %arrayidx4528.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9531.1
  %7849 = load double, double* %arrayidx4528.2, align 8, !tbaa !62
  %cmp4529.2 = fcmp une double %7848, %7849
  br i1 %cmp4529.2, label %cleanup4548, label %for.inc4538.2

for.inc4538.2:                                    ; preds = %for.inc4538.1
  %indvars.iv.next9531.2 = or i64 %indvars.iv9530, 3
  %arrayidx4526.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9531.2
  %7850 = load double, double* %arrayidx4526.3, align 8, !tbaa !62
  %arrayidx4528.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9531.2
  %7851 = load double, double* %arrayidx4528.3, align 8, !tbaa !62
  %cmp4529.3 = fcmp une double %7850, %7851
  br i1 %cmp4529.3, label %cleanup4548, label %for.inc4538.3

for.inc4538.3:                                    ; preds = %for.inc4538.2
  %indvars.iv.next9531.3 = or i64 %indvars.iv9530, 4
  %arrayidx4526.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9531.3
  %7852 = load double, double* %arrayidx4526.4, align 8, !tbaa !62
  %arrayidx4528.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9531.3
  %7853 = load double, double* %arrayidx4528.4, align 8, !tbaa !62
  %cmp4529.4 = fcmp une double %7852, %7853
  br i1 %cmp4529.4, label %cleanup4548, label %for.inc4538.4

for.inc4538.4:                                    ; preds = %for.inc4538.3
  %indvars.iv.next9531.4 = or i64 %indvars.iv9530, 5
  %arrayidx4526.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9531.4
  %7854 = load double, double* %arrayidx4526.5, align 8, !tbaa !62
  %arrayidx4528.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9531.4
  %7855 = load double, double* %arrayidx4528.5, align 8, !tbaa !62
  %cmp4529.5 = fcmp une double %7854, %7855
  br i1 %cmp4529.5, label %cleanup4548, label %for.inc4538.5

for.inc4538.5:                                    ; preds = %for.inc4538.4
  %indvars.iv.next9531.5 = or i64 %indvars.iv9530, 6
  %arrayidx4526.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9531.5
  %7856 = load double, double* %arrayidx4526.6, align 8, !tbaa !62
  %arrayidx4528.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9531.5
  %7857 = load double, double* %arrayidx4528.6, align 8, !tbaa !62
  %cmp4529.6 = fcmp une double %7856, %7857
  br i1 %cmp4529.6, label %cleanup4548, label %for.inc4538.6

for.inc4538.6:                                    ; preds = %for.inc4538.5
  %indvars.iv.next9531.6 = or i64 %indvars.iv9530, 7
  %arrayidx4526.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9531.6
  %7858 = load double, double* %arrayidx4526.7, align 8, !tbaa !62
  %arrayidx4528.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9531.6
  %7859 = load double, double* %arrayidx4528.7, align 8, !tbaa !62
  %cmp4529.7 = fcmp une double %7858, %7859
  br i1 %cmp4529.7, label %cleanup4548, label %for.inc4538.7

for.inc4538.7:                                    ; preds = %for.inc4538.6
  %indvars.iv.next9531.7 = add nuw nsw i64 %indvars.iv9530, 8
  %cmp4521.7 = icmp ult i64 %indvars.iv.next9531.7, %indvars.iv9532
  br i1 %cmp4521.7, label %for.body4524, label %for.inc4546

for.inc4413.1:                                    ; preds = %for.inc4413
  %indvars.iv.next9542.1 = or i64 %indvars.iv9541, 2
  %arrayidx4401.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9542.1
  %7860 = load double, double* %arrayidx4401.2, align 8, !tbaa !62
  %arrayidx4403.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9542.1
  %7861 = load double, double* %arrayidx4403.2, align 8, !tbaa !62
  %cmp4404.2 = fcmp une double %7860, %7861
  br i1 %cmp4404.2, label %cleanup4423, label %for.inc4413.2

for.inc4413.2:                                    ; preds = %for.inc4413.1
  %indvars.iv.next9542.2 = or i64 %indvars.iv9541, 3
  %arrayidx4401.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9542.2
  %7862 = load double, double* %arrayidx4401.3, align 8, !tbaa !62
  %arrayidx4403.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9542.2
  %7863 = load double, double* %arrayidx4403.3, align 8, !tbaa !62
  %cmp4404.3 = fcmp une double %7862, %7863
  br i1 %cmp4404.3, label %cleanup4423, label %for.inc4413.3

for.inc4413.3:                                    ; preds = %for.inc4413.2
  %indvars.iv.next9542.3 = or i64 %indvars.iv9541, 4
  %arrayidx4401.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9542.3
  %7864 = load double, double* %arrayidx4401.4, align 8, !tbaa !62
  %arrayidx4403.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9542.3
  %7865 = load double, double* %arrayidx4403.4, align 8, !tbaa !62
  %cmp4404.4 = fcmp une double %7864, %7865
  br i1 %cmp4404.4, label %cleanup4423, label %for.inc4413.4

for.inc4413.4:                                    ; preds = %for.inc4413.3
  %indvars.iv.next9542.4 = or i64 %indvars.iv9541, 5
  %arrayidx4401.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9542.4
  %7866 = load double, double* %arrayidx4401.5, align 8, !tbaa !62
  %arrayidx4403.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9542.4
  %7867 = load double, double* %arrayidx4403.5, align 8, !tbaa !62
  %cmp4404.5 = fcmp une double %7866, %7867
  br i1 %cmp4404.5, label %cleanup4423, label %for.inc4413.5

for.inc4413.5:                                    ; preds = %for.inc4413.4
  %indvars.iv.next9542.5 = or i64 %indvars.iv9541, 6
  %arrayidx4401.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9542.5
  %7868 = load double, double* %arrayidx4401.6, align 8, !tbaa !62
  %arrayidx4403.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9542.5
  %7869 = load double, double* %arrayidx4403.6, align 8, !tbaa !62
  %cmp4404.6 = fcmp une double %7868, %7869
  br i1 %cmp4404.6, label %cleanup4423, label %for.inc4413.6

for.inc4413.6:                                    ; preds = %for.inc4413.5
  %indvars.iv.next9542.6 = or i64 %indvars.iv9541, 7
  %arrayidx4401.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9542.6
  %7870 = load double, double* %arrayidx4401.7, align 8, !tbaa !62
  %arrayidx4403.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9542.6
  %7871 = load double, double* %arrayidx4403.7, align 8, !tbaa !62
  %cmp4404.7 = fcmp une double %7870, %7871
  br i1 %cmp4404.7, label %cleanup4423, label %for.inc4413.7

for.inc4413.7:                                    ; preds = %for.inc4413.6
  %indvars.iv.next9542.7 = add nuw nsw i64 %indvars.iv9541, 8
  %cmp4396.7 = icmp ult i64 %indvars.iv.next9542.7, %indvars.iv9543
  br i1 %cmp4396.7, label %for.body4399, label %for.inc4421

for.inc4301.1:                                    ; preds = %for.inc4301
  %indvars.iv.next9550.1 = or i64 %indvars.iv9549, 2
  %arrayidx4289.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9550.1
  %7872 = load double, double* %arrayidx4289.2, align 8, !tbaa !62
  %arrayidx4291.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9550.1
  %7873 = load double, double* %arrayidx4291.2, align 8, !tbaa !62
  %cmp4292.2 = fcmp une double %7872, %7873
  br i1 %cmp4292.2, label %cleanup4311, label %for.inc4301.2

for.inc4301.2:                                    ; preds = %for.inc4301.1
  %indvars.iv.next9550.2 = or i64 %indvars.iv9549, 3
  %arrayidx4289.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9550.2
  %7874 = load double, double* %arrayidx4289.3, align 8, !tbaa !62
  %arrayidx4291.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9550.2
  %7875 = load double, double* %arrayidx4291.3, align 8, !tbaa !62
  %cmp4292.3 = fcmp une double %7874, %7875
  br i1 %cmp4292.3, label %cleanup4311, label %for.inc4301.3

for.inc4301.3:                                    ; preds = %for.inc4301.2
  %indvars.iv.next9550.3 = or i64 %indvars.iv9549, 4
  %arrayidx4289.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9550.3
  %7876 = load double, double* %arrayidx4289.4, align 8, !tbaa !62
  %arrayidx4291.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9550.3
  %7877 = load double, double* %arrayidx4291.4, align 8, !tbaa !62
  %cmp4292.4 = fcmp une double %7876, %7877
  br i1 %cmp4292.4, label %cleanup4311, label %for.inc4301.4

for.inc4301.4:                                    ; preds = %for.inc4301.3
  %indvars.iv.next9550.4 = or i64 %indvars.iv9549, 5
  %arrayidx4289.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9550.4
  %7878 = load double, double* %arrayidx4289.5, align 8, !tbaa !62
  %arrayidx4291.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9550.4
  %7879 = load double, double* %arrayidx4291.5, align 8, !tbaa !62
  %cmp4292.5 = fcmp une double %7878, %7879
  br i1 %cmp4292.5, label %cleanup4311, label %for.inc4301.5

for.inc4301.5:                                    ; preds = %for.inc4301.4
  %indvars.iv.next9550.5 = or i64 %indvars.iv9549, 6
  %arrayidx4289.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9550.5
  %7880 = load double, double* %arrayidx4289.6, align 8, !tbaa !62
  %arrayidx4291.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9550.5
  %7881 = load double, double* %arrayidx4291.6, align 8, !tbaa !62
  %cmp4292.6 = fcmp une double %7880, %7881
  br i1 %cmp4292.6, label %cleanup4311, label %for.inc4301.6

for.inc4301.6:                                    ; preds = %for.inc4301.5
  %indvars.iv.next9550.6 = or i64 %indvars.iv9549, 7
  %arrayidx4289.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9550.6
  %7882 = load double, double* %arrayidx4289.7, align 8, !tbaa !62
  %arrayidx4291.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9550.6
  %7883 = load double, double* %arrayidx4291.7, align 8, !tbaa !62
  %cmp4292.7 = fcmp une double %7882, %7883
  br i1 %cmp4292.7, label %cleanup4311, label %for.inc4301.7

for.inc4301.7:                                    ; preds = %for.inc4301.6
  %indvars.iv.next9550.7 = add nuw nsw i64 %indvars.iv9549, 8
  %cmp4284.7 = icmp ult i64 %indvars.iv.next9550.7, %indvars.iv9551
  br i1 %cmp4284.7, label %for.body4287, label %for.inc4309

for.inc4176.1:                                    ; preds = %for.inc4176
  %indvars.iv.next9561.1 = or i64 %indvars.iv9560, 2
  %arrayidx4164.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9561.1
  %7884 = load double, double* %arrayidx4164.2, align 8, !tbaa !62
  %arrayidx4166.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9561.1
  %7885 = load double, double* %arrayidx4166.2, align 8, !tbaa !62
  %cmp4167.2 = fcmp une double %7884, %7885
  br i1 %cmp4167.2, label %cleanup4186, label %for.inc4176.2

for.inc4176.2:                                    ; preds = %for.inc4176.1
  %indvars.iv.next9561.2 = or i64 %indvars.iv9560, 3
  %arrayidx4164.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9561.2
  %7886 = load double, double* %arrayidx4164.3, align 8, !tbaa !62
  %arrayidx4166.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9561.2
  %7887 = load double, double* %arrayidx4166.3, align 8, !tbaa !62
  %cmp4167.3 = fcmp une double %7886, %7887
  br i1 %cmp4167.3, label %cleanup4186, label %for.inc4176.3

for.inc4176.3:                                    ; preds = %for.inc4176.2
  %indvars.iv.next9561.3 = or i64 %indvars.iv9560, 4
  %arrayidx4164.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9561.3
  %7888 = load double, double* %arrayidx4164.4, align 8, !tbaa !62
  %arrayidx4166.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9561.3
  %7889 = load double, double* %arrayidx4166.4, align 8, !tbaa !62
  %cmp4167.4 = fcmp une double %7888, %7889
  br i1 %cmp4167.4, label %cleanup4186, label %for.inc4176.4

for.inc4176.4:                                    ; preds = %for.inc4176.3
  %indvars.iv.next9561.4 = or i64 %indvars.iv9560, 5
  %arrayidx4164.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9561.4
  %7890 = load double, double* %arrayidx4164.5, align 8, !tbaa !62
  %arrayidx4166.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9561.4
  %7891 = load double, double* %arrayidx4166.5, align 8, !tbaa !62
  %cmp4167.5 = fcmp une double %7890, %7891
  br i1 %cmp4167.5, label %cleanup4186, label %for.inc4176.5

for.inc4176.5:                                    ; preds = %for.inc4176.4
  %indvars.iv.next9561.5 = or i64 %indvars.iv9560, 6
  %arrayidx4164.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9561.5
  %7892 = load double, double* %arrayidx4164.6, align 8, !tbaa !62
  %arrayidx4166.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9561.5
  %7893 = load double, double* %arrayidx4166.6, align 8, !tbaa !62
  %cmp4167.6 = fcmp une double %7892, %7893
  br i1 %cmp4167.6, label %cleanup4186, label %for.inc4176.6

for.inc4176.6:                                    ; preds = %for.inc4176.5
  %indvars.iv.next9561.6 = or i64 %indvars.iv9560, 7
  %arrayidx4164.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9561.6
  %7894 = load double, double* %arrayidx4164.7, align 8, !tbaa !62
  %arrayidx4166.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9561.6
  %7895 = load double, double* %arrayidx4166.7, align 8, !tbaa !62
  %cmp4167.7 = fcmp une double %7894, %7895
  br i1 %cmp4167.7, label %cleanup4186, label %for.inc4176.7

for.inc4176.7:                                    ; preds = %for.inc4176.6
  %indvars.iv.next9561.7 = add nuw nsw i64 %indvars.iv9560, 8
  %cmp4159.7 = icmp ult i64 %indvars.iv.next9561.7, %indvars.iv9562
  br i1 %cmp4159.7, label %for.body4162, label %for.inc4184

for.inc4064.1:                                    ; preds = %for.inc4064
  %indvars.iv.next9569.1 = or i64 %indvars.iv9568, 2
  %arrayidx4052.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9569.1
  %7896 = load double, double* %arrayidx4052.2, align 8, !tbaa !62
  %arrayidx4054.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9569.1
  %7897 = load double, double* %arrayidx4054.2, align 8, !tbaa !62
  %cmp4055.2 = fcmp une double %7896, %7897
  br i1 %cmp4055.2, label %cleanup4074, label %for.inc4064.2

for.inc4064.2:                                    ; preds = %for.inc4064.1
  %indvars.iv.next9569.2 = or i64 %indvars.iv9568, 3
  %arrayidx4052.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9569.2
  %7898 = load double, double* %arrayidx4052.3, align 8, !tbaa !62
  %arrayidx4054.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9569.2
  %7899 = load double, double* %arrayidx4054.3, align 8, !tbaa !62
  %cmp4055.3 = fcmp une double %7898, %7899
  br i1 %cmp4055.3, label %cleanup4074, label %for.inc4064.3

for.inc4064.3:                                    ; preds = %for.inc4064.2
  %indvars.iv.next9569.3 = or i64 %indvars.iv9568, 4
  %arrayidx4052.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9569.3
  %7900 = load double, double* %arrayidx4052.4, align 8, !tbaa !62
  %arrayidx4054.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9569.3
  %7901 = load double, double* %arrayidx4054.4, align 8, !tbaa !62
  %cmp4055.4 = fcmp une double %7900, %7901
  br i1 %cmp4055.4, label %cleanup4074, label %for.inc4064.4

for.inc4064.4:                                    ; preds = %for.inc4064.3
  %indvars.iv.next9569.4 = or i64 %indvars.iv9568, 5
  %arrayidx4052.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9569.4
  %7902 = load double, double* %arrayidx4052.5, align 8, !tbaa !62
  %arrayidx4054.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9569.4
  %7903 = load double, double* %arrayidx4054.5, align 8, !tbaa !62
  %cmp4055.5 = fcmp une double %7902, %7903
  br i1 %cmp4055.5, label %cleanup4074, label %for.inc4064.5

for.inc4064.5:                                    ; preds = %for.inc4064.4
  %indvars.iv.next9569.5 = or i64 %indvars.iv9568, 6
  %arrayidx4052.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9569.5
  %7904 = load double, double* %arrayidx4052.6, align 8, !tbaa !62
  %arrayidx4054.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9569.5
  %7905 = load double, double* %arrayidx4054.6, align 8, !tbaa !62
  %cmp4055.6 = fcmp une double %7904, %7905
  br i1 %cmp4055.6, label %cleanup4074, label %for.inc4064.6

for.inc4064.6:                                    ; preds = %for.inc4064.5
  %indvars.iv.next9569.6 = or i64 %indvars.iv9568, 7
  %arrayidx4052.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9569.6
  %7906 = load double, double* %arrayidx4052.7, align 8, !tbaa !62
  %arrayidx4054.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9569.6
  %7907 = load double, double* %arrayidx4054.7, align 8, !tbaa !62
  %cmp4055.7 = fcmp une double %7906, %7907
  br i1 %cmp4055.7, label %cleanup4074, label %for.inc4064.7

for.inc4064.7:                                    ; preds = %for.inc4064.6
  %indvars.iv.next9569.7 = add nuw nsw i64 %indvars.iv9568, 8
  %cmp4047.7 = icmp ult i64 %indvars.iv.next9569.7, %indvars.iv9570
  br i1 %cmp4047.7, label %for.body4050, label %for.inc4072

for.inc3940.1:                                    ; preds = %for.inc3940
  %indvars.iv.next9580.1 = or i64 %indvars.iv9579, 2
  %arrayidx3928.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9580.1
  %7908 = load double, double* %arrayidx3928.2, align 8, !tbaa !62
  %arrayidx3930.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9580.1
  %7909 = load double, double* %arrayidx3930.2, align 8, !tbaa !62
  %cmp3931.2 = fcmp une double %7908, %7909
  br i1 %cmp3931.2, label %cleanup3950, label %for.inc3940.2

for.inc3940.2:                                    ; preds = %for.inc3940.1
  %indvars.iv.next9580.2 = or i64 %indvars.iv9579, 3
  %arrayidx3928.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9580.2
  %7910 = load double, double* %arrayidx3928.3, align 8, !tbaa !62
  %arrayidx3930.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9580.2
  %7911 = load double, double* %arrayidx3930.3, align 8, !tbaa !62
  %cmp3931.3 = fcmp une double %7910, %7911
  br i1 %cmp3931.3, label %cleanup3950, label %for.inc3940.3

for.inc3940.3:                                    ; preds = %for.inc3940.2
  %indvars.iv.next9580.3 = or i64 %indvars.iv9579, 4
  %arrayidx3928.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9580.3
  %7912 = load double, double* %arrayidx3928.4, align 8, !tbaa !62
  %arrayidx3930.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9580.3
  %7913 = load double, double* %arrayidx3930.4, align 8, !tbaa !62
  %cmp3931.4 = fcmp une double %7912, %7913
  br i1 %cmp3931.4, label %cleanup3950, label %for.inc3940.4

for.inc3940.4:                                    ; preds = %for.inc3940.3
  %indvars.iv.next9580.4 = or i64 %indvars.iv9579, 5
  %arrayidx3928.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9580.4
  %7914 = load double, double* %arrayidx3928.5, align 8, !tbaa !62
  %arrayidx3930.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9580.4
  %7915 = load double, double* %arrayidx3930.5, align 8, !tbaa !62
  %cmp3931.5 = fcmp une double %7914, %7915
  br i1 %cmp3931.5, label %cleanup3950, label %for.inc3940.5

for.inc3940.5:                                    ; preds = %for.inc3940.4
  %indvars.iv.next9580.5 = or i64 %indvars.iv9579, 6
  %arrayidx3928.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9580.5
  %7916 = load double, double* %arrayidx3928.6, align 8, !tbaa !62
  %arrayidx3930.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9580.5
  %7917 = load double, double* %arrayidx3930.6, align 8, !tbaa !62
  %cmp3931.6 = fcmp une double %7916, %7917
  br i1 %cmp3931.6, label %cleanup3950, label %for.inc3940.6

for.inc3940.6:                                    ; preds = %for.inc3940.5
  %indvars.iv.next9580.6 = or i64 %indvars.iv9579, 7
  %arrayidx3928.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9580.6
  %7918 = load double, double* %arrayidx3928.7, align 8, !tbaa !62
  %arrayidx3930.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9580.6
  %7919 = load double, double* %arrayidx3930.7, align 8, !tbaa !62
  %cmp3931.7 = fcmp une double %7918, %7919
  br i1 %cmp3931.7, label %cleanup3950, label %for.inc3940.7

for.inc3940.7:                                    ; preds = %for.inc3940.6
  %indvars.iv.next9580.7 = add nuw nsw i64 %indvars.iv9579, 8
  %cmp3923.7 = icmp ult i64 %indvars.iv.next9580.7, %indvars.iv9581
  br i1 %cmp3923.7, label %for.body3926, label %for.inc3948

for.inc3831.1:                                    ; preds = %for.inc3831
  %indvars.iv.next9591.1 = or i64 %indvars.iv9590, 2
  %arrayidx3819.2 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9591.1
  %7920 = load double, double* %arrayidx3819.2, align 8, !tbaa !62
  %arrayidx3821.2 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9591.1
  %7921 = load double, double* %arrayidx3821.2, align 8, !tbaa !62
  %cmp3822.2 = fcmp une double %7920, %7921
  br i1 %cmp3822.2, label %cleanup3838, label %for.inc3831.2

for.inc3831.2:                                    ; preds = %for.inc3831.1
  %indvars.iv.next9591.2 = or i64 %indvars.iv9590, 3
  %arrayidx3819.3 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9591.2
  %7922 = load double, double* %arrayidx3819.3, align 8, !tbaa !62
  %arrayidx3821.3 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9591.2
  %7923 = load double, double* %arrayidx3821.3, align 8, !tbaa !62
  %cmp3822.3 = fcmp une double %7922, %7923
  br i1 %cmp3822.3, label %cleanup3838, label %for.inc3831.3

for.inc3831.3:                                    ; preds = %for.inc3831.2
  %indvars.iv.next9591.3 = or i64 %indvars.iv9590, 4
  %arrayidx3819.4 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9591.3
  %7924 = load double, double* %arrayidx3819.4, align 8, !tbaa !62
  %arrayidx3821.4 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9591.3
  %7925 = load double, double* %arrayidx3821.4, align 8, !tbaa !62
  %cmp3822.4 = fcmp une double %7924, %7925
  br i1 %cmp3822.4, label %cleanup3838, label %for.inc3831.4

for.inc3831.4:                                    ; preds = %for.inc3831.3
  %indvars.iv.next9591.4 = or i64 %indvars.iv9590, 5
  %arrayidx3819.5 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9591.4
  %7926 = load double, double* %arrayidx3819.5, align 8, !tbaa !62
  %arrayidx3821.5 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9591.4
  %7927 = load double, double* %arrayidx3821.5, align 8, !tbaa !62
  %cmp3822.5 = fcmp une double %7926, %7927
  br i1 %cmp3822.5, label %cleanup3838, label %for.inc3831.5

for.inc3831.5:                                    ; preds = %for.inc3831.4
  %indvars.iv.next9591.5 = or i64 %indvars.iv9590, 6
  %arrayidx3819.6 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9591.5
  %7928 = load double, double* %arrayidx3819.6, align 8, !tbaa !62
  %arrayidx3821.6 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9591.5
  %7929 = load double, double* %arrayidx3821.6, align 8, !tbaa !62
  %cmp3822.6 = fcmp une double %7928, %7929
  br i1 %cmp3822.6, label %cleanup3838, label %for.inc3831.6

for.inc3831.6:                                    ; preds = %for.inc3831.5
  %indvars.iv.next9591.6 = or i64 %indvars.iv9590, 7
  %arrayidx3819.7 = getelementptr inbounds double, double* %1795, i64 %indvars.iv.next9591.6
  %7930 = load double, double* %arrayidx3819.7, align 8, !tbaa !62
  %arrayidx3821.7 = getelementptr inbounds double, double* %1794, i64 %indvars.iv.next9591.6
  %7931 = load double, double* %arrayidx3821.7, align 8, !tbaa !62
  %cmp3822.7 = fcmp une double %7930, %7931
  br i1 %cmp3822.7, label %cleanup3838, label %for.inc3831.7

for.inc3831.7:                                    ; preds = %for.inc3831.6
  %indvars.iv.next9591.7 = add nuw nsw i64 %indvars.iv9590, 8
  %cmp3814.7 = icmp ult i64 %indvars.iv.next9591.7, %indvars.iv9592
  br i1 %cmp3814.7, label %for.body3817, label %for.inc3836
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined.(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us.preheader, label %omp.inner.for.body

omp.inner.for.body.us.preheader:                  ; preds = %omp.inner.for.body.lr.ph
  %scevgep17 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 1
  %scevgep23 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 1
  %scevgep28 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 1
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 1
  %scevgep38 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 1
  br label %omp.inner.for.body.us

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.us.preheader, %.omp_outlined..7.exit.us
  %.omp.iv.012.us = phi i32 [ %add.us, %.omp_outlined..7.exit.us ], [ %7, %omp.inner.for.body.us.preheader ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !131
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !131
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !131
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !131
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !131
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !131
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !131
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !131
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !131
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !131
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !131
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !131
  %cmp329.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp329.i.us, label %.omp_outlined..7.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  %19 = icmp sgt i64 %18, %17
  %smax = select i1 %19, i64 %18, i64 %17
  %20 = sub nsw i64 1, %17
  %21 = add nsw i64 %20, %smax
  %min.iters.check = icmp ult i64 %21, 12
  br i1 %min.iters.check, label %omp.inner.for.body.i.us.preheader, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader.i.us
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %17
  %scevgep19 = getelementptr double, double* %scevgep17, i64 %smax
  %scevgep21 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %17
  %scevgep24 = getelementptr double, double* %scevgep23, i64 %smax
  %scevgep26 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %17
  %scevgep29 = getelementptr double, double* %scevgep28, i64 %smax
  %scevgep31 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %17
  %scevgep34 = getelementptr double, double* %scevgep33, i64 %smax
  %scevgep36 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %17
  %scevgep39 = getelementptr double, double* %scevgep38, i64 %smax
  %bound0 = icmp ult double* %scevgep, %scevgep24
  %bound1 = icmp ult double* %scevgep21, %scevgep19
  %found.conflict = and i1 %bound0, %bound1
  %bound041 = icmp ult double* %scevgep, %scevgep29
  %bound142 = icmp ult double* %scevgep26, %scevgep19
  %found.conflict43 = and i1 %bound041, %bound142
  %conflict.rdx = or i1 %found.conflict, %found.conflict43
  %bound044 = icmp ult double* %scevgep, %scevgep34
  %bound145 = icmp ult double* %scevgep31, %scevgep19
  %found.conflict46 = and i1 %bound044, %bound145
  %conflict.rdx47 = or i1 %found.conflict46, %conflict.rdx
  %bound048 = icmp ult double* %scevgep, %scevgep39
  %bound149 = icmp ult double* %scevgep36, %scevgep19
  %found.conflict50 = and i1 %bound048, %bound149
  %conflict.rdx51 = or i1 %found.conflict50, %conflict.rdx47
  %bound052 = icmp ult double* %scevgep21, %scevgep29
  %bound153 = icmp ult double* %scevgep26, %scevgep24
  %found.conflict54 = and i1 %bound052, %bound153
  %conflict.rdx55 = or i1 %found.conflict54, %conflict.rdx51
  %bound056 = icmp ult double* %scevgep21, %scevgep34
  %bound157 = icmp ult double* %scevgep31, %scevgep24
  %found.conflict58 = and i1 %bound056, %bound157
  %conflict.rdx59 = or i1 %found.conflict58, %conflict.rdx55
  %bound060 = icmp ult double* %scevgep21, %scevgep39
  %bound161 = icmp ult double* %scevgep36, %scevgep24
  %found.conflict62 = and i1 %bound060, %bound161
  %conflict.rdx63 = or i1 %found.conflict62, %conflict.rdx59
  br i1 %conflict.rdx63, label %omp.inner.for.body.i.us.preheader, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 12
  %n.vec = sub nsw i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %17
  %22 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !134, !noalias !131
  %24 = getelementptr inbounds double, double* %22, i64 2
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !134, !noalias !131
  %26 = getelementptr inbounds double, double* %22, i64 4
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !134, !noalias !131
  %28 = getelementptr inbounds double, double* %22, i64 6
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !134, !noalias !131
  %30 = getelementptr inbounds double, double* %22, i64 8
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !134, !noalias !131
  %32 = getelementptr inbounds double, double* %22, i64 10
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !134, !noalias !131
  %34 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !137, !noalias !131
  %36 = getelementptr inbounds double, double* %34, i64 2
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !137, !noalias !131
  %38 = getelementptr inbounds double, double* %34, i64 4
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !137, !noalias !131
  %40 = getelementptr inbounds double, double* %34, i64 6
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !137, !noalias !131
  %42 = getelementptr inbounds double, double* %34, i64 8
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !137, !noalias !131
  %44 = getelementptr inbounds double, double* %34, i64 10
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !137, !noalias !131
  %46 = fadd <2 x double> %wide.load, %wide.load74
  %47 = fadd <2 x double> %wide.load69, %wide.load75
  %48 = fadd <2 x double> %wide.load70, %wide.load76
  %49 = fadd <2 x double> %wide.load71, %wide.load77
  %50 = fadd <2 x double> %wide.load72, %wide.load78
  %51 = fadd <2 x double> %wide.load73, %wide.load79
  %52 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  %54 = getelementptr inbounds double, double* %52, i64 2
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  %56 = getelementptr inbounds double, double* %52, i64 4
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  %58 = getelementptr inbounds double, double* %52, i64 6
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  %60 = getelementptr inbounds double, double* %52, i64 8
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  %62 = getelementptr inbounds double, double* %52, i64 10
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  %64 = fadd <2 x double> %46, %wide.load80
  %65 = fadd <2 x double> %47, %wide.load81
  %66 = fadd <2 x double> %48, %wide.load82
  %67 = fadd <2 x double> %49, %wide.load83
  %68 = fadd <2 x double> %50, %wide.load84
  %69 = fadd <2 x double> %51, %wide.load85
  store <2 x double> %64, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  store <2 x double> %65, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  store <2 x double> %66, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  store <2 x double> %67, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  store <2 x double> %68, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  store <2 x double> %69, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !139, !noalias !141
  %70 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !144, !noalias !131
  %72 = getelementptr inbounds double, double* %70, i64 2
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !144, !noalias !131
  %74 = getelementptr inbounds double, double* %70, i64 4
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !144, !noalias !131
  %76 = getelementptr inbounds double, double* %70, i64 6
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !144, !noalias !131
  %78 = getelementptr inbounds double, double* %70, i64 8
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !144, !noalias !131
  %80 = getelementptr inbounds double, double* %70, i64 10
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !144, !noalias !131
  %82 = fadd <2 x double> %wide.load74, %wide.load92
  %83 = fadd <2 x double> %wide.load75, %wide.load93
  %84 = fadd <2 x double> %wide.load76, %wide.load94
  %85 = fadd <2 x double> %wide.load77, %wide.load95
  %86 = fadd <2 x double> %wide.load78, %wide.load96
  %87 = fadd <2 x double> %wide.load79, %wide.load97
  %88 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  %90 = getelementptr inbounds double, double* %88, i64 2
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  %92 = getelementptr inbounds double, double* %88, i64 4
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  %94 = getelementptr inbounds double, double* %88, i64 6
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  %96 = getelementptr inbounds double, double* %88, i64 8
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  %98 = getelementptr inbounds double, double* %88, i64 10
  %99 = bitcast double* %98 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  %100 = fadd <2 x double> %82, %wide.load98
  %101 = fadd <2 x double> %83, %wide.load99
  %102 = fadd <2 x double> %84, %wide.load100
  %103 = fadd <2 x double> %85, %wide.load101
  %104 = fadd <2 x double> %86, %wide.load102
  %105 = fadd <2 x double> %87, %wide.load103
  store <2 x double> %100, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  store <2 x double> %101, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  store <2 x double> %102, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  store <2 x double> %103, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  store <2 x double> %104, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  store <2 x double> %105, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !145, !noalias !146
  %index.next = add i64 %index, 12
  %106 = icmp eq i64 %index.next, %n.vec
  br i1 %106, label %middle.block, label %vector.body, !llvm.loop !147

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %17
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %.omp_outlined..7.exit.us, label %omp.inner.for.body.i.us.preheader

omp.inner.for.body.i.us.preheader:                ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us.ph = phi i64 [ %ind.end, %middle.block ], [ %17, %omp.inner.for.body.preheader.i.us ], [ %17, %vector.memcheck ]
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.body.i.us.preheader, %omp.inner.for.body.i.us
  %indvars.iv.i.us = phi i64 [ %indvars.iv.next.i.us, %omp.inner.for.body.i.us ], [ %indvars.iv.i.us.ph, %omp.inner.for.body.i.us.preheader ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %107 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !131
  %arrayidx6.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %108 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !131
  %add7.i.us = fadd double %107, %108
  %arrayidx9.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %109 = load double, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !131
  %add10.i.us = fadd double %109, %add7.i.us
  store double %add10.i.us, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !131
  %110 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !131
  %arrayidx14.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %111 = load double, double* %arrayidx14.i.us, align 8, !tbaa !62, !noalias !131
  %add15.i.us = fadd double %110, %111
  %arrayidx17.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %112 = load double, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !131
  %add18.i.us = fadd double %112, %add15.i.us
  store double %add18.i.us, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !131
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp3.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp3.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..7.exit.us, !llvm.loop !148

.omp_outlined..7.exit.us:                         ; preds = %omp.inner.for.body.i.us, %middle.block, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !131
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !131
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !131
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !131
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !131
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %113 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %113, %.omp.iv.012.us
  %114 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %114
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %115 = load i32, i32* %.omp.comb.lb, align 4
  %116 = load i32, i32* %.omp.comb.ub, align 4
  %117 = zext i32 %116 to i64
  %118 = zext i32 %115 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..7 to void (i32*, i32*, ...)*), i64 %118, i64 %117, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %119 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %119, %.omp.iv.012
  %120 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %120
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..7.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

declare void @__kmpc_for_static_init_4(%struct.ident_t*, i32, i32, i32*, i32*, i32*, i32*, i32, i32) local_unnamed_addr

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..7(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp329 = icmp sgt i32 %7, %cond
  br i1 %cmp329, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  %10 = icmp sgt i64 %9, %8
  %smax = select i1 %10, i64 %9, i64 %8
  %11 = add nsw i64 %smax, 1
  %12 = sub nsw i64 %11, %8
  %min.iters.check = icmp ult i64 %12, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %8
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %11
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %8
  %scevgep37 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %11
  %scevgep39 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %8
  %scevgep41 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %11
  %scevgep43 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %8
  %scevgep45 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %11
  %scevgep47 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %8
  %scevgep49 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %11
  %bound0 = icmp ult double* %scevgep, %scevgep37
  %bound1 = icmp ult double* %scevgep35, %scevgep33
  %found.conflict = and i1 %bound0, %bound1
  %bound051 = icmp ult double* %scevgep, %scevgep41
  %bound152 = icmp ult double* %scevgep39, %scevgep33
  %found.conflict53 = and i1 %bound051, %bound152
  %conflict.rdx = or i1 %found.conflict, %found.conflict53
  %bound054 = icmp ult double* %scevgep, %scevgep45
  %bound155 = icmp ult double* %scevgep43, %scevgep33
  %found.conflict56 = and i1 %bound054, %bound155
  %conflict.rdx57 = or i1 %found.conflict56, %conflict.rdx
  %bound058 = icmp ult double* %scevgep, %scevgep49
  %bound159 = icmp ult double* %scevgep47, %scevgep33
  %found.conflict60 = and i1 %bound058, %bound159
  %conflict.rdx61 = or i1 %found.conflict60, %conflict.rdx57
  %bound062 = icmp ult double* %scevgep35, %scevgep41
  %bound163 = icmp ult double* %scevgep39, %scevgep37
  %found.conflict64 = and i1 %bound062, %bound163
  %conflict.rdx65 = or i1 %found.conflict64, %conflict.rdx61
  %bound066 = icmp ult double* %scevgep35, %scevgep45
  %bound167 = icmp ult double* %scevgep43, %scevgep37
  %found.conflict68 = and i1 %bound066, %bound167
  %conflict.rdx69 = or i1 %found.conflict68, %conflict.rdx65
  %bound070 = icmp ult double* %scevgep35, %scevgep49
  %bound171 = icmp ult double* %scevgep47, %scevgep37
  %found.conflict72 = and i1 %bound070, %bound171
  %conflict.rdx73 = or i1 %found.conflict72, %conflict.rdx69
  br i1 %conflict.rdx73, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %12, 12
  %n.vec = sub nsw i64 %12, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %8
  %13 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %14 = bitcast double* %13 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %14, align 8, !tbaa !62, !alias.scope !149
  %15 = getelementptr inbounds double, double* %13, i64 2
  %16 = bitcast double* %15 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %16, align 8, !tbaa !62, !alias.scope !149
  %17 = getelementptr inbounds double, double* %13, i64 4
  %18 = bitcast double* %17 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %18, align 8, !tbaa !62, !alias.scope !149
  %19 = getelementptr inbounds double, double* %13, i64 6
  %20 = bitcast double* %19 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %20, align 8, !tbaa !62, !alias.scope !149
  %21 = getelementptr inbounds double, double* %13, i64 8
  %22 = bitcast double* %21 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %22, align 8, !tbaa !62, !alias.scope !149
  %23 = getelementptr inbounds double, double* %13, i64 10
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !149
  %25 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !152
  %27 = getelementptr inbounds double, double* %25, i64 2
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !152
  %29 = getelementptr inbounds double, double* %25, i64 4
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !152
  %31 = getelementptr inbounds double, double* %25, i64 6
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !152
  %33 = getelementptr inbounds double, double* %25, i64 8
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !152
  %35 = getelementptr inbounds double, double* %25, i64 10
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !152
  %37 = fadd <2 x double> %wide.load, %wide.load84
  %38 = fadd <2 x double> %wide.load79, %wide.load85
  %39 = fadd <2 x double> %wide.load80, %wide.load86
  %40 = fadd <2 x double> %wide.load81, %wide.load87
  %41 = fadd <2 x double> %wide.load82, %wide.load88
  %42 = fadd <2 x double> %wide.load83, %wide.load89
  %43 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  %45 = getelementptr inbounds double, double* %43, i64 2
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  %47 = getelementptr inbounds double, double* %43, i64 4
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  %49 = getelementptr inbounds double, double* %43, i64 6
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  %51 = getelementptr inbounds double, double* %43, i64 8
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  %53 = getelementptr inbounds double, double* %43, i64 10
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  %55 = fadd <2 x double> %37, %wide.load90
  %56 = fadd <2 x double> %38, %wide.load91
  %57 = fadd <2 x double> %39, %wide.load92
  %58 = fadd <2 x double> %40, %wide.load93
  %59 = fadd <2 x double> %41, %wide.load94
  %60 = fadd <2 x double> %42, %wide.load95
  store <2 x double> %55, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  store <2 x double> %56, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  store <2 x double> %57, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  store <2 x double> %58, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  store <2 x double> %59, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  store <2 x double> %60, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !154, !noalias !156
  %61 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !159
  %63 = getelementptr inbounds double, double* %61, i64 2
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !159
  %65 = getelementptr inbounds double, double* %61, i64 4
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !159
  %67 = getelementptr inbounds double, double* %61, i64 6
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !159
  %69 = getelementptr inbounds double, double* %61, i64 8
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !159
  %71 = getelementptr inbounds double, double* %61, i64 10
  %72 = bitcast double* %71 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !159
  %73 = fadd <2 x double> %wide.load84, %wide.load102
  %74 = fadd <2 x double> %wide.load85, %wide.load103
  %75 = fadd <2 x double> %wide.load86, %wide.load104
  %76 = fadd <2 x double> %wide.load87, %wide.load105
  %77 = fadd <2 x double> %wide.load88, %wide.load106
  %78 = fadd <2 x double> %wide.load89, %wide.load107
  %79 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %80 = bitcast double* %79 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  %81 = getelementptr inbounds double, double* %79, i64 2
  %82 = bitcast double* %81 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  %83 = getelementptr inbounds double, double* %79, i64 4
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load110 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  %85 = getelementptr inbounds double, double* %79, i64 6
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load111 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  %87 = getelementptr inbounds double, double* %79, i64 8
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load112 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  %89 = getelementptr inbounds double, double* %79, i64 10
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load113 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  %91 = fadd <2 x double> %73, %wide.load108
  %92 = fadd <2 x double> %74, %wide.load109
  %93 = fadd <2 x double> %75, %wide.load110
  %94 = fadd <2 x double> %76, %wide.load111
  %95 = fadd <2 x double> %77, %wide.load112
  %96 = fadd <2 x double> %78, %wide.load113
  store <2 x double> %91, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  store <2 x double> %92, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  store <2 x double> %93, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  store <2 x double> %94, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  store <2 x double> %95, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  store <2 x double> %96, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !160, !noalias !161
  %index.next = add i64 %index, 12
  %97 = icmp eq i64 %index.next, %n.vec
  br i1 %97, label %middle.block, label %vector.body, !llvm.loop !162

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %8
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %8, %omp.inner.for.body.preheader ], [ %8, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %98 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx6, align 8, !tbaa !62
  %add7 = fadd double %98, %99
  %arrayidx9 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx9, align 8, !tbaa !62
  %add10 = fadd double %100, %add7
  store double %add10, double* %arrayidx9, align 8, !tbaa !62
  %101 = load double, double* %arrayidx6, align 8, !tbaa !62
  %arrayidx14 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %101, %102
  %arrayidx17 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %103, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp3 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp3, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !163

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

declare void @__kmpc_for_static_fini(%struct.ident_t*, i32) local_unnamed_addr

declare void @__kmpc_push_num_threads(%struct.ident_t*, i32, i32) local_unnamed_addr

declare void @__kmpc_push_proc_bind(%struct.ident_t*, i32, i32) local_unnamed_addr

declare void @__kmpc_fork_call(%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) local_unnamed_addr

declare void @__kmpc_serialized_parallel(%struct.ident_t*, i32) local_unnamed_addr

declare void @__kmpc_end_serialized_parallel(%struct.ident_t*, i32) local_unnamed_addr

declare i32 @__kmpc_global_thread_num(%struct.ident_t*) local_unnamed_addr

declare i32 @__kmpc_push_num_teams(%struct.ident_t*, i32, i32, i32) local_unnamed_addr

declare void @__kmpc_fork_teams(%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) local_unnamed_addr

declare i32 @__tgt_target_teams(i64, i8*, i32, i8**, i8**, i64*, i64*, i32, i32) local_unnamed_addr

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..14(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..15(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..15 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..15(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 38, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !164
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !165

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !164
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !164
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !164
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !164
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !164
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !164
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !164
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !164
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !166

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

declare void @__kmpc_dispatch_init_4(%struct.ident_t*, i32, i32, i32, i32, i32, i32) local_unnamed_addr

declare i32 @__kmpc_dispatch_next_4(%struct.ident_t*, i32, i32*, i32*, i32*, i32*) local_unnamed_addr

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..18(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..19(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..19 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..19(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 35, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !167
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !168

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !167
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !167
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !167
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !167
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !167
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !167
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !167
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !167
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !169

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..22(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..23(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..23 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..23(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 36, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !170
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !171

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !170
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !170
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !170
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !170
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !170
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !170
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !170
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !170
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !172

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..26(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..27(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..27 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..27(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 37, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !173
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !174

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !173
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !173
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !173
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !173
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !173
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !173
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !173
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !173
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !175

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..30(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us.preheader, label %omp.inner.for.body

omp.inner.for.body.us.preheader:                  ; preds = %omp.inner.for.body.lr.ph
  %scevgep17 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 1
  %scevgep23 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 1
  %scevgep28 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 1
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 1
  %scevgep38 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 1
  br label %omp.inner.for.body.us

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.us.preheader, %.omp_outlined..31.exit.us
  %.omp.iv.012.us = phi i32 [ %add.us, %.omp_outlined..31.exit.us ], [ %7, %omp.inner.for.body.us.preheader ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !176
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !176
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !176
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !176
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !176
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !176
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !176
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !176
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !176
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !176
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !176
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !176
  %cmp329.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp329.i.us, label %.omp_outlined..31.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  %19 = icmp sgt i64 %18, %17
  %smax = select i1 %19, i64 %18, i64 %17
  %20 = sub nsw i64 1, %17
  %21 = add nsw i64 %20, %smax
  %min.iters.check = icmp ult i64 %21, 12
  br i1 %min.iters.check, label %omp.inner.for.body.i.us.preheader, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader.i.us
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %17
  %scevgep19 = getelementptr double, double* %scevgep17, i64 %smax
  %scevgep21 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %17
  %scevgep24 = getelementptr double, double* %scevgep23, i64 %smax
  %scevgep26 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %17
  %scevgep29 = getelementptr double, double* %scevgep28, i64 %smax
  %scevgep31 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %17
  %scevgep34 = getelementptr double, double* %scevgep33, i64 %smax
  %scevgep36 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %17
  %scevgep39 = getelementptr double, double* %scevgep38, i64 %smax
  %bound0 = icmp ult double* %scevgep, %scevgep24
  %bound1 = icmp ult double* %scevgep21, %scevgep19
  %found.conflict = and i1 %bound0, %bound1
  %bound041 = icmp ult double* %scevgep, %scevgep29
  %bound142 = icmp ult double* %scevgep26, %scevgep19
  %found.conflict43 = and i1 %bound041, %bound142
  %conflict.rdx = or i1 %found.conflict, %found.conflict43
  %bound044 = icmp ult double* %scevgep, %scevgep34
  %bound145 = icmp ult double* %scevgep31, %scevgep19
  %found.conflict46 = and i1 %bound044, %bound145
  %conflict.rdx47 = or i1 %found.conflict46, %conflict.rdx
  %bound048 = icmp ult double* %scevgep, %scevgep39
  %bound149 = icmp ult double* %scevgep36, %scevgep19
  %found.conflict50 = and i1 %bound048, %bound149
  %conflict.rdx51 = or i1 %found.conflict50, %conflict.rdx47
  %bound052 = icmp ult double* %scevgep21, %scevgep29
  %bound153 = icmp ult double* %scevgep26, %scevgep24
  %found.conflict54 = and i1 %bound052, %bound153
  %conflict.rdx55 = or i1 %found.conflict54, %conflict.rdx51
  %bound056 = icmp ult double* %scevgep21, %scevgep34
  %bound157 = icmp ult double* %scevgep31, %scevgep24
  %found.conflict58 = and i1 %bound056, %bound157
  %conflict.rdx59 = or i1 %found.conflict58, %conflict.rdx55
  %bound060 = icmp ult double* %scevgep21, %scevgep39
  %bound161 = icmp ult double* %scevgep36, %scevgep24
  %found.conflict62 = and i1 %bound060, %bound161
  %conflict.rdx63 = or i1 %found.conflict62, %conflict.rdx59
  br i1 %conflict.rdx63, label %omp.inner.for.body.i.us.preheader, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 12
  %n.vec = sub nsw i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %17
  %22 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !179, !noalias !176
  %24 = getelementptr inbounds double, double* %22, i64 2
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !179, !noalias !176
  %26 = getelementptr inbounds double, double* %22, i64 4
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !179, !noalias !176
  %28 = getelementptr inbounds double, double* %22, i64 6
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !179, !noalias !176
  %30 = getelementptr inbounds double, double* %22, i64 8
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !179, !noalias !176
  %32 = getelementptr inbounds double, double* %22, i64 10
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !179, !noalias !176
  %34 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !182, !noalias !176
  %36 = getelementptr inbounds double, double* %34, i64 2
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !182, !noalias !176
  %38 = getelementptr inbounds double, double* %34, i64 4
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !182, !noalias !176
  %40 = getelementptr inbounds double, double* %34, i64 6
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !182, !noalias !176
  %42 = getelementptr inbounds double, double* %34, i64 8
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !182, !noalias !176
  %44 = getelementptr inbounds double, double* %34, i64 10
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !182, !noalias !176
  %46 = fadd <2 x double> %wide.load, %wide.load74
  %47 = fadd <2 x double> %wide.load69, %wide.load75
  %48 = fadd <2 x double> %wide.load70, %wide.load76
  %49 = fadd <2 x double> %wide.load71, %wide.load77
  %50 = fadd <2 x double> %wide.load72, %wide.load78
  %51 = fadd <2 x double> %wide.load73, %wide.load79
  %52 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  %54 = getelementptr inbounds double, double* %52, i64 2
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  %56 = getelementptr inbounds double, double* %52, i64 4
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  %58 = getelementptr inbounds double, double* %52, i64 6
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  %60 = getelementptr inbounds double, double* %52, i64 8
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  %62 = getelementptr inbounds double, double* %52, i64 10
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  %64 = fadd <2 x double> %46, %wide.load80
  %65 = fadd <2 x double> %47, %wide.load81
  %66 = fadd <2 x double> %48, %wide.load82
  %67 = fadd <2 x double> %49, %wide.load83
  %68 = fadd <2 x double> %50, %wide.load84
  %69 = fadd <2 x double> %51, %wide.load85
  store <2 x double> %64, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  store <2 x double> %65, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  store <2 x double> %66, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  store <2 x double> %67, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  store <2 x double> %68, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  store <2 x double> %69, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !184, !noalias !186
  %70 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !189, !noalias !176
  %72 = getelementptr inbounds double, double* %70, i64 2
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !189, !noalias !176
  %74 = getelementptr inbounds double, double* %70, i64 4
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !189, !noalias !176
  %76 = getelementptr inbounds double, double* %70, i64 6
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !189, !noalias !176
  %78 = getelementptr inbounds double, double* %70, i64 8
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !189, !noalias !176
  %80 = getelementptr inbounds double, double* %70, i64 10
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !189, !noalias !176
  %82 = fadd <2 x double> %wide.load74, %wide.load92
  %83 = fadd <2 x double> %wide.load75, %wide.load93
  %84 = fadd <2 x double> %wide.load76, %wide.load94
  %85 = fadd <2 x double> %wide.load77, %wide.load95
  %86 = fadd <2 x double> %wide.load78, %wide.load96
  %87 = fadd <2 x double> %wide.load79, %wide.load97
  %88 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  %90 = getelementptr inbounds double, double* %88, i64 2
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  %92 = getelementptr inbounds double, double* %88, i64 4
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  %94 = getelementptr inbounds double, double* %88, i64 6
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  %96 = getelementptr inbounds double, double* %88, i64 8
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  %98 = getelementptr inbounds double, double* %88, i64 10
  %99 = bitcast double* %98 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  %100 = fadd <2 x double> %82, %wide.load98
  %101 = fadd <2 x double> %83, %wide.load99
  %102 = fadd <2 x double> %84, %wide.load100
  %103 = fadd <2 x double> %85, %wide.load101
  %104 = fadd <2 x double> %86, %wide.load102
  %105 = fadd <2 x double> %87, %wide.load103
  store <2 x double> %100, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  store <2 x double> %101, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  store <2 x double> %102, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  store <2 x double> %103, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  store <2 x double> %104, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  store <2 x double> %105, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !190, !noalias !191
  %index.next = add i64 %index, 12
  %106 = icmp eq i64 %index.next, %n.vec
  br i1 %106, label %middle.block, label %vector.body, !llvm.loop !192

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %17
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %.omp_outlined..31.exit.us, label %omp.inner.for.body.i.us.preheader

omp.inner.for.body.i.us.preheader:                ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us.ph = phi i64 [ %ind.end, %middle.block ], [ %17, %omp.inner.for.body.preheader.i.us ], [ %17, %vector.memcheck ]
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.body.i.us.preheader, %omp.inner.for.body.i.us
  %indvars.iv.i.us = phi i64 [ %indvars.iv.next.i.us, %omp.inner.for.body.i.us ], [ %indvars.iv.i.us.ph, %omp.inner.for.body.i.us.preheader ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %107 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !176
  %arrayidx6.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %108 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !176
  %add7.i.us = fadd double %107, %108
  %arrayidx9.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %109 = load double, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !176
  %add10.i.us = fadd double %109, %add7.i.us
  store double %add10.i.us, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !176
  %110 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !176
  %arrayidx14.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %111 = load double, double* %arrayidx14.i.us, align 8, !tbaa !62, !noalias !176
  %add15.i.us = fadd double %110, %111
  %arrayidx17.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %112 = load double, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !176
  %add18.i.us = fadd double %112, %add15.i.us
  store double %add18.i.us, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !176
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp3.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp3.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..31.exit.us, !llvm.loop !193

.omp_outlined..31.exit.us:                        ; preds = %omp.inner.for.body.i.us, %middle.block, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !176
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !176
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !176
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !176
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !176
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %113 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %113, %.omp.iv.012.us
  %114 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %114
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 2) #6
  %115 = load i32, i32* %.omp.comb.lb, align 4
  %116 = load i32, i32* %.omp.comb.ub, align 4
  %117 = zext i32 %116 to i64
  %118 = zext i32 %115 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..31 to void (i32*, i32*, ...)*), i64 %118, i64 %117, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %119 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %119, %.omp.iv.012
  %120 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %120
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..31.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..31(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp329 = icmp sgt i32 %7, %cond
  br i1 %cmp329, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  %10 = icmp sgt i64 %9, %8
  %smax = select i1 %10, i64 %9, i64 %8
  %11 = add nsw i64 %smax, 1
  %12 = sub nsw i64 %11, %8
  %min.iters.check = icmp ult i64 %12, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %8
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %11
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %8
  %scevgep37 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %11
  %scevgep39 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %8
  %scevgep41 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %11
  %scevgep43 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %8
  %scevgep45 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %11
  %scevgep47 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %8
  %scevgep49 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %11
  %bound0 = icmp ult double* %scevgep, %scevgep37
  %bound1 = icmp ult double* %scevgep35, %scevgep33
  %found.conflict = and i1 %bound0, %bound1
  %bound051 = icmp ult double* %scevgep, %scevgep41
  %bound152 = icmp ult double* %scevgep39, %scevgep33
  %found.conflict53 = and i1 %bound051, %bound152
  %conflict.rdx = or i1 %found.conflict, %found.conflict53
  %bound054 = icmp ult double* %scevgep, %scevgep45
  %bound155 = icmp ult double* %scevgep43, %scevgep33
  %found.conflict56 = and i1 %bound054, %bound155
  %conflict.rdx57 = or i1 %found.conflict56, %conflict.rdx
  %bound058 = icmp ult double* %scevgep, %scevgep49
  %bound159 = icmp ult double* %scevgep47, %scevgep33
  %found.conflict60 = and i1 %bound058, %bound159
  %conflict.rdx61 = or i1 %found.conflict60, %conflict.rdx57
  %bound062 = icmp ult double* %scevgep35, %scevgep41
  %bound163 = icmp ult double* %scevgep39, %scevgep37
  %found.conflict64 = and i1 %bound062, %bound163
  %conflict.rdx65 = or i1 %found.conflict64, %conflict.rdx61
  %bound066 = icmp ult double* %scevgep35, %scevgep45
  %bound167 = icmp ult double* %scevgep43, %scevgep37
  %found.conflict68 = and i1 %bound066, %bound167
  %conflict.rdx69 = or i1 %found.conflict68, %conflict.rdx65
  %bound070 = icmp ult double* %scevgep35, %scevgep49
  %bound171 = icmp ult double* %scevgep47, %scevgep37
  %found.conflict72 = and i1 %bound070, %bound171
  %conflict.rdx73 = or i1 %found.conflict72, %conflict.rdx69
  br i1 %conflict.rdx73, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %12, 12
  %n.vec = sub nsw i64 %12, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %8
  %13 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %14 = bitcast double* %13 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %14, align 8, !tbaa !62, !alias.scope !194
  %15 = getelementptr inbounds double, double* %13, i64 2
  %16 = bitcast double* %15 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %16, align 8, !tbaa !62, !alias.scope !194
  %17 = getelementptr inbounds double, double* %13, i64 4
  %18 = bitcast double* %17 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %18, align 8, !tbaa !62, !alias.scope !194
  %19 = getelementptr inbounds double, double* %13, i64 6
  %20 = bitcast double* %19 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %20, align 8, !tbaa !62, !alias.scope !194
  %21 = getelementptr inbounds double, double* %13, i64 8
  %22 = bitcast double* %21 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %22, align 8, !tbaa !62, !alias.scope !194
  %23 = getelementptr inbounds double, double* %13, i64 10
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !194
  %25 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !197
  %27 = getelementptr inbounds double, double* %25, i64 2
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !197
  %29 = getelementptr inbounds double, double* %25, i64 4
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !197
  %31 = getelementptr inbounds double, double* %25, i64 6
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !197
  %33 = getelementptr inbounds double, double* %25, i64 8
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !197
  %35 = getelementptr inbounds double, double* %25, i64 10
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !197
  %37 = fadd <2 x double> %wide.load, %wide.load84
  %38 = fadd <2 x double> %wide.load79, %wide.load85
  %39 = fadd <2 x double> %wide.load80, %wide.load86
  %40 = fadd <2 x double> %wide.load81, %wide.load87
  %41 = fadd <2 x double> %wide.load82, %wide.load88
  %42 = fadd <2 x double> %wide.load83, %wide.load89
  %43 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  %45 = getelementptr inbounds double, double* %43, i64 2
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  %47 = getelementptr inbounds double, double* %43, i64 4
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  %49 = getelementptr inbounds double, double* %43, i64 6
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  %51 = getelementptr inbounds double, double* %43, i64 8
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  %53 = getelementptr inbounds double, double* %43, i64 10
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  %55 = fadd <2 x double> %37, %wide.load90
  %56 = fadd <2 x double> %38, %wide.load91
  %57 = fadd <2 x double> %39, %wide.load92
  %58 = fadd <2 x double> %40, %wide.load93
  %59 = fadd <2 x double> %41, %wide.load94
  %60 = fadd <2 x double> %42, %wide.load95
  store <2 x double> %55, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  store <2 x double> %56, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  store <2 x double> %57, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  store <2 x double> %58, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  store <2 x double> %59, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  store <2 x double> %60, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !199, !noalias !201
  %61 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !204
  %63 = getelementptr inbounds double, double* %61, i64 2
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !204
  %65 = getelementptr inbounds double, double* %61, i64 4
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !204
  %67 = getelementptr inbounds double, double* %61, i64 6
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !204
  %69 = getelementptr inbounds double, double* %61, i64 8
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !204
  %71 = getelementptr inbounds double, double* %61, i64 10
  %72 = bitcast double* %71 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !204
  %73 = fadd <2 x double> %wide.load84, %wide.load102
  %74 = fadd <2 x double> %wide.load85, %wide.load103
  %75 = fadd <2 x double> %wide.load86, %wide.load104
  %76 = fadd <2 x double> %wide.load87, %wide.load105
  %77 = fadd <2 x double> %wide.load88, %wide.load106
  %78 = fadd <2 x double> %wide.load89, %wide.load107
  %79 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %80 = bitcast double* %79 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  %81 = getelementptr inbounds double, double* %79, i64 2
  %82 = bitcast double* %81 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  %83 = getelementptr inbounds double, double* %79, i64 4
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load110 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  %85 = getelementptr inbounds double, double* %79, i64 6
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load111 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  %87 = getelementptr inbounds double, double* %79, i64 8
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load112 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  %89 = getelementptr inbounds double, double* %79, i64 10
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load113 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  %91 = fadd <2 x double> %73, %wide.load108
  %92 = fadd <2 x double> %74, %wide.load109
  %93 = fadd <2 x double> %75, %wide.load110
  %94 = fadd <2 x double> %76, %wide.load111
  %95 = fadd <2 x double> %77, %wide.load112
  %96 = fadd <2 x double> %78, %wide.load113
  store <2 x double> %91, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  store <2 x double> %92, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  store <2 x double> %93, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  store <2 x double> %94, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  store <2 x double> %95, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  store <2 x double> %96, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !205, !noalias !206
  %index.next = add i64 %index, 12
  %97 = icmp eq i64 %index.next, %n.vec
  br i1 %97, label %middle.block, label %vector.body, !llvm.loop !207

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %8
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %8, %omp.inner.for.body.preheader ], [ %8, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %98 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx6, align 8, !tbaa !62
  %add7 = fadd double %98, %99
  %arrayidx9 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx9, align 8, !tbaa !62
  %add10 = fadd double %100, %add7
  store double %add10, double* %arrayidx9, align 8, !tbaa !62
  %101 = load double, double* %arrayidx6, align 8, !tbaa !62
  %arrayidx14 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %101, %102
  %arrayidx17 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %103, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp3 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp3, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !208

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..34(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us.preheader, label %omp.inner.for.body

omp.inner.for.body.us.preheader:                  ; preds = %omp.inner.for.body.lr.ph
  %scevgep17 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 1
  %scevgep23 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 1
  %scevgep28 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 1
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 1
  %scevgep38 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 1
  br label %omp.inner.for.body.us

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.us.preheader, %.omp_outlined..35.exit.us
  %.omp.iv.012.us = phi i32 [ %add.us, %.omp_outlined..35.exit.us ], [ %7, %omp.inner.for.body.us.preheader ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !209
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !209
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !209
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !209
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !209
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !209
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !209
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !209
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !209
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !209
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !209
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !209
  %cmp329.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp329.i.us, label %.omp_outlined..35.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  %19 = icmp sgt i64 %18, %17
  %smax = select i1 %19, i64 %18, i64 %17
  %20 = sub nsw i64 1, %17
  %21 = add nsw i64 %20, %smax
  %min.iters.check = icmp ult i64 %21, 12
  br i1 %min.iters.check, label %omp.inner.for.body.i.us.preheader, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader.i.us
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %17
  %scevgep19 = getelementptr double, double* %scevgep17, i64 %smax
  %scevgep21 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %17
  %scevgep24 = getelementptr double, double* %scevgep23, i64 %smax
  %scevgep26 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %17
  %scevgep29 = getelementptr double, double* %scevgep28, i64 %smax
  %scevgep31 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %17
  %scevgep34 = getelementptr double, double* %scevgep33, i64 %smax
  %scevgep36 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %17
  %scevgep39 = getelementptr double, double* %scevgep38, i64 %smax
  %bound0 = icmp ult double* %scevgep, %scevgep24
  %bound1 = icmp ult double* %scevgep21, %scevgep19
  %found.conflict = and i1 %bound0, %bound1
  %bound041 = icmp ult double* %scevgep, %scevgep29
  %bound142 = icmp ult double* %scevgep26, %scevgep19
  %found.conflict43 = and i1 %bound041, %bound142
  %conflict.rdx = or i1 %found.conflict, %found.conflict43
  %bound044 = icmp ult double* %scevgep, %scevgep34
  %bound145 = icmp ult double* %scevgep31, %scevgep19
  %found.conflict46 = and i1 %bound044, %bound145
  %conflict.rdx47 = or i1 %found.conflict46, %conflict.rdx
  %bound048 = icmp ult double* %scevgep, %scevgep39
  %bound149 = icmp ult double* %scevgep36, %scevgep19
  %found.conflict50 = and i1 %bound048, %bound149
  %conflict.rdx51 = or i1 %found.conflict50, %conflict.rdx47
  %bound052 = icmp ult double* %scevgep21, %scevgep29
  %bound153 = icmp ult double* %scevgep26, %scevgep24
  %found.conflict54 = and i1 %bound052, %bound153
  %conflict.rdx55 = or i1 %found.conflict54, %conflict.rdx51
  %bound056 = icmp ult double* %scevgep21, %scevgep34
  %bound157 = icmp ult double* %scevgep31, %scevgep24
  %found.conflict58 = and i1 %bound056, %bound157
  %conflict.rdx59 = or i1 %found.conflict58, %conflict.rdx55
  %bound060 = icmp ult double* %scevgep21, %scevgep39
  %bound161 = icmp ult double* %scevgep36, %scevgep24
  %found.conflict62 = and i1 %bound060, %bound161
  %conflict.rdx63 = or i1 %found.conflict62, %conflict.rdx59
  br i1 %conflict.rdx63, label %omp.inner.for.body.i.us.preheader, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 12
  %n.vec = sub nsw i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %17
  %22 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !212, !noalias !209
  %24 = getelementptr inbounds double, double* %22, i64 2
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !212, !noalias !209
  %26 = getelementptr inbounds double, double* %22, i64 4
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !212, !noalias !209
  %28 = getelementptr inbounds double, double* %22, i64 6
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !212, !noalias !209
  %30 = getelementptr inbounds double, double* %22, i64 8
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !212, !noalias !209
  %32 = getelementptr inbounds double, double* %22, i64 10
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !212, !noalias !209
  %34 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !215, !noalias !209
  %36 = getelementptr inbounds double, double* %34, i64 2
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !215, !noalias !209
  %38 = getelementptr inbounds double, double* %34, i64 4
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !215, !noalias !209
  %40 = getelementptr inbounds double, double* %34, i64 6
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !215, !noalias !209
  %42 = getelementptr inbounds double, double* %34, i64 8
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !215, !noalias !209
  %44 = getelementptr inbounds double, double* %34, i64 10
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !215, !noalias !209
  %46 = fadd <2 x double> %wide.load, %wide.load74
  %47 = fadd <2 x double> %wide.load69, %wide.load75
  %48 = fadd <2 x double> %wide.load70, %wide.load76
  %49 = fadd <2 x double> %wide.load71, %wide.load77
  %50 = fadd <2 x double> %wide.load72, %wide.load78
  %51 = fadd <2 x double> %wide.load73, %wide.load79
  %52 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  %54 = getelementptr inbounds double, double* %52, i64 2
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  %56 = getelementptr inbounds double, double* %52, i64 4
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  %58 = getelementptr inbounds double, double* %52, i64 6
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  %60 = getelementptr inbounds double, double* %52, i64 8
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  %62 = getelementptr inbounds double, double* %52, i64 10
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  %64 = fadd <2 x double> %46, %wide.load80
  %65 = fadd <2 x double> %47, %wide.load81
  %66 = fadd <2 x double> %48, %wide.load82
  %67 = fadd <2 x double> %49, %wide.load83
  %68 = fadd <2 x double> %50, %wide.load84
  %69 = fadd <2 x double> %51, %wide.load85
  store <2 x double> %64, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  store <2 x double> %65, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  store <2 x double> %66, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  store <2 x double> %67, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  store <2 x double> %68, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  store <2 x double> %69, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !217, !noalias !219
  %70 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !222, !noalias !209
  %72 = getelementptr inbounds double, double* %70, i64 2
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !222, !noalias !209
  %74 = getelementptr inbounds double, double* %70, i64 4
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !222, !noalias !209
  %76 = getelementptr inbounds double, double* %70, i64 6
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !222, !noalias !209
  %78 = getelementptr inbounds double, double* %70, i64 8
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !222, !noalias !209
  %80 = getelementptr inbounds double, double* %70, i64 10
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !222, !noalias !209
  %82 = fadd <2 x double> %wide.load74, %wide.load92
  %83 = fadd <2 x double> %wide.load75, %wide.load93
  %84 = fadd <2 x double> %wide.load76, %wide.load94
  %85 = fadd <2 x double> %wide.load77, %wide.load95
  %86 = fadd <2 x double> %wide.load78, %wide.load96
  %87 = fadd <2 x double> %wide.load79, %wide.load97
  %88 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  %90 = getelementptr inbounds double, double* %88, i64 2
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  %92 = getelementptr inbounds double, double* %88, i64 4
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  %94 = getelementptr inbounds double, double* %88, i64 6
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  %96 = getelementptr inbounds double, double* %88, i64 8
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  %98 = getelementptr inbounds double, double* %88, i64 10
  %99 = bitcast double* %98 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  %100 = fadd <2 x double> %82, %wide.load98
  %101 = fadd <2 x double> %83, %wide.load99
  %102 = fadd <2 x double> %84, %wide.load100
  %103 = fadd <2 x double> %85, %wide.load101
  %104 = fadd <2 x double> %86, %wide.load102
  %105 = fadd <2 x double> %87, %wide.load103
  store <2 x double> %100, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  store <2 x double> %101, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  store <2 x double> %102, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  store <2 x double> %103, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  store <2 x double> %104, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  store <2 x double> %105, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !223, !noalias !224
  %index.next = add i64 %index, 12
  %106 = icmp eq i64 %index.next, %n.vec
  br i1 %106, label %middle.block, label %vector.body, !llvm.loop !225

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %17
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %.omp_outlined..35.exit.us, label %omp.inner.for.body.i.us.preheader

omp.inner.for.body.i.us.preheader:                ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us.ph = phi i64 [ %ind.end, %middle.block ], [ %17, %omp.inner.for.body.preheader.i.us ], [ %17, %vector.memcheck ]
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.body.i.us.preheader, %omp.inner.for.body.i.us
  %indvars.iv.i.us = phi i64 [ %indvars.iv.next.i.us, %omp.inner.for.body.i.us ], [ %indvars.iv.i.us.ph, %omp.inner.for.body.i.us.preheader ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %107 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !209
  %arrayidx6.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %108 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !209
  %add7.i.us = fadd double %107, %108
  %arrayidx9.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %109 = load double, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !209
  %add10.i.us = fadd double %109, %add7.i.us
  store double %add10.i.us, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !209
  %110 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !209
  %arrayidx14.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %111 = load double, double* %arrayidx14.i.us, align 8, !tbaa !62, !noalias !209
  %add15.i.us = fadd double %110, %111
  %arrayidx17.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %112 = load double, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !209
  %add18.i.us = fadd double %112, %add15.i.us
  store double %add18.i.us, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !209
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp3.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp3.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..35.exit.us, !llvm.loop !226

.omp_outlined..35.exit.us:                        ; preds = %omp.inner.for.body.i.us, %middle.block, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !209
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !209
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !209
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !209
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !209
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %113 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %113, %.omp.iv.012.us
  %114 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %114
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %115 = load i32, i32* %.omp.comb.lb, align 4
  %116 = load i32, i32* %.omp.comb.ub, align 4
  %117 = zext i32 %116 to i64
  %118 = zext i32 %115 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..35 to void (i32*, i32*, ...)*), i64 %118, i64 %117, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %119 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %119, %.omp.iv.012
  %120 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %120
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..35.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..35(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp329 = icmp sgt i32 %7, %cond
  br i1 %cmp329, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  %10 = icmp sgt i64 %9, %8
  %smax = select i1 %10, i64 %9, i64 %8
  %11 = add nsw i64 %smax, 1
  %12 = sub nsw i64 %11, %8
  %min.iters.check = icmp ult i64 %12, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %8
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %11
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %8
  %scevgep37 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %11
  %scevgep39 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %8
  %scevgep41 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %11
  %scevgep43 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %8
  %scevgep45 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %11
  %scevgep47 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %8
  %scevgep49 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %11
  %bound0 = icmp ult double* %scevgep, %scevgep37
  %bound1 = icmp ult double* %scevgep35, %scevgep33
  %found.conflict = and i1 %bound0, %bound1
  %bound051 = icmp ult double* %scevgep, %scevgep41
  %bound152 = icmp ult double* %scevgep39, %scevgep33
  %found.conflict53 = and i1 %bound051, %bound152
  %conflict.rdx = or i1 %found.conflict, %found.conflict53
  %bound054 = icmp ult double* %scevgep, %scevgep45
  %bound155 = icmp ult double* %scevgep43, %scevgep33
  %found.conflict56 = and i1 %bound054, %bound155
  %conflict.rdx57 = or i1 %found.conflict56, %conflict.rdx
  %bound058 = icmp ult double* %scevgep, %scevgep49
  %bound159 = icmp ult double* %scevgep47, %scevgep33
  %found.conflict60 = and i1 %bound058, %bound159
  %conflict.rdx61 = or i1 %found.conflict60, %conflict.rdx57
  %bound062 = icmp ult double* %scevgep35, %scevgep41
  %bound163 = icmp ult double* %scevgep39, %scevgep37
  %found.conflict64 = and i1 %bound062, %bound163
  %conflict.rdx65 = or i1 %found.conflict64, %conflict.rdx61
  %bound066 = icmp ult double* %scevgep35, %scevgep45
  %bound167 = icmp ult double* %scevgep43, %scevgep37
  %found.conflict68 = and i1 %bound066, %bound167
  %conflict.rdx69 = or i1 %found.conflict68, %conflict.rdx65
  %bound070 = icmp ult double* %scevgep35, %scevgep49
  %bound171 = icmp ult double* %scevgep47, %scevgep37
  %found.conflict72 = and i1 %bound070, %bound171
  %conflict.rdx73 = or i1 %found.conflict72, %conflict.rdx69
  br i1 %conflict.rdx73, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %12, 12
  %n.vec = sub nsw i64 %12, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %8
  %13 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %14 = bitcast double* %13 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %14, align 8, !tbaa !62, !alias.scope !227
  %15 = getelementptr inbounds double, double* %13, i64 2
  %16 = bitcast double* %15 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %16, align 8, !tbaa !62, !alias.scope !227
  %17 = getelementptr inbounds double, double* %13, i64 4
  %18 = bitcast double* %17 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %18, align 8, !tbaa !62, !alias.scope !227
  %19 = getelementptr inbounds double, double* %13, i64 6
  %20 = bitcast double* %19 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %20, align 8, !tbaa !62, !alias.scope !227
  %21 = getelementptr inbounds double, double* %13, i64 8
  %22 = bitcast double* %21 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %22, align 8, !tbaa !62, !alias.scope !227
  %23 = getelementptr inbounds double, double* %13, i64 10
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !227
  %25 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !230
  %27 = getelementptr inbounds double, double* %25, i64 2
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !230
  %29 = getelementptr inbounds double, double* %25, i64 4
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !230
  %31 = getelementptr inbounds double, double* %25, i64 6
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !230
  %33 = getelementptr inbounds double, double* %25, i64 8
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !230
  %35 = getelementptr inbounds double, double* %25, i64 10
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !230
  %37 = fadd <2 x double> %wide.load, %wide.load84
  %38 = fadd <2 x double> %wide.load79, %wide.load85
  %39 = fadd <2 x double> %wide.load80, %wide.load86
  %40 = fadd <2 x double> %wide.load81, %wide.load87
  %41 = fadd <2 x double> %wide.load82, %wide.load88
  %42 = fadd <2 x double> %wide.load83, %wide.load89
  %43 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  %45 = getelementptr inbounds double, double* %43, i64 2
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  %47 = getelementptr inbounds double, double* %43, i64 4
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  %49 = getelementptr inbounds double, double* %43, i64 6
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  %51 = getelementptr inbounds double, double* %43, i64 8
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  %53 = getelementptr inbounds double, double* %43, i64 10
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  %55 = fadd <2 x double> %37, %wide.load90
  %56 = fadd <2 x double> %38, %wide.load91
  %57 = fadd <2 x double> %39, %wide.load92
  %58 = fadd <2 x double> %40, %wide.load93
  %59 = fadd <2 x double> %41, %wide.load94
  %60 = fadd <2 x double> %42, %wide.load95
  store <2 x double> %55, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  store <2 x double> %56, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  store <2 x double> %57, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  store <2 x double> %58, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  store <2 x double> %59, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  store <2 x double> %60, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !232, !noalias !234
  %61 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !237
  %63 = getelementptr inbounds double, double* %61, i64 2
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !237
  %65 = getelementptr inbounds double, double* %61, i64 4
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !237
  %67 = getelementptr inbounds double, double* %61, i64 6
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !237
  %69 = getelementptr inbounds double, double* %61, i64 8
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !237
  %71 = getelementptr inbounds double, double* %61, i64 10
  %72 = bitcast double* %71 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !237
  %73 = fadd <2 x double> %wide.load84, %wide.load102
  %74 = fadd <2 x double> %wide.load85, %wide.load103
  %75 = fadd <2 x double> %wide.load86, %wide.load104
  %76 = fadd <2 x double> %wide.load87, %wide.load105
  %77 = fadd <2 x double> %wide.load88, %wide.load106
  %78 = fadd <2 x double> %wide.load89, %wide.load107
  %79 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %80 = bitcast double* %79 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  %81 = getelementptr inbounds double, double* %79, i64 2
  %82 = bitcast double* %81 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  %83 = getelementptr inbounds double, double* %79, i64 4
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load110 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  %85 = getelementptr inbounds double, double* %79, i64 6
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load111 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  %87 = getelementptr inbounds double, double* %79, i64 8
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load112 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  %89 = getelementptr inbounds double, double* %79, i64 10
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load113 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  %91 = fadd <2 x double> %73, %wide.load108
  %92 = fadd <2 x double> %74, %wide.load109
  %93 = fadd <2 x double> %75, %wide.load110
  %94 = fadd <2 x double> %76, %wide.load111
  %95 = fadd <2 x double> %77, %wide.load112
  %96 = fadd <2 x double> %78, %wide.load113
  store <2 x double> %91, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  store <2 x double> %92, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  store <2 x double> %93, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  store <2 x double> %94, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  store <2 x double> %95, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  store <2 x double> %96, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !238, !noalias !239
  %index.next = add i64 %index, 12
  %97 = icmp eq i64 %index.next, %n.vec
  br i1 %97, label %middle.block, label %vector.body, !llvm.loop !240

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %8
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %8, %omp.inner.for.body.preheader ], [ %8, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %98 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx6, align 8, !tbaa !62
  %add7 = fadd double %98, %99
  %arrayidx9 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx9, align 8, !tbaa !62
  %add10 = fadd double %100, %add7
  store double %add10, double* %arrayidx9, align 8, !tbaa !62
  %101 = load double, double* %arrayidx6, align 8, !tbaa !62
  %arrayidx14 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %101, %102
  %arrayidx17 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %103, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp3 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp3, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !241

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..38(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..39(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..39 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..39(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 38, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !242
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !243

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !242
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !242
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !242
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !242
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !242
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !242
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !242
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !242
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !244

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..42(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..43(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..43 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..43(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 35, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !245
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !246

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !245
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !245
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !245
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !245
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !245
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !245
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !245
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !245
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !247

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..46(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..47(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..47 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..47(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 36, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !248
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !249

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !248
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !248
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !248
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !248
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !248
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !248
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !248
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !248
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !250

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..50(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..51(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..51 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..51(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 37, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !251
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !252

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !251
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !251
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !251
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !251
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !251
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !251
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !251
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !251
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !253

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..54(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us.preheader, label %omp.inner.for.body

omp.inner.for.body.us.preheader:                  ; preds = %omp.inner.for.body.lr.ph
  %scevgep17 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 1
  %scevgep23 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 1
  %scevgep28 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 1
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 1
  %scevgep38 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 1
  br label %omp.inner.for.body.us

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.us.preheader, %.omp_outlined..55.exit.us
  %.omp.iv.012.us = phi i32 [ %add.us, %.omp_outlined..55.exit.us ], [ %7, %omp.inner.for.body.us.preheader ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !254
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !254
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !254
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !254
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !254
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !254
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !254
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !254
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !254
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !254
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !254
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !254
  %cmp329.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp329.i.us, label %.omp_outlined..55.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  %19 = icmp sgt i64 %18, %17
  %smax = select i1 %19, i64 %18, i64 %17
  %20 = sub nsw i64 1, %17
  %21 = add nsw i64 %20, %smax
  %min.iters.check = icmp ult i64 %21, 12
  br i1 %min.iters.check, label %omp.inner.for.body.i.us.preheader, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader.i.us
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %17
  %scevgep19 = getelementptr double, double* %scevgep17, i64 %smax
  %scevgep21 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %17
  %scevgep24 = getelementptr double, double* %scevgep23, i64 %smax
  %scevgep26 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %17
  %scevgep29 = getelementptr double, double* %scevgep28, i64 %smax
  %scevgep31 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %17
  %scevgep34 = getelementptr double, double* %scevgep33, i64 %smax
  %scevgep36 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %17
  %scevgep39 = getelementptr double, double* %scevgep38, i64 %smax
  %bound0 = icmp ult double* %scevgep, %scevgep24
  %bound1 = icmp ult double* %scevgep21, %scevgep19
  %found.conflict = and i1 %bound0, %bound1
  %bound041 = icmp ult double* %scevgep, %scevgep29
  %bound142 = icmp ult double* %scevgep26, %scevgep19
  %found.conflict43 = and i1 %bound041, %bound142
  %conflict.rdx = or i1 %found.conflict, %found.conflict43
  %bound044 = icmp ult double* %scevgep, %scevgep34
  %bound145 = icmp ult double* %scevgep31, %scevgep19
  %found.conflict46 = and i1 %bound044, %bound145
  %conflict.rdx47 = or i1 %found.conflict46, %conflict.rdx
  %bound048 = icmp ult double* %scevgep, %scevgep39
  %bound149 = icmp ult double* %scevgep36, %scevgep19
  %found.conflict50 = and i1 %bound048, %bound149
  %conflict.rdx51 = or i1 %found.conflict50, %conflict.rdx47
  %bound052 = icmp ult double* %scevgep21, %scevgep29
  %bound153 = icmp ult double* %scevgep26, %scevgep24
  %found.conflict54 = and i1 %bound052, %bound153
  %conflict.rdx55 = or i1 %found.conflict54, %conflict.rdx51
  %bound056 = icmp ult double* %scevgep21, %scevgep34
  %bound157 = icmp ult double* %scevgep31, %scevgep24
  %found.conflict58 = and i1 %bound056, %bound157
  %conflict.rdx59 = or i1 %found.conflict58, %conflict.rdx55
  %bound060 = icmp ult double* %scevgep21, %scevgep39
  %bound161 = icmp ult double* %scevgep36, %scevgep24
  %found.conflict62 = and i1 %bound060, %bound161
  %conflict.rdx63 = or i1 %found.conflict62, %conflict.rdx59
  br i1 %conflict.rdx63, label %omp.inner.for.body.i.us.preheader, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 12
  %n.vec = sub nsw i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %17
  %22 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !257, !noalias !254
  %24 = getelementptr inbounds double, double* %22, i64 2
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !257, !noalias !254
  %26 = getelementptr inbounds double, double* %22, i64 4
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !257, !noalias !254
  %28 = getelementptr inbounds double, double* %22, i64 6
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !257, !noalias !254
  %30 = getelementptr inbounds double, double* %22, i64 8
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !257, !noalias !254
  %32 = getelementptr inbounds double, double* %22, i64 10
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !257, !noalias !254
  %34 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !260, !noalias !254
  %36 = getelementptr inbounds double, double* %34, i64 2
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !260, !noalias !254
  %38 = getelementptr inbounds double, double* %34, i64 4
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !260, !noalias !254
  %40 = getelementptr inbounds double, double* %34, i64 6
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !260, !noalias !254
  %42 = getelementptr inbounds double, double* %34, i64 8
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !260, !noalias !254
  %44 = getelementptr inbounds double, double* %34, i64 10
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !260, !noalias !254
  %46 = fadd <2 x double> %wide.load, %wide.load74
  %47 = fadd <2 x double> %wide.load69, %wide.load75
  %48 = fadd <2 x double> %wide.load70, %wide.load76
  %49 = fadd <2 x double> %wide.load71, %wide.load77
  %50 = fadd <2 x double> %wide.load72, %wide.load78
  %51 = fadd <2 x double> %wide.load73, %wide.load79
  %52 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  %54 = getelementptr inbounds double, double* %52, i64 2
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  %56 = getelementptr inbounds double, double* %52, i64 4
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  %58 = getelementptr inbounds double, double* %52, i64 6
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  %60 = getelementptr inbounds double, double* %52, i64 8
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  %62 = getelementptr inbounds double, double* %52, i64 10
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  %64 = fadd <2 x double> %46, %wide.load80
  %65 = fadd <2 x double> %47, %wide.load81
  %66 = fadd <2 x double> %48, %wide.load82
  %67 = fadd <2 x double> %49, %wide.load83
  %68 = fadd <2 x double> %50, %wide.load84
  %69 = fadd <2 x double> %51, %wide.load85
  store <2 x double> %64, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  store <2 x double> %65, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  store <2 x double> %66, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  store <2 x double> %67, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  store <2 x double> %68, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  store <2 x double> %69, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !262, !noalias !264
  %70 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !267, !noalias !254
  %72 = getelementptr inbounds double, double* %70, i64 2
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !267, !noalias !254
  %74 = getelementptr inbounds double, double* %70, i64 4
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !267, !noalias !254
  %76 = getelementptr inbounds double, double* %70, i64 6
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !267, !noalias !254
  %78 = getelementptr inbounds double, double* %70, i64 8
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !267, !noalias !254
  %80 = getelementptr inbounds double, double* %70, i64 10
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !267, !noalias !254
  %82 = fadd <2 x double> %wide.load74, %wide.load92
  %83 = fadd <2 x double> %wide.load75, %wide.load93
  %84 = fadd <2 x double> %wide.load76, %wide.load94
  %85 = fadd <2 x double> %wide.load77, %wide.load95
  %86 = fadd <2 x double> %wide.load78, %wide.load96
  %87 = fadd <2 x double> %wide.load79, %wide.load97
  %88 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  %90 = getelementptr inbounds double, double* %88, i64 2
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  %92 = getelementptr inbounds double, double* %88, i64 4
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  %94 = getelementptr inbounds double, double* %88, i64 6
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  %96 = getelementptr inbounds double, double* %88, i64 8
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  %98 = getelementptr inbounds double, double* %88, i64 10
  %99 = bitcast double* %98 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  %100 = fadd <2 x double> %82, %wide.load98
  %101 = fadd <2 x double> %83, %wide.load99
  %102 = fadd <2 x double> %84, %wide.load100
  %103 = fadd <2 x double> %85, %wide.load101
  %104 = fadd <2 x double> %86, %wide.load102
  %105 = fadd <2 x double> %87, %wide.load103
  store <2 x double> %100, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  store <2 x double> %101, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  store <2 x double> %102, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  store <2 x double> %103, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  store <2 x double> %104, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  store <2 x double> %105, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !268, !noalias !269
  %index.next = add i64 %index, 12
  %106 = icmp eq i64 %index.next, %n.vec
  br i1 %106, label %middle.block, label %vector.body, !llvm.loop !270

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %17
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %.omp_outlined..55.exit.us, label %omp.inner.for.body.i.us.preheader

omp.inner.for.body.i.us.preheader:                ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us.ph = phi i64 [ %ind.end, %middle.block ], [ %17, %omp.inner.for.body.preheader.i.us ], [ %17, %vector.memcheck ]
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.body.i.us.preheader, %omp.inner.for.body.i.us
  %indvars.iv.i.us = phi i64 [ %indvars.iv.next.i.us, %omp.inner.for.body.i.us ], [ %indvars.iv.i.us.ph, %omp.inner.for.body.i.us.preheader ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %107 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !254
  %arrayidx6.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %108 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !254
  %add7.i.us = fadd double %107, %108
  %arrayidx9.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %109 = load double, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !254
  %add10.i.us = fadd double %109, %add7.i.us
  store double %add10.i.us, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !254
  %110 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !254
  %arrayidx14.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %111 = load double, double* %arrayidx14.i.us, align 8, !tbaa !62, !noalias !254
  %add15.i.us = fadd double %110, %111
  %arrayidx17.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %112 = load double, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !254
  %add18.i.us = fadd double %112, %add15.i.us
  store double %add18.i.us, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !254
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp3.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp3.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..55.exit.us, !llvm.loop !271

.omp_outlined..55.exit.us:                        ; preds = %omp.inner.for.body.i.us, %middle.block, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !254
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !254
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !254
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !254
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !254
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %113 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %113, %.omp.iv.012.us
  %114 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %114
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 3) #6
  %115 = load i32, i32* %.omp.comb.lb, align 4
  %116 = load i32, i32* %.omp.comb.ub, align 4
  %117 = zext i32 %116 to i64
  %118 = zext i32 %115 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..55 to void (i32*, i32*, ...)*), i64 %118, i64 %117, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %119 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %119, %.omp.iv.012
  %120 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %120
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..55.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..55(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp329 = icmp sgt i32 %7, %cond
  br i1 %cmp329, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  %10 = icmp sgt i64 %9, %8
  %smax = select i1 %10, i64 %9, i64 %8
  %11 = add nsw i64 %smax, 1
  %12 = sub nsw i64 %11, %8
  %min.iters.check = icmp ult i64 %12, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %8
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %11
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %8
  %scevgep37 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %11
  %scevgep39 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %8
  %scevgep41 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %11
  %scevgep43 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %8
  %scevgep45 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %11
  %scevgep47 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %8
  %scevgep49 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %11
  %bound0 = icmp ult double* %scevgep, %scevgep37
  %bound1 = icmp ult double* %scevgep35, %scevgep33
  %found.conflict = and i1 %bound0, %bound1
  %bound051 = icmp ult double* %scevgep, %scevgep41
  %bound152 = icmp ult double* %scevgep39, %scevgep33
  %found.conflict53 = and i1 %bound051, %bound152
  %conflict.rdx = or i1 %found.conflict, %found.conflict53
  %bound054 = icmp ult double* %scevgep, %scevgep45
  %bound155 = icmp ult double* %scevgep43, %scevgep33
  %found.conflict56 = and i1 %bound054, %bound155
  %conflict.rdx57 = or i1 %found.conflict56, %conflict.rdx
  %bound058 = icmp ult double* %scevgep, %scevgep49
  %bound159 = icmp ult double* %scevgep47, %scevgep33
  %found.conflict60 = and i1 %bound058, %bound159
  %conflict.rdx61 = or i1 %found.conflict60, %conflict.rdx57
  %bound062 = icmp ult double* %scevgep35, %scevgep41
  %bound163 = icmp ult double* %scevgep39, %scevgep37
  %found.conflict64 = and i1 %bound062, %bound163
  %conflict.rdx65 = or i1 %found.conflict64, %conflict.rdx61
  %bound066 = icmp ult double* %scevgep35, %scevgep45
  %bound167 = icmp ult double* %scevgep43, %scevgep37
  %found.conflict68 = and i1 %bound066, %bound167
  %conflict.rdx69 = or i1 %found.conflict68, %conflict.rdx65
  %bound070 = icmp ult double* %scevgep35, %scevgep49
  %bound171 = icmp ult double* %scevgep47, %scevgep37
  %found.conflict72 = and i1 %bound070, %bound171
  %conflict.rdx73 = or i1 %found.conflict72, %conflict.rdx69
  br i1 %conflict.rdx73, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %12, 12
  %n.vec = sub nsw i64 %12, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %8
  %13 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %14 = bitcast double* %13 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %14, align 8, !tbaa !62, !alias.scope !272
  %15 = getelementptr inbounds double, double* %13, i64 2
  %16 = bitcast double* %15 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %16, align 8, !tbaa !62, !alias.scope !272
  %17 = getelementptr inbounds double, double* %13, i64 4
  %18 = bitcast double* %17 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %18, align 8, !tbaa !62, !alias.scope !272
  %19 = getelementptr inbounds double, double* %13, i64 6
  %20 = bitcast double* %19 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %20, align 8, !tbaa !62, !alias.scope !272
  %21 = getelementptr inbounds double, double* %13, i64 8
  %22 = bitcast double* %21 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %22, align 8, !tbaa !62, !alias.scope !272
  %23 = getelementptr inbounds double, double* %13, i64 10
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !272
  %25 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !275
  %27 = getelementptr inbounds double, double* %25, i64 2
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !275
  %29 = getelementptr inbounds double, double* %25, i64 4
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !275
  %31 = getelementptr inbounds double, double* %25, i64 6
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !275
  %33 = getelementptr inbounds double, double* %25, i64 8
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !275
  %35 = getelementptr inbounds double, double* %25, i64 10
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !275
  %37 = fadd <2 x double> %wide.load, %wide.load84
  %38 = fadd <2 x double> %wide.load79, %wide.load85
  %39 = fadd <2 x double> %wide.load80, %wide.load86
  %40 = fadd <2 x double> %wide.load81, %wide.load87
  %41 = fadd <2 x double> %wide.load82, %wide.load88
  %42 = fadd <2 x double> %wide.load83, %wide.load89
  %43 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  %45 = getelementptr inbounds double, double* %43, i64 2
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  %47 = getelementptr inbounds double, double* %43, i64 4
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  %49 = getelementptr inbounds double, double* %43, i64 6
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  %51 = getelementptr inbounds double, double* %43, i64 8
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  %53 = getelementptr inbounds double, double* %43, i64 10
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  %55 = fadd <2 x double> %37, %wide.load90
  %56 = fadd <2 x double> %38, %wide.load91
  %57 = fadd <2 x double> %39, %wide.load92
  %58 = fadd <2 x double> %40, %wide.load93
  %59 = fadd <2 x double> %41, %wide.load94
  %60 = fadd <2 x double> %42, %wide.load95
  store <2 x double> %55, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  store <2 x double> %56, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  store <2 x double> %57, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  store <2 x double> %58, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  store <2 x double> %59, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  store <2 x double> %60, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !277, !noalias !279
  %61 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !282
  %63 = getelementptr inbounds double, double* %61, i64 2
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !282
  %65 = getelementptr inbounds double, double* %61, i64 4
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !282
  %67 = getelementptr inbounds double, double* %61, i64 6
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !282
  %69 = getelementptr inbounds double, double* %61, i64 8
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !282
  %71 = getelementptr inbounds double, double* %61, i64 10
  %72 = bitcast double* %71 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !282
  %73 = fadd <2 x double> %wide.load84, %wide.load102
  %74 = fadd <2 x double> %wide.load85, %wide.load103
  %75 = fadd <2 x double> %wide.load86, %wide.load104
  %76 = fadd <2 x double> %wide.load87, %wide.load105
  %77 = fadd <2 x double> %wide.load88, %wide.load106
  %78 = fadd <2 x double> %wide.load89, %wide.load107
  %79 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %80 = bitcast double* %79 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  %81 = getelementptr inbounds double, double* %79, i64 2
  %82 = bitcast double* %81 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  %83 = getelementptr inbounds double, double* %79, i64 4
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load110 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  %85 = getelementptr inbounds double, double* %79, i64 6
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load111 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  %87 = getelementptr inbounds double, double* %79, i64 8
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load112 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  %89 = getelementptr inbounds double, double* %79, i64 10
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load113 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  %91 = fadd <2 x double> %73, %wide.load108
  %92 = fadd <2 x double> %74, %wide.load109
  %93 = fadd <2 x double> %75, %wide.load110
  %94 = fadd <2 x double> %76, %wide.load111
  %95 = fadd <2 x double> %77, %wide.load112
  %96 = fadd <2 x double> %78, %wide.load113
  store <2 x double> %91, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  store <2 x double> %92, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  store <2 x double> %93, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  store <2 x double> %94, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  store <2 x double> %95, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  store <2 x double> %96, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !283, !noalias !284
  %index.next = add i64 %index, 12
  %97 = icmp eq i64 %index.next, %n.vec
  br i1 %97, label %middle.block, label %vector.body, !llvm.loop !285

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %8
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %8, %omp.inner.for.body.preheader ], [ %8, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %98 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx6, align 8, !tbaa !62
  %add7 = fadd double %98, %99
  %arrayidx9 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx9, align 8, !tbaa !62
  %add10 = fadd double %100, %add7
  store double %add10, double* %arrayidx9, align 8, !tbaa !62
  %101 = load double, double* %arrayidx6, align 8, !tbaa !62
  %arrayidx14 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %101, %102
  %arrayidx17 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %103, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp3 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp3, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !286

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..58(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us.preheader, label %omp.inner.for.body

omp.inner.for.body.us.preheader:                  ; preds = %omp.inner.for.body.lr.ph
  %scevgep17 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 1
  %scevgep23 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 1
  %scevgep28 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 1
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 1
  %scevgep38 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 1
  br label %omp.inner.for.body.us

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.us.preheader, %.omp_outlined..59.exit.us
  %.omp.iv.012.us = phi i32 [ %add.us, %.omp_outlined..59.exit.us ], [ %7, %omp.inner.for.body.us.preheader ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !287
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !287
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !287
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !287
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !287
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !287
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !287
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !287
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !287
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !287
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !287
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !287
  %cmp329.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp329.i.us, label %.omp_outlined..59.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  %19 = icmp sgt i64 %18, %17
  %smax = select i1 %19, i64 %18, i64 %17
  %20 = sub nsw i64 1, %17
  %21 = add nsw i64 %20, %smax
  %min.iters.check = icmp ult i64 %21, 12
  br i1 %min.iters.check, label %omp.inner.for.body.i.us.preheader, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader.i.us
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %17
  %scevgep19 = getelementptr double, double* %scevgep17, i64 %smax
  %scevgep21 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %17
  %scevgep24 = getelementptr double, double* %scevgep23, i64 %smax
  %scevgep26 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %17
  %scevgep29 = getelementptr double, double* %scevgep28, i64 %smax
  %scevgep31 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %17
  %scevgep34 = getelementptr double, double* %scevgep33, i64 %smax
  %scevgep36 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %17
  %scevgep39 = getelementptr double, double* %scevgep38, i64 %smax
  %bound0 = icmp ult double* %scevgep, %scevgep24
  %bound1 = icmp ult double* %scevgep21, %scevgep19
  %found.conflict = and i1 %bound0, %bound1
  %bound041 = icmp ult double* %scevgep, %scevgep29
  %bound142 = icmp ult double* %scevgep26, %scevgep19
  %found.conflict43 = and i1 %bound041, %bound142
  %conflict.rdx = or i1 %found.conflict, %found.conflict43
  %bound044 = icmp ult double* %scevgep, %scevgep34
  %bound145 = icmp ult double* %scevgep31, %scevgep19
  %found.conflict46 = and i1 %bound044, %bound145
  %conflict.rdx47 = or i1 %found.conflict46, %conflict.rdx
  %bound048 = icmp ult double* %scevgep, %scevgep39
  %bound149 = icmp ult double* %scevgep36, %scevgep19
  %found.conflict50 = and i1 %bound048, %bound149
  %conflict.rdx51 = or i1 %found.conflict50, %conflict.rdx47
  %bound052 = icmp ult double* %scevgep21, %scevgep29
  %bound153 = icmp ult double* %scevgep26, %scevgep24
  %found.conflict54 = and i1 %bound052, %bound153
  %conflict.rdx55 = or i1 %found.conflict54, %conflict.rdx51
  %bound056 = icmp ult double* %scevgep21, %scevgep34
  %bound157 = icmp ult double* %scevgep31, %scevgep24
  %found.conflict58 = and i1 %bound056, %bound157
  %conflict.rdx59 = or i1 %found.conflict58, %conflict.rdx55
  %bound060 = icmp ult double* %scevgep21, %scevgep39
  %bound161 = icmp ult double* %scevgep36, %scevgep24
  %found.conflict62 = and i1 %bound060, %bound161
  %conflict.rdx63 = or i1 %found.conflict62, %conflict.rdx59
  br i1 %conflict.rdx63, label %omp.inner.for.body.i.us.preheader, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 12
  %n.vec = sub nsw i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %17
  %22 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !290, !noalias !287
  %24 = getelementptr inbounds double, double* %22, i64 2
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !290, !noalias !287
  %26 = getelementptr inbounds double, double* %22, i64 4
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !290, !noalias !287
  %28 = getelementptr inbounds double, double* %22, i64 6
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !290, !noalias !287
  %30 = getelementptr inbounds double, double* %22, i64 8
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !290, !noalias !287
  %32 = getelementptr inbounds double, double* %22, i64 10
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !290, !noalias !287
  %34 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !293, !noalias !287
  %36 = getelementptr inbounds double, double* %34, i64 2
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !293, !noalias !287
  %38 = getelementptr inbounds double, double* %34, i64 4
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !293, !noalias !287
  %40 = getelementptr inbounds double, double* %34, i64 6
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !293, !noalias !287
  %42 = getelementptr inbounds double, double* %34, i64 8
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !293, !noalias !287
  %44 = getelementptr inbounds double, double* %34, i64 10
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !293, !noalias !287
  %46 = fadd <2 x double> %wide.load, %wide.load74
  %47 = fadd <2 x double> %wide.load69, %wide.load75
  %48 = fadd <2 x double> %wide.load70, %wide.load76
  %49 = fadd <2 x double> %wide.load71, %wide.load77
  %50 = fadd <2 x double> %wide.load72, %wide.load78
  %51 = fadd <2 x double> %wide.load73, %wide.load79
  %52 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  %54 = getelementptr inbounds double, double* %52, i64 2
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  %56 = getelementptr inbounds double, double* %52, i64 4
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  %58 = getelementptr inbounds double, double* %52, i64 6
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  %60 = getelementptr inbounds double, double* %52, i64 8
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  %62 = getelementptr inbounds double, double* %52, i64 10
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  %64 = fadd <2 x double> %46, %wide.load80
  %65 = fadd <2 x double> %47, %wide.load81
  %66 = fadd <2 x double> %48, %wide.load82
  %67 = fadd <2 x double> %49, %wide.load83
  %68 = fadd <2 x double> %50, %wide.load84
  %69 = fadd <2 x double> %51, %wide.load85
  store <2 x double> %64, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  store <2 x double> %65, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  store <2 x double> %66, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  store <2 x double> %67, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  store <2 x double> %68, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  store <2 x double> %69, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !295, !noalias !297
  %70 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !300, !noalias !287
  %72 = getelementptr inbounds double, double* %70, i64 2
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !300, !noalias !287
  %74 = getelementptr inbounds double, double* %70, i64 4
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !300, !noalias !287
  %76 = getelementptr inbounds double, double* %70, i64 6
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !300, !noalias !287
  %78 = getelementptr inbounds double, double* %70, i64 8
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !300, !noalias !287
  %80 = getelementptr inbounds double, double* %70, i64 10
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !300, !noalias !287
  %82 = fadd <2 x double> %wide.load74, %wide.load92
  %83 = fadd <2 x double> %wide.load75, %wide.load93
  %84 = fadd <2 x double> %wide.load76, %wide.load94
  %85 = fadd <2 x double> %wide.load77, %wide.load95
  %86 = fadd <2 x double> %wide.load78, %wide.load96
  %87 = fadd <2 x double> %wide.load79, %wide.load97
  %88 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  %90 = getelementptr inbounds double, double* %88, i64 2
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  %92 = getelementptr inbounds double, double* %88, i64 4
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  %94 = getelementptr inbounds double, double* %88, i64 6
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  %96 = getelementptr inbounds double, double* %88, i64 8
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  %98 = getelementptr inbounds double, double* %88, i64 10
  %99 = bitcast double* %98 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  %100 = fadd <2 x double> %82, %wide.load98
  %101 = fadd <2 x double> %83, %wide.load99
  %102 = fadd <2 x double> %84, %wide.load100
  %103 = fadd <2 x double> %85, %wide.load101
  %104 = fadd <2 x double> %86, %wide.load102
  %105 = fadd <2 x double> %87, %wide.load103
  store <2 x double> %100, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  store <2 x double> %101, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  store <2 x double> %102, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  store <2 x double> %103, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  store <2 x double> %104, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  store <2 x double> %105, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !301, !noalias !302
  %index.next = add i64 %index, 12
  %106 = icmp eq i64 %index.next, %n.vec
  br i1 %106, label %middle.block, label %vector.body, !llvm.loop !303

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %17
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %.omp_outlined..59.exit.us, label %omp.inner.for.body.i.us.preheader

omp.inner.for.body.i.us.preheader:                ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us.ph = phi i64 [ %ind.end, %middle.block ], [ %17, %omp.inner.for.body.preheader.i.us ], [ %17, %vector.memcheck ]
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.body.i.us.preheader, %omp.inner.for.body.i.us
  %indvars.iv.i.us = phi i64 [ %indvars.iv.next.i.us, %omp.inner.for.body.i.us ], [ %indvars.iv.i.us.ph, %omp.inner.for.body.i.us.preheader ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %107 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !287
  %arrayidx6.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %108 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !287
  %add7.i.us = fadd double %107, %108
  %arrayidx9.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %109 = load double, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !287
  %add10.i.us = fadd double %109, %add7.i.us
  store double %add10.i.us, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !287
  %110 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !287
  %arrayidx14.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %111 = load double, double* %arrayidx14.i.us, align 8, !tbaa !62, !noalias !287
  %add15.i.us = fadd double %110, %111
  %arrayidx17.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %112 = load double, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !287
  %add18.i.us = fadd double %112, %add15.i.us
  store double %add18.i.us, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !287
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp3.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp3.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..59.exit.us, !llvm.loop !304

.omp_outlined..59.exit.us:                        ; preds = %omp.inner.for.body.i.us, %middle.block, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !287
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !287
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !287
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !287
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !287
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %113 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %113, %.omp.iv.012.us
  %114 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %114
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %115 = load i32, i32* %.omp.comb.lb, align 4
  %116 = load i32, i32* %.omp.comb.ub, align 4
  %117 = zext i32 %116 to i64
  %118 = zext i32 %115 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..59 to void (i32*, i32*, ...)*), i64 %118, i64 %117, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %119 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %119, %.omp.iv.012
  %120 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %120
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..59.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..59(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp329 = icmp sgt i32 %7, %cond
  br i1 %cmp329, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  %10 = icmp sgt i64 %9, %8
  %smax = select i1 %10, i64 %9, i64 %8
  %11 = add nsw i64 %smax, 1
  %12 = sub nsw i64 %11, %8
  %min.iters.check = icmp ult i64 %12, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %8
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %11
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %8
  %scevgep37 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %11
  %scevgep39 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %8
  %scevgep41 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %11
  %scevgep43 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %8
  %scevgep45 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %11
  %scevgep47 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %8
  %scevgep49 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %11
  %bound0 = icmp ult double* %scevgep, %scevgep37
  %bound1 = icmp ult double* %scevgep35, %scevgep33
  %found.conflict = and i1 %bound0, %bound1
  %bound051 = icmp ult double* %scevgep, %scevgep41
  %bound152 = icmp ult double* %scevgep39, %scevgep33
  %found.conflict53 = and i1 %bound051, %bound152
  %conflict.rdx = or i1 %found.conflict, %found.conflict53
  %bound054 = icmp ult double* %scevgep, %scevgep45
  %bound155 = icmp ult double* %scevgep43, %scevgep33
  %found.conflict56 = and i1 %bound054, %bound155
  %conflict.rdx57 = or i1 %found.conflict56, %conflict.rdx
  %bound058 = icmp ult double* %scevgep, %scevgep49
  %bound159 = icmp ult double* %scevgep47, %scevgep33
  %found.conflict60 = and i1 %bound058, %bound159
  %conflict.rdx61 = or i1 %found.conflict60, %conflict.rdx57
  %bound062 = icmp ult double* %scevgep35, %scevgep41
  %bound163 = icmp ult double* %scevgep39, %scevgep37
  %found.conflict64 = and i1 %bound062, %bound163
  %conflict.rdx65 = or i1 %found.conflict64, %conflict.rdx61
  %bound066 = icmp ult double* %scevgep35, %scevgep45
  %bound167 = icmp ult double* %scevgep43, %scevgep37
  %found.conflict68 = and i1 %bound066, %bound167
  %conflict.rdx69 = or i1 %found.conflict68, %conflict.rdx65
  %bound070 = icmp ult double* %scevgep35, %scevgep49
  %bound171 = icmp ult double* %scevgep47, %scevgep37
  %found.conflict72 = and i1 %bound070, %bound171
  %conflict.rdx73 = or i1 %found.conflict72, %conflict.rdx69
  br i1 %conflict.rdx73, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %12, 12
  %n.vec = sub nsw i64 %12, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %8
  %13 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %14 = bitcast double* %13 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %14, align 8, !tbaa !62, !alias.scope !305
  %15 = getelementptr inbounds double, double* %13, i64 2
  %16 = bitcast double* %15 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %16, align 8, !tbaa !62, !alias.scope !305
  %17 = getelementptr inbounds double, double* %13, i64 4
  %18 = bitcast double* %17 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %18, align 8, !tbaa !62, !alias.scope !305
  %19 = getelementptr inbounds double, double* %13, i64 6
  %20 = bitcast double* %19 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %20, align 8, !tbaa !62, !alias.scope !305
  %21 = getelementptr inbounds double, double* %13, i64 8
  %22 = bitcast double* %21 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %22, align 8, !tbaa !62, !alias.scope !305
  %23 = getelementptr inbounds double, double* %13, i64 10
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !305
  %25 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !308
  %27 = getelementptr inbounds double, double* %25, i64 2
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !308
  %29 = getelementptr inbounds double, double* %25, i64 4
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !308
  %31 = getelementptr inbounds double, double* %25, i64 6
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !308
  %33 = getelementptr inbounds double, double* %25, i64 8
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !308
  %35 = getelementptr inbounds double, double* %25, i64 10
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !308
  %37 = fadd <2 x double> %wide.load, %wide.load84
  %38 = fadd <2 x double> %wide.load79, %wide.load85
  %39 = fadd <2 x double> %wide.load80, %wide.load86
  %40 = fadd <2 x double> %wide.load81, %wide.load87
  %41 = fadd <2 x double> %wide.load82, %wide.load88
  %42 = fadd <2 x double> %wide.load83, %wide.load89
  %43 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  %45 = getelementptr inbounds double, double* %43, i64 2
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  %47 = getelementptr inbounds double, double* %43, i64 4
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  %49 = getelementptr inbounds double, double* %43, i64 6
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  %51 = getelementptr inbounds double, double* %43, i64 8
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  %53 = getelementptr inbounds double, double* %43, i64 10
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  %55 = fadd <2 x double> %37, %wide.load90
  %56 = fadd <2 x double> %38, %wide.load91
  %57 = fadd <2 x double> %39, %wide.load92
  %58 = fadd <2 x double> %40, %wide.load93
  %59 = fadd <2 x double> %41, %wide.load94
  %60 = fadd <2 x double> %42, %wide.load95
  store <2 x double> %55, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  store <2 x double> %56, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  store <2 x double> %57, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  store <2 x double> %58, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  store <2 x double> %59, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  store <2 x double> %60, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !310, !noalias !312
  %61 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !315
  %63 = getelementptr inbounds double, double* %61, i64 2
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !315
  %65 = getelementptr inbounds double, double* %61, i64 4
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !315
  %67 = getelementptr inbounds double, double* %61, i64 6
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !315
  %69 = getelementptr inbounds double, double* %61, i64 8
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !315
  %71 = getelementptr inbounds double, double* %61, i64 10
  %72 = bitcast double* %71 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !315
  %73 = fadd <2 x double> %wide.load84, %wide.load102
  %74 = fadd <2 x double> %wide.load85, %wide.load103
  %75 = fadd <2 x double> %wide.load86, %wide.load104
  %76 = fadd <2 x double> %wide.load87, %wide.load105
  %77 = fadd <2 x double> %wide.load88, %wide.load106
  %78 = fadd <2 x double> %wide.load89, %wide.load107
  %79 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %80 = bitcast double* %79 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  %81 = getelementptr inbounds double, double* %79, i64 2
  %82 = bitcast double* %81 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  %83 = getelementptr inbounds double, double* %79, i64 4
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load110 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  %85 = getelementptr inbounds double, double* %79, i64 6
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load111 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  %87 = getelementptr inbounds double, double* %79, i64 8
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load112 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  %89 = getelementptr inbounds double, double* %79, i64 10
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load113 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  %91 = fadd <2 x double> %73, %wide.load108
  %92 = fadd <2 x double> %74, %wide.load109
  %93 = fadd <2 x double> %75, %wide.load110
  %94 = fadd <2 x double> %76, %wide.load111
  %95 = fadd <2 x double> %77, %wide.load112
  %96 = fadd <2 x double> %78, %wide.load113
  store <2 x double> %91, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  store <2 x double> %92, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  store <2 x double> %93, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  store <2 x double> %94, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  store <2 x double> %95, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  store <2 x double> %96, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !316, !noalias !317
  %index.next = add i64 %index, 12
  %97 = icmp eq i64 %index.next, %n.vec
  br i1 %97, label %middle.block, label %vector.body, !llvm.loop !318

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %8
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %8, %omp.inner.for.body.preheader ], [ %8, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %98 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx6, align 8, !tbaa !62
  %add7 = fadd double %98, %99
  %arrayidx9 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx9, align 8, !tbaa !62
  %add10 = fadd double %100, %add7
  store double %add10, double* %arrayidx9, align 8, !tbaa !62
  %101 = load double, double* %arrayidx6, align 8, !tbaa !62
  %arrayidx14 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %101, %102
  %arrayidx17 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %103, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp3 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp3, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !319

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..62(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..63(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..63 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..63(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 38, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !320
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !321

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !320
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !320
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !320
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !320
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !320
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !320
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !320
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !320
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !322

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..66(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..67(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..67 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..67(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 35, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !323
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !324

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !323
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !323
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !323
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !323
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !323
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !323
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !323
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !323
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !325

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..70(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..71(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..71 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..71(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 36, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !326
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !327

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !326
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !326
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !326
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !326
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !326
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !326
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !326
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !326
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !328

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..74(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.012.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..75(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.012.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..75 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.012
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..75(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 37, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool29 = icmp eq i32 %5, 0
  br i1 %tobool29, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !329
  %cmp27 = icmp sgt i32 %7, %8
  br i1 %cmp27, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load35 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load36 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load40
  %39 = fadd <2 x double> %wide.load35, %wide.load41
  %40 = fadd <2 x double> %wide.load36, %wide.load42
  %41 = fadd <2 x double> %wide.load37, %wide.load43
  %42 = fadd <2 x double> %wide.load38, %wide.load44
  %43 = fadd <2 x double> %wide.load39, %wide.load45
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load46
  %57 = fadd <2 x double> %39, %wide.load47
  %58 = fadd <2 x double> %40, %wide.load48
  %59 = fadd <2 x double> %41, %wide.load49
  %60 = fadd <2 x double> %42, %wide.load50
  %61 = fadd <2 x double> %43, %wide.load51
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load52 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load53 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load54 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load55 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load56 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load57 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load52, %wide.load58
  %75 = fadd <2 x double> %wide.load53, %wide.load59
  %76 = fadd <2 x double> %wide.load54, %wide.load60
  %77 = fadd <2 x double> %wide.load55, %wide.load61
  %78 = fadd <2 x double> %wide.load56, %wide.load62
  %79 = fadd <2 x double> %wide.load57, %wide.load63
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load64
  %93 = fadd <2 x double> %75, %wide.load65
  %94 = fadd <2 x double> %76, %wide.load66
  %95 = fadd <2 x double> %77, %wide.load67
  %96 = fadd <2 x double> %78, %wide.load68
  %97 = fadd <2 x double> %79, %wide.load69
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !330

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !329
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !329
  %add5 = fadd double %99, %100
  %arrayidx7 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !329
  %add8 = fadd double %101, %add5
  store double %add8, double* %arrayidx7, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !329
  %102 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !329
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !329
  %add13 = fadd double %102, %103
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !329
  %add16 = fadd double %104, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !329
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !331

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..78(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp511 = icmp sgt i32 %7, %cond
  br i1 %cmp511, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us.preheader, label %omp.inner.for.body

omp.inner.for.body.us.preheader:                  ; preds = %omp.inner.for.body.lr.ph
  %scevgep17 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 1
  %scevgep23 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 1
  %scevgep28 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 1
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 1
  %scevgep38 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 1
  br label %omp.inner.for.body.us

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.us.preheader, %.omp_outlined..79.exit.us
  %.omp.iv.012.us = phi i32 [ %add.us, %.omp_outlined..79.exit.us ], [ %7, %omp.inner.for.body.us.preheader ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !332
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !332
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !332
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !332
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !332
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !332
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !332
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !332
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !332
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !332
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !332
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !332
  %cmp329.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp329.i.us, label %.omp_outlined..79.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  %19 = icmp sgt i64 %18, %17
  %smax = select i1 %19, i64 %18, i64 %17
  %20 = sub nsw i64 1, %17
  %21 = add nsw i64 %20, %smax
  %min.iters.check = icmp ult i64 %21, 12
  br i1 %min.iters.check, label %omp.inner.for.body.i.us.preheader, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader.i.us
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %17
  %scevgep19 = getelementptr double, double* %scevgep17, i64 %smax
  %scevgep21 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %17
  %scevgep24 = getelementptr double, double* %scevgep23, i64 %smax
  %scevgep26 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %17
  %scevgep29 = getelementptr double, double* %scevgep28, i64 %smax
  %scevgep31 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %17
  %scevgep34 = getelementptr double, double* %scevgep33, i64 %smax
  %scevgep36 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %17
  %scevgep39 = getelementptr double, double* %scevgep38, i64 %smax
  %bound0 = icmp ult double* %scevgep, %scevgep24
  %bound1 = icmp ult double* %scevgep21, %scevgep19
  %found.conflict = and i1 %bound0, %bound1
  %bound041 = icmp ult double* %scevgep, %scevgep29
  %bound142 = icmp ult double* %scevgep26, %scevgep19
  %found.conflict43 = and i1 %bound041, %bound142
  %conflict.rdx = or i1 %found.conflict, %found.conflict43
  %bound044 = icmp ult double* %scevgep, %scevgep34
  %bound145 = icmp ult double* %scevgep31, %scevgep19
  %found.conflict46 = and i1 %bound044, %bound145
  %conflict.rdx47 = or i1 %found.conflict46, %conflict.rdx
  %bound048 = icmp ult double* %scevgep, %scevgep39
  %bound149 = icmp ult double* %scevgep36, %scevgep19
  %found.conflict50 = and i1 %bound048, %bound149
  %conflict.rdx51 = or i1 %found.conflict50, %conflict.rdx47
  %bound052 = icmp ult double* %scevgep21, %scevgep29
  %bound153 = icmp ult double* %scevgep26, %scevgep24
  %found.conflict54 = and i1 %bound052, %bound153
  %conflict.rdx55 = or i1 %found.conflict54, %conflict.rdx51
  %bound056 = icmp ult double* %scevgep21, %scevgep34
  %bound157 = icmp ult double* %scevgep31, %scevgep24
  %found.conflict58 = and i1 %bound056, %bound157
  %conflict.rdx59 = or i1 %found.conflict58, %conflict.rdx55
  %bound060 = icmp ult double* %scevgep21, %scevgep39
  %bound161 = icmp ult double* %scevgep36, %scevgep24
  %found.conflict62 = and i1 %bound060, %bound161
  %conflict.rdx63 = or i1 %found.conflict62, %conflict.rdx59
  br i1 %conflict.rdx63, label %omp.inner.for.body.i.us.preheader, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 12
  %n.vec = sub nsw i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %17
  %22 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !335, !noalias !332
  %24 = getelementptr inbounds double, double* %22, i64 2
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !335, !noalias !332
  %26 = getelementptr inbounds double, double* %22, i64 4
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !335, !noalias !332
  %28 = getelementptr inbounds double, double* %22, i64 6
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !335, !noalias !332
  %30 = getelementptr inbounds double, double* %22, i64 8
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !335, !noalias !332
  %32 = getelementptr inbounds double, double* %22, i64 10
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !335, !noalias !332
  %34 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !338, !noalias !332
  %36 = getelementptr inbounds double, double* %34, i64 2
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !338, !noalias !332
  %38 = getelementptr inbounds double, double* %34, i64 4
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !338, !noalias !332
  %40 = getelementptr inbounds double, double* %34, i64 6
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !338, !noalias !332
  %42 = getelementptr inbounds double, double* %34, i64 8
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !338, !noalias !332
  %44 = getelementptr inbounds double, double* %34, i64 10
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !338, !noalias !332
  %46 = fadd <2 x double> %wide.load, %wide.load74
  %47 = fadd <2 x double> %wide.load69, %wide.load75
  %48 = fadd <2 x double> %wide.load70, %wide.load76
  %49 = fadd <2 x double> %wide.load71, %wide.load77
  %50 = fadd <2 x double> %wide.load72, %wide.load78
  %51 = fadd <2 x double> %wide.load73, %wide.load79
  %52 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  %54 = getelementptr inbounds double, double* %52, i64 2
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  %56 = getelementptr inbounds double, double* %52, i64 4
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  %58 = getelementptr inbounds double, double* %52, i64 6
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  %60 = getelementptr inbounds double, double* %52, i64 8
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  %62 = getelementptr inbounds double, double* %52, i64 10
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  %64 = fadd <2 x double> %46, %wide.load80
  %65 = fadd <2 x double> %47, %wide.load81
  %66 = fadd <2 x double> %48, %wide.load82
  %67 = fadd <2 x double> %49, %wide.load83
  %68 = fadd <2 x double> %50, %wide.load84
  %69 = fadd <2 x double> %51, %wide.load85
  store <2 x double> %64, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  store <2 x double> %65, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  store <2 x double> %66, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  store <2 x double> %67, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  store <2 x double> %68, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  store <2 x double> %69, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !340, !noalias !342
  %70 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !345, !noalias !332
  %72 = getelementptr inbounds double, double* %70, i64 2
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !345, !noalias !332
  %74 = getelementptr inbounds double, double* %70, i64 4
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !345, !noalias !332
  %76 = getelementptr inbounds double, double* %70, i64 6
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !345, !noalias !332
  %78 = getelementptr inbounds double, double* %70, i64 8
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !345, !noalias !332
  %80 = getelementptr inbounds double, double* %70, i64 10
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !345, !noalias !332
  %82 = fadd <2 x double> %wide.load74, %wide.load92
  %83 = fadd <2 x double> %wide.load75, %wide.load93
  %84 = fadd <2 x double> %wide.load76, %wide.load94
  %85 = fadd <2 x double> %wide.load77, %wide.load95
  %86 = fadd <2 x double> %wide.load78, %wide.load96
  %87 = fadd <2 x double> %wide.load79, %wide.load97
  %88 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  %90 = getelementptr inbounds double, double* %88, i64 2
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  %92 = getelementptr inbounds double, double* %88, i64 4
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  %94 = getelementptr inbounds double, double* %88, i64 6
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  %96 = getelementptr inbounds double, double* %88, i64 8
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  %98 = getelementptr inbounds double, double* %88, i64 10
  %99 = bitcast double* %98 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  %100 = fadd <2 x double> %82, %wide.load98
  %101 = fadd <2 x double> %83, %wide.load99
  %102 = fadd <2 x double> %84, %wide.load100
  %103 = fadd <2 x double> %85, %wide.load101
  %104 = fadd <2 x double> %86, %wide.load102
  %105 = fadd <2 x double> %87, %wide.load103
  store <2 x double> %100, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  store <2 x double> %101, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  store <2 x double> %102, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  store <2 x double> %103, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  store <2 x double> %104, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  store <2 x double> %105, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !346, !noalias !347
  %index.next = add i64 %index, 12
  %106 = icmp eq i64 %index.next, %n.vec
  br i1 %106, label %middle.block, label %vector.body, !llvm.loop !348

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %17
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %.omp_outlined..79.exit.us, label %omp.inner.for.body.i.us.preheader

omp.inner.for.body.i.us.preheader:                ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us.ph = phi i64 [ %ind.end, %middle.block ], [ %17, %omp.inner.for.body.preheader.i.us ], [ %17, %vector.memcheck ]
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.body.i.us.preheader, %omp.inner.for.body.i.us
  %indvars.iv.i.us = phi i64 [ %indvars.iv.next.i.us, %omp.inner.for.body.i.us ], [ %indvars.iv.i.us.ph, %omp.inner.for.body.i.us.preheader ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %107 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !332
  %arrayidx6.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %108 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !332
  %add7.i.us = fadd double %107, %108
  %arrayidx9.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %109 = load double, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !332
  %add10.i.us = fadd double %109, %add7.i.us
  store double %add10.i.us, double* %arrayidx9.i.us, align 8, !tbaa !62, !noalias !332
  %110 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !332
  %arrayidx14.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %111 = load double, double* %arrayidx14.i.us, align 8, !tbaa !62, !noalias !332
  %add15.i.us = fadd double %110, %111
  %arrayidx17.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %112 = load double, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !332
  %add18.i.us = fadd double %112, %add15.i.us
  store double %add18.i.us, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !332
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp3.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp3.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..79.exit.us, !llvm.loop !349

.omp_outlined..79.exit.us:                        ; preds = %omp.inner.for.body.i.us, %middle.block, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !332
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !332
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !332
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !332
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !332
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %113 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %113, %.omp.iv.012.us
  %114 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %114
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.012 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  call void @__kmpc_push_proc_bind(%struct.ident_t* nonnull @2, i32 %4, i32 4) #6
  %115 = load i32, i32* %.omp.comb.lb, align 4
  %116 = load i32, i32* %.omp.comb.ub, align 4
  %117 = zext i32 %116 to i64
  %118 = zext i32 %115 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..79 to void (i32*, i32*, ...)*), i64 %118, i64 %117, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %119 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %119, %.omp.iv.012
  %120 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %120
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..79.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..79(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp329 = icmp sgt i32 %7, %cond
  br i1 %cmp329, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  %10 = icmp sgt i64 %9, %8
  %smax = select i1 %10, i64 %9, i64 %8
  %11 = add nsw i64 %smax, 1
  %12 = sub nsw i64 %11, %8
  %min.iters.check = icmp ult i64 %12, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %8
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %11
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %8
  %scevgep37 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %11
  %scevgep39 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %8
  %scevgep41 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %11
  %scevgep43 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %8
  %scevgep45 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %11
  %scevgep47 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %8
  %scevgep49 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %11
  %bound0 = icmp ult double* %scevgep, %scevgep37
  %bound1 = icmp ult double* %scevgep35, %scevgep33
  %found.conflict = and i1 %bound0, %bound1
  %bound051 = icmp ult double* %scevgep, %scevgep41
  %bound152 = icmp ult double* %scevgep39, %scevgep33
  %found.conflict53 = and i1 %bound051, %bound152
  %conflict.rdx = or i1 %found.conflict, %found.conflict53
  %bound054 = icmp ult double* %scevgep, %scevgep45
  %bound155 = icmp ult double* %scevgep43, %scevgep33
  %found.conflict56 = and i1 %bound054, %bound155
  %conflict.rdx57 = or i1 %found.conflict56, %conflict.rdx
  %bound058 = icmp ult double* %scevgep, %scevgep49
  %bound159 = icmp ult double* %scevgep47, %scevgep33
  %found.conflict60 = and i1 %bound058, %bound159
  %conflict.rdx61 = or i1 %found.conflict60, %conflict.rdx57
  %bound062 = icmp ult double* %scevgep35, %scevgep41
  %bound163 = icmp ult double* %scevgep39, %scevgep37
  %found.conflict64 = and i1 %bound062, %bound163
  %conflict.rdx65 = or i1 %found.conflict64, %conflict.rdx61
  %bound066 = icmp ult double* %scevgep35, %scevgep45
  %bound167 = icmp ult double* %scevgep43, %scevgep37
  %found.conflict68 = and i1 %bound066, %bound167
  %conflict.rdx69 = or i1 %found.conflict68, %conflict.rdx65
  %bound070 = icmp ult double* %scevgep35, %scevgep49
  %bound171 = icmp ult double* %scevgep47, %scevgep37
  %found.conflict72 = and i1 %bound070, %bound171
  %conflict.rdx73 = or i1 %found.conflict72, %conflict.rdx69
  br i1 %conflict.rdx73, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %12, 12
  %n.vec = sub nsw i64 %12, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %8
  %13 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %14 = bitcast double* %13 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %14, align 8, !tbaa !62, !alias.scope !350
  %15 = getelementptr inbounds double, double* %13, i64 2
  %16 = bitcast double* %15 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %16, align 8, !tbaa !62, !alias.scope !350
  %17 = getelementptr inbounds double, double* %13, i64 4
  %18 = bitcast double* %17 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %18, align 8, !tbaa !62, !alias.scope !350
  %19 = getelementptr inbounds double, double* %13, i64 6
  %20 = bitcast double* %19 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %20, align 8, !tbaa !62, !alias.scope !350
  %21 = getelementptr inbounds double, double* %13, i64 8
  %22 = bitcast double* %21 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %22, align 8, !tbaa !62, !alias.scope !350
  %23 = getelementptr inbounds double, double* %13, i64 10
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !350
  %25 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !353
  %27 = getelementptr inbounds double, double* %25, i64 2
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !353
  %29 = getelementptr inbounds double, double* %25, i64 4
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !353
  %31 = getelementptr inbounds double, double* %25, i64 6
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !353
  %33 = getelementptr inbounds double, double* %25, i64 8
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !353
  %35 = getelementptr inbounds double, double* %25, i64 10
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !353
  %37 = fadd <2 x double> %wide.load, %wide.load84
  %38 = fadd <2 x double> %wide.load79, %wide.load85
  %39 = fadd <2 x double> %wide.load80, %wide.load86
  %40 = fadd <2 x double> %wide.load81, %wide.load87
  %41 = fadd <2 x double> %wide.load82, %wide.load88
  %42 = fadd <2 x double> %wide.load83, %wide.load89
  %43 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  %45 = getelementptr inbounds double, double* %43, i64 2
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  %47 = getelementptr inbounds double, double* %43, i64 4
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  %49 = getelementptr inbounds double, double* %43, i64 6
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  %51 = getelementptr inbounds double, double* %43, i64 8
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  %53 = getelementptr inbounds double, double* %43, i64 10
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  %55 = fadd <2 x double> %37, %wide.load90
  %56 = fadd <2 x double> %38, %wide.load91
  %57 = fadd <2 x double> %39, %wide.load92
  %58 = fadd <2 x double> %40, %wide.load93
  %59 = fadd <2 x double> %41, %wide.load94
  %60 = fadd <2 x double> %42, %wide.load95
  store <2 x double> %55, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  store <2 x double> %56, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  store <2 x double> %57, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  store <2 x double> %58, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  store <2 x double> %59, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  store <2 x double> %60, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !355, !noalias !357
  %61 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !360
  %63 = getelementptr inbounds double, double* %61, i64 2
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !360
  %65 = getelementptr inbounds double, double* %61, i64 4
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !360
  %67 = getelementptr inbounds double, double* %61, i64 6
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !360
  %69 = getelementptr inbounds double, double* %61, i64 8
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !360
  %71 = getelementptr inbounds double, double* %61, i64 10
  %72 = bitcast double* %71 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !360
  %73 = fadd <2 x double> %wide.load84, %wide.load102
  %74 = fadd <2 x double> %wide.load85, %wide.load103
  %75 = fadd <2 x double> %wide.load86, %wide.load104
  %76 = fadd <2 x double> %wide.load87, %wide.load105
  %77 = fadd <2 x double> %wide.load88, %wide.load106
  %78 = fadd <2 x double> %wide.load89, %wide.load107
  %79 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %80 = bitcast double* %79 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  %81 = getelementptr inbounds double, double* %79, i64 2
  %82 = bitcast double* %81 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  %83 = getelementptr inbounds double, double* %79, i64 4
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load110 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  %85 = getelementptr inbounds double, double* %79, i64 6
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load111 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  %87 = getelementptr inbounds double, double* %79, i64 8
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load112 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  %89 = getelementptr inbounds double, double* %79, i64 10
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load113 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  %91 = fadd <2 x double> %73, %wide.load108
  %92 = fadd <2 x double> %74, %wide.load109
  %93 = fadd <2 x double> %75, %wide.load110
  %94 = fadd <2 x double> %76, %wide.load111
  %95 = fadd <2 x double> %77, %wide.load112
  %96 = fadd <2 x double> %78, %wide.load113
  store <2 x double> %91, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  store <2 x double> %92, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  store <2 x double> %93, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  store <2 x double> %94, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  store <2 x double> %95, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  store <2 x double> %96, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !361, !noalias !362
  %index.next = add i64 %index, 12
  %97 = icmp eq i64 %index.next, %n.vec
  br i1 %97, label %middle.block, label %vector.body, !llvm.loop !363

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %8
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %8, %omp.inner.for.body.preheader ], [ %8, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %98 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx6, align 8, !tbaa !62
  %add7 = fadd double %98, %99
  %arrayidx9 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx9, align 8, !tbaa !62
  %add10 = fadd double %100, %add7
  store double %add10, double* %arrayidx9, align 8, !tbaa !62
  %101 = load double, double* %arrayidx6, align 8, !tbaa !62
  %arrayidx14 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %101, %102
  %arrayidx17 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %103, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp3 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp3, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !364

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..82(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp513 = icmp sgt i32 %7, %cond
  br i1 %cmp513, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us.preheader, label %omp.inner.for.body

omp.inner.for.body.us.preheader:                  ; preds = %omp.inner.for.body.lr.ph
  %scevgep19 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 1
  %scevgep25 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 1
  %scevgep30 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 1
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 1
  %scevgep40 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 1
  br label %omp.inner.for.body.us

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.us.preheader, %.omp_outlined..83.exit.us
  %.omp.iv.014.us = phi i32 [ %add.us, %.omp_outlined..83.exit.us ], [ %7, %omp.inner.for.body.us.preheader ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !365
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !365
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !365
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !365
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !365
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !365
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !365
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !365
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !365
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !365
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !365
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !365
  %cmp331.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp331.i.us, label %.omp_outlined..83.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  %19 = icmp sgt i64 %18, %17
  %smax = select i1 %19, i64 %18, i64 %17
  %20 = sub nsw i64 1, %17
  %21 = add nsw i64 %20, %smax
  %min.iters.check = icmp ult i64 %21, 12
  br i1 %min.iters.check, label %omp.inner.for.body.i.us.preheader, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader.i.us
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %17
  %scevgep21 = getelementptr double, double* %scevgep19, i64 %smax
  %scevgep23 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %17
  %scevgep26 = getelementptr double, double* %scevgep25, i64 %smax
  %scevgep28 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %17
  %scevgep31 = getelementptr double, double* %scevgep30, i64 %smax
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %17
  %scevgep36 = getelementptr double, double* %scevgep35, i64 %smax
  %scevgep38 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %17
  %scevgep41 = getelementptr double, double* %scevgep40, i64 %smax
  %bound0 = icmp ult double* %scevgep, %scevgep26
  %bound1 = icmp ult double* %scevgep23, %scevgep21
  %found.conflict = and i1 %bound0, %bound1
  %bound043 = icmp ult double* %scevgep, %scevgep31
  %bound144 = icmp ult double* %scevgep28, %scevgep21
  %found.conflict45 = and i1 %bound043, %bound144
  %conflict.rdx = or i1 %found.conflict, %found.conflict45
  %bound046 = icmp ult double* %scevgep, %scevgep36
  %bound147 = icmp ult double* %scevgep33, %scevgep21
  %found.conflict48 = and i1 %bound046, %bound147
  %conflict.rdx49 = or i1 %found.conflict48, %conflict.rdx
  %bound050 = icmp ult double* %scevgep, %scevgep41
  %bound151 = icmp ult double* %scevgep38, %scevgep21
  %found.conflict52 = and i1 %bound050, %bound151
  %conflict.rdx53 = or i1 %found.conflict52, %conflict.rdx49
  %bound054 = icmp ult double* %scevgep23, %scevgep31
  %bound155 = icmp ult double* %scevgep28, %scevgep26
  %found.conflict56 = and i1 %bound054, %bound155
  %conflict.rdx57 = or i1 %found.conflict56, %conflict.rdx53
  %bound058 = icmp ult double* %scevgep23, %scevgep36
  %bound159 = icmp ult double* %scevgep33, %scevgep26
  %found.conflict60 = and i1 %bound058, %bound159
  %conflict.rdx61 = or i1 %found.conflict60, %conflict.rdx57
  %bound062 = icmp ult double* %scevgep23, %scevgep41
  %bound163 = icmp ult double* %scevgep38, %scevgep26
  %found.conflict64 = and i1 %bound062, %bound163
  %conflict.rdx65 = or i1 %found.conflict64, %conflict.rdx61
  br i1 %conflict.rdx65, label %omp.inner.for.body.i.us.preheader, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 12
  %n.vec = sub nsw i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %17
  %22 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !368, !noalias !365
  %24 = getelementptr inbounds double, double* %22, i64 2
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !368, !noalias !365
  %26 = getelementptr inbounds double, double* %22, i64 4
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !368, !noalias !365
  %28 = getelementptr inbounds double, double* %22, i64 6
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !368, !noalias !365
  %30 = getelementptr inbounds double, double* %22, i64 8
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !368, !noalias !365
  %32 = getelementptr inbounds double, double* %22, i64 10
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !368, !noalias !365
  %34 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !371, !noalias !365
  %36 = getelementptr inbounds double, double* %34, i64 2
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !371, !noalias !365
  %38 = getelementptr inbounds double, double* %34, i64 4
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !371, !noalias !365
  %40 = getelementptr inbounds double, double* %34, i64 6
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !371, !noalias !365
  %42 = getelementptr inbounds double, double* %34, i64 8
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !371, !noalias !365
  %44 = getelementptr inbounds double, double* %34, i64 10
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !371, !noalias !365
  %46 = fadd <2 x double> %wide.load, %wide.load76
  %47 = fadd <2 x double> %wide.load71, %wide.load77
  %48 = fadd <2 x double> %wide.load72, %wide.load78
  %49 = fadd <2 x double> %wide.load73, %wide.load79
  %50 = fadd <2 x double> %wide.load74, %wide.load80
  %51 = fadd <2 x double> %wide.load75, %wide.load81
  %52 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !373, !noalias !365
  %54 = getelementptr inbounds double, double* %52, i64 2
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !373, !noalias !365
  %56 = getelementptr inbounds double, double* %52, i64 4
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !373, !noalias !365
  %58 = getelementptr inbounds double, double* %52, i64 6
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !373, !noalias !365
  %60 = getelementptr inbounds double, double* %52, i64 8
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !373, !noalias !365
  %62 = getelementptr inbounds double, double* %52, i64 10
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !373, !noalias !365
  %64 = fadd <2 x double> %wide.load76, %wide.load82
  %65 = fadd <2 x double> %wide.load77, %wide.load83
  %66 = fadd <2 x double> %wide.load78, %wide.load84
  %67 = fadd <2 x double> %wide.load79, %wide.load85
  %68 = fadd <2 x double> %wide.load80, %wide.load86
  %69 = fadd <2 x double> %wide.load81, %wide.load87
  %70 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  %72 = getelementptr inbounds double, double* %70, i64 2
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  %74 = getelementptr inbounds double, double* %70, i64 4
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  %76 = getelementptr inbounds double, double* %70, i64 6
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  %78 = getelementptr inbounds double, double* %70, i64 8
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  %80 = getelementptr inbounds double, double* %70, i64 10
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  %82 = fadd <2 x double> %46, %wide.load88
  %83 = fadd <2 x double> %47, %wide.load89
  %84 = fadd <2 x double> %48, %wide.load90
  %85 = fadd <2 x double> %49, %wide.load91
  %86 = fadd <2 x double> %50, %wide.load92
  %87 = fadd <2 x double> %51, %wide.load93
  store <2 x double> %82, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  store <2 x double> %83, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  store <2 x double> %84, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  store <2 x double> %85, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  store <2 x double> %86, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  store <2 x double> %87, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !375, !noalias !377
  %88 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  %90 = getelementptr inbounds double, double* %88, i64 2
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  %92 = getelementptr inbounds double, double* %88, i64 4
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  %94 = getelementptr inbounds double, double* %88, i64 6
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  %96 = getelementptr inbounds double, double* %88, i64 8
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  %98 = getelementptr inbounds double, double* %88, i64 10
  %99 = bitcast double* %98 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  %100 = fadd <2 x double> %64, %wide.load94
  %101 = fadd <2 x double> %65, %wide.load95
  %102 = fadd <2 x double> %66, %wide.load96
  %103 = fadd <2 x double> %67, %wide.load97
  %104 = fadd <2 x double> %68, %wide.load98
  %105 = fadd <2 x double> %69, %wide.load99
  store <2 x double> %100, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  store <2 x double> %101, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  store <2 x double> %102, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  store <2 x double> %103, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  store <2 x double> %104, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  store <2 x double> %105, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !379, !noalias !380
  %index.next = add i64 %index, 12
  %106 = icmp eq i64 %index.next, %n.vec
  br i1 %106, label %middle.block, label %vector.body, !llvm.loop !381

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %17
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %.omp_outlined..83.exit.us, label %omp.inner.for.body.i.us.preheader

omp.inner.for.body.i.us.preheader:                ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us.ph = phi i64 [ %ind.end, %middle.block ], [ %17, %omp.inner.for.body.preheader.i.us ], [ %17, %vector.memcheck ]
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.body.i.us.preheader, %omp.inner.for.body.i.us
  %indvars.iv.i.us = phi i64 [ %indvars.iv.next.i.us, %omp.inner.for.body.i.us ], [ %indvars.iv.i.us.ph, %omp.inner.for.body.i.us.preheader ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %107 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !365
  %arrayidx6.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %108 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !365
  %add7.i.us = fadd double %107, %108
  %arrayidx11.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %109 = load double, double* %arrayidx11.i.us, align 8, !tbaa !62, !noalias !365
  %add12.i.us = fadd double %108, %109
  %arrayidx14.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %110 = load double, double* %arrayidx14.i.us, align 8, !tbaa !62, !noalias !365
  %add15.i.us = fadd double %add7.i.us, %110
  store double %add15.i.us, double* %arrayidx14.i.us, align 8, !tbaa !62, !noalias !365
  %arrayidx17.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %111 = load double, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !365
  %add18.i.us = fadd double %add12.i.us, %111
  store double %add18.i.us, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !365
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp3.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp3.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..83.exit.us, !llvm.loop !382

.omp_outlined..83.exit.us:                        ; preds = %omp.inner.for.body.i.us, %middle.block, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !365
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !365
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !365
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !365
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !365
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %112 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %112, %.omp.iv.014.us
  %113 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %113
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %114 = load i32, i32* %.omp.comb.lb, align 4
  %115 = load i32, i32* %.omp.comb.ub, align 4
  %116 = zext i32 %115 to i64
  %117 = zext i32 %114 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..83 to void (i32*, i32*, ...)*), i64 %117, i64 %116, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B) #6
  %118 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %118, %.omp.iv.014
  %119 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %119
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..83.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..83(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture readonly dereferenceable(24576) %E, [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture dereferenceable(24576) %B) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp331 = icmp sgt i32 %7, %cond
  br i1 %cmp331, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  %10 = icmp sgt i64 %9, %8
  %smax = select i1 %10, i64 %9, i64 %8
  %11 = add nsw i64 %smax, 1
  %12 = sub nsw i64 %11, %8
  %min.iters.check = icmp ult i64 %12, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %8
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %11
  %scevgep37 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %8
  %scevgep39 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %11
  %scevgep41 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %8
  %scevgep43 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %11
  %scevgep45 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %8
  %scevgep47 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %11
  %scevgep49 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %8
  %scevgep51 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %11
  %bound0 = icmp ult double* %scevgep, %scevgep39
  %bound1 = icmp ult double* %scevgep37, %scevgep35
  %found.conflict = and i1 %bound0, %bound1
  %bound053 = icmp ult double* %scevgep, %scevgep43
  %bound154 = icmp ult double* %scevgep41, %scevgep35
  %found.conflict55 = and i1 %bound053, %bound154
  %conflict.rdx = or i1 %found.conflict, %found.conflict55
  %bound056 = icmp ult double* %scevgep, %scevgep47
  %bound157 = icmp ult double* %scevgep45, %scevgep35
  %found.conflict58 = and i1 %bound056, %bound157
  %conflict.rdx59 = or i1 %found.conflict58, %conflict.rdx
  %bound060 = icmp ult double* %scevgep, %scevgep51
  %bound161 = icmp ult double* %scevgep49, %scevgep35
  %found.conflict62 = and i1 %bound060, %bound161
  %conflict.rdx63 = or i1 %found.conflict62, %conflict.rdx59
  %bound064 = icmp ult double* %scevgep37, %scevgep43
  %bound165 = icmp ult double* %scevgep41, %scevgep39
  %found.conflict66 = and i1 %bound064, %bound165
  %conflict.rdx67 = or i1 %found.conflict66, %conflict.rdx63
  %bound068 = icmp ult double* %scevgep37, %scevgep47
  %bound169 = icmp ult double* %scevgep45, %scevgep39
  %found.conflict70 = and i1 %bound068, %bound169
  %conflict.rdx71 = or i1 %found.conflict70, %conflict.rdx67
  %bound072 = icmp ult double* %scevgep37, %scevgep51
  %bound173 = icmp ult double* %scevgep49, %scevgep39
  %found.conflict74 = and i1 %bound072, %bound173
  %conflict.rdx75 = or i1 %found.conflict74, %conflict.rdx71
  br i1 %conflict.rdx75, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %12, 12
  %n.vec = sub nsw i64 %12, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %8
  %13 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %14 = bitcast double* %13 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %14, align 8, !tbaa !62, !alias.scope !383
  %15 = getelementptr inbounds double, double* %13, i64 2
  %16 = bitcast double* %15 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %16, align 8, !tbaa !62, !alias.scope !383
  %17 = getelementptr inbounds double, double* %13, i64 4
  %18 = bitcast double* %17 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %18, align 8, !tbaa !62, !alias.scope !383
  %19 = getelementptr inbounds double, double* %13, i64 6
  %20 = bitcast double* %19 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %20, align 8, !tbaa !62, !alias.scope !383
  %21 = getelementptr inbounds double, double* %13, i64 8
  %22 = bitcast double* %21 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %22, align 8, !tbaa !62, !alias.scope !383
  %23 = getelementptr inbounds double, double* %13, i64 10
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !383
  %25 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !386
  %27 = getelementptr inbounds double, double* %25, i64 2
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !386
  %29 = getelementptr inbounds double, double* %25, i64 4
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !386
  %31 = getelementptr inbounds double, double* %25, i64 6
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !386
  %33 = getelementptr inbounds double, double* %25, i64 8
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !386
  %35 = getelementptr inbounds double, double* %25, i64 10
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !386
  %37 = fadd <2 x double> %wide.load, %wide.load86
  %38 = fadd <2 x double> %wide.load81, %wide.load87
  %39 = fadd <2 x double> %wide.load82, %wide.load88
  %40 = fadd <2 x double> %wide.load83, %wide.load89
  %41 = fadd <2 x double> %wide.load84, %wide.load90
  %42 = fadd <2 x double> %wide.load85, %wide.load91
  %43 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !388
  %45 = getelementptr inbounds double, double* %43, i64 2
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !388
  %47 = getelementptr inbounds double, double* %43, i64 4
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !388
  %49 = getelementptr inbounds double, double* %43, i64 6
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !388
  %51 = getelementptr inbounds double, double* %43, i64 8
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !388
  %53 = getelementptr inbounds double, double* %43, i64 10
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !388
  %55 = fadd <2 x double> %wide.load86, %wide.load92
  %56 = fadd <2 x double> %wide.load87, %wide.load93
  %57 = fadd <2 x double> %wide.load88, %wide.load94
  %58 = fadd <2 x double> %wide.load89, %wide.load95
  %59 = fadd <2 x double> %wide.load90, %wide.load96
  %60 = fadd <2 x double> %wide.load91, %wide.load97
  %61 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  %63 = getelementptr inbounds double, double* %61, i64 2
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  %65 = getelementptr inbounds double, double* %61, i64 4
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  %67 = getelementptr inbounds double, double* %61, i64 6
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  %69 = getelementptr inbounds double, double* %61, i64 8
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  %71 = getelementptr inbounds double, double* %61, i64 10
  %72 = bitcast double* %71 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  %73 = fadd <2 x double> %37, %wide.load98
  %74 = fadd <2 x double> %38, %wide.load99
  %75 = fadd <2 x double> %39, %wide.load100
  %76 = fadd <2 x double> %40, %wide.load101
  %77 = fadd <2 x double> %41, %wide.load102
  %78 = fadd <2 x double> %42, %wide.load103
  store <2 x double> %73, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  store <2 x double> %74, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  store <2 x double> %75, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  store <2 x double> %76, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  store <2 x double> %77, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  store <2 x double> %78, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !390, !noalias !392
  %79 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %80 = bitcast double* %79 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  %81 = getelementptr inbounds double, double* %79, i64 2
  %82 = bitcast double* %81 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  %83 = getelementptr inbounds double, double* %79, i64 4
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  %85 = getelementptr inbounds double, double* %79, i64 6
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  %87 = getelementptr inbounds double, double* %79, i64 8
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  %89 = getelementptr inbounds double, double* %79, i64 10
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  %91 = fadd <2 x double> %55, %wide.load104
  %92 = fadd <2 x double> %56, %wide.load105
  %93 = fadd <2 x double> %57, %wide.load106
  %94 = fadd <2 x double> %58, %wide.load107
  %95 = fadd <2 x double> %59, %wide.load108
  %96 = fadd <2 x double> %60, %wide.load109
  store <2 x double> %91, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  store <2 x double> %92, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  store <2 x double> %93, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  store <2 x double> %94, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  store <2 x double> %95, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  store <2 x double> %96, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !394, !noalias !395
  %index.next = add i64 %index, 12
  %97 = icmp eq i64 %index.next, %n.vec
  br i1 %97, label %middle.block, label %vector.body, !llvm.loop !396

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %8
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %8, %omp.inner.for.body.preheader ], [ %8, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %98 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx6, align 8, !tbaa !62
  %add7 = fadd double %98, %99
  %arrayidx11 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx11, align 8, !tbaa !62
  %add12 = fadd double %99, %100
  %arrayidx14 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %add7, %101
  store double %add15, double* %arrayidx14, align 8, !tbaa !62
  %arrayidx17 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %add12, %102
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp3 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp3, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !397

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..86(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp513 = icmp sgt i32 %7, %cond
  br i1 %cmp513, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.014.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..87(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.014.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..87 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.014
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..87(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture readonly dereferenceable(24576) %E, [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture dereferenceable(24576) %B) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 38, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool31 = icmp eq i32 %5, 0
  br i1 %tobool31, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !398
  %cmp29 = icmp sgt i32 %7, %8
  br i1 %cmp29, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load42
  %39 = fadd <2 x double> %wide.load37, %wide.load43
  %40 = fadd <2 x double> %wide.load38, %wide.load44
  %41 = fadd <2 x double> %wide.load39, %wide.load45
  %42 = fadd <2 x double> %wide.load40, %wide.load46
  %43 = fadd <2 x double> %wide.load41, %wide.load47
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %wide.load42, %wide.load48
  %57 = fadd <2 x double> %wide.load43, %wide.load49
  %58 = fadd <2 x double> %wide.load44, %wide.load50
  %59 = fadd <2 x double> %wide.load45, %wide.load51
  %60 = fadd <2 x double> %wide.load46, %wide.load52
  %61 = fadd <2 x double> %wide.load47, %wide.load53
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %38, %wide.load54
  %75 = fadd <2 x double> %39, %wide.load55
  %76 = fadd <2 x double> %40, %wide.load56
  %77 = fadd <2 x double> %41, %wide.load57
  %78 = fadd <2 x double> %42, %wide.load58
  %79 = fadd <2 x double> %43, %wide.load59
  store <2 x double> %74, <2 x double>* %63, align 8, !tbaa !62
  store <2 x double> %75, <2 x double>* %65, align 8, !tbaa !62
  store <2 x double> %76, <2 x double>* %67, align 8, !tbaa !62
  store <2 x double> %77, <2 x double>* %69, align 8, !tbaa !62
  store <2 x double> %78, <2 x double>* %71, align 8, !tbaa !62
  store <2 x double> %79, <2 x double>* %73, align 8, !tbaa !62
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %56, %wide.load60
  %93 = fadd <2 x double> %57, %wide.load61
  %94 = fadd <2 x double> %58, %wide.load62
  %95 = fadd <2 x double> %59, %wide.load63
  %96 = fadd <2 x double> %60, %wide.load64
  %97 = fadd <2 x double> %61, %wide.load65
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !399

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !398
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !398
  %add5 = fadd double %99, %100
  %arrayidx9 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx9, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !398
  %add10 = fadd double %100, %101
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !398
  %add13 = fadd double %add5, %102
  store double %add13, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !398
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !398
  %add16 = fadd double %add10, %103
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !398
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !400

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..90(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp513 = icmp sgt i32 %7, %cond
  br i1 %cmp513, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.014.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..91(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.014.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..91 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.014
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..91(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture readonly dereferenceable(24576) %E, [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture dereferenceable(24576) %B) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 35, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool31 = icmp eq i32 %5, 0
  br i1 %tobool31, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !401
  %cmp29 = icmp sgt i32 %7, %8
  br i1 %cmp29, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load42
  %39 = fadd <2 x double> %wide.load37, %wide.load43
  %40 = fadd <2 x double> %wide.load38, %wide.load44
  %41 = fadd <2 x double> %wide.load39, %wide.load45
  %42 = fadd <2 x double> %wide.load40, %wide.load46
  %43 = fadd <2 x double> %wide.load41, %wide.load47
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %wide.load42, %wide.load48
  %57 = fadd <2 x double> %wide.load43, %wide.load49
  %58 = fadd <2 x double> %wide.load44, %wide.load50
  %59 = fadd <2 x double> %wide.load45, %wide.load51
  %60 = fadd <2 x double> %wide.load46, %wide.load52
  %61 = fadd <2 x double> %wide.load47, %wide.load53
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %38, %wide.load54
  %75 = fadd <2 x double> %39, %wide.load55
  %76 = fadd <2 x double> %40, %wide.load56
  %77 = fadd <2 x double> %41, %wide.load57
  %78 = fadd <2 x double> %42, %wide.load58
  %79 = fadd <2 x double> %43, %wide.load59
  store <2 x double> %74, <2 x double>* %63, align 8, !tbaa !62
  store <2 x double> %75, <2 x double>* %65, align 8, !tbaa !62
  store <2 x double> %76, <2 x double>* %67, align 8, !tbaa !62
  store <2 x double> %77, <2 x double>* %69, align 8, !tbaa !62
  store <2 x double> %78, <2 x double>* %71, align 8, !tbaa !62
  store <2 x double> %79, <2 x double>* %73, align 8, !tbaa !62
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %56, %wide.load60
  %93 = fadd <2 x double> %57, %wide.load61
  %94 = fadd <2 x double> %58, %wide.load62
  %95 = fadd <2 x double> %59, %wide.load63
  %96 = fadd <2 x double> %60, %wide.load64
  %97 = fadd <2 x double> %61, %wide.load65
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !402

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !401
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !401
  %add5 = fadd double %99, %100
  %arrayidx9 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx9, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !401
  %add10 = fadd double %100, %101
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !401
  %add13 = fadd double %add5, %102
  store double %add13, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !401
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !401
  %add16 = fadd double %add10, %103
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !401
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !403

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..94(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp513 = icmp sgt i32 %7, %cond
  br i1 %cmp513, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.014.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..95(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.014.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..95 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.014
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..95(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture readonly dereferenceable(24576) %E, [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture dereferenceable(24576) %B) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 36, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool31 = icmp eq i32 %5, 0
  br i1 %tobool31, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !404
  %cmp29 = icmp sgt i32 %7, %8
  br i1 %cmp29, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load42
  %39 = fadd <2 x double> %wide.load37, %wide.load43
  %40 = fadd <2 x double> %wide.load38, %wide.load44
  %41 = fadd <2 x double> %wide.load39, %wide.load45
  %42 = fadd <2 x double> %wide.load40, %wide.load46
  %43 = fadd <2 x double> %wide.load41, %wide.load47
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %wide.load42, %wide.load48
  %57 = fadd <2 x double> %wide.load43, %wide.load49
  %58 = fadd <2 x double> %wide.load44, %wide.load50
  %59 = fadd <2 x double> %wide.load45, %wide.load51
  %60 = fadd <2 x double> %wide.load46, %wide.load52
  %61 = fadd <2 x double> %wide.load47, %wide.load53
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %38, %wide.load54
  %75 = fadd <2 x double> %39, %wide.load55
  %76 = fadd <2 x double> %40, %wide.load56
  %77 = fadd <2 x double> %41, %wide.load57
  %78 = fadd <2 x double> %42, %wide.load58
  %79 = fadd <2 x double> %43, %wide.load59
  store <2 x double> %74, <2 x double>* %63, align 8, !tbaa !62
  store <2 x double> %75, <2 x double>* %65, align 8, !tbaa !62
  store <2 x double> %76, <2 x double>* %67, align 8, !tbaa !62
  store <2 x double> %77, <2 x double>* %69, align 8, !tbaa !62
  store <2 x double> %78, <2 x double>* %71, align 8, !tbaa !62
  store <2 x double> %79, <2 x double>* %73, align 8, !tbaa !62
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %56, %wide.load60
  %93 = fadd <2 x double> %57, %wide.load61
  %94 = fadd <2 x double> %58, %wide.load62
  %95 = fadd <2 x double> %59, %wide.load63
  %96 = fadd <2 x double> %60, %wide.load64
  %97 = fadd <2 x double> %61, %wide.load65
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !405

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !404
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !404
  %add5 = fadd double %99, %100
  %arrayidx9 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx9, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !404
  %add10 = fadd double %100, %101
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !404
  %add13 = fadd double %add5, %102
  store double %add13, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !404
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !404
  %add16 = fadd double %add10, %103
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !404
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !406

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..98(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp513 = icmp sgt i32 %7, %cond
  br i1 %cmp513, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.014.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..99(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.014.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %13
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..99 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.014
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %19
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..99(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture readonly dereferenceable(24576) %E, [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture dereferenceable(24576) %B) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 37, i32 %conv, i32 %conv1, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool31 = icmp eq i32 %5, 0
  br i1 %tobool31, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !407
  %cmp29 = icmp sgt i32 %7, %8
  br i1 %cmp29, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load37 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load38 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load39 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load40 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load41 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load42 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load42
  %39 = fadd <2 x double> %wide.load37, %wide.load43
  %40 = fadd <2 x double> %wide.load38, %wide.load44
  %41 = fadd <2 x double> %wide.load39, %wide.load45
  %42 = fadd <2 x double> %wide.load40, %wide.load46
  %43 = fadd <2 x double> %wide.load41, %wide.load47
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %wide.load42, %wide.load48
  %57 = fadd <2 x double> %wide.load43, %wide.load49
  %58 = fadd <2 x double> %wide.load44, %wide.load50
  %59 = fadd <2 x double> %wide.load45, %wide.load51
  %60 = fadd <2 x double> %wide.load46, %wide.load52
  %61 = fadd <2 x double> %wide.load47, %wide.load53
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %38, %wide.load54
  %75 = fadd <2 x double> %39, %wide.load55
  %76 = fadd <2 x double> %40, %wide.load56
  %77 = fadd <2 x double> %41, %wide.load57
  %78 = fadd <2 x double> %42, %wide.load58
  %79 = fadd <2 x double> %43, %wide.load59
  store <2 x double> %74, <2 x double>* %63, align 8, !tbaa !62
  store <2 x double> %75, <2 x double>* %65, align 8, !tbaa !62
  store <2 x double> %76, <2 x double>* %67, align 8, !tbaa !62
  store <2 x double> %77, <2 x double>* %69, align 8, !tbaa !62
  store <2 x double> %78, <2 x double>* %71, align 8, !tbaa !62
  store <2 x double> %79, <2 x double>* %73, align 8, !tbaa !62
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %56, %wide.load60
  %93 = fadd <2 x double> %57, %wide.load61
  %94 = fadd <2 x double> %58, %wide.load62
  %95 = fadd <2 x double> %59, %wide.load63
  %96 = fadd <2 x double> %60, %wide.load64
  %97 = fadd <2 x double> %61, %wide.load65
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !408

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !407
  %arrayidx4 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx4, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !407
  %add5 = fadd double %99, %100
  %arrayidx9 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx9, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !407
  %add10 = fadd double %100, %101
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !407
  %add13 = fadd double %add5, %102
  store double %add13, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !407
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !407
  %add16 = fadd double %add10, %103
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !407
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !409

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..102(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %E, [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %B, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp513 = icmp sgt i32 %7, %cond
  br i1 %cmp513, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us.preheader, label %omp.inner.for.body

omp.inner.for.body.us.preheader:                  ; preds = %omp.inner.for.body.lr.ph
  %scevgep19 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 1
  %scevgep25 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 1
  %scevgep30 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 1
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 1
  %scevgep40 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 1
  br label %omp.inner.for.body.us

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.us.preheader, %.omp_outlined..103.exit.us
  %.omp.iv.014.us = phi i32 [ %add.us, %.omp_outlined..103.exit.us ], [ %7, %omp.inner.for.body.us.preheader ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !410
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !410
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !410
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !410
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !410
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !410
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !410
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !410
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !410
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !410
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !410
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !410
  %cmp331.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp331.i.us, label %.omp_outlined..103.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  %19 = icmp sgt i64 %18, %17
  %smax = select i1 %19, i64 %18, i64 %17
  %20 = sub nsw i64 1, %17
  %21 = add nsw i64 %20, %smax
  %min.iters.check = icmp ult i64 %21, 12
  br i1 %min.iters.check, label %omp.inner.for.body.i.us.preheader, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader.i.us
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %17
  %scevgep21 = getelementptr double, double* %scevgep19, i64 %smax
  %scevgep23 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %17
  %scevgep26 = getelementptr double, double* %scevgep25, i64 %smax
  %scevgep28 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %17
  %scevgep31 = getelementptr double, double* %scevgep30, i64 %smax
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %17
  %scevgep36 = getelementptr double, double* %scevgep35, i64 %smax
  %scevgep38 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %17
  %scevgep41 = getelementptr double, double* %scevgep40, i64 %smax
  %bound0 = icmp ult double* %scevgep, %scevgep26
  %bound1 = icmp ult double* %scevgep23, %scevgep21
  %found.conflict = and i1 %bound0, %bound1
  %bound043 = icmp ult double* %scevgep, %scevgep31
  %bound144 = icmp ult double* %scevgep28, %scevgep21
  %found.conflict45 = and i1 %bound043, %bound144
  %conflict.rdx = or i1 %found.conflict, %found.conflict45
  %bound046 = icmp ult double* %scevgep, %scevgep36
  %bound147 = icmp ult double* %scevgep33, %scevgep21
  %found.conflict48 = and i1 %bound046, %bound147
  %conflict.rdx49 = or i1 %found.conflict48, %conflict.rdx
  %bound050 = icmp ult double* %scevgep, %scevgep41
  %bound151 = icmp ult double* %scevgep38, %scevgep21
  %found.conflict52 = and i1 %bound050, %bound151
  %conflict.rdx53 = or i1 %found.conflict52, %conflict.rdx49
  %bound054 = icmp ult double* %scevgep23, %scevgep31
  %bound155 = icmp ult double* %scevgep28, %scevgep26
  %found.conflict56 = and i1 %bound054, %bound155
  %conflict.rdx57 = or i1 %found.conflict56, %conflict.rdx53
  %bound058 = icmp ult double* %scevgep23, %scevgep36
  %bound159 = icmp ult double* %scevgep33, %scevgep26
  %found.conflict60 = and i1 %bound058, %bound159
  %conflict.rdx61 = or i1 %found.conflict60, %conflict.rdx57
  %bound062 = icmp ult double* %scevgep23, %scevgep41
  %bound163 = icmp ult double* %scevgep38, %scevgep26
  %found.conflict64 = and i1 %bound062, %bound163
  %conflict.rdx65 = or i1 %found.conflict64, %conflict.rdx61
  br i1 %conflict.rdx65, label %omp.inner.for.body.i.us.preheader, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 12
  %n.vec = sub nsw i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %17
  %22 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !413, !noalias !410
  %24 = getelementptr inbounds double, double* %22, i64 2
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !413, !noalias !410
  %26 = getelementptr inbounds double, double* %22, i64 4
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !413, !noalias !410
  %28 = getelementptr inbounds double, double* %22, i64 6
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !413, !noalias !410
  %30 = getelementptr inbounds double, double* %22, i64 8
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !413, !noalias !410
  %32 = getelementptr inbounds double, double* %22, i64 10
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !413, !noalias !410
  %34 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !416, !noalias !410
  %36 = getelementptr inbounds double, double* %34, i64 2
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !416, !noalias !410
  %38 = getelementptr inbounds double, double* %34, i64 4
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !416, !noalias !410
  %40 = getelementptr inbounds double, double* %34, i64 6
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !416, !noalias !410
  %42 = getelementptr inbounds double, double* %34, i64 8
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !416, !noalias !410
  %44 = getelementptr inbounds double, double* %34, i64 10
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !416, !noalias !410
  %46 = fadd <2 x double> %wide.load, %wide.load76
  %47 = fadd <2 x double> %wide.load71, %wide.load77
  %48 = fadd <2 x double> %wide.load72, %wide.load78
  %49 = fadd <2 x double> %wide.load73, %wide.load79
  %50 = fadd <2 x double> %wide.load74, %wide.load80
  %51 = fadd <2 x double> %wide.load75, %wide.load81
  %52 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !418, !noalias !410
  %54 = getelementptr inbounds double, double* %52, i64 2
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !418, !noalias !410
  %56 = getelementptr inbounds double, double* %52, i64 4
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !418, !noalias !410
  %58 = getelementptr inbounds double, double* %52, i64 6
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !418, !noalias !410
  %60 = getelementptr inbounds double, double* %52, i64 8
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !418, !noalias !410
  %62 = getelementptr inbounds double, double* %52, i64 10
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !418, !noalias !410
  %64 = fadd <2 x double> %wide.load76, %wide.load82
  %65 = fadd <2 x double> %wide.load77, %wide.load83
  %66 = fadd <2 x double> %wide.load78, %wide.load84
  %67 = fadd <2 x double> %wide.load79, %wide.load85
  %68 = fadd <2 x double> %wide.load80, %wide.load86
  %69 = fadd <2 x double> %wide.load81, %wide.load87
  %70 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  %72 = getelementptr inbounds double, double* %70, i64 2
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  %74 = getelementptr inbounds double, double* %70, i64 4
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  %76 = getelementptr inbounds double, double* %70, i64 6
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  %78 = getelementptr inbounds double, double* %70, i64 8
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  %80 = getelementptr inbounds double, double* %70, i64 10
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  %82 = fadd <2 x double> %46, %wide.load88
  %83 = fadd <2 x double> %47, %wide.load89
  %84 = fadd <2 x double> %48, %wide.load90
  %85 = fadd <2 x double> %49, %wide.load91
  %86 = fadd <2 x double> %50, %wide.load92
  %87 = fadd <2 x double> %51, %wide.load93
  store <2 x double> %82, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  store <2 x double> %83, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  store <2 x double> %84, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  store <2 x double> %85, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  store <2 x double> %86, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  store <2 x double> %87, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !420, !noalias !422
  %88 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  %90 = getelementptr inbounds double, double* %88, i64 2
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  %92 = getelementptr inbounds double, double* %88, i64 4
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  %94 = getelementptr inbounds double, double* %88, i64 6
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  %96 = getelementptr inbounds double, double* %88, i64 8
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  %98 = getelementptr inbounds double, double* %88, i64 10
  %99 = bitcast double* %98 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  %100 = fadd <2 x double> %64, %wide.load94
  %101 = fadd <2 x double> %65, %wide.load95
  %102 = fadd <2 x double> %66, %wide.load96
  %103 = fadd <2 x double> %67, %wide.load97
  %104 = fadd <2 x double> %68, %wide.load98
  %105 = fadd <2 x double> %69, %wide.load99
  store <2 x double> %100, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  store <2 x double> %101, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  store <2 x double> %102, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  store <2 x double> %103, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  store <2 x double> %104, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  store <2 x double> %105, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !424, !noalias !425
  %index.next = add i64 %index, 12
  %106 = icmp eq i64 %index.next, %n.vec
  br i1 %106, label %middle.block, label %vector.body, !llvm.loop !426

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %17
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %.omp_outlined..103.exit.us, label %omp.inner.for.body.i.us.preheader

omp.inner.for.body.i.us.preheader:                ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us.ph = phi i64 [ %ind.end, %middle.block ], [ %17, %omp.inner.for.body.preheader.i.us ], [ %17, %vector.memcheck ]
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.body.i.us.preheader, %omp.inner.for.body.i.us
  %indvars.iv.i.us = phi i64 [ %indvars.iv.next.i.us, %omp.inner.for.body.i.us ], [ %indvars.iv.i.us.ph, %omp.inner.for.body.i.us.preheader ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %107 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !410
  %arrayidx6.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %108 = load double, double* %arrayidx6.i.us, align 8, !tbaa !62, !noalias !410
  %add7.i.us = fadd double %107, %108
  %arrayidx11.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %109 = load double, double* %arrayidx11.i.us, align 8, !tbaa !62, !noalias !410
  %add12.i.us = fadd double %108, %109
  %arrayidx14.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %110 = load double, double* %arrayidx14.i.us, align 8, !tbaa !62, !noalias !410
  %add15.i.us = fadd double %add7.i.us, %110
  store double %add15.i.us, double* %arrayidx14.i.us, align 8, !tbaa !62, !noalias !410
  %arrayidx17.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %111 = load double, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !410
  %add18.i.us = fadd double %add12.i.us, %111
  store double %add18.i.us, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !410
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp3.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp3.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..103.exit.us, !llvm.loop !427

.omp_outlined..103.exit.us:                       ; preds = %omp.inner.for.body.i.us, %middle.block, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !410
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !410
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !410
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !410
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !410
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %112 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %112, %.omp.iv.014.us
  %113 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5.us = icmp sgt i32 %add.us, %113
  br i1 %cmp5.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %114 = load i32, i32* %.omp.comb.lb, align 4
  %115 = load i32, i32* %.omp.comb.ub, align 4
  %116 = zext i32 %115 to i64
  %117 = zext i32 %114 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..103 to void (i32*, i32*, ...)*), i64 %117, i64 %116, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %E, [3072 x double]* nonnull %A, [3072 x double]* nonnull %B) #6
  %118 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %118, %.omp.iv.014
  %119 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp5 = icmp sgt i32 %add, %119
  br i1 %cmp5, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..103.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..103(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture readonly dereferenceable(24576) %E, [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture dereferenceable(24576) %B) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv1 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv1, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp331 = icmp sgt i32 %7, %cond
  br i1 %cmp331, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  %10 = icmp sgt i64 %9, %8
  %smax = select i1 %10, i64 %9, i64 %8
  %11 = add nsw i64 %smax, 1
  %12 = sub nsw i64 %11, %8
  %min.iters.check = icmp ult i64 %12, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %8
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %11
  %scevgep37 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %8
  %scevgep39 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %11
  %scevgep41 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %8
  %scevgep43 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %11
  %scevgep45 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %8
  %scevgep47 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %11
  %scevgep49 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %8
  %scevgep51 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %11
  %bound0 = icmp ult double* %scevgep, %scevgep39
  %bound1 = icmp ult double* %scevgep37, %scevgep35
  %found.conflict = and i1 %bound0, %bound1
  %bound053 = icmp ult double* %scevgep, %scevgep43
  %bound154 = icmp ult double* %scevgep41, %scevgep35
  %found.conflict55 = and i1 %bound053, %bound154
  %conflict.rdx = or i1 %found.conflict, %found.conflict55
  %bound056 = icmp ult double* %scevgep, %scevgep47
  %bound157 = icmp ult double* %scevgep45, %scevgep35
  %found.conflict58 = and i1 %bound056, %bound157
  %conflict.rdx59 = or i1 %found.conflict58, %conflict.rdx
  %bound060 = icmp ult double* %scevgep, %scevgep51
  %bound161 = icmp ult double* %scevgep49, %scevgep35
  %found.conflict62 = and i1 %bound060, %bound161
  %conflict.rdx63 = or i1 %found.conflict62, %conflict.rdx59
  %bound064 = icmp ult double* %scevgep37, %scevgep43
  %bound165 = icmp ult double* %scevgep41, %scevgep39
  %found.conflict66 = and i1 %bound064, %bound165
  %conflict.rdx67 = or i1 %found.conflict66, %conflict.rdx63
  %bound068 = icmp ult double* %scevgep37, %scevgep47
  %bound169 = icmp ult double* %scevgep45, %scevgep39
  %found.conflict70 = and i1 %bound068, %bound169
  %conflict.rdx71 = or i1 %found.conflict70, %conflict.rdx67
  %bound072 = icmp ult double* %scevgep37, %scevgep51
  %bound173 = icmp ult double* %scevgep49, %scevgep39
  %found.conflict74 = and i1 %bound072, %bound173
  %conflict.rdx75 = or i1 %found.conflict74, %conflict.rdx71
  br i1 %conflict.rdx75, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %12, 12
  %n.vec = sub nsw i64 %12, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %8
  %13 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %14 = bitcast double* %13 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %14, align 8, !tbaa !62, !alias.scope !428
  %15 = getelementptr inbounds double, double* %13, i64 2
  %16 = bitcast double* %15 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %16, align 8, !tbaa !62, !alias.scope !428
  %17 = getelementptr inbounds double, double* %13, i64 4
  %18 = bitcast double* %17 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %18, align 8, !tbaa !62, !alias.scope !428
  %19 = getelementptr inbounds double, double* %13, i64 6
  %20 = bitcast double* %19 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %20, align 8, !tbaa !62, !alias.scope !428
  %21 = getelementptr inbounds double, double* %13, i64 8
  %22 = bitcast double* %21 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %22, align 8, !tbaa !62, !alias.scope !428
  %23 = getelementptr inbounds double, double* %13, i64 10
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !428
  %25 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !431
  %27 = getelementptr inbounds double, double* %25, i64 2
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !431
  %29 = getelementptr inbounds double, double* %25, i64 4
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !431
  %31 = getelementptr inbounds double, double* %25, i64 6
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !431
  %33 = getelementptr inbounds double, double* %25, i64 8
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !431
  %35 = getelementptr inbounds double, double* %25, i64 10
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !431
  %37 = fadd <2 x double> %wide.load, %wide.load86
  %38 = fadd <2 x double> %wide.load81, %wide.load87
  %39 = fadd <2 x double> %wide.load82, %wide.load88
  %40 = fadd <2 x double> %wide.load83, %wide.load89
  %41 = fadd <2 x double> %wide.load84, %wide.load90
  %42 = fadd <2 x double> %wide.load85, %wide.load91
  %43 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !433
  %45 = getelementptr inbounds double, double* %43, i64 2
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !433
  %47 = getelementptr inbounds double, double* %43, i64 4
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !433
  %49 = getelementptr inbounds double, double* %43, i64 6
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !433
  %51 = getelementptr inbounds double, double* %43, i64 8
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !433
  %53 = getelementptr inbounds double, double* %43, i64 10
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !433
  %55 = fadd <2 x double> %wide.load86, %wide.load92
  %56 = fadd <2 x double> %wide.load87, %wide.load93
  %57 = fadd <2 x double> %wide.load88, %wide.load94
  %58 = fadd <2 x double> %wide.load89, %wide.load95
  %59 = fadd <2 x double> %wide.load90, %wide.load96
  %60 = fadd <2 x double> %wide.load91, %wide.load97
  %61 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  %63 = getelementptr inbounds double, double* %61, i64 2
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  %65 = getelementptr inbounds double, double* %61, i64 4
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  %67 = getelementptr inbounds double, double* %61, i64 6
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  %69 = getelementptr inbounds double, double* %61, i64 8
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  %71 = getelementptr inbounds double, double* %61, i64 10
  %72 = bitcast double* %71 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  %73 = fadd <2 x double> %37, %wide.load98
  %74 = fadd <2 x double> %38, %wide.load99
  %75 = fadd <2 x double> %39, %wide.load100
  %76 = fadd <2 x double> %40, %wide.load101
  %77 = fadd <2 x double> %41, %wide.load102
  %78 = fadd <2 x double> %42, %wide.load103
  store <2 x double> %73, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  store <2 x double> %74, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  store <2 x double> %75, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  store <2 x double> %76, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  store <2 x double> %77, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  store <2 x double> %78, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !435, !noalias !437
  %79 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %80 = bitcast double* %79 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  %81 = getelementptr inbounds double, double* %79, i64 2
  %82 = bitcast double* %81 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  %83 = getelementptr inbounds double, double* %79, i64 4
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  %85 = getelementptr inbounds double, double* %79, i64 6
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  %87 = getelementptr inbounds double, double* %79, i64 8
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  %89 = getelementptr inbounds double, double* %79, i64 10
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  %91 = fadd <2 x double> %55, %wide.load104
  %92 = fadd <2 x double> %56, %wide.load105
  %93 = fadd <2 x double> %57, %wide.load106
  %94 = fadd <2 x double> %58, %wide.load107
  %95 = fadd <2 x double> %59, %wide.load108
  %96 = fadd <2 x double> %60, %wide.load109
  store <2 x double> %91, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  store <2 x double> %92, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  store <2 x double> %93, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  store <2 x double> %94, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  store <2 x double> %95, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  store <2 x double> %96, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !439, !noalias !440
  %index.next = add i64 %index, 12
  %97 = icmp eq i64 %index.next, %n.vec
  br i1 %97, label %middle.block, label %vector.body, !llvm.loop !441

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %8
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %8, %omp.inner.for.body.preheader ], [ %8, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %98 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx6, align 8, !tbaa !62
  %add7 = fadd double %98, %99
  %arrayidx11 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx11, align 8, !tbaa !62
  %add12 = fadd double %99, %100
  %arrayidx14 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %add7, %101
  store double %add15, double* %arrayidx14, align 8, !tbaa !62
  %arrayidx17 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %add12, %102
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp3 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp3, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !442

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..106(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp715 = icmp sgt i32 %7, %cond
  br i1 %cmp715, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %.omp_outlined..107.exit.us
  %.omp.iv.016.us = phi i32 [ %add.us, %.omp_outlined..107.exit.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !443
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !443
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !443
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !443
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !443
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !443
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !443
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !443
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !443
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !443
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !443
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !443
  %cmp540.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp540.i.us, label %.omp_outlined..107.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.inc.i.us, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us = phi i64 [ %17, %omp.inner.for.body.preheader.i.us ], [ %indvars.iv.next.i.us, %omp.inner.for.inc.i.us ]
  %p.addr.043.i.us = phi i64 [ %p, %omp.inner.for.body.preheader.i.us ], [ %p.addr.1.i.us, %omp.inner.for.inc.i.us ]
  %q.addr.041.i.us = phi i64 [ %q, %omp.inner.for.body.preheader.i.us ], [ %q.addr.1.i.us, %omp.inner.for.inc.i.us ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %19 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !443
  %arrayidx8.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %20 = load double, double* %arrayidx8.i.us, align 8, !tbaa !62, !noalias !443
  %add9.i.us = fadd double %19, %20
  %21 = bitcast i64 %p.addr.043.i.us to double
  %add10.i.us = fadd double %add9.i.us, %21
  %arrayidx12.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %22 = load double, double* %arrayidx12.i.us, align 8, !tbaa !62, !noalias !443
  %add13.i.us = fadd double %22, %add10.i.us
  store double %add13.i.us, double* %arrayidx12.i.us, align 8, !tbaa !62, !noalias !443
  %23 = load double, double* %arrayidx8.i.us, align 8, !tbaa !62, !noalias !443
  %arrayidx17.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %24 = load double, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !443
  %add18.i.us = fadd double %23, %24
  %25 = bitcast i64 %q.addr.041.i.us to double
  %add19.i.us = fadd double %add18.i.us, %25
  %arrayidx21.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %26 = load double, double* %arrayidx21.i.us, align 8, !tbaa !62, !noalias !443
  %add22.i.us = fadd double %26, %add19.i.us
  store double %add22.i.us, double* %arrayidx21.i.us, align 8, !tbaa !62, !noalias !443
  %27 = icmp eq i64 %indvars.iv.i.us, 3071
  br i1 %27, label %if.then.i.us, label %omp.inner.for.inc.i.us

if.then.i.us:                                     ; preds = %omp.inner.for.body.i.us
  %add25.i.us = fadd double %21, 6.000000e+00
  %28 = bitcast double %add25.i.us to i64
  %add26.i.us = fadd double %25, 9.000000e+00
  %29 = bitcast double %add26.i.us to i64
  br label %omp.inner.for.inc.i.us

omp.inner.for.inc.i.us:                           ; preds = %if.then.i.us, %omp.inner.for.body.i.us
  %q.addr.1.i.us = phi i64 [ %29, %if.then.i.us ], [ %q.addr.041.i.us, %omp.inner.for.body.i.us ]
  %p.addr.1.i.us = phi i64 [ %28, %if.then.i.us ], [ %p.addr.043.i.us, %omp.inner.for.body.i.us ]
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp5.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp5.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..107.exit.us

.omp_outlined..107.exit.us:                       ; preds = %omp.inner.for.inc.i.us, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !443
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !443
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !443
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !443
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !443
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %30 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %30, %.omp.iv.016.us
  %31 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7.us = icmp sgt i32 %add.us, %31
  br i1 %cmp7.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.016 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %32 = load i32, i32* %.omp.comb.lb, align 4
  %33 = load i32, i32* %.omp.comb.ub, align 4
  %34 = zext i32 %33 to i64
  %35 = zext i32 %32 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64)* @.omp_outlined..107 to void (i32*, i32*, ...)*), i64 %35, i64 %34, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 %p, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 %q) #6
  %36 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %36, %.omp.iv.016
  %37 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7 = icmp sgt i32 %add, %37
  br i1 %cmp7, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..107.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..107(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, i64 %p, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E, i64 %q) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv2 = trunc i64 %.previous.lb. to i32
  %conv3 = trunc i64 %.previous.ub. to i32
  store i32 %conv2, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv3, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp540 = icmp sgt i32 %7, %cond
  br i1 %cmp540, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.inc
  %indvars.iv = phi i64 [ %8, %omp.inner.for.body.preheader ], [ %indvars.iv.next, %omp.inner.for.inc ]
  %p.addr.043 = phi i64 [ %p, %omp.inner.for.body.preheader ], [ %p.addr.1, %omp.inner.for.inc ]
  %q.addr.041 = phi i64 [ %q, %omp.inner.for.body.preheader ], [ %q.addr.1, %omp.inner.for.inc ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %10 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx8 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %11 = load double, double* %arrayidx8, align 8, !tbaa !62
  %add9 = fadd double %10, %11
  %12 = bitcast i64 %p.addr.043 to double
  %add10 = fadd double %add9, %12
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %13 = load double, double* %arrayidx12, align 8, !tbaa !62
  %add13 = fadd double %13, %add10
  store double %add13, double* %arrayidx12, align 8, !tbaa !62
  %14 = load double, double* %arrayidx8, align 8, !tbaa !62
  %arrayidx17 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %15 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %14, %15
  %16 = bitcast i64 %q.addr.041 to double
  %add19 = fadd double %add18, %16
  %arrayidx21 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %17 = load double, double* %arrayidx21, align 8, !tbaa !62
  %add22 = fadd double %17, %add19
  store double %add22, double* %arrayidx21, align 8, !tbaa !62
  %18 = icmp eq i64 %indvars.iv, 3071
  br i1 %18, label %if.then, label %omp.inner.for.inc

if.then:                                          ; preds = %omp.inner.for.body
  %add25 = fadd double %12, 6.000000e+00
  %19 = bitcast double %add25 to i64
  %add26 = fadd double %16, 9.000000e+00
  %20 = bitcast double %add26 to i64
  br label %omp.inner.for.inc

omp.inner.for.inc:                                ; preds = %if.then, %omp.inner.for.body
  %q.addr.1 = phi i64 [ %20, %if.then ], [ %q.addr.041, %omp.inner.for.body ]
  %p.addr.1 = phi i64 [ %19, %if.then ], [ %p.addr.043, %omp.inner.for.body ]
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp5 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp5, label %omp.inner.for.body, label %omp.loop.exit

omp.loop.exit:                                    ; preds = %omp.inner.for.inc, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..110(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp715 = icmp sgt i32 %7, %cond
  br i1 %cmp715, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.016.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..111(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 %p, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 %q) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.016.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7.us = icmp sgt i32 %add.us, %13
  br i1 %cmp7.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.016 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64)* @.omp_outlined..111 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 %p, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 %q) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.016
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7 = icmp sgt i32 %add, %19
  br i1 %cmp7, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..111(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, i64 %p, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E, i64 %q) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv2 = trunc i64 %.previous.lb. to i32
  %conv3 = trunc i64 %.previous.ub. to i32
  store i32 %conv2, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv3, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 38, i32 %conv2, i32 %conv3, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool43 = icmp eq i32 %5, 0
  br i1 %tobool43, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.inc, %omp.dispatch.body
  %q.addr.1.lcssa = phi i64 [ %q.addr.044, %omp.dispatch.body ], [ %q.addr.2, %omp.inner.for.inc ]
  %p.addr.1.lcssa = phi i64 [ %p.addr.045, %omp.dispatch.body ], [ %p.addr.2, %omp.inner.for.inc ]
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %p.addr.045 = phi i64 [ %p.addr.1.lcssa, %omp.dispatch.cond.loopexit ], [ %p, %entry ]
  %q.addr.044 = phi i64 [ %q.addr.1.lcssa, %omp.dispatch.cond.loopexit ], [ %q, %entry ]
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !446
  %cmp38 = icmp sgt i32 %7, %8
  br i1 %cmp38, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.inc, %omp.inner.for.body.lr.ph
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.inc ], [ %9, %omp.inner.for.body.lr.ph ]
  %p.addr.141 = phi i64 [ %p.addr.2, %omp.inner.for.inc ], [ %p.addr.045, %omp.inner.for.body.lr.ph ]
  %q.addr.139 = phi i64 [ %q.addr.2, %omp.inner.for.inc ], [ %q.addr.044, %omp.inner.for.body.lr.ph ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %11 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !446
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %12 = load double, double* %arrayidx6, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !446
  %add7 = fadd double %11, %12
  %13 = bitcast i64 %p.addr.141 to double
  %add8 = fadd double %add7, %13
  %arrayidx10 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %14 = load double, double* %arrayidx10, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !446
  %add11 = fadd double %14, %add8
  store double %add11, double* %arrayidx10, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !446
  %15 = load double, double* %arrayidx6, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !446
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %16 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !446
  %add16 = fadd double %15, %16
  %17 = bitcast i64 %q.addr.139 to double
  %add17 = fadd double %add16, %17
  %arrayidx19 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %18 = load double, double* %arrayidx19, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !446
  %add20 = fadd double %18, %add17
  store double %add20, double* %arrayidx19, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !446
  %19 = icmp eq i64 %indvars.iv, 3071
  br i1 %19, label %if.then, label %omp.inner.for.inc

if.then:                                          ; preds = %omp.inner.for.body
  %add23 = fadd double %13, 6.000000e+00
  %20 = bitcast double %add23 to i64
  %add24 = fadd double %17, 9.000000e+00
  %21 = bitcast double %add24 to i64
  br label %omp.inner.for.inc

omp.inner.for.inc:                                ; preds = %if.then, %omp.inner.for.body
  %q.addr.2 = phi i64 [ %21, %if.then ], [ %q.addr.139, %omp.inner.for.body ]
  %p.addr.2 = phi i64 [ %20, %if.then ], [ %p.addr.141, %omp.inner.for.body ]
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !446

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..114(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp715 = icmp sgt i32 %7, %cond
  br i1 %cmp715, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.016.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..115(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 %p, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 %q) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.016.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7.us = icmp sgt i32 %add.us, %13
  br i1 %cmp7.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.016 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64)* @.omp_outlined..115 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 %p, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 %q) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.016
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7 = icmp sgt i32 %add, %19
  br i1 %cmp7, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..115(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, i64 %p, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E, i64 %q) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv2 = trunc i64 %.previous.lb. to i32
  %conv3 = trunc i64 %.previous.ub. to i32
  store i32 %conv2, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv3, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 35, i32 %conv2, i32 %conv3, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool43 = icmp eq i32 %5, 0
  br i1 %tobool43, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.inc, %omp.dispatch.body
  %q.addr.1.lcssa = phi i64 [ %q.addr.044, %omp.dispatch.body ], [ %q.addr.2, %omp.inner.for.inc ]
  %p.addr.1.lcssa = phi i64 [ %p.addr.045, %omp.dispatch.body ], [ %p.addr.2, %omp.inner.for.inc ]
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %p.addr.045 = phi i64 [ %p.addr.1.lcssa, %omp.dispatch.cond.loopexit ], [ %p, %entry ]
  %q.addr.044 = phi i64 [ %q.addr.1.lcssa, %omp.dispatch.cond.loopexit ], [ %q, %entry ]
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !447
  %cmp38 = icmp sgt i32 %7, %8
  br i1 %cmp38, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.inc, %omp.inner.for.body.lr.ph
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.inc ], [ %9, %omp.inner.for.body.lr.ph ]
  %p.addr.141 = phi i64 [ %p.addr.2, %omp.inner.for.inc ], [ %p.addr.045, %omp.inner.for.body.lr.ph ]
  %q.addr.139 = phi i64 [ %q.addr.2, %omp.inner.for.inc ], [ %q.addr.044, %omp.inner.for.body.lr.ph ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %11 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !447
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %12 = load double, double* %arrayidx6, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !447
  %add7 = fadd double %11, %12
  %13 = bitcast i64 %p.addr.141 to double
  %add8 = fadd double %add7, %13
  %arrayidx10 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %14 = load double, double* %arrayidx10, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !447
  %add11 = fadd double %14, %add8
  store double %add11, double* %arrayidx10, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !447
  %15 = load double, double* %arrayidx6, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !447
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %16 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !447
  %add16 = fadd double %15, %16
  %17 = bitcast i64 %q.addr.139 to double
  %add17 = fadd double %add16, %17
  %arrayidx19 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %18 = load double, double* %arrayidx19, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !447
  %add20 = fadd double %18, %add17
  store double %add20, double* %arrayidx19, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !447
  %19 = icmp eq i64 %indvars.iv, 3071
  br i1 %19, label %if.then, label %omp.inner.for.inc

if.then:                                          ; preds = %omp.inner.for.body
  %add23 = fadd double %13, 6.000000e+00
  %20 = bitcast double %add23 to i64
  %add24 = fadd double %17, 9.000000e+00
  %21 = bitcast double %add24 to i64
  br label %omp.inner.for.inc

omp.inner.for.inc:                                ; preds = %if.then, %omp.inner.for.body
  %q.addr.2 = phi i64 [ %21, %if.then ], [ %q.addr.139, %omp.inner.for.body ]
  %p.addr.2 = phi i64 [ %20, %if.then ], [ %p.addr.141, %omp.inner.for.body ]
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !447

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..118(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp715 = icmp sgt i32 %7, %cond
  br i1 %cmp715, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.016.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..119(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 %p, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 %q) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.016.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7.us = icmp sgt i32 %add.us, %13
  br i1 %cmp7.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.016 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64)* @.omp_outlined..119 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 %p, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 %q) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.016
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7 = icmp sgt i32 %add, %19
  br i1 %cmp7, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..119(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, i64 %p, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E, i64 %q) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv2 = trunc i64 %.previous.lb. to i32
  %conv3 = trunc i64 %.previous.ub. to i32
  store i32 %conv2, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv3, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 36, i32 %conv2, i32 %conv3, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool43 = icmp eq i32 %5, 0
  br i1 %tobool43, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.inc, %omp.dispatch.body
  %q.addr.1.lcssa = phi i64 [ %q.addr.044, %omp.dispatch.body ], [ %q.addr.2, %omp.inner.for.inc ]
  %p.addr.1.lcssa = phi i64 [ %p.addr.045, %omp.dispatch.body ], [ %p.addr.2, %omp.inner.for.inc ]
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %p.addr.045 = phi i64 [ %p.addr.1.lcssa, %omp.dispatch.cond.loopexit ], [ %p, %entry ]
  %q.addr.044 = phi i64 [ %q.addr.1.lcssa, %omp.dispatch.cond.loopexit ], [ %q, %entry ]
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !448
  %cmp38 = icmp sgt i32 %7, %8
  br i1 %cmp38, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.inc, %omp.inner.for.body.lr.ph
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.inc ], [ %9, %omp.inner.for.body.lr.ph ]
  %p.addr.141 = phi i64 [ %p.addr.2, %omp.inner.for.inc ], [ %p.addr.045, %omp.inner.for.body.lr.ph ]
  %q.addr.139 = phi i64 [ %q.addr.2, %omp.inner.for.inc ], [ %q.addr.044, %omp.inner.for.body.lr.ph ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %11 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !448
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %12 = load double, double* %arrayidx6, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !448
  %add7 = fadd double %11, %12
  %13 = bitcast i64 %p.addr.141 to double
  %add8 = fadd double %add7, %13
  %arrayidx10 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %14 = load double, double* %arrayidx10, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !448
  %add11 = fadd double %14, %add8
  store double %add11, double* %arrayidx10, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !448
  %15 = load double, double* %arrayidx6, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !448
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %16 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !448
  %add16 = fadd double %15, %16
  %17 = bitcast i64 %q.addr.139 to double
  %add17 = fadd double %add16, %17
  %arrayidx19 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %18 = load double, double* %arrayidx19, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !448
  %add20 = fadd double %18, %add17
  store double %add20, double* %arrayidx19, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !448
  %19 = icmp eq i64 %indvars.iv, 3071
  br i1 %19, label %if.then, label %omp.inner.for.inc

if.then:                                          ; preds = %omp.inner.for.body
  %add23 = fadd double %13, 6.000000e+00
  %20 = bitcast double %add23 to i64
  %add24 = fadd double %17, 9.000000e+00
  %21 = bitcast double %add24 to i64
  br label %omp.inner.for.inc

omp.inner.for.inc:                                ; preds = %if.then, %omp.inner.for.body
  %q.addr.2 = phi i64 [ %21, %if.then ], [ %q.addr.139, %omp.inner.for.body ]
  %p.addr.2 = phi i64 [ %20, %if.then ], [ %p.addr.141, %omp.inner.for.body ]
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !448

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..122(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp715 = icmp sgt i32 %7, %cond
  br i1 %cmp715, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.016.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..123(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 %p, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 %q) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.016.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7.us = icmp sgt i32 %add.us, %13
  br i1 %cmp7.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.016 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64)* @.omp_outlined..123 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 %p, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 %q) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.016
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7 = icmp sgt i32 %add, %19
  br i1 %cmp7, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..123(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, i64 %p, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E, i64 %q) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv2 = trunc i64 %.previous.lb. to i32
  %conv3 = trunc i64 %.previous.ub. to i32
  store i32 %conv2, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv3, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 37, i32 %conv2, i32 %conv3, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool43 = icmp eq i32 %5, 0
  br i1 %tobool43, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.inc, %omp.dispatch.body
  %q.addr.1.lcssa = phi i64 [ %q.addr.044, %omp.dispatch.body ], [ %q.addr.2, %omp.inner.for.inc ]
  %p.addr.1.lcssa = phi i64 [ %p.addr.045, %omp.dispatch.body ], [ %p.addr.2, %omp.inner.for.inc ]
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %p.addr.045 = phi i64 [ %p.addr.1.lcssa, %omp.dispatch.cond.loopexit ], [ %p, %entry ]
  %q.addr.044 = phi i64 [ %q.addr.1.lcssa, %omp.dispatch.cond.loopexit ], [ %q, %entry ]
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !449
  %cmp38 = icmp sgt i32 %7, %8
  br i1 %cmp38, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.inc, %omp.inner.for.body.lr.ph
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.inc ], [ %9, %omp.inner.for.body.lr.ph ]
  %p.addr.141 = phi i64 [ %p.addr.2, %omp.inner.for.inc ], [ %p.addr.045, %omp.inner.for.body.lr.ph ]
  %q.addr.139 = phi i64 [ %q.addr.2, %omp.inner.for.inc ], [ %q.addr.044, %omp.inner.for.body.lr.ph ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %11 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !449
  %arrayidx6 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %12 = load double, double* %arrayidx6, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !449
  %add7 = fadd double %11, %12
  %13 = bitcast i64 %p.addr.141 to double
  %add8 = fadd double %add7, %13
  %arrayidx10 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %14 = load double, double* %arrayidx10, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !449
  %add11 = fadd double %14, %add8
  store double %add11, double* %arrayidx10, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !449
  %15 = load double, double* %arrayidx6, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !449
  %arrayidx15 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %16 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !449
  %add16 = fadd double %15, %16
  %17 = bitcast i64 %q.addr.139 to double
  %add17 = fadd double %add16, %17
  %arrayidx19 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %18 = load double, double* %arrayidx19, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !449
  %add20 = fadd double %18, %add17
  store double %add20, double* %arrayidx19, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !449
  %19 = icmp eq i64 %indvars.iv, 3071
  br i1 %19, label %if.then, label %omp.inner.for.inc

if.then:                                          ; preds = %omp.inner.for.body
  %add23 = fadd double %13, 6.000000e+00
  %20 = bitcast double %add23 to i64
  %add24 = fadd double %17, 9.000000e+00
  %21 = bitcast double %add24 to i64
  br label %omp.inner.for.inc

omp.inner.for.inc:                                ; preds = %if.then, %omp.inner.for.body
  %q.addr.2 = phi i64 [ %21, %if.then ], [ %q.addr.139, %omp.inner.for.body ]
  %p.addr.2 = phi i64 [ %20, %if.then ], [ %p.addr.141, %omp.inner.for.body ]
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !449

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..126(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, i64 %p, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %q, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp715 = icmp sgt i32 %7, %cond
  br i1 %cmp715, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %.omp_outlined..127.exit.us
  %.omp.iv.016.us = phi i32 [ %add.us, %.omp_outlined..127.exit.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !450
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !450
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !450
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !450
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !450
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !450
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !450
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !450
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !450
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !450
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !450
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !450
  %cmp540.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp540.i.us, label %.omp_outlined..127.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.inc.i.us, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us = phi i64 [ %17, %omp.inner.for.body.preheader.i.us ], [ %indvars.iv.next.i.us, %omp.inner.for.inc.i.us ]
  %p.addr.043.i.us = phi i64 [ %p, %omp.inner.for.body.preheader.i.us ], [ %p.addr.1.i.us, %omp.inner.for.inc.i.us ]
  %q.addr.041.i.us = phi i64 [ %q, %omp.inner.for.body.preheader.i.us ], [ %q.addr.1.i.us, %omp.inner.for.inc.i.us ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %19 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !450
  %arrayidx8.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %20 = load double, double* %arrayidx8.i.us, align 8, !tbaa !62, !noalias !450
  %add9.i.us = fadd double %19, %20
  %21 = bitcast i64 %p.addr.043.i.us to double
  %add10.i.us = fadd double %add9.i.us, %21
  %arrayidx12.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %22 = load double, double* %arrayidx12.i.us, align 8, !tbaa !62, !noalias !450
  %add13.i.us = fadd double %22, %add10.i.us
  store double %add13.i.us, double* %arrayidx12.i.us, align 8, !tbaa !62, !noalias !450
  %23 = load double, double* %arrayidx8.i.us, align 8, !tbaa !62, !noalias !450
  %arrayidx17.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %24 = load double, double* %arrayidx17.i.us, align 8, !tbaa !62, !noalias !450
  %add18.i.us = fadd double %23, %24
  %25 = bitcast i64 %q.addr.041.i.us to double
  %add19.i.us = fadd double %add18.i.us, %25
  %arrayidx21.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %26 = load double, double* %arrayidx21.i.us, align 8, !tbaa !62, !noalias !450
  %add22.i.us = fadd double %26, %add19.i.us
  store double %add22.i.us, double* %arrayidx21.i.us, align 8, !tbaa !62, !noalias !450
  %27 = icmp eq i64 %indvars.iv.i.us, 3071
  br i1 %27, label %if.then.i.us, label %omp.inner.for.inc.i.us

if.then.i.us:                                     ; preds = %omp.inner.for.body.i.us
  %add25.i.us = fadd double %21, 6.000000e+00
  %28 = bitcast double %add25.i.us to i64
  %add26.i.us = fadd double %25, 9.000000e+00
  %29 = bitcast double %add26.i.us to i64
  br label %omp.inner.for.inc.i.us

omp.inner.for.inc.i.us:                           ; preds = %if.then.i.us, %omp.inner.for.body.i.us
  %q.addr.1.i.us = phi i64 [ %29, %if.then.i.us ], [ %q.addr.041.i.us, %omp.inner.for.body.i.us ]
  %p.addr.1.i.us = phi i64 [ %28, %if.then.i.us ], [ %p.addr.043.i.us, %omp.inner.for.body.i.us ]
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp5.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp5.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..127.exit.us

.omp_outlined..127.exit.us:                       ; preds = %omp.inner.for.inc.i.us, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !450
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !450
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !450
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !450
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !450
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %30 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %30, %.omp.iv.016.us
  %31 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7.us = icmp sgt i32 %add.us, %31
  br i1 %cmp7.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.016 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %32 = load i32, i32* %.omp.comb.lb, align 4
  %33 = load i32, i32* %.omp.comb.ub, align 4
  %34 = zext i32 %33 to i64
  %35 = zext i32 %32 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 9, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, i64, [3072 x double]*, [3072 x double]*, i64)* @.omp_outlined..127 to void (i32*, i32*, ...)*), i64 %35, i64 %34, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, i64 %p, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E, i64 %q) #6
  %36 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %36, %.omp.iv.016
  %37 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp7 = icmp sgt i32 %add, %37
  br i1 %cmp7, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..127.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..127(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, i64 %p, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E, i64 %q) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv2 = trunc i64 %.previous.lb. to i32
  %conv3 = trunc i64 %.previous.ub. to i32
  store i32 %conv2, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv3, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp540 = icmp sgt i32 %7, %cond
  br i1 %cmp540, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.inc
  %indvars.iv = phi i64 [ %8, %omp.inner.for.body.preheader ], [ %indvars.iv.next, %omp.inner.for.inc ]
  %p.addr.043 = phi i64 [ %p, %omp.inner.for.body.preheader ], [ %p.addr.1, %omp.inner.for.inc ]
  %q.addr.041 = phi i64 [ %q, %omp.inner.for.body.preheader ], [ %q.addr.1, %omp.inner.for.inc ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %10 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx8 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %11 = load double, double* %arrayidx8, align 8, !tbaa !62
  %add9 = fadd double %10, %11
  %12 = bitcast i64 %p.addr.043 to double
  %add10 = fadd double %add9, %12
  %arrayidx12 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %13 = load double, double* %arrayidx12, align 8, !tbaa !62
  %add13 = fadd double %13, %add10
  store double %add13, double* %arrayidx12, align 8, !tbaa !62
  %14 = load double, double* %arrayidx8, align 8, !tbaa !62
  %arrayidx17 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %15 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %14, %15
  %16 = bitcast i64 %q.addr.041 to double
  %add19 = fadd double %add18, %16
  %arrayidx21 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %17 = load double, double* %arrayidx21, align 8, !tbaa !62
  %add22 = fadd double %17, %add19
  store double %add22, double* %arrayidx21, align 8, !tbaa !62
  %18 = icmp eq i64 %indvars.iv, 3071
  br i1 %18, label %if.then, label %omp.inner.for.inc

if.then:                                          ; preds = %omp.inner.for.body
  %add25 = fadd double %12, 6.000000e+00
  %19 = bitcast double %add25 to i64
  %add26 = fadd double %16, 9.000000e+00
  %20 = bitcast double %add26 to i64
  br label %omp.inner.for.inc

omp.inner.for.inc:                                ; preds = %if.then, %omp.inner.for.body
  %q.addr.1 = phi i64 [ %20, %if.then ], [ %q.addr.041, %omp.inner.for.body ]
  %p.addr.1 = phi i64 [ %19, %if.then ], [ %p.addr.043, %omp.inner.for.body ]
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp5 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp5, label %omp.inner.for.body, label %omp.loop.exit

omp.loop.exit:                                    ; preds = %omp.inner.for.inc, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..130(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp613 = icmp sgt i32 %7, %cond
  br i1 %cmp613, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us.preheader, label %omp.inner.for.body

omp.inner.for.body.us.preheader:                  ; preds = %omp.inner.for.body.lr.ph
  %scevgep19 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 1
  %scevgep25 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 1
  %scevgep30 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 1
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 1
  %scevgep40 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 1
  br label %omp.inner.for.body.us

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.us.preheader, %.omp_outlined..131.exit.us
  %.omp.iv.014.us = phi i32 [ %add.us, %.omp_outlined..131.exit.us ], [ %7, %omp.inner.for.body.us.preheader ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !453
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !453
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !453
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !453
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !453
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !453
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !453
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !453
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !453
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !453
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !453
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !453
  %cmp451.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp451.i.us, label %.omp_outlined..131.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  %19 = icmp sgt i64 %18, %17
  %smax = select i1 %19, i64 %18, i64 %17
  %20 = sub nsw i64 1, %17
  %21 = add nsw i64 %20, %smax
  %min.iters.check = icmp ult i64 %21, 12
  br i1 %min.iters.check, label %omp.inner.for.body.i.us.preheader, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader.i.us
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %17
  %scevgep21 = getelementptr double, double* %scevgep19, i64 %smax
  %scevgep23 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %17
  %scevgep26 = getelementptr double, double* %scevgep25, i64 %smax
  %scevgep28 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %17
  %scevgep31 = getelementptr double, double* %scevgep30, i64 %smax
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %17
  %scevgep36 = getelementptr double, double* %scevgep35, i64 %smax
  %scevgep38 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %17
  %scevgep41 = getelementptr double, double* %scevgep40, i64 %smax
  %bound0 = icmp ult double* %scevgep, %scevgep26
  %bound1 = icmp ult double* %scevgep23, %scevgep21
  %found.conflict = and i1 %bound0, %bound1
  %bound043 = icmp ult double* %scevgep, %scevgep31
  %bound144 = icmp ult double* %scevgep28, %scevgep21
  %found.conflict45 = and i1 %bound043, %bound144
  %conflict.rdx = or i1 %found.conflict, %found.conflict45
  %bound046 = icmp ult double* %scevgep, %scevgep36
  %bound147 = icmp ult double* %scevgep33, %scevgep21
  %found.conflict48 = and i1 %bound046, %bound147
  %conflict.rdx49 = or i1 %found.conflict48, %conflict.rdx
  %bound050 = icmp ult double* %scevgep, %scevgep41
  %bound151 = icmp ult double* %scevgep38, %scevgep21
  %found.conflict52 = and i1 %bound050, %bound151
  %conflict.rdx53 = or i1 %found.conflict52, %conflict.rdx49
  %bound054 = icmp ult double* %scevgep23, %scevgep31
  %bound155 = icmp ult double* %scevgep28, %scevgep26
  %found.conflict56 = and i1 %bound054, %bound155
  %conflict.rdx57 = or i1 %found.conflict56, %conflict.rdx53
  %bound058 = icmp ult double* %scevgep23, %scevgep36
  %bound159 = icmp ult double* %scevgep33, %scevgep26
  %found.conflict60 = and i1 %bound058, %bound159
  %conflict.rdx61 = or i1 %found.conflict60, %conflict.rdx57
  %bound062 = icmp ult double* %scevgep23, %scevgep41
  %bound163 = icmp ult double* %scevgep38, %scevgep26
  %found.conflict64 = and i1 %bound062, %bound163
  %conflict.rdx65 = or i1 %found.conflict64, %conflict.rdx61
  br i1 %conflict.rdx65, label %omp.inner.for.body.i.us.preheader, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 12
  %n.vec = sub nsw i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %17
  %22 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !456, !noalias !453
  %24 = getelementptr inbounds double, double* %22, i64 2
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !456, !noalias !453
  %26 = getelementptr inbounds double, double* %22, i64 4
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !456, !noalias !453
  %28 = getelementptr inbounds double, double* %22, i64 6
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !456, !noalias !453
  %30 = getelementptr inbounds double, double* %22, i64 8
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !456, !noalias !453
  %32 = getelementptr inbounds double, double* %22, i64 10
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !456, !noalias !453
  %34 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !459, !noalias !453
  %36 = getelementptr inbounds double, double* %34, i64 2
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !459, !noalias !453
  %38 = getelementptr inbounds double, double* %34, i64 4
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !459, !noalias !453
  %40 = getelementptr inbounds double, double* %34, i64 6
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !459, !noalias !453
  %42 = getelementptr inbounds double, double* %34, i64 8
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !459, !noalias !453
  %44 = getelementptr inbounds double, double* %34, i64 10
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !459, !noalias !453
  %46 = fadd <2 x double> %wide.load, %wide.load76
  %47 = fadd <2 x double> %wide.load71, %wide.load77
  %48 = fadd <2 x double> %wide.load72, %wide.load78
  %49 = fadd <2 x double> %wide.load73, %wide.load79
  %50 = fadd <2 x double> %wide.load74, %wide.load80
  %51 = fadd <2 x double> %wide.load75, %wide.load81
  %52 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  %54 = getelementptr inbounds double, double* %52, i64 2
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  %56 = getelementptr inbounds double, double* %52, i64 4
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  %58 = getelementptr inbounds double, double* %52, i64 6
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  %60 = getelementptr inbounds double, double* %52, i64 8
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  %62 = getelementptr inbounds double, double* %52, i64 10
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  %64 = fadd <2 x double> %46, %wide.load82
  %65 = fadd <2 x double> %47, %wide.load83
  %66 = fadd <2 x double> %48, %wide.load84
  %67 = fadd <2 x double> %49, %wide.load85
  %68 = fadd <2 x double> %50, %wide.load86
  %69 = fadd <2 x double> %51, %wide.load87
  store <2 x double> %64, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  store <2 x double> %65, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  store <2 x double> %66, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  store <2 x double> %67, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  store <2 x double> %68, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  store <2 x double> %69, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !461, !noalias !463
  %70 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !466, !noalias !453
  %72 = getelementptr inbounds double, double* %70, i64 2
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !466, !noalias !453
  %74 = getelementptr inbounds double, double* %70, i64 4
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !466, !noalias !453
  %76 = getelementptr inbounds double, double* %70, i64 6
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !466, !noalias !453
  %78 = getelementptr inbounds double, double* %70, i64 8
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !466, !noalias !453
  %80 = getelementptr inbounds double, double* %70, i64 10
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !466, !noalias !453
  %82 = fadd <2 x double> %wide.load76, %wide.load94
  %83 = fadd <2 x double> %wide.load77, %wide.load95
  %84 = fadd <2 x double> %wide.load78, %wide.load96
  %85 = fadd <2 x double> %wide.load79, %wide.load97
  %86 = fadd <2 x double> %wide.load80, %wide.load98
  %87 = fadd <2 x double> %wide.load81, %wide.load99
  %88 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  %90 = getelementptr inbounds double, double* %88, i64 2
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  %92 = getelementptr inbounds double, double* %88, i64 4
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  %94 = getelementptr inbounds double, double* %88, i64 6
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  %96 = getelementptr inbounds double, double* %88, i64 8
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  %98 = getelementptr inbounds double, double* %88, i64 10
  %99 = bitcast double* %98 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  %100 = fadd <2 x double> %82, %wide.load100
  %101 = fadd <2 x double> %83, %wide.load101
  %102 = fadd <2 x double> %84, %wide.load102
  %103 = fadd <2 x double> %85, %wide.load103
  %104 = fadd <2 x double> %86, %wide.load104
  %105 = fadd <2 x double> %87, %wide.load105
  store <2 x double> %100, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  store <2 x double> %101, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  store <2 x double> %102, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  store <2 x double> %103, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  store <2 x double> %104, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  store <2 x double> %105, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !467, !noalias !468
  %index.next = add i64 %index, 12
  %106 = icmp eq i64 %index.next, %n.vec
  br i1 %106, label %middle.block, label %vector.body, !llvm.loop !469

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %17
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %.omp_outlined..131.exit.us, label %omp.inner.for.body.i.us.preheader

omp.inner.for.body.i.us.preheader:                ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us.ph = phi i64 [ %ind.end, %middle.block ], [ %17, %omp.inner.for.body.preheader.i.us ], [ %17, %vector.memcheck ]
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.body.i.us.preheader, %omp.inner.for.body.i.us
  %indvars.iv.i.us = phi i64 [ %indvars.iv.next.i.us, %omp.inner.for.body.i.us ], [ %indvars.iv.i.us.ph, %omp.inner.for.body.i.us.preheader ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %107 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !453
  %arrayidx13.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %108 = load double, double* %arrayidx13.i.us, align 8, !tbaa !62, !noalias !453
  %add14.i.us = fadd double %107, %108
  %arrayidx18.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %109 = load double, double* %arrayidx18.i.us, align 8, !tbaa !62, !noalias !453
  %add19.i.us = fadd double %109, %add14.i.us
  store double %add19.i.us, double* %arrayidx18.i.us, align 8, !tbaa !62, !noalias !453
  %110 = load double, double* %arrayidx13.i.us, align 8, !tbaa !62, !noalias !453
  %arrayidx27.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %111 = load double, double* %arrayidx27.i.us, align 8, !tbaa !62, !noalias !453
  %add28.i.us = fadd double %110, %111
  %arrayidx32.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %112 = load double, double* %arrayidx32.i.us, align 8, !tbaa !62, !noalias !453
  %add33.i.us = fadd double %112, %add28.i.us
  store double %add33.i.us, double* %arrayidx32.i.us, align 8, !tbaa !62, !noalias !453
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp4.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp4.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..131.exit.us, !llvm.loop !470

.omp_outlined..131.exit.us:                       ; preds = %omp.inner.for.body.i.us, %middle.block, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !453
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !453
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !453
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !453
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !453
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %113 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %113, %.omp.iv.014.us
  %114 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6.us = icmp sgt i32 %add.us, %114
  br i1 %cmp6.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %115 = load i32, i32* %.omp.comb.lb, align 4
  %116 = load i32, i32* %.omp.comb.ub, align 4
  %117 = zext i32 %116 to i64
  %118 = zext i32 %115 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..131 to void (i32*, i32*, ...)*), i64 %118, i64 %117, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %119 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %119, %.omp.iv.014
  %120 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6 = icmp sgt i32 %add, %120
  br i1 %cmp6, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..131.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..131(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv2 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv2, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp451 = icmp sgt i32 %7, %cond
  br i1 %cmp451, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  %10 = icmp sgt i64 %9, %8
  %smax = select i1 %10, i64 %9, i64 %8
  %11 = add nsw i64 %smax, 1
  %12 = sub nsw i64 %11, %8
  %min.iters.check = icmp ult i64 %12, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %8
  %scevgep55 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %11
  %scevgep57 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %8
  %scevgep59 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %11
  %scevgep61 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %8
  %scevgep63 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %11
  %scevgep65 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %8
  %scevgep67 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %11
  %scevgep69 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %8
  %scevgep71 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %11
  %bound0 = icmp ult double* %scevgep, %scevgep59
  %bound1 = icmp ult double* %scevgep57, %scevgep55
  %found.conflict = and i1 %bound0, %bound1
  %bound073 = icmp ult double* %scevgep, %scevgep63
  %bound174 = icmp ult double* %scevgep61, %scevgep55
  %found.conflict75 = and i1 %bound073, %bound174
  %conflict.rdx = or i1 %found.conflict, %found.conflict75
  %bound076 = icmp ult double* %scevgep, %scevgep67
  %bound177 = icmp ult double* %scevgep65, %scevgep55
  %found.conflict78 = and i1 %bound076, %bound177
  %conflict.rdx79 = or i1 %found.conflict78, %conflict.rdx
  %bound080 = icmp ult double* %scevgep, %scevgep71
  %bound181 = icmp ult double* %scevgep69, %scevgep55
  %found.conflict82 = and i1 %bound080, %bound181
  %conflict.rdx83 = or i1 %found.conflict82, %conflict.rdx79
  %bound084 = icmp ult double* %scevgep57, %scevgep63
  %bound185 = icmp ult double* %scevgep61, %scevgep59
  %found.conflict86 = and i1 %bound084, %bound185
  %conflict.rdx87 = or i1 %found.conflict86, %conflict.rdx83
  %bound088 = icmp ult double* %scevgep57, %scevgep67
  %bound189 = icmp ult double* %scevgep65, %scevgep59
  %found.conflict90 = and i1 %bound088, %bound189
  %conflict.rdx91 = or i1 %found.conflict90, %conflict.rdx87
  %bound092 = icmp ult double* %scevgep57, %scevgep71
  %bound193 = icmp ult double* %scevgep69, %scevgep59
  %found.conflict94 = and i1 %bound092, %bound193
  %conflict.rdx95 = or i1 %found.conflict94, %conflict.rdx91
  br i1 %conflict.rdx95, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %12, 12
  %n.vec = sub nsw i64 %12, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %8
  %13 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %14 = bitcast double* %13 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %14, align 8, !tbaa !62, !alias.scope !471
  %15 = getelementptr inbounds double, double* %13, i64 2
  %16 = bitcast double* %15 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %16, align 8, !tbaa !62, !alias.scope !471
  %17 = getelementptr inbounds double, double* %13, i64 4
  %18 = bitcast double* %17 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %18, align 8, !tbaa !62, !alias.scope !471
  %19 = getelementptr inbounds double, double* %13, i64 6
  %20 = bitcast double* %19 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %20, align 8, !tbaa !62, !alias.scope !471
  %21 = getelementptr inbounds double, double* %13, i64 8
  %22 = bitcast double* %21 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %22, align 8, !tbaa !62, !alias.scope !471
  %23 = getelementptr inbounds double, double* %13, i64 10
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !471
  %25 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !474
  %27 = getelementptr inbounds double, double* %25, i64 2
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !474
  %29 = getelementptr inbounds double, double* %25, i64 4
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !474
  %31 = getelementptr inbounds double, double* %25, i64 6
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !474
  %33 = getelementptr inbounds double, double* %25, i64 8
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load110 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !474
  %35 = getelementptr inbounds double, double* %25, i64 10
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load111 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !474
  %37 = fadd <2 x double> %wide.load, %wide.load106
  %38 = fadd <2 x double> %wide.load101, %wide.load107
  %39 = fadd <2 x double> %wide.load102, %wide.load108
  %40 = fadd <2 x double> %wide.load103, %wide.load109
  %41 = fadd <2 x double> %wide.load104, %wide.load110
  %42 = fadd <2 x double> %wide.load105, %wide.load111
  %43 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load112 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  %45 = getelementptr inbounds double, double* %43, i64 2
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load113 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  %47 = getelementptr inbounds double, double* %43, i64 4
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load114 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  %49 = getelementptr inbounds double, double* %43, i64 6
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load115 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  %51 = getelementptr inbounds double, double* %43, i64 8
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load116 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  %53 = getelementptr inbounds double, double* %43, i64 10
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load117 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  %55 = fadd <2 x double> %37, %wide.load112
  %56 = fadd <2 x double> %38, %wide.load113
  %57 = fadd <2 x double> %39, %wide.load114
  %58 = fadd <2 x double> %40, %wide.load115
  %59 = fadd <2 x double> %41, %wide.load116
  %60 = fadd <2 x double> %42, %wide.load117
  store <2 x double> %55, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  store <2 x double> %56, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  store <2 x double> %57, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  store <2 x double> %58, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  store <2 x double> %59, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  store <2 x double> %60, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !476, !noalias !478
  %61 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load124 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !481
  %63 = getelementptr inbounds double, double* %61, i64 2
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load125 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !481
  %65 = getelementptr inbounds double, double* %61, i64 4
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load126 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !481
  %67 = getelementptr inbounds double, double* %61, i64 6
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load127 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !481
  %69 = getelementptr inbounds double, double* %61, i64 8
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load128 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !481
  %71 = getelementptr inbounds double, double* %61, i64 10
  %72 = bitcast double* %71 to <2 x double>*
  %wide.load129 = load <2 x double>, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !481
  %73 = fadd <2 x double> %wide.load106, %wide.load124
  %74 = fadd <2 x double> %wide.load107, %wide.load125
  %75 = fadd <2 x double> %wide.load108, %wide.load126
  %76 = fadd <2 x double> %wide.load109, %wide.load127
  %77 = fadd <2 x double> %wide.load110, %wide.load128
  %78 = fadd <2 x double> %wide.load111, %wide.load129
  %79 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %80 = bitcast double* %79 to <2 x double>*
  %wide.load130 = load <2 x double>, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  %81 = getelementptr inbounds double, double* %79, i64 2
  %82 = bitcast double* %81 to <2 x double>*
  %wide.load131 = load <2 x double>, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  %83 = getelementptr inbounds double, double* %79, i64 4
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load132 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  %85 = getelementptr inbounds double, double* %79, i64 6
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load133 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  %87 = getelementptr inbounds double, double* %79, i64 8
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load134 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  %89 = getelementptr inbounds double, double* %79, i64 10
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load135 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  %91 = fadd <2 x double> %73, %wide.load130
  %92 = fadd <2 x double> %74, %wide.load131
  %93 = fadd <2 x double> %75, %wide.load132
  %94 = fadd <2 x double> %76, %wide.load133
  %95 = fadd <2 x double> %77, %wide.load134
  %96 = fadd <2 x double> %78, %wide.load135
  store <2 x double> %91, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  store <2 x double> %92, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  store <2 x double> %93, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  store <2 x double> %94, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  store <2 x double> %95, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  store <2 x double> %96, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !482, !noalias !483
  %index.next = add i64 %index, 12
  %97 = icmp eq i64 %index.next, %n.vec
  br i1 %97, label %middle.block, label %vector.body, !llvm.loop !484

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %8
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %8, %omp.inner.for.body.preheader ], [ %8, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %98 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx13 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx13, align 8, !tbaa !62
  %add14 = fadd double %98, %99
  %arrayidx18 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx18, align 8, !tbaa !62
  %add19 = fadd double %100, %add14
  store double %add19, double* %arrayidx18, align 8, !tbaa !62
  %101 = load double, double* %arrayidx13, align 8, !tbaa !62
  %arrayidx27 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx27, align 8, !tbaa !62
  %add28 = fadd double %101, %102
  %arrayidx32 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx32, align 8, !tbaa !62
  %add33 = fadd double %103, %add28
  store double %add33, double* %arrayidx32, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp4 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp4, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !485

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..134(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp613 = icmp sgt i32 %7, %cond
  br i1 %cmp613, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.014.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..135(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.014.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6.us = icmp sgt i32 %add.us, %13
  br i1 %cmp6.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..135 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.014
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6 = icmp sgt i32 %add, %19
  br i1 %cmp6, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..135(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv2 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv2, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 38, i32 %conv, i32 %conv2, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool51 = icmp eq i32 %5, 0
  br i1 %tobool51, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !486
  %cmp49 = icmp sgt i32 %7, %8
  br i1 %cmp49, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load62
  %39 = fadd <2 x double> %wide.load57, %wide.load63
  %40 = fadd <2 x double> %wide.load58, %wide.load64
  %41 = fadd <2 x double> %wide.load59, %wide.load65
  %42 = fadd <2 x double> %wide.load60, %wide.load66
  %43 = fadd <2 x double> %wide.load61, %wide.load67
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load68
  %57 = fadd <2 x double> %39, %wide.load69
  %58 = fadd <2 x double> %40, %wide.load70
  %59 = fadd <2 x double> %41, %wide.load71
  %60 = fadd <2 x double> %42, %wide.load72
  %61 = fadd <2 x double> %43, %wide.load73
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load74 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load75 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load76 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load77 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load78 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load79 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load74, %wide.load80
  %75 = fadd <2 x double> %wide.load75, %wide.load81
  %76 = fadd <2 x double> %wide.load76, %wide.load82
  %77 = fadd <2 x double> %wide.load77, %wide.load83
  %78 = fadd <2 x double> %wide.load78, %wide.load84
  %79 = fadd <2 x double> %wide.load79, %wide.load85
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load86
  %93 = fadd <2 x double> %75, %wide.load87
  %94 = fadd <2 x double> %76, %wide.load88
  %95 = fadd <2 x double> %77, %wide.load89
  %96 = fadd <2 x double> %78, %wide.load90
  %97 = fadd <2 x double> %79, %wide.load91
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !487

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !486
  %arrayidx11 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx11, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !486
  %add12 = fadd double %99, %100
  %arrayidx16 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx16, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !486
  %add17 = fadd double %101, %add12
  store double %add17, double* %arrayidx16, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !486
  %102 = load double, double* %arrayidx11, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !486
  %arrayidx25 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx25, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !486
  %add26 = fadd double %102, %103
  %arrayidx30 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx30, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !486
  %add31 = fadd double %104, %add26
  store double %add31, double* %arrayidx30, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !486
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !488

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..138(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp613 = icmp sgt i32 %7, %cond
  br i1 %cmp613, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.014.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..139(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.014.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6.us = icmp sgt i32 %add.us, %13
  br i1 %cmp6.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..139 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.014
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6 = icmp sgt i32 %add, %19
  br i1 %cmp6, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..139(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv2 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv2, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 35, i32 %conv, i32 %conv2, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool51 = icmp eq i32 %5, 0
  br i1 %tobool51, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !489
  %cmp49 = icmp sgt i32 %7, %8
  br i1 %cmp49, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load62
  %39 = fadd <2 x double> %wide.load57, %wide.load63
  %40 = fadd <2 x double> %wide.load58, %wide.load64
  %41 = fadd <2 x double> %wide.load59, %wide.load65
  %42 = fadd <2 x double> %wide.load60, %wide.load66
  %43 = fadd <2 x double> %wide.load61, %wide.load67
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load68
  %57 = fadd <2 x double> %39, %wide.load69
  %58 = fadd <2 x double> %40, %wide.load70
  %59 = fadd <2 x double> %41, %wide.load71
  %60 = fadd <2 x double> %42, %wide.load72
  %61 = fadd <2 x double> %43, %wide.load73
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load74 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load75 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load76 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load77 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load78 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load79 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load74, %wide.load80
  %75 = fadd <2 x double> %wide.load75, %wide.load81
  %76 = fadd <2 x double> %wide.load76, %wide.load82
  %77 = fadd <2 x double> %wide.load77, %wide.load83
  %78 = fadd <2 x double> %wide.load78, %wide.load84
  %79 = fadd <2 x double> %wide.load79, %wide.load85
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load86
  %93 = fadd <2 x double> %75, %wide.load87
  %94 = fadd <2 x double> %76, %wide.load88
  %95 = fadd <2 x double> %77, %wide.load89
  %96 = fadd <2 x double> %78, %wide.load90
  %97 = fadd <2 x double> %79, %wide.load91
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !490

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !489
  %arrayidx11 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx11, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !489
  %add12 = fadd double %99, %100
  %arrayidx16 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx16, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !489
  %add17 = fadd double %101, %add12
  store double %add17, double* %arrayidx16, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !489
  %102 = load double, double* %arrayidx11, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !489
  %arrayidx25 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx25, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !489
  %add26 = fadd double %102, %103
  %arrayidx30 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx30, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !489
  %add31 = fadd double %104, %add26
  store double %add31, double* %arrayidx30, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !489
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !491

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..142(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp613 = icmp sgt i32 %7, %cond
  br i1 %cmp613, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.014.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..143(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.014.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6.us = icmp sgt i32 %add.us, %13
  br i1 %cmp6.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..143 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.014
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6 = icmp sgt i32 %add, %19
  br i1 %cmp6, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..143(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv2 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv2, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 36, i32 %conv, i32 %conv2, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool51 = icmp eq i32 %5, 0
  br i1 %tobool51, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !492
  %cmp49 = icmp sgt i32 %7, %8
  br i1 %cmp49, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load62
  %39 = fadd <2 x double> %wide.load57, %wide.load63
  %40 = fadd <2 x double> %wide.load58, %wide.load64
  %41 = fadd <2 x double> %wide.load59, %wide.load65
  %42 = fadd <2 x double> %wide.load60, %wide.load66
  %43 = fadd <2 x double> %wide.load61, %wide.load67
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load68
  %57 = fadd <2 x double> %39, %wide.load69
  %58 = fadd <2 x double> %40, %wide.load70
  %59 = fadd <2 x double> %41, %wide.load71
  %60 = fadd <2 x double> %42, %wide.load72
  %61 = fadd <2 x double> %43, %wide.load73
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load74 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load75 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load76 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load77 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load78 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load79 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load74, %wide.load80
  %75 = fadd <2 x double> %wide.load75, %wide.load81
  %76 = fadd <2 x double> %wide.load76, %wide.load82
  %77 = fadd <2 x double> %wide.load77, %wide.load83
  %78 = fadd <2 x double> %wide.load78, %wide.load84
  %79 = fadd <2 x double> %wide.load79, %wide.load85
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load86
  %93 = fadd <2 x double> %75, %wide.load87
  %94 = fadd <2 x double> %76, %wide.load88
  %95 = fadd <2 x double> %77, %wide.load89
  %96 = fadd <2 x double> %78, %wide.load90
  %97 = fadd <2 x double> %79, %wide.load91
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !493

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !492
  %arrayidx11 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx11, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !492
  %add12 = fadd double %99, %100
  %arrayidx16 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx16, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !492
  %add17 = fadd double %101, %add12
  store double %add17, double* %arrayidx16, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !492
  %102 = load double, double* %arrayidx11, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !492
  %arrayidx25 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx25, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !492
  %add26 = fadd double %102, %103
  %arrayidx30 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx30, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !492
  %add31 = fadd double %104, %add26
  store double %add31, double* %arrayidx30, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !492
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !494

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..146(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.zero.addr = alloca i32, align 4
  store i32 0, i32* %.zero.addr, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp613 = icmp sgt i32 %7, %cond
  br i1 %cmp613, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  br i1 %tobool, label %omp.inner.for.body.us, label %omp.inner.for.body

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body.us
  %.omp.iv.014.us = phi i32 [ %add.us, %omp.inner.for.body.us ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %8 = load i32, i32* %.omp.comb.lb, align 4
  %9 = zext i32 %8 to i64
  %10 = load i32, i32* %.omp.comb.ub, align 4
  %11 = zext i32 %10 to i64
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @.omp_outlined..147(i32* %.global_tid., i32* nonnull %.zero.addr, i64 %9, i64 %11, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %12 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %12, %.omp.iv.014.us
  %13 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6.us = icmp sgt i32 %add.us, %13
  br i1 %cmp6.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %14 = load i32, i32* %.omp.comb.lb, align 4
  %15 = zext i32 %14 to i64
  %16 = load i32, i32* %.omp.comb.ub, align 4
  %17 = zext i32 %16 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..147 to void (i32*, i32*, ...)*), i64 %15, i64 %17, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %18 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %18, %.omp.iv.014
  %19 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6 = icmp sgt i32 %add, %19
  br i1 %cmp6, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.inner.for.body.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..147(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv2 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv2, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 37, i32 %conv, i32 %conv2, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool51 = icmp eq i32 %5, 0
  br i1 %tobool51, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %entry, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !495
  %cmp49 = icmp sgt i32 %7, %8
  br i1 %cmp49, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 12
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %26, i64 2
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %26, i64 4
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %26, i64 6
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %26, i64 8
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %26, i64 10
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = fadd <2 x double> %wide.load, %wide.load62
  %39 = fadd <2 x double> %wide.load57, %wide.load63
  %40 = fadd <2 x double> %wide.load58, %wide.load64
  %41 = fadd <2 x double> %wide.load59, %wide.load65
  %42 = fadd <2 x double> %wide.load60, %wide.load66
  %43 = fadd <2 x double> %wide.load61, %wide.load67
  %44 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %44, i64 2
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %44, i64 4
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %44, i64 6
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %44, i64 8
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %44, i64 10
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = fadd <2 x double> %38, %wide.load68
  %57 = fadd <2 x double> %39, %wide.load69
  %58 = fadd <2 x double> %40, %wide.load70
  %59 = fadd <2 x double> %41, %wide.load71
  %60 = fadd <2 x double> %42, %wide.load72
  %61 = fadd <2 x double> %43, %wide.load73
  store <2 x double> %56, <2 x double>* %45, align 8, !tbaa !62
  store <2 x double> %57, <2 x double>* %47, align 8, !tbaa !62
  store <2 x double> %58, <2 x double>* %49, align 8, !tbaa !62
  store <2 x double> %59, <2 x double>* %51, align 8, !tbaa !62
  store <2 x double> %60, <2 x double>* %53, align 8, !tbaa !62
  store <2 x double> %61, <2 x double>* %55, align 8, !tbaa !62
  %wide.load74 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %wide.load75 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %wide.load76 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %wide.load77 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %wide.load78 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %wide.load79 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %62 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62
  %64 = getelementptr inbounds double, double* %62, i64 2
  %65 = bitcast double* %64 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %65, align 8, !tbaa !62
  %66 = getelementptr inbounds double, double* %62, i64 4
  %67 = bitcast double* %66 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %67, align 8, !tbaa !62
  %68 = getelementptr inbounds double, double* %62, i64 6
  %69 = bitcast double* %68 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %69, align 8, !tbaa !62
  %70 = getelementptr inbounds double, double* %62, i64 8
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62
  %72 = getelementptr inbounds double, double* %62, i64 10
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62
  %74 = fadd <2 x double> %wide.load74, %wide.load80
  %75 = fadd <2 x double> %wide.load75, %wide.load81
  %76 = fadd <2 x double> %wide.load76, %wide.load82
  %77 = fadd <2 x double> %wide.load77, %wide.load83
  %78 = fadd <2 x double> %wide.load78, %wide.load84
  %79 = fadd <2 x double> %wide.load79, %wide.load85
  %80 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %80, i64 2
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %80, i64 4
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %80, i64 6
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %80, i64 8
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %80, i64 10
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = fadd <2 x double> %74, %wide.load86
  %93 = fadd <2 x double> %75, %wide.load87
  %94 = fadd <2 x double> %76, %wide.load88
  %95 = fadd <2 x double> %77, %wide.load89
  %96 = fadd <2 x double> %78, %wide.load90
  %97 = fadd <2 x double> %79, %wide.load91
  store <2 x double> %92, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %93, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %94, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %95, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %96, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %97, <2 x double>* %91, align 8, !tbaa !62
  %index.next = add i64 %index, 12
  %98 = icmp eq i64 %index.next, %n.vec
  br i1 %98, label %middle.block, label %vector.body, !llvm.loop !496

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !495
  %arrayidx11 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx11, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !495
  %add12 = fadd double %99, %100
  %arrayidx16 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %101 = load double, double* %arrayidx16, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !495
  %add17 = fadd double %101, %add12
  store double %add17, double* %arrayidx16, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !495
  %102 = load double, double* %arrayidx11, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !495
  %arrayidx25 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx25, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !495
  %add26 = fadd double %102, %103
  %arrayidx30 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %104 = load double, double* %arrayidx30, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !495
  %add31 = fadd double %104, %add26
  store double %add31, double* %arrayidx30, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !495
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp = icmp slt i64 %indvars.iv, %10
  br i1 %cmp, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !497

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %entry
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..150(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., [3072 x double]* dereferenceable(24576) %A, [3072 x double]* dereferenceable(24576) %C, [3072 x double]* dereferenceable(24576) %D, [3072 x double]* dereferenceable(24576) %B, [3072 x double]* dereferenceable(24576) %E, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb.i = alloca i32, align 4
  %.omp.ub.i = alloca i32, align 4
  %.omp.stride.i = alloca i32, align 4
  %.omp.is_last.i = alloca i32, align 4
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 3071, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp613 = icmp sgt i32 %7, %cond
  br i1 %cmp613, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %tobool = icmp eq i32 %.capture_expr..addr.sroa.0.0.extract.trunc, 0
  %8 = bitcast i32* %.omp.lb.i to i8*
  %9 = bitcast i32* %.omp.ub.i to i8*
  %10 = bitcast i32* %.omp.stride.i to i8*
  %11 = bitcast i32* %.omp.is_last.i to i8*
  br i1 %tobool, label %omp.inner.for.body.us.preheader, label %omp.inner.for.body

omp.inner.for.body.us.preheader:                  ; preds = %omp.inner.for.body.lr.ph
  %scevgep19 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 1
  %scevgep25 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 1
  %scevgep30 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 1
  %scevgep35 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 1
  %scevgep40 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 1
  br label %omp.inner.for.body.us

omp.inner.for.body.us:                            ; preds = %omp.inner.for.body.us.preheader, %.omp_outlined..151.exit.us
  %.omp.iv.014.us = phi i32 [ %add.us, %.omp_outlined..151.exit.us ], [ %7, %omp.inner.for.body.us.preheader ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %12 = load i32, i32* %.omp.comb.lb, align 4
  %13 = load i32, i32* %.omp.comb.ub, align 4
  call void @__kmpc_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %8) #6, !noalias !498
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %9) #6, !noalias !498
  store i32 %12, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !498
  store i32 %13, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !498
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %10) #6, !noalias !498
  store i32 1, i32* %.omp.stride.i, align 4, !tbaa !58, !noalias !498
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %11) #6, !noalias !498
  store i32 0, i32* %.omp.is_last.i, align 4, !tbaa !58, !noalias !498
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last.i, i32* nonnull %.omp.lb.i, i32* nonnull %.omp.ub.i, i32* nonnull %.omp.stride.i, i32 1, i32 1) #6, !noalias !498
  %14 = load i32, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !498
  %15 = icmp slt i32 %14, 3071
  %cond.i.us = select i1 %15, i32 %14, i32 3071
  store i32 %cond.i.us, i32* %.omp.ub.i, align 4, !tbaa !58, !noalias !498
  %16 = load i32, i32* %.omp.lb.i, align 4, !tbaa !58, !noalias !498
  %cmp451.i.us = icmp sgt i32 %16, %cond.i.us
  br i1 %cmp451.i.us, label %.omp_outlined..151.exit.us, label %omp.inner.for.body.preheader.i.us

omp.inner.for.body.preheader.i.us:                ; preds = %omp.inner.for.body.us
  %17 = sext i32 %16 to i64
  %18 = sext i32 %cond.i.us to i64
  %19 = icmp sgt i64 %18, %17
  %smax = select i1 %19, i64 %18, i64 %17
  %20 = sub nsw i64 1, %17
  %21 = add nsw i64 %20, %smax
  %min.iters.check = icmp ult i64 %21, 12
  br i1 %min.iters.check, label %omp.inner.for.body.i.us.preheader, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader.i.us
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %17
  %scevgep21 = getelementptr double, double* %scevgep19, i64 %smax
  %scevgep23 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %17
  %scevgep26 = getelementptr double, double* %scevgep25, i64 %smax
  %scevgep28 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %17
  %scevgep31 = getelementptr double, double* %scevgep30, i64 %smax
  %scevgep33 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %17
  %scevgep36 = getelementptr double, double* %scevgep35, i64 %smax
  %scevgep38 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %17
  %scevgep41 = getelementptr double, double* %scevgep40, i64 %smax
  %bound0 = icmp ult double* %scevgep, %scevgep26
  %bound1 = icmp ult double* %scevgep23, %scevgep21
  %found.conflict = and i1 %bound0, %bound1
  %bound043 = icmp ult double* %scevgep, %scevgep31
  %bound144 = icmp ult double* %scevgep28, %scevgep21
  %found.conflict45 = and i1 %bound043, %bound144
  %conflict.rdx = or i1 %found.conflict, %found.conflict45
  %bound046 = icmp ult double* %scevgep, %scevgep36
  %bound147 = icmp ult double* %scevgep33, %scevgep21
  %found.conflict48 = and i1 %bound046, %bound147
  %conflict.rdx49 = or i1 %found.conflict48, %conflict.rdx
  %bound050 = icmp ult double* %scevgep, %scevgep41
  %bound151 = icmp ult double* %scevgep38, %scevgep21
  %found.conflict52 = and i1 %bound050, %bound151
  %conflict.rdx53 = or i1 %found.conflict52, %conflict.rdx49
  %bound054 = icmp ult double* %scevgep23, %scevgep31
  %bound155 = icmp ult double* %scevgep28, %scevgep26
  %found.conflict56 = and i1 %bound054, %bound155
  %conflict.rdx57 = or i1 %found.conflict56, %conflict.rdx53
  %bound058 = icmp ult double* %scevgep23, %scevgep36
  %bound159 = icmp ult double* %scevgep33, %scevgep26
  %found.conflict60 = and i1 %bound058, %bound159
  %conflict.rdx61 = or i1 %found.conflict60, %conflict.rdx57
  %bound062 = icmp ult double* %scevgep23, %scevgep41
  %bound163 = icmp ult double* %scevgep38, %scevgep26
  %found.conflict64 = and i1 %bound062, %bound163
  %conflict.rdx65 = or i1 %found.conflict64, %conflict.rdx61
  br i1 %conflict.rdx65, label %omp.inner.for.body.i.us.preheader, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 12
  %n.vec = sub nsw i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %17
  %22 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !501, !noalias !498
  %24 = getelementptr inbounds double, double* %22, i64 2
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !501, !noalias !498
  %26 = getelementptr inbounds double, double* %22, i64 4
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !501, !noalias !498
  %28 = getelementptr inbounds double, double* %22, i64 6
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !501, !noalias !498
  %30 = getelementptr inbounds double, double* %22, i64 8
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !501, !noalias !498
  %32 = getelementptr inbounds double, double* %22, i64 10
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !501, !noalias !498
  %34 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !504, !noalias !498
  %36 = getelementptr inbounds double, double* %34, i64 2
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !504, !noalias !498
  %38 = getelementptr inbounds double, double* %34, i64 4
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !504, !noalias !498
  %40 = getelementptr inbounds double, double* %34, i64 6
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !504, !noalias !498
  %42 = getelementptr inbounds double, double* %34, i64 8
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !504, !noalias !498
  %44 = getelementptr inbounds double, double* %34, i64 10
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !504, !noalias !498
  %46 = fadd <2 x double> %wide.load, %wide.load76
  %47 = fadd <2 x double> %wide.load71, %wide.load77
  %48 = fadd <2 x double> %wide.load72, %wide.load78
  %49 = fadd <2 x double> %wide.load73, %wide.load79
  %50 = fadd <2 x double> %wide.load74, %wide.load80
  %51 = fadd <2 x double> %wide.load75, %wide.load81
  %52 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  %54 = getelementptr inbounds double, double* %52, i64 2
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  %56 = getelementptr inbounds double, double* %52, i64 4
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  %58 = getelementptr inbounds double, double* %52, i64 6
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  %60 = getelementptr inbounds double, double* %52, i64 8
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  %62 = getelementptr inbounds double, double* %52, i64 10
  %63 = bitcast double* %62 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  %64 = fadd <2 x double> %46, %wide.load82
  %65 = fadd <2 x double> %47, %wide.load83
  %66 = fadd <2 x double> %48, %wide.load84
  %67 = fadd <2 x double> %49, %wide.load85
  %68 = fadd <2 x double> %50, %wide.load86
  %69 = fadd <2 x double> %51, %wide.load87
  store <2 x double> %64, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  store <2 x double> %65, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  store <2 x double> %66, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  store <2 x double> %67, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  store <2 x double> %68, <2 x double>* %61, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  store <2 x double> %69, <2 x double>* %63, align 8, !tbaa !62, !alias.scope !506, !noalias !508
  %70 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %71 = bitcast double* %70 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %71, align 8, !tbaa !62, !alias.scope !511, !noalias !498
  %72 = getelementptr inbounds double, double* %70, i64 2
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !511, !noalias !498
  %74 = getelementptr inbounds double, double* %70, i64 4
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !511, !noalias !498
  %76 = getelementptr inbounds double, double* %70, i64 6
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !511, !noalias !498
  %78 = getelementptr inbounds double, double* %70, i64 8
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !511, !noalias !498
  %80 = getelementptr inbounds double, double* %70, i64 10
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !511, !noalias !498
  %82 = fadd <2 x double> %wide.load76, %wide.load94
  %83 = fadd <2 x double> %wide.load77, %wide.load95
  %84 = fadd <2 x double> %wide.load78, %wide.load96
  %85 = fadd <2 x double> %wide.load79, %wide.load97
  %86 = fadd <2 x double> %wide.load80, %wide.load98
  %87 = fadd <2 x double> %wide.load81, %wide.load99
  %88 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  %90 = getelementptr inbounds double, double* %88, i64 2
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  %92 = getelementptr inbounds double, double* %88, i64 4
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  %94 = getelementptr inbounds double, double* %88, i64 6
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  %96 = getelementptr inbounds double, double* %88, i64 8
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  %98 = getelementptr inbounds double, double* %88, i64 10
  %99 = bitcast double* %98 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  %100 = fadd <2 x double> %82, %wide.load100
  %101 = fadd <2 x double> %83, %wide.load101
  %102 = fadd <2 x double> %84, %wide.load102
  %103 = fadd <2 x double> %85, %wide.load103
  %104 = fadd <2 x double> %86, %wide.load104
  %105 = fadd <2 x double> %87, %wide.load105
  store <2 x double> %100, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  store <2 x double> %101, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  store <2 x double> %102, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  store <2 x double> %103, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  store <2 x double> %104, <2 x double>* %97, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  store <2 x double> %105, <2 x double>* %99, align 8, !tbaa !62, !alias.scope !512, !noalias !513
  %index.next = add i64 %index, 12
  %106 = icmp eq i64 %index.next, %n.vec
  br i1 %106, label %middle.block, label %vector.body, !llvm.loop !514

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %17
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %.omp_outlined..151.exit.us, label %omp.inner.for.body.i.us.preheader

omp.inner.for.body.i.us.preheader:                ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader.i.us
  %indvars.iv.i.us.ph = phi i64 [ %ind.end, %middle.block ], [ %17, %omp.inner.for.body.preheader.i.us ], [ %17, %vector.memcheck ]
  br label %omp.inner.for.body.i.us

omp.inner.for.body.i.us:                          ; preds = %omp.inner.for.body.i.us.preheader, %omp.inner.for.body.i.us
  %indvars.iv.i.us = phi i64 [ %indvars.iv.next.i.us, %omp.inner.for.body.i.us ], [ %indvars.iv.i.us.ph, %omp.inner.for.body.i.us.preheader ]
  %arrayidx.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv.i.us
  %107 = load double, double* %arrayidx.i.us, align 8, !tbaa !62, !noalias !498
  %arrayidx13.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv.i.us
  %108 = load double, double* %arrayidx13.i.us, align 8, !tbaa !62, !noalias !498
  %add14.i.us = fadd double %107, %108
  %arrayidx18.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv.i.us
  %109 = load double, double* %arrayidx18.i.us, align 8, !tbaa !62, !noalias !498
  %add19.i.us = fadd double %109, %add14.i.us
  store double %add19.i.us, double* %arrayidx18.i.us, align 8, !tbaa !62, !noalias !498
  %110 = load double, double* %arrayidx13.i.us, align 8, !tbaa !62, !noalias !498
  %arrayidx27.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv.i.us
  %111 = load double, double* %arrayidx27.i.us, align 8, !tbaa !62, !noalias !498
  %add28.i.us = fadd double %110, %111
  %arrayidx32.i.us = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv.i.us
  %112 = load double, double* %arrayidx32.i.us, align 8, !tbaa !62, !noalias !498
  %add33.i.us = fadd double %112, %add28.i.us
  store double %add33.i.us, double* %arrayidx32.i.us, align 8, !tbaa !62, !noalias !498
  %indvars.iv.next.i.us = add nsw i64 %indvars.iv.i.us, 1
  %cmp4.i.us = icmp slt i64 %indvars.iv.i.us, %18
  br i1 %cmp4.i.us, label %omp.inner.for.body.i.us, label %.omp_outlined..151.exit.us, !llvm.loop !515

.omp_outlined..151.exit.us:                       ; preds = %omp.inner.for.body.i.us, %middle.block, %omp.inner.for.body.us
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6, !noalias !498
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %11) #6, !noalias !498
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %10) #6, !noalias !498
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %9) #6, !noalias !498
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %8) #6, !noalias !498
  call void @__kmpc_end_serialized_parallel(%struct.ident_t* nonnull @2, i32 %4) #6
  %113 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add.us = add nsw i32 %113, %.omp.iv.014.us
  %114 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6.us = icmp sgt i32 %add.us, %114
  br i1 %cmp6.us, label %omp.loop.exit, label %omp.inner.for.body.us

omp.inner.for.body:                               ; preds = %omp.inner.for.body.lr.ph, %omp.inner.for.body
  %.omp.iv.014 = phi i32 [ %add, %omp.inner.for.body ], [ %7, %omp.inner.for.body.lr.ph ]
  call void @__kmpc_push_num_threads(%struct.ident_t* nonnull @2, i32 %4, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %115 = load i32, i32* %.omp.comb.lb, align 4
  %116 = load i32, i32* %.omp.comb.ub, align 4
  %117 = zext i32 %116 to i64
  %118 = zext i32 %115 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*, [3072 x double]*)* @.omp_outlined..151 to void (i32*, i32*, ...)*), i64 %118, i64 %117, [3072 x double]* nonnull %A, [3072 x double]* nonnull %C, [3072 x double]* nonnull %D, [3072 x double]* nonnull %B, [3072 x double]* nonnull %E) #6
  %119 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add = add nsw i32 %119, %.omp.iv.014
  %120 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6 = icmp sgt i32 %add, %120
  br i1 %cmp6, label %omp.loop.exit, label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %.omp_outlined..151.exit.us, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..151(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., [3072 x double]* nocapture dereferenceable(24576) %A, [3072 x double]* nocapture readonly dereferenceable(24576) %C, [3072 x double]* nocapture readonly dereferenceable(24576) %D, [3072 x double]* nocapture dereferenceable(24576) %B, [3072 x double]* nocapture readonly dereferenceable(24576) %E) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv = trunc i64 %.previous.lb. to i32
  %conv2 = trunc i64 %.previous.ub. to i32
  store i32 %conv, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv2, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %6 = icmp slt i32 %5, 3071
  %cond = select i1 %6, i32 %5, i32 3071
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp451 = icmp sgt i32 %7, %cond
  br i1 %cmp451, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %entry
  %8 = sext i32 %7 to i64
  %9 = sext i32 %cond to i64
  %10 = icmp sgt i64 %9, %8
  %smax = select i1 %10, i64 %9, i64 %8
  %11 = add nsw i64 %smax, 1
  %12 = sub nsw i64 %11, %8
  %min.iters.check = icmp ult i64 %12, 12
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %8
  %scevgep55 = getelementptr [3072 x double], [3072 x double]* %A, i64 0, i64 %11
  %scevgep57 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %8
  %scevgep59 = getelementptr [3072 x double], [3072 x double]* %B, i64 0, i64 %11
  %scevgep61 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %8
  %scevgep63 = getelementptr [3072 x double], [3072 x double]* %C, i64 0, i64 %11
  %scevgep65 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %8
  %scevgep67 = getelementptr [3072 x double], [3072 x double]* %D, i64 0, i64 %11
  %scevgep69 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %8
  %scevgep71 = getelementptr [3072 x double], [3072 x double]* %E, i64 0, i64 %11
  %bound0 = icmp ult double* %scevgep, %scevgep59
  %bound1 = icmp ult double* %scevgep57, %scevgep55
  %found.conflict = and i1 %bound0, %bound1
  %bound073 = icmp ult double* %scevgep, %scevgep63
  %bound174 = icmp ult double* %scevgep61, %scevgep55
  %found.conflict75 = and i1 %bound073, %bound174
  %conflict.rdx = or i1 %found.conflict, %found.conflict75
  %bound076 = icmp ult double* %scevgep, %scevgep67
  %bound177 = icmp ult double* %scevgep65, %scevgep55
  %found.conflict78 = and i1 %bound076, %bound177
  %conflict.rdx79 = or i1 %found.conflict78, %conflict.rdx
  %bound080 = icmp ult double* %scevgep, %scevgep71
  %bound181 = icmp ult double* %scevgep69, %scevgep55
  %found.conflict82 = and i1 %bound080, %bound181
  %conflict.rdx83 = or i1 %found.conflict82, %conflict.rdx79
  %bound084 = icmp ult double* %scevgep57, %scevgep63
  %bound185 = icmp ult double* %scevgep61, %scevgep59
  %found.conflict86 = and i1 %bound084, %bound185
  %conflict.rdx87 = or i1 %found.conflict86, %conflict.rdx83
  %bound088 = icmp ult double* %scevgep57, %scevgep67
  %bound189 = icmp ult double* %scevgep65, %scevgep59
  %found.conflict90 = and i1 %bound088, %bound189
  %conflict.rdx91 = or i1 %found.conflict90, %conflict.rdx87
  %bound092 = icmp ult double* %scevgep57, %scevgep71
  %bound193 = icmp ult double* %scevgep69, %scevgep59
  %found.conflict94 = and i1 %bound092, %bound193
  %conflict.rdx95 = or i1 %found.conflict94, %conflict.rdx91
  br i1 %conflict.rdx95, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %12, 12
  %n.vec = sub nsw i64 %12, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %8
  %13 = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %offset.idx
  %14 = bitcast double* %13 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %14, align 8, !tbaa !62, !alias.scope !516
  %15 = getelementptr inbounds double, double* %13, i64 2
  %16 = bitcast double* %15 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %16, align 8, !tbaa !62, !alias.scope !516
  %17 = getelementptr inbounds double, double* %13, i64 4
  %18 = bitcast double* %17 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %18, align 8, !tbaa !62, !alias.scope !516
  %19 = getelementptr inbounds double, double* %13, i64 6
  %20 = bitcast double* %19 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %20, align 8, !tbaa !62, !alias.scope !516
  %21 = getelementptr inbounds double, double* %13, i64 8
  %22 = bitcast double* %21 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %22, align 8, !tbaa !62, !alias.scope !516
  %23 = getelementptr inbounds double, double* %13, i64 10
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !516
  %25 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %offset.idx
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !519
  %27 = getelementptr inbounds double, double* %25, i64 2
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !519
  %29 = getelementptr inbounds double, double* %25, i64 4
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !519
  %31 = getelementptr inbounds double, double* %25, i64 6
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !519
  %33 = getelementptr inbounds double, double* %25, i64 8
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load110 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !519
  %35 = getelementptr inbounds double, double* %25, i64 10
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load111 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !519
  %37 = fadd <2 x double> %wide.load, %wide.load106
  %38 = fadd <2 x double> %wide.load101, %wide.load107
  %39 = fadd <2 x double> %wide.load102, %wide.load108
  %40 = fadd <2 x double> %wide.load103, %wide.load109
  %41 = fadd <2 x double> %wide.load104, %wide.load110
  %42 = fadd <2 x double> %wide.load105, %wide.load111
  %43 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %offset.idx
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load112 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  %45 = getelementptr inbounds double, double* %43, i64 2
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load113 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  %47 = getelementptr inbounds double, double* %43, i64 4
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load114 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  %49 = getelementptr inbounds double, double* %43, i64 6
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load115 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  %51 = getelementptr inbounds double, double* %43, i64 8
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load116 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  %53 = getelementptr inbounds double, double* %43, i64 10
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load117 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  %55 = fadd <2 x double> %37, %wide.load112
  %56 = fadd <2 x double> %38, %wide.load113
  %57 = fadd <2 x double> %39, %wide.load114
  %58 = fadd <2 x double> %40, %wide.load115
  %59 = fadd <2 x double> %41, %wide.load116
  %60 = fadd <2 x double> %42, %wide.load117
  store <2 x double> %55, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  store <2 x double> %56, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  store <2 x double> %57, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  store <2 x double> %58, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  store <2 x double> %59, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  store <2 x double> %60, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !521, !noalias !523
  %61 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %offset.idx
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load124 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !526
  %63 = getelementptr inbounds double, double* %61, i64 2
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load125 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !526
  %65 = getelementptr inbounds double, double* %61, i64 4
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load126 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !526
  %67 = getelementptr inbounds double, double* %61, i64 6
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load127 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !526
  %69 = getelementptr inbounds double, double* %61, i64 8
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load128 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !526
  %71 = getelementptr inbounds double, double* %61, i64 10
  %72 = bitcast double* %71 to <2 x double>*
  %wide.load129 = load <2 x double>, <2 x double>* %72, align 8, !tbaa !62, !alias.scope !526
  %73 = fadd <2 x double> %wide.load106, %wide.load124
  %74 = fadd <2 x double> %wide.load107, %wide.load125
  %75 = fadd <2 x double> %wide.load108, %wide.load126
  %76 = fadd <2 x double> %wide.load109, %wide.load127
  %77 = fadd <2 x double> %wide.load110, %wide.load128
  %78 = fadd <2 x double> %wide.load111, %wide.load129
  %79 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %offset.idx
  %80 = bitcast double* %79 to <2 x double>*
  %wide.load130 = load <2 x double>, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  %81 = getelementptr inbounds double, double* %79, i64 2
  %82 = bitcast double* %81 to <2 x double>*
  %wide.load131 = load <2 x double>, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  %83 = getelementptr inbounds double, double* %79, i64 4
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load132 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  %85 = getelementptr inbounds double, double* %79, i64 6
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load133 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  %87 = getelementptr inbounds double, double* %79, i64 8
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load134 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  %89 = getelementptr inbounds double, double* %79, i64 10
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load135 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  %91 = fadd <2 x double> %73, %wide.load130
  %92 = fadd <2 x double> %74, %wide.load131
  %93 = fadd <2 x double> %75, %wide.load132
  %94 = fadd <2 x double> %76, %wide.load133
  %95 = fadd <2 x double> %77, %wide.load134
  %96 = fadd <2 x double> %78, %wide.load135
  store <2 x double> %91, <2 x double>* %80, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  store <2 x double> %92, <2 x double>* %82, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  store <2 x double> %93, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  store <2 x double> %94, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  store <2 x double> %95, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  store <2 x double> %96, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !527, !noalias !528
  %index.next = add i64 %index, 12
  %97 = icmp eq i64 %index.next, %n.vec
  br i1 %97, label %middle.block, label %vector.body, !llvm.loop !529

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %8
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %8, %omp.inner.for.body.preheader ], [ %8, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds [3072 x double], [3072 x double]* %C, i64 0, i64 %indvars.iv
  %98 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx13 = getelementptr inbounds [3072 x double], [3072 x double]* %D, i64 0, i64 %indvars.iv
  %99 = load double, double* %arrayidx13, align 8, !tbaa !62
  %add14 = fadd double %98, %99
  %arrayidx18 = getelementptr inbounds [3072 x double], [3072 x double]* %A, i64 0, i64 %indvars.iv
  %100 = load double, double* %arrayidx18, align 8, !tbaa !62
  %add19 = fadd double %100, %add14
  store double %add19, double* %arrayidx18, align 8, !tbaa !62
  %101 = load double, double* %arrayidx13, align 8, !tbaa !62
  %arrayidx27 = getelementptr inbounds [3072 x double], [3072 x double]* %E, i64 0, i64 %indvars.iv
  %102 = load double, double* %arrayidx27, align 8, !tbaa !62
  %add28 = fadd double %101, %102
  %arrayidx32 = getelementptr inbounds [3072 x double], [3072 x double]* %B, i64 0, i64 %indvars.iv
  %103 = load double, double* %arrayidx32, align 8, !tbaa !62
  %add33 = fadd double %103, %add28
  store double %add33, double* %arrayidx32, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp4 = icmp slt i64 %indvars.iv, %9
  br i1 %cmp4, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !530

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %entry
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  ret void
}

; Function Attrs: nounwind
declare noalias i8* @malloc(i64) local_unnamed_addr #4

declare void @__tgt_target_data_begin(i64, i32, i8**, i8**, i64*, i64*) local_unnamed_addr

declare void @__tgt_target_data_update(i64, i32, i8**, i8**, i64*, i64*) local_unnamed_addr

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..158(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub2 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre25 = bitcast i32* %.omp.is_last to i8*
  %.pre26 = bitcast i32* %.omp.stride to i8*
  %.pre28 = bitcast i32* %.omp.comb.ub to i8*
  %.pre30 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub2, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6 = icmp sgt i32 %5, %sub2
  %cond = select i1 %cmp6, i32 %sub2, i32 %5
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp823 = icmp sgt i32 %6, %cond
  br i1 %cmp823, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.omp.inner.for.body_crit_edge, %omp.inner.for.body.lr.ph
  %7 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %.pre, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %8 = phi i32 [ %cond, %omp.inner.for.body.lr.ph ], [ %12, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %.omp.iv.024 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %add11, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %9 = zext i32 %7 to i64
  %10 = zext i32 %8 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 6, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*)* @.omp_outlined..159 to void (i32*, i32*, ...)*), i64 %9, i64 %10, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c) #6
  %11 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add11 = add nsw i32 %11, %.omp.iv.024
  %12 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %add11, %12
  br i1 %cmp8, label %omp.loop.exit, label %omp.inner.for.body.omp.inner.for.body_crit_edge

omp.inner.for.body.omp.inner.for.body_crit_edge:  ; preds = %omp.inner.for.body
  %.pre = load i32, i32* %.omp.comb.lb, align 4
  br label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.loop.exit
  %.pre-phi31 = phi i8* [ %.pre30, %entry.omp.precond.end_crit_edge ], [ %0, %omp.loop.exit ]
  %.pre-phi29 = phi i8* [ %.pre28, %entry.omp.precond.end_crit_edge ], [ %1, %omp.loop.exit ]
  %.pre-phi27 = phi i8* [ %.pre26, %entry.omp.precond.end_crit_edge ], [ %2, %omp.loop.exit ]
  %.pre-phi = phi i8* [ %.pre25, %entry.omp.precond.end_crit_edge ], [ %3, %omp.loop.exit ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi27) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi29) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi31) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..159(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub2 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv5 = trunc i64 %.previous.lb. to i32
  %conv6 = trunc i64 %.previous.ub. to i32
  store i32 %conv5, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv6, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %5, %sub2
  %cond = select i1 %cmp8, i32 %sub2, i32 %5
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp1033 = icmp sgt i32 %6, %cond
  br i1 %cmp1033, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %omp.precond.then
  %7 = sext i32 %6 to i64
  %8 = sext i32 %cond to i64
  %9 = icmp sgt i64 %7, %8
  %smax = select i1 %9, i64 %7, i64 %8
  %10 = add nsw i64 %smax, 1
  %11 = sub nsw i64 %10, %7
  %min.iters.check = icmp ult i64 %11, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr double, double* %a, i64 %7
  %scevgep37 = getelementptr double, double* %a, i64 %10
  %scevgep39 = getelementptr double, double* %b, i64 %7
  %scevgep41 = getelementptr double, double* %b, i64 %10
  %scevgep43 = getelementptr double, double* %c, i64 %7
  %scevgep45 = getelementptr double, double* %c, i64 %10
  %bound0 = icmp ult double* %scevgep, %scevgep41
  %bound1 = icmp ult double* %scevgep39, %scevgep37
  %found.conflict = and i1 %bound0, %bound1
  %bound047 = icmp ult double* %scevgep, %scevgep45
  %bound148 = icmp ult double* %scevgep43, %scevgep37
  %found.conflict49 = and i1 %bound047, %bound148
  %conflict.rdx = or i1 %found.conflict, %found.conflict49
  br i1 %conflict.rdx, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %11, 24
  %n.vec = sub nsw i64 %11, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %7
  %12 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %13 = bitcast double* %12 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %13, align 8, !tbaa !62, !alias.scope !531
  %14 = getelementptr inbounds double, double* %12, i64 2
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62, !alias.scope !531
  %16 = getelementptr inbounds double, double* %12, i64 4
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62, !alias.scope !531
  %18 = getelementptr inbounds double, double* %12, i64 6
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62, !alias.scope !531
  %20 = getelementptr inbounds double, double* %12, i64 8
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62, !alias.scope !531
  %22 = getelementptr inbounds double, double* %12, i64 10
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !531
  %24 = getelementptr inbounds double, double* %12, i64 12
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !531
  %26 = getelementptr inbounds double, double* %12, i64 14
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !531
  %28 = getelementptr inbounds double, double* %12, i64 16
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !531
  %30 = getelementptr inbounds double, double* %12, i64 18
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !531
  %32 = getelementptr inbounds double, double* %12, i64 20
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !531
  %34 = getelementptr inbounds double, double* %12, i64 22
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !531
  %36 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !534
  %38 = getelementptr inbounds double, double* %36, i64 2
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !534
  %40 = getelementptr inbounds double, double* %36, i64 4
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !534
  %42 = getelementptr inbounds double, double* %36, i64 6
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !534
  %44 = getelementptr inbounds double, double* %36, i64 8
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !534
  %46 = getelementptr inbounds double, double* %36, i64 10
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62, !alias.scope !534
  %48 = getelementptr inbounds double, double* %36, i64 12
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62, !alias.scope !534
  %50 = getelementptr inbounds double, double* %36, i64 14
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62, !alias.scope !534
  %52 = getelementptr inbounds double, double* %36, i64 16
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !534
  %54 = getelementptr inbounds double, double* %36, i64 18
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !534
  %56 = getelementptr inbounds double, double* %36, i64 20
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !534
  %58 = getelementptr inbounds double, double* %36, i64 22
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !534
  %60 = fadd <2 x double> %wide.load, %wide.load72
  %61 = fadd <2 x double> %wide.load61, %wide.load73
  %62 = fadd <2 x double> %wide.load62, %wide.load74
  %63 = fadd <2 x double> %wide.load63, %wide.load75
  %64 = fadd <2 x double> %wide.load64, %wide.load76
  %65 = fadd <2 x double> %wide.load65, %wide.load77
  %66 = fadd <2 x double> %wide.load66, %wide.load78
  %67 = fadd <2 x double> %wide.load67, %wide.load79
  %68 = fadd <2 x double> %wide.load68, %wide.load80
  %69 = fadd <2 x double> %wide.load69, %wide.load81
  %70 = fadd <2 x double> %wide.load70, %wide.load82
  %71 = fadd <2 x double> %wide.load71, %wide.load83
  %72 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %74 = getelementptr inbounds double, double* %72, i64 2
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %76 = getelementptr inbounds double, double* %72, i64 4
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %78 = getelementptr inbounds double, double* %72, i64 6
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %80 = getelementptr inbounds double, double* %72, i64 8
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %82 = getelementptr inbounds double, double* %72, i64 10
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %84 = getelementptr inbounds double, double* %72, i64 12
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %86 = getelementptr inbounds double, double* %72, i64 14
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %88 = getelementptr inbounds double, double* %72, i64 16
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %90 = getelementptr inbounds double, double* %72, i64 18
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %92 = getelementptr inbounds double, double* %72, i64 20
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %94 = getelementptr inbounds double, double* %72, i64 22
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %96 = fadd <2 x double> %60, %wide.load84
  %97 = fadd <2 x double> %61, %wide.load85
  %98 = fadd <2 x double> %62, %wide.load86
  %99 = fadd <2 x double> %63, %wide.load87
  %100 = fadd <2 x double> %64, %wide.load88
  %101 = fadd <2 x double> %65, %wide.load89
  %102 = fadd <2 x double> %66, %wide.load90
  %103 = fadd <2 x double> %67, %wide.load91
  %104 = fadd <2 x double> %68, %wide.load92
  %105 = fadd <2 x double> %69, %wide.load93
  %106 = fadd <2 x double> %70, %wide.load94
  %107 = fadd <2 x double> %71, %wide.load95
  store <2 x double> %96, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  store <2 x double> %97, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  store <2 x double> %98, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  store <2 x double> %99, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  store <2 x double> %100, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  store <2 x double> %101, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  store <2 x double> %102, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  store <2 x double> %103, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  store <2 x double> %104, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  store <2 x double> %105, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  store <2 x double> %106, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  store <2 x double> %107, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !536, !noalias !538
  %index.next = add i64 %index, 24
  %108 = icmp eq i64 %index.next, %n.vec
  br i1 %108, label %middle.block, label %vector.body, !llvm.loop !539

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %7
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %7, %omp.inner.for.body.preheader ], [ %7, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %109 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx14 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %110 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %109, %110
  %arrayidx17 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %111 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %111, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp10 = icmp slt i64 %indvars.iv, %8
  br i1 %cmp10, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !540

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.loop.exit, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..166(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub2 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre25 = bitcast i32* %.omp.is_last to i8*
  %.pre26 = bitcast i32* %.omp.stride to i8*
  %.pre28 = bitcast i32* %.omp.comb.ub to i8*
  %.pre30 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub2, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6 = icmp sgt i32 %5, %sub2
  %cond = select i1 %cmp6, i32 %sub2, i32 %5
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp823 = icmp sgt i32 %6, %cond
  br i1 %cmp823, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.omp.inner.for.body_crit_edge, %omp.inner.for.body.lr.ph
  %7 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %.pre, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %8 = phi i32 [ %cond, %omp.inner.for.body.lr.ph ], [ %12, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %.omp.iv.024 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %add11, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %9 = zext i32 %7 to i64
  %10 = zext i32 %8 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 6, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*)* @.omp_outlined..167 to void (i32*, i32*, ...)*), i64 %9, i64 %10, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c) #6
  %11 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add11 = add nsw i32 %11, %.omp.iv.024
  %12 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %add11, %12
  br i1 %cmp8, label %omp.loop.exit, label %omp.inner.for.body.omp.inner.for.body_crit_edge

omp.inner.for.body.omp.inner.for.body_crit_edge:  ; preds = %omp.inner.for.body
  %.pre = load i32, i32* %.omp.comb.lb, align 4
  br label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.loop.exit
  %.pre-phi31 = phi i8* [ %.pre30, %entry.omp.precond.end_crit_edge ], [ %0, %omp.loop.exit ]
  %.pre-phi29 = phi i8* [ %.pre28, %entry.omp.precond.end_crit_edge ], [ %1, %omp.loop.exit ]
  %.pre-phi27 = phi i8* [ %.pre26, %entry.omp.precond.end_crit_edge ], [ %2, %omp.loop.exit ]
  %.pre-phi = phi i8* [ %.pre25, %entry.omp.precond.end_crit_edge ], [ %3, %omp.loop.exit ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi27) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi29) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi31) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..167(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub2 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv5 = trunc i64 %.previous.lb. to i32
  %conv6 = trunc i64 %.previous.ub. to i32
  store i32 %conv5, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv6, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %5, %sub2
  %cond = select i1 %cmp8, i32 %sub2, i32 %5
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp1033 = icmp sgt i32 %6, %cond
  br i1 %cmp1033, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %omp.precond.then
  %7 = sext i32 %6 to i64
  %8 = sext i32 %cond to i64
  %9 = icmp sgt i64 %7, %8
  %smax = select i1 %9, i64 %7, i64 %8
  %10 = add nsw i64 %smax, 1
  %11 = sub nsw i64 %10, %7
  %min.iters.check = icmp ult i64 %11, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr double, double* %a, i64 %7
  %scevgep37 = getelementptr double, double* %a, i64 %10
  %scevgep39 = getelementptr double, double* %b, i64 %7
  %scevgep41 = getelementptr double, double* %b, i64 %10
  %scevgep43 = getelementptr double, double* %c, i64 %7
  %scevgep45 = getelementptr double, double* %c, i64 %10
  %bound0 = icmp ult double* %scevgep, %scevgep41
  %bound1 = icmp ult double* %scevgep39, %scevgep37
  %found.conflict = and i1 %bound0, %bound1
  %bound047 = icmp ult double* %scevgep, %scevgep45
  %bound148 = icmp ult double* %scevgep43, %scevgep37
  %found.conflict49 = and i1 %bound047, %bound148
  %conflict.rdx = or i1 %found.conflict, %found.conflict49
  br i1 %conflict.rdx, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %11, 24
  %n.vec = sub nsw i64 %11, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %7
  %12 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %13 = bitcast double* %12 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %13, align 8, !tbaa !62, !alias.scope !541
  %14 = getelementptr inbounds double, double* %12, i64 2
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62, !alias.scope !541
  %16 = getelementptr inbounds double, double* %12, i64 4
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62, !alias.scope !541
  %18 = getelementptr inbounds double, double* %12, i64 6
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62, !alias.scope !541
  %20 = getelementptr inbounds double, double* %12, i64 8
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62, !alias.scope !541
  %22 = getelementptr inbounds double, double* %12, i64 10
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !541
  %24 = getelementptr inbounds double, double* %12, i64 12
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !541
  %26 = getelementptr inbounds double, double* %12, i64 14
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !541
  %28 = getelementptr inbounds double, double* %12, i64 16
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !541
  %30 = getelementptr inbounds double, double* %12, i64 18
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !541
  %32 = getelementptr inbounds double, double* %12, i64 20
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !541
  %34 = getelementptr inbounds double, double* %12, i64 22
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !541
  %36 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !544
  %38 = getelementptr inbounds double, double* %36, i64 2
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !544
  %40 = getelementptr inbounds double, double* %36, i64 4
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !544
  %42 = getelementptr inbounds double, double* %36, i64 6
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !544
  %44 = getelementptr inbounds double, double* %36, i64 8
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !544
  %46 = getelementptr inbounds double, double* %36, i64 10
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62, !alias.scope !544
  %48 = getelementptr inbounds double, double* %36, i64 12
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62, !alias.scope !544
  %50 = getelementptr inbounds double, double* %36, i64 14
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62, !alias.scope !544
  %52 = getelementptr inbounds double, double* %36, i64 16
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !544
  %54 = getelementptr inbounds double, double* %36, i64 18
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !544
  %56 = getelementptr inbounds double, double* %36, i64 20
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !544
  %58 = getelementptr inbounds double, double* %36, i64 22
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !544
  %60 = fadd <2 x double> %wide.load, %wide.load72
  %61 = fadd <2 x double> %wide.load61, %wide.load73
  %62 = fadd <2 x double> %wide.load62, %wide.load74
  %63 = fadd <2 x double> %wide.load63, %wide.load75
  %64 = fadd <2 x double> %wide.load64, %wide.load76
  %65 = fadd <2 x double> %wide.load65, %wide.load77
  %66 = fadd <2 x double> %wide.load66, %wide.load78
  %67 = fadd <2 x double> %wide.load67, %wide.load79
  %68 = fadd <2 x double> %wide.load68, %wide.load80
  %69 = fadd <2 x double> %wide.load69, %wide.load81
  %70 = fadd <2 x double> %wide.load70, %wide.load82
  %71 = fadd <2 x double> %wide.load71, %wide.load83
  %72 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %74 = getelementptr inbounds double, double* %72, i64 2
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %76 = getelementptr inbounds double, double* %72, i64 4
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %78 = getelementptr inbounds double, double* %72, i64 6
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %80 = getelementptr inbounds double, double* %72, i64 8
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %82 = getelementptr inbounds double, double* %72, i64 10
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %84 = getelementptr inbounds double, double* %72, i64 12
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %86 = getelementptr inbounds double, double* %72, i64 14
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %88 = getelementptr inbounds double, double* %72, i64 16
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %90 = getelementptr inbounds double, double* %72, i64 18
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %92 = getelementptr inbounds double, double* %72, i64 20
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %94 = getelementptr inbounds double, double* %72, i64 22
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %96 = fadd <2 x double> %60, %wide.load84
  %97 = fadd <2 x double> %61, %wide.load85
  %98 = fadd <2 x double> %62, %wide.load86
  %99 = fadd <2 x double> %63, %wide.load87
  %100 = fadd <2 x double> %64, %wide.load88
  %101 = fadd <2 x double> %65, %wide.load89
  %102 = fadd <2 x double> %66, %wide.load90
  %103 = fadd <2 x double> %67, %wide.load91
  %104 = fadd <2 x double> %68, %wide.load92
  %105 = fadd <2 x double> %69, %wide.load93
  %106 = fadd <2 x double> %70, %wide.load94
  %107 = fadd <2 x double> %71, %wide.load95
  store <2 x double> %96, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  store <2 x double> %97, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  store <2 x double> %98, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  store <2 x double> %99, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  store <2 x double> %100, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  store <2 x double> %101, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  store <2 x double> %102, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  store <2 x double> %103, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  store <2 x double> %104, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  store <2 x double> %105, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  store <2 x double> %106, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  store <2 x double> %107, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !546, !noalias !548
  %index.next = add i64 %index, 24
  %108 = icmp eq i64 %index.next, %n.vec
  br i1 %108, label %middle.block, label %vector.body, !llvm.loop !549

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %7
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %7, %omp.inner.for.body.preheader ], [ %7, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %109 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx14 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %110 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %109, %110
  %arrayidx17 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %111 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %111, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp10 = icmp slt i64 %indvars.iv, %8
  br i1 %cmp10, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !550

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.loop.exit, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..173(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub4 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre28 = bitcast i32* %.omp.is_last to i8*
  %.pre29 = bitcast i32* %.omp.stride to i8*
  %.pre31 = bitcast i32* %.omp.comb.ub to i8*
  %.pre33 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub4, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %5, %sub4
  %cond = select i1 %cmp8, i32 %sub4, i32 %5
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1026 = icmp sgt i32 %6, %cond
  br i1 %cmp1026, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.omp.inner.for.body_crit_edge, %omp.inner.for.body.lr.ph
  %7 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %.pre, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %8 = phi i32 [ %cond, %omp.inner.for.body.lr.ph ], [ %12, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %.omp.iv.027 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %add14, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %9 = zext i32 %7 to i64
  %10 = zext i32 %8 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64)* @.omp_outlined..174 to void (i32*, i32*, ...)*), i64 %9, i64 %10, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext) #6
  %11 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add14 = add nsw i32 %11, %.omp.iv.027
  %12 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp10 = icmp sgt i32 %add14, %12
  br i1 %cmp10, label %omp.loop.exit, label %omp.inner.for.body.omp.inner.for.body_crit_edge

omp.inner.for.body.omp.inner.for.body_crit_edge:  ; preds = %omp.inner.for.body
  %.pre = load i32, i32* %.omp.comb.lb, align 4
  br label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.loop.exit
  %.pre-phi34 = phi i8* [ %.pre33, %entry.omp.precond.end_crit_edge ], [ %0, %omp.loop.exit ]
  %.pre-phi32 = phi i8* [ %.pre31, %entry.omp.precond.end_crit_edge ], [ %1, %omp.loop.exit ]
  %.pre-phi30 = phi i8* [ %.pre29, %entry.omp.precond.end_crit_edge ], [ %2, %omp.loop.exit ]
  %.pre-phi = phi i8* [ %.pre28, %entry.omp.precond.end_crit_edge ], [ %3, %omp.loop.exit ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi30) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi32) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi34) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..174(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv7 = trunc i64 %.previous.lb. to i32
  %conv8 = trunc i64 %.previous.ub. to i32
  store i32 %conv7, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv8, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 33, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %conv1045 = sext i32 %5 to i64
  %cmp1146 = icmp ugt i64 %conv1045, %.previous.ub.
  %cond47 = select i1 %cmp1146, i64 %.previous.ub., i64 %conv1045
  %conv1448 = trunc i64 %cond47 to i32
  store i32 %conv1448, i32* %.omp.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp1549 = icmp sgt i32 %6, %conv1448
  br i1 %cmp1549, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %7 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %8 = sext i32 %6 to i64
  %9 = sext i32 %7 to i64
  %10 = sub nsw i64 1, %8
  %11 = sub nsw i64 0, %9
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %indvar = phi i64 [ 0, %omp.inner.for.cond.preheader.lr.ph ], [ %indvar.next, %omp.dispatch.inc ]
  %indvars.iv = phi i64 [ %8, %omp.inner.for.cond.preheader.lr.ph ], [ %indvars.iv.next, %omp.dispatch.inc ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add27, %omp.dispatch.inc ]
  %conv1450 = phi i32 [ %conv1448, %omp.inner.for.cond.preheader.lr.ph ], [ %conv14, %omp.dispatch.inc ]
  %13 = mul i64 %indvar, %9
  %14 = add i64 %13, %8
  %scevgep = getelementptr double, double* %a, i64 %14
  %15 = add i64 %14, 1
  %scevgep54 = getelementptr double, double* %a, i64 %15
  %16 = mul i64 %indvar, %11
  %17 = sub i64 %16, %8
  %scevgep58 = getelementptr double, double* %b, i64 %14
  %scevgep60 = getelementptr double, double* %b, i64 %15
  %scevgep63 = getelementptr double, double* %c, i64 %14
  %scevgep65 = getelementptr double, double* %c, i64 %15
  %cmp1743 = icmp sgt i32 %12, %conv1450
  br i1 %cmp1743, label %omp.dispatch.inc, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %omp.inner.for.cond.preheader
  %18 = add i64 %10, %16
  %19 = sext i32 %conv1450 to i64
  %20 = icmp sgt i64 %14, %19
  %smax = select i1 %20, i64 %14, i64 %19
  %21 = add i64 %18, %smax
  %min.iters.check = icmp ult i64 %21, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %22 = add i64 %smax, %17
  %scevgep56 = getelementptr double, double* %scevgep54, i64 %22
  %scevgep61 = getelementptr double, double* %scevgep60, i64 %22
  %scevgep66 = getelementptr double, double* %scevgep65, i64 %22
  %bound0 = icmp ult double* %scevgep, %scevgep61
  %bound1 = icmp ult double* %scevgep58, %scevgep56
  %found.conflict = and i1 %bound0, %bound1
  %bound068 = icmp ult double* %scevgep, %scevgep66
  %bound169 = icmp ult double* %scevgep63, %scevgep56
  %found.conflict70 = and i1 %bound068, %bound169
  %conflict.rdx = or i1 %found.conflict, %found.conflict70
  br i1 %conflict.rdx, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 24
  %n.vec = sub i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %indvars.iv
  %23 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !551
  %25 = getelementptr inbounds double, double* %23, i64 2
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !551
  %27 = getelementptr inbounds double, double* %23, i64 4
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !551
  %29 = getelementptr inbounds double, double* %23, i64 6
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !551
  %31 = getelementptr inbounds double, double* %23, i64 8
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !551
  %33 = getelementptr inbounds double, double* %23, i64 10
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !551
  %35 = getelementptr inbounds double, double* %23, i64 12
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !551
  %37 = getelementptr inbounds double, double* %23, i64 14
  %38 = bitcast double* %37 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %38, align 8, !tbaa !62, !alias.scope !551
  %39 = getelementptr inbounds double, double* %23, i64 16
  %40 = bitcast double* %39 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %40, align 8, !tbaa !62, !alias.scope !551
  %41 = getelementptr inbounds double, double* %23, i64 18
  %42 = bitcast double* %41 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %42, align 8, !tbaa !62, !alias.scope !551
  %43 = getelementptr inbounds double, double* %23, i64 20
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !551
  %45 = getelementptr inbounds double, double* %23, i64 22
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !551
  %47 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !554
  %49 = getelementptr inbounds double, double* %47, i64 2
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !554
  %51 = getelementptr inbounds double, double* %47, i64 4
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !554
  %53 = getelementptr inbounds double, double* %47, i64 6
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !554
  %55 = getelementptr inbounds double, double* %47, i64 8
  %56 = bitcast double* %55 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %56, align 8, !tbaa !62, !alias.scope !554
  %57 = getelementptr inbounds double, double* %47, i64 10
  %58 = bitcast double* %57 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %58, align 8, !tbaa !62, !alias.scope !554
  %59 = getelementptr inbounds double, double* %47, i64 12
  %60 = bitcast double* %59 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %60, align 8, !tbaa !62, !alias.scope !554
  %61 = getelementptr inbounds double, double* %47, i64 14
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !554
  %63 = getelementptr inbounds double, double* %47, i64 16
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !554
  %65 = getelementptr inbounds double, double* %47, i64 18
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !554
  %67 = getelementptr inbounds double, double* %47, i64 20
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !554
  %69 = getelementptr inbounds double, double* %47, i64 22
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !554
  %71 = fadd <2 x double> %wide.load, %wide.load93
  %72 = fadd <2 x double> %wide.load82, %wide.load94
  %73 = fadd <2 x double> %wide.load83, %wide.load95
  %74 = fadd <2 x double> %wide.load84, %wide.load96
  %75 = fadd <2 x double> %wide.load85, %wide.load97
  %76 = fadd <2 x double> %wide.load86, %wide.load98
  %77 = fadd <2 x double> %wide.load87, %wide.load99
  %78 = fadd <2 x double> %wide.load88, %wide.load100
  %79 = fadd <2 x double> %wide.load89, %wide.load101
  %80 = fadd <2 x double> %wide.load90, %wide.load102
  %81 = fadd <2 x double> %wide.load91, %wide.load103
  %82 = fadd <2 x double> %wide.load92, %wide.load104
  %83 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %85 = getelementptr inbounds double, double* %83, i64 2
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %87 = getelementptr inbounds double, double* %83, i64 4
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %89 = getelementptr inbounds double, double* %83, i64 6
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %91 = getelementptr inbounds double, double* %83, i64 8
  %92 = bitcast double* %91 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %92, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %93 = getelementptr inbounds double, double* %83, i64 10
  %94 = bitcast double* %93 to <2 x double>*
  %wide.load110 = load <2 x double>, <2 x double>* %94, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %95 = getelementptr inbounds double, double* %83, i64 12
  %96 = bitcast double* %95 to <2 x double>*
  %wide.load111 = load <2 x double>, <2 x double>* %96, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %97 = getelementptr inbounds double, double* %83, i64 14
  %98 = bitcast double* %97 to <2 x double>*
  %wide.load112 = load <2 x double>, <2 x double>* %98, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %99 = getelementptr inbounds double, double* %83, i64 16
  %100 = bitcast double* %99 to <2 x double>*
  %wide.load113 = load <2 x double>, <2 x double>* %100, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %101 = getelementptr inbounds double, double* %83, i64 18
  %102 = bitcast double* %101 to <2 x double>*
  %wide.load114 = load <2 x double>, <2 x double>* %102, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %103 = getelementptr inbounds double, double* %83, i64 20
  %104 = bitcast double* %103 to <2 x double>*
  %wide.load115 = load <2 x double>, <2 x double>* %104, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %105 = getelementptr inbounds double, double* %83, i64 22
  %106 = bitcast double* %105 to <2 x double>*
  %wide.load116 = load <2 x double>, <2 x double>* %106, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %107 = fadd <2 x double> %71, %wide.load105
  %108 = fadd <2 x double> %72, %wide.load106
  %109 = fadd <2 x double> %73, %wide.load107
  %110 = fadd <2 x double> %74, %wide.load108
  %111 = fadd <2 x double> %75, %wide.load109
  %112 = fadd <2 x double> %76, %wide.load110
  %113 = fadd <2 x double> %77, %wide.load111
  %114 = fadd <2 x double> %78, %wide.load112
  %115 = fadd <2 x double> %79, %wide.load113
  %116 = fadd <2 x double> %80, %wide.load114
  %117 = fadd <2 x double> %81, %wide.load115
  %118 = fadd <2 x double> %82, %wide.load116
  store <2 x double> %107, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  store <2 x double> %108, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  store <2 x double> %109, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  store <2 x double> %110, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  store <2 x double> %111, <2 x double>* %92, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  store <2 x double> %112, <2 x double>* %94, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  store <2 x double> %113, <2 x double>* %96, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  store <2 x double> %114, <2 x double>* %98, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  store <2 x double> %115, <2 x double>* %100, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  store <2 x double> %116, <2 x double>* %102, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  store <2 x double> %117, <2 x double>* %104, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  store <2 x double> %118, <2 x double>* %106, align 8, !tbaa !62, !alias.scope !556, !noalias !558
  %index.next = add i64 %index, 24
  %119 = icmp eq i64 %index.next, %n.vec
  br i1 %119, label %middle.block, label %vector.body, !llvm.loop !559

middle.block:                                     ; preds = %vector.body
  %ind.end = add i64 %n.vec, %indvars.iv
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.inc, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv51.ph = phi i64 [ %ind.end, %middle.block ], [ %indvars.iv, %omp.inner.for.body.preheader ], [ %indvars.iv, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv51 = phi i64 [ %indvars.iv.next52, %omp.inner.for.body ], [ %indvars.iv51.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv51
  %120 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx21 = getelementptr inbounds double, double* %c, i64 %indvars.iv51
  %121 = load double, double* %arrayidx21, align 8, !tbaa !62
  %add22 = fadd double %120, %121
  %arrayidx24 = getelementptr inbounds double, double* %a, i64 %indvars.iv51
  %122 = load double, double* %arrayidx24, align 8, !tbaa !62
  %add25 = fadd double %122, %add22
  store double %add25, double* %arrayidx24, align 8, !tbaa !62
  %indvars.iv.next52 = add i64 %indvars.iv51, 1
  %cmp17 = icmp slt i64 %indvars.iv51, %19
  br i1 %cmp17, label %omp.inner.for.body, label %omp.dispatch.inc, !llvm.loop !560

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %middle.block, %omp.inner.for.cond.preheader
  %add27 = add nsw i32 %12, %7
  %add28 = add nsw i32 %conv1450, %7
  %conv10 = sext i32 %add28 to i64
  %cmp11 = icmp ugt i64 %conv10, %.previous.ub.
  %cond = select i1 %cmp11, i64 %.previous.ub., i64 %conv10
  %conv14 = trunc i64 %cond to i32
  %cmp15 = icmp sgt i32 %add27, %conv14
  %indvars.iv.next = add i64 %indvars.iv, %9
  %indvar.next = add i64 %indvar, 1
  br i1 %cmp15, label %omp.dispatch.cond.omp.dispatch.end_crit_edge, label %omp.inner.for.cond.preheader

omp.dispatch.cond.omp.dispatch.end_crit_edge:     ; preds = %omp.dispatch.inc
  %conv14.le = trunc i64 %cond to i32
  store i32 %add27, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv14.le, i32* %.omp.ub, align 4, !tbaa !58
  br label %omp.dispatch.end

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.omp.dispatch.end_crit_edge, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.dispatch.end, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..180(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub2 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre25 = bitcast i32* %.omp.is_last to i8*
  %.pre26 = bitcast i32* %.omp.stride to i8*
  %.pre28 = bitcast i32* %.omp.comb.ub to i8*
  %.pre30 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub2, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6 = icmp sgt i32 %5, %sub2
  %cond = select i1 %cmp6, i32 %sub2, i32 %5
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp823 = icmp sgt i32 %6, %cond
  br i1 %cmp823, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.omp.inner.for.body_crit_edge, %omp.inner.for.body.lr.ph
  %7 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %.pre, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %8 = phi i32 [ %cond, %omp.inner.for.body.lr.ph ], [ %12, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %.omp.iv.024 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %add11, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %9 = zext i32 %7 to i64
  %10 = zext i32 %8 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 6, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*)* @.omp_outlined..181 to void (i32*, i32*, ...)*), i64 %9, i64 %10, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c) #6
  %11 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add11 = add nsw i32 %11, %.omp.iv.024
  %12 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %add11, %12
  br i1 %cmp8, label %omp.loop.exit, label %omp.inner.for.body.omp.inner.for.body_crit_edge

omp.inner.for.body.omp.inner.for.body_crit_edge:  ; preds = %omp.inner.for.body
  %.pre = load i32, i32* %.omp.comb.lb, align 4
  br label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.loop.exit
  %.pre-phi31 = phi i8* [ %.pre30, %entry.omp.precond.end_crit_edge ], [ %0, %omp.loop.exit ]
  %.pre-phi29 = phi i8* [ %.pre28, %entry.omp.precond.end_crit_edge ], [ %1, %omp.loop.exit ]
  %.pre-phi27 = phi i8* [ %.pre26, %entry.omp.precond.end_crit_edge ], [ %2, %omp.loop.exit ]
  %.pre-phi = phi i8* [ %.pre25, %entry.omp.precond.end_crit_edge ], [ %3, %omp.loop.exit ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi27) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi29) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi31) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..181(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv5 = trunc i64 %.previous.lb. to i32
  %conv6 = trunc i64 %.previous.ub. to i32
  store i32 %conv5, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv6, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 35, i32 %conv5, i32 %conv6, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool31 = icmp eq i32 %5, 0
  br i1 %tobool31, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %omp.precond.then, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !561
  %cmp829 = icmp sgt i32 %7, %8
  br i1 %cmp829, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 24
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load43 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load44 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds double, double* %14, i64 12
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %14, i64 14
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %14, i64 16
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %14, i64 18
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %14, i64 20
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %14, i64 22
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62
  %40 = getelementptr inbounds double, double* %38, i64 2
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62
  %42 = getelementptr inbounds double, double* %38, i64 4
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62
  %44 = getelementptr inbounds double, double* %38, i64 6
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %38, i64 8
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %38, i64 10
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %38, i64 12
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %38, i64 14
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %38, i64 16
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = getelementptr inbounds double, double* %38, i64 18
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62
  %58 = getelementptr inbounds double, double* %38, i64 20
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62
  %60 = getelementptr inbounds double, double* %38, i64 22
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62
  %62 = fadd <2 x double> %wide.load, %wide.load54
  %63 = fadd <2 x double> %wide.load43, %wide.load55
  %64 = fadd <2 x double> %wide.load44, %wide.load56
  %65 = fadd <2 x double> %wide.load45, %wide.load57
  %66 = fadd <2 x double> %wide.load46, %wide.load58
  %67 = fadd <2 x double> %wide.load47, %wide.load59
  %68 = fadd <2 x double> %wide.load48, %wide.load60
  %69 = fadd <2 x double> %wide.load49, %wide.load61
  %70 = fadd <2 x double> %wide.load50, %wide.load62
  %71 = fadd <2 x double> %wide.load51, %wide.load63
  %72 = fadd <2 x double> %wide.load52, %wide.load64
  %73 = fadd <2 x double> %wide.load53, %wide.load65
  %74 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62
  %76 = getelementptr inbounds double, double* %74, i64 2
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62
  %78 = getelementptr inbounds double, double* %74, i64 4
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62
  %80 = getelementptr inbounds double, double* %74, i64 6
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %74, i64 8
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %74, i64 10
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %74, i64 12
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %74, i64 14
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %74, i64 16
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = getelementptr inbounds double, double* %74, i64 18
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62
  %94 = getelementptr inbounds double, double* %74, i64 20
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62
  %96 = getelementptr inbounds double, double* %74, i64 22
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62
  %98 = fadd <2 x double> %62, %wide.load66
  %99 = fadd <2 x double> %63, %wide.load67
  %100 = fadd <2 x double> %64, %wide.load68
  %101 = fadd <2 x double> %65, %wide.load69
  %102 = fadd <2 x double> %66, %wide.load70
  %103 = fadd <2 x double> %67, %wide.load71
  %104 = fadd <2 x double> %68, %wide.load72
  %105 = fadd <2 x double> %69, %wide.load73
  %106 = fadd <2 x double> %70, %wide.load74
  %107 = fadd <2 x double> %71, %wide.load75
  %108 = fadd <2 x double> %72, %wide.load76
  %109 = fadd <2 x double> %73, %wide.load77
  store <2 x double> %98, <2 x double>* %75, align 8, !tbaa !62
  store <2 x double> %99, <2 x double>* %77, align 8, !tbaa !62
  store <2 x double> %100, <2 x double>* %79, align 8, !tbaa !62
  store <2 x double> %101, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %102, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %103, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %104, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %105, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %106, <2 x double>* %91, align 8, !tbaa !62
  store <2 x double> %107, <2 x double>* %93, align 8, !tbaa !62
  store <2 x double> %108, <2 x double>* %95, align 8, !tbaa !62
  store <2 x double> %109, <2 x double>* %97, align 8, !tbaa !62
  %index.next = add i64 %index, 24
  %110 = icmp eq i64 %index.next, %n.vec
  br i1 %110, label %middle.block, label %vector.body, !llvm.loop !562

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %111 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !561
  %arrayidx12 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %112 = load double, double* %arrayidx12, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !561
  %add13 = fadd double %111, %112
  %arrayidx15 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %113 = load double, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !561
  %add16 = fadd double %113, %add13
  store double %add16, double* %arrayidx15, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !561
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp8 = icmp slt i64 %indvars.iv, %10
  br i1 %cmp8, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !563

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %omp.precond.then
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.dispatch.end, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..187(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub4 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre28 = bitcast i32* %.omp.is_last to i8*
  %.pre29 = bitcast i32* %.omp.stride to i8*
  %.pre31 = bitcast i32* %.omp.comb.ub to i8*
  %.pre33 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub4, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %5, %sub4
  %cond = select i1 %cmp8, i32 %sub4, i32 %5
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1026 = icmp sgt i32 %6, %cond
  br i1 %cmp1026, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.omp.inner.for.body_crit_edge, %omp.inner.for.body.lr.ph
  %7 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %.pre, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %8 = phi i32 [ %cond, %omp.inner.for.body.lr.ph ], [ %12, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %.omp.iv.027 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %add14, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %9 = zext i32 %7 to i64
  %10 = zext i32 %8 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64)* @.omp_outlined..188 to void (i32*, i32*, ...)*), i64 %9, i64 %10, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext) #6
  %11 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add14 = add nsw i32 %11, %.omp.iv.027
  %12 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp10 = icmp sgt i32 %add14, %12
  br i1 %cmp10, label %omp.loop.exit, label %omp.inner.for.body.omp.inner.for.body_crit_edge

omp.inner.for.body.omp.inner.for.body_crit_edge:  ; preds = %omp.inner.for.body
  %.pre = load i32, i32* %.omp.comb.lb, align 4
  br label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.loop.exit
  %.pre-phi34 = phi i8* [ %.pre33, %entry.omp.precond.end_crit_edge ], [ %0, %omp.loop.exit ]
  %.pre-phi32 = phi i8* [ %.pre31, %entry.omp.precond.end_crit_edge ], [ %1, %omp.loop.exit ]
  %.pre-phi30 = phi i8* [ %.pre29, %entry.omp.precond.end_crit_edge ], [ %2, %omp.loop.exit ]
  %.pre-phi = phi i8* [ %.pre28, %entry.omp.precond.end_crit_edge ], [ %3, %omp.loop.exit ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi30) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi32) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi34) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..188(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv7 = trunc i64 %.previous.lb. to i32
  %conv8 = trunc i64 %.previous.ub. to i32
  store i32 %conv7, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv8, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 35, i32 %conv7, i32 %conv8, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool33 = icmp eq i32 %5, 0
  br i1 %tobool33, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %omp.precond.then, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !564
  %cmp1031 = icmp sgt i32 %7, %8
  br i1 %cmp1031, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 24
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds double, double* %14, i64 12
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %14, i64 14
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %14, i64 16
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %14, i64 18
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %14, i64 20
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %14, i64 22
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62
  %40 = getelementptr inbounds double, double* %38, i64 2
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62
  %42 = getelementptr inbounds double, double* %38, i64 4
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62
  %44 = getelementptr inbounds double, double* %38, i64 6
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %38, i64 8
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %38, i64 10
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %38, i64 12
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %38, i64 14
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %38, i64 16
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = getelementptr inbounds double, double* %38, i64 18
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62
  %58 = getelementptr inbounds double, double* %38, i64 20
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62
  %60 = getelementptr inbounds double, double* %38, i64 22
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62
  %62 = fadd <2 x double> %wide.load, %wide.load56
  %63 = fadd <2 x double> %wide.load45, %wide.load57
  %64 = fadd <2 x double> %wide.load46, %wide.load58
  %65 = fadd <2 x double> %wide.load47, %wide.load59
  %66 = fadd <2 x double> %wide.load48, %wide.load60
  %67 = fadd <2 x double> %wide.load49, %wide.load61
  %68 = fadd <2 x double> %wide.load50, %wide.load62
  %69 = fadd <2 x double> %wide.load51, %wide.load63
  %70 = fadd <2 x double> %wide.load52, %wide.load64
  %71 = fadd <2 x double> %wide.load53, %wide.load65
  %72 = fadd <2 x double> %wide.load54, %wide.load66
  %73 = fadd <2 x double> %wide.load55, %wide.load67
  %74 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62
  %76 = getelementptr inbounds double, double* %74, i64 2
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62
  %78 = getelementptr inbounds double, double* %74, i64 4
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62
  %80 = getelementptr inbounds double, double* %74, i64 6
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %74, i64 8
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %74, i64 10
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %74, i64 12
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %74, i64 14
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %74, i64 16
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = getelementptr inbounds double, double* %74, i64 18
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62
  %94 = getelementptr inbounds double, double* %74, i64 20
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62
  %96 = getelementptr inbounds double, double* %74, i64 22
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62
  %98 = fadd <2 x double> %62, %wide.load68
  %99 = fadd <2 x double> %63, %wide.load69
  %100 = fadd <2 x double> %64, %wide.load70
  %101 = fadd <2 x double> %65, %wide.load71
  %102 = fadd <2 x double> %66, %wide.load72
  %103 = fadd <2 x double> %67, %wide.load73
  %104 = fadd <2 x double> %68, %wide.load74
  %105 = fadd <2 x double> %69, %wide.load75
  %106 = fadd <2 x double> %70, %wide.load76
  %107 = fadd <2 x double> %71, %wide.load77
  %108 = fadd <2 x double> %72, %wide.load78
  %109 = fadd <2 x double> %73, %wide.load79
  store <2 x double> %98, <2 x double>* %75, align 8, !tbaa !62
  store <2 x double> %99, <2 x double>* %77, align 8, !tbaa !62
  store <2 x double> %100, <2 x double>* %79, align 8, !tbaa !62
  store <2 x double> %101, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %102, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %103, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %104, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %105, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %106, <2 x double>* %91, align 8, !tbaa !62
  store <2 x double> %107, <2 x double>* %93, align 8, !tbaa !62
  store <2 x double> %108, <2 x double>* %95, align 8, !tbaa !62
  store <2 x double> %109, <2 x double>* %97, align 8, !tbaa !62
  %index.next = add i64 %index, 24
  %110 = icmp eq i64 %index.next, %n.vec
  br i1 %110, label %middle.block, label %vector.body, !llvm.loop !565

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %111 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !564
  %arrayidx14 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %112 = load double, double* %arrayidx14, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !564
  %add15 = fadd double %111, %112
  %arrayidx17 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %113 = load double, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !564
  %add18 = fadd double %113, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !564
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp10 = icmp slt i64 %indvars.iv, %10
  br i1 %cmp10, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !566

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %omp.precond.then
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.dispatch.end, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..194(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub2 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre25 = bitcast i32* %.omp.is_last to i8*
  %.pre26 = bitcast i32* %.omp.stride to i8*
  %.pre28 = bitcast i32* %.omp.comb.ub to i8*
  %.pre30 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub2, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6 = icmp sgt i32 %5, %sub2
  %cond = select i1 %cmp6, i32 %sub2, i32 %5
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp823 = icmp sgt i32 %6, %cond
  br i1 %cmp823, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.omp.inner.for.body_crit_edge, %omp.inner.for.body.lr.ph
  %7 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %.pre, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %8 = phi i32 [ %cond, %omp.inner.for.body.lr.ph ], [ %12, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %.omp.iv.024 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %add11, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %9 = zext i32 %7 to i64
  %10 = zext i32 %8 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 6, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*)* @.omp_outlined..195 to void (i32*, i32*, ...)*), i64 %9, i64 %10, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c) #6
  %11 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add11 = add nsw i32 %11, %.omp.iv.024
  %12 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %add11, %12
  br i1 %cmp8, label %omp.loop.exit, label %omp.inner.for.body.omp.inner.for.body_crit_edge

omp.inner.for.body.omp.inner.for.body_crit_edge:  ; preds = %omp.inner.for.body
  %.pre = load i32, i32* %.omp.comb.lb, align 4
  br label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.loop.exit
  %.pre-phi31 = phi i8* [ %.pre30, %entry.omp.precond.end_crit_edge ], [ %0, %omp.loop.exit ]
  %.pre-phi29 = phi i8* [ %.pre28, %entry.omp.precond.end_crit_edge ], [ %1, %omp.loop.exit ]
  %.pre-phi27 = phi i8* [ %.pre26, %entry.omp.precond.end_crit_edge ], [ %2, %omp.loop.exit ]
  %.pre-phi = phi i8* [ %.pre25, %entry.omp.precond.end_crit_edge ], [ %3, %omp.loop.exit ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi27) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi29) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi31) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..195(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub2 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv5 = trunc i64 %.previous.lb. to i32
  %conv6 = trunc i64 %.previous.ub. to i32
  store i32 %conv5, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv6, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %5, %sub2
  %cond = select i1 %cmp8, i32 %sub2, i32 %5
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp1033 = icmp sgt i32 %6, %cond
  br i1 %cmp1033, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %omp.precond.then
  %7 = sext i32 %6 to i64
  %8 = sext i32 %cond to i64
  %9 = icmp sgt i64 %7, %8
  %smax = select i1 %9, i64 %7, i64 %8
  %10 = add nsw i64 %smax, 1
  %11 = sub nsw i64 %10, %7
  %min.iters.check = icmp ult i64 %11, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr double, double* %a, i64 %7
  %scevgep37 = getelementptr double, double* %a, i64 %10
  %scevgep39 = getelementptr double, double* %b, i64 %7
  %scevgep41 = getelementptr double, double* %b, i64 %10
  %scevgep43 = getelementptr double, double* %c, i64 %7
  %scevgep45 = getelementptr double, double* %c, i64 %10
  %bound0 = icmp ult double* %scevgep, %scevgep41
  %bound1 = icmp ult double* %scevgep39, %scevgep37
  %found.conflict = and i1 %bound0, %bound1
  %bound047 = icmp ult double* %scevgep, %scevgep45
  %bound148 = icmp ult double* %scevgep43, %scevgep37
  %found.conflict49 = and i1 %bound047, %bound148
  %conflict.rdx = or i1 %found.conflict, %found.conflict49
  br i1 %conflict.rdx, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %11, 24
  %n.vec = sub nsw i64 %11, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %7
  %12 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %13 = bitcast double* %12 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %13, align 8, !tbaa !62, !alias.scope !567
  %14 = getelementptr inbounds double, double* %12, i64 2
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62, !alias.scope !567
  %16 = getelementptr inbounds double, double* %12, i64 4
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62, !alias.scope !567
  %18 = getelementptr inbounds double, double* %12, i64 6
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62, !alias.scope !567
  %20 = getelementptr inbounds double, double* %12, i64 8
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62, !alias.scope !567
  %22 = getelementptr inbounds double, double* %12, i64 10
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !567
  %24 = getelementptr inbounds double, double* %12, i64 12
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !567
  %26 = getelementptr inbounds double, double* %12, i64 14
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !567
  %28 = getelementptr inbounds double, double* %12, i64 16
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !567
  %30 = getelementptr inbounds double, double* %12, i64 18
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !567
  %32 = getelementptr inbounds double, double* %12, i64 20
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !567
  %34 = getelementptr inbounds double, double* %12, i64 22
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !567
  %36 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !570
  %38 = getelementptr inbounds double, double* %36, i64 2
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !570
  %40 = getelementptr inbounds double, double* %36, i64 4
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !570
  %42 = getelementptr inbounds double, double* %36, i64 6
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !570
  %44 = getelementptr inbounds double, double* %36, i64 8
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !570
  %46 = getelementptr inbounds double, double* %36, i64 10
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62, !alias.scope !570
  %48 = getelementptr inbounds double, double* %36, i64 12
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62, !alias.scope !570
  %50 = getelementptr inbounds double, double* %36, i64 14
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62, !alias.scope !570
  %52 = getelementptr inbounds double, double* %36, i64 16
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !570
  %54 = getelementptr inbounds double, double* %36, i64 18
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !570
  %56 = getelementptr inbounds double, double* %36, i64 20
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !570
  %58 = getelementptr inbounds double, double* %36, i64 22
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !570
  %60 = fadd <2 x double> %wide.load, %wide.load72
  %61 = fadd <2 x double> %wide.load61, %wide.load73
  %62 = fadd <2 x double> %wide.load62, %wide.load74
  %63 = fadd <2 x double> %wide.load63, %wide.load75
  %64 = fadd <2 x double> %wide.load64, %wide.load76
  %65 = fadd <2 x double> %wide.load65, %wide.load77
  %66 = fadd <2 x double> %wide.load66, %wide.load78
  %67 = fadd <2 x double> %wide.load67, %wide.load79
  %68 = fadd <2 x double> %wide.load68, %wide.load80
  %69 = fadd <2 x double> %wide.load69, %wide.load81
  %70 = fadd <2 x double> %wide.load70, %wide.load82
  %71 = fadd <2 x double> %wide.load71, %wide.load83
  %72 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %74 = getelementptr inbounds double, double* %72, i64 2
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %76 = getelementptr inbounds double, double* %72, i64 4
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %78 = getelementptr inbounds double, double* %72, i64 6
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %80 = getelementptr inbounds double, double* %72, i64 8
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %82 = getelementptr inbounds double, double* %72, i64 10
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %84 = getelementptr inbounds double, double* %72, i64 12
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %86 = getelementptr inbounds double, double* %72, i64 14
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %88 = getelementptr inbounds double, double* %72, i64 16
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %90 = getelementptr inbounds double, double* %72, i64 18
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %92 = getelementptr inbounds double, double* %72, i64 20
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %94 = getelementptr inbounds double, double* %72, i64 22
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %96 = fadd <2 x double> %60, %wide.load84
  %97 = fadd <2 x double> %61, %wide.load85
  %98 = fadd <2 x double> %62, %wide.load86
  %99 = fadd <2 x double> %63, %wide.load87
  %100 = fadd <2 x double> %64, %wide.load88
  %101 = fadd <2 x double> %65, %wide.load89
  %102 = fadd <2 x double> %66, %wide.load90
  %103 = fadd <2 x double> %67, %wide.load91
  %104 = fadd <2 x double> %68, %wide.load92
  %105 = fadd <2 x double> %69, %wide.load93
  %106 = fadd <2 x double> %70, %wide.load94
  %107 = fadd <2 x double> %71, %wide.load95
  store <2 x double> %96, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  store <2 x double> %97, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  store <2 x double> %98, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  store <2 x double> %99, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  store <2 x double> %100, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  store <2 x double> %101, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  store <2 x double> %102, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  store <2 x double> %103, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  store <2 x double> %104, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  store <2 x double> %105, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  store <2 x double> %106, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  store <2 x double> %107, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !572, !noalias !574
  %index.next = add i64 %index, 24
  %108 = icmp eq i64 %index.next, %n.vec
  br i1 %108, label %middle.block, label %vector.body, !llvm.loop !575

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %7
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %7, %omp.inner.for.body.preheader ], [ %7, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %109 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx14 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %110 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %109, %110
  %arrayidx17 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %111 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %111, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp10 = icmp slt i64 %indvars.iv, %8
  br i1 %cmp10, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !576

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.loop.exit, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..201(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub4 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre41 = bitcast i32* %.omp.is_last to i8*
  %.pre42 = bitcast i32* %.omp.stride to i8*
  %.pre44 = bitcast i32* %.omp.comb.ub to i8*
  %.pre46 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub4, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 91, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp836 = icmp sgt i32 %5, %sub4
  %cond37 = select i1 %cmp836, i32 %sub4, i32 %5
  store i32 %cond37, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1038 = icmp sgt i32 %6, %cond37
  br i1 %cmp1038, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %7 = phi i32 [ %cond37, %omp.inner.for.cond.preheader.lr.ph ], [ %cond, %omp.dispatch.inc ]
  %8 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add17, %omp.dispatch.inc ]
  %cmp1233 = icmp sgt i32 %8, %7
  br i1 %cmp1233, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge: ; preds = %omp.inner.for.cond.preheader
  %.pre = load i32, i32* %.omp.stride, align 4, !tbaa !58
  br label %omp.dispatch.inc

omp.inner.for.body:                               ; preds = %omp.inner.for.cond.preheader, %omp.inner.for.body
  %9 = phi i32 [ %15, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %10 = phi i32 [ %14, %omp.inner.for.body ], [ %7, %omp.inner.for.cond.preheader ]
  %.omp.iv.034 = phi i32 [ %add16, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %11 = zext i32 %9 to i64
  %12 = zext i32 %10 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64)* @.omp_outlined..202 to void (i32*, i32*, ...)*), i64 %11, i64 %12, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext) #6, !llvm.mem.parallel_loop_access !577
  %13 = load i32, i32* %.omp.stride, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !577
  %add16 = add nsw i32 %13, %.omp.iv.034
  %14 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !577
  %cmp12 = icmp sgt i32 %add16, %14
  %15 = load i32, i32* %.omp.comb.lb, align 4
  br i1 %cmp12, label %omp.dispatch.inc, label %omp.inner.for.body, !llvm.loop !577

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge
  %16 = phi i32 [ %.pre, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %13, %omp.inner.for.body ]
  %.lcssa32 = phi i32 [ %7, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %14, %omp.inner.for.body ]
  %.lcssa = phi i32 [ %8, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %15, %omp.inner.for.body ]
  %add17 = add nsw i32 %.lcssa, %16
  store i32 %add17, i32* %.omp.comb.lb, align 4, !tbaa !58
  %add18 = add nsw i32 %.lcssa32, %16
  %cmp8 = icmp sgt i32 %add18, %sub4
  %cond = select i1 %cmp8, i32 %sub4, i32 %add18
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp10 = icmp sgt i32 %add17, %cond
  br i1 %cmp10, label %omp.dispatch.end, label %omp.inner.for.cond.preheader

omp.dispatch.end:                                 ; preds = %omp.dispatch.inc, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.dispatch.end
  %.pre-phi47 = phi i8* [ %.pre46, %entry.omp.precond.end_crit_edge ], [ %0, %omp.dispatch.end ]
  %.pre-phi45 = phi i8* [ %.pre44, %entry.omp.precond.end_crit_edge ], [ %1, %omp.dispatch.end ]
  %.pre-phi43 = phi i8* [ %.pre42, %entry.omp.precond.end_crit_edge ], [ %2, %omp.dispatch.end ]
  %.pre-phi = phi i8* [ %.pre41, %entry.omp.precond.end_crit_edge ], [ %3, %omp.dispatch.end ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi43) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi45) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi47) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..202(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub4 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv7 = trunc i64 %.previous.lb. to i32
  %conv8 = trunc i64 %.previous.ub. to i32
  store i32 %conv7, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv8, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %cmp10 = icmp sgt i32 %5, %sub4
  %cond = select i1 %cmp10, i32 %sub4, i32 %5
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp1235 = icmp sgt i32 %6, %cond
  br i1 %cmp1235, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %omp.precond.then
  %7 = sext i32 %6 to i64
  %8 = sext i32 %cond to i64
  %9 = icmp sgt i64 %7, %8
  %smax = select i1 %9, i64 %7, i64 %8
  %10 = add nsw i64 %smax, 1
  %11 = sub nsw i64 %10, %7
  %min.iters.check = icmp ult i64 %11, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr double, double* %a, i64 %7
  %scevgep39 = getelementptr double, double* %a, i64 %10
  %scevgep41 = getelementptr double, double* %b, i64 %7
  %scevgep43 = getelementptr double, double* %b, i64 %10
  %scevgep45 = getelementptr double, double* %c, i64 %7
  %scevgep47 = getelementptr double, double* %c, i64 %10
  %bound0 = icmp ult double* %scevgep, %scevgep43
  %bound1 = icmp ult double* %scevgep41, %scevgep39
  %found.conflict = and i1 %bound0, %bound1
  %bound049 = icmp ult double* %scevgep, %scevgep47
  %bound150 = icmp ult double* %scevgep45, %scevgep39
  %found.conflict51 = and i1 %bound049, %bound150
  %conflict.rdx = or i1 %found.conflict, %found.conflict51
  br i1 %conflict.rdx, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %11, 24
  %n.vec = sub nsw i64 %11, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %7
  %12 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %13 = bitcast double* %12 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %13, align 8, !tbaa !62, !alias.scope !578
  %14 = getelementptr inbounds double, double* %12, i64 2
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62, !alias.scope !578
  %16 = getelementptr inbounds double, double* %12, i64 4
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62, !alias.scope !578
  %18 = getelementptr inbounds double, double* %12, i64 6
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62, !alias.scope !578
  %20 = getelementptr inbounds double, double* %12, i64 8
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62, !alias.scope !578
  %22 = getelementptr inbounds double, double* %12, i64 10
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !578
  %24 = getelementptr inbounds double, double* %12, i64 12
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !578
  %26 = getelementptr inbounds double, double* %12, i64 14
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !578
  %28 = getelementptr inbounds double, double* %12, i64 16
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !578
  %30 = getelementptr inbounds double, double* %12, i64 18
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !578
  %32 = getelementptr inbounds double, double* %12, i64 20
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !578
  %34 = getelementptr inbounds double, double* %12, i64 22
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !578
  %36 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !581
  %38 = getelementptr inbounds double, double* %36, i64 2
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !581
  %40 = getelementptr inbounds double, double* %36, i64 4
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !581
  %42 = getelementptr inbounds double, double* %36, i64 6
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !581
  %44 = getelementptr inbounds double, double* %36, i64 8
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !581
  %46 = getelementptr inbounds double, double* %36, i64 10
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62, !alias.scope !581
  %48 = getelementptr inbounds double, double* %36, i64 12
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62, !alias.scope !581
  %50 = getelementptr inbounds double, double* %36, i64 14
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62, !alias.scope !581
  %52 = getelementptr inbounds double, double* %36, i64 16
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !581
  %54 = getelementptr inbounds double, double* %36, i64 18
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !581
  %56 = getelementptr inbounds double, double* %36, i64 20
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !581
  %58 = getelementptr inbounds double, double* %36, i64 22
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !581
  %60 = fadd <2 x double> %wide.load, %wide.load74
  %61 = fadd <2 x double> %wide.load63, %wide.load75
  %62 = fadd <2 x double> %wide.load64, %wide.load76
  %63 = fadd <2 x double> %wide.load65, %wide.load77
  %64 = fadd <2 x double> %wide.load66, %wide.load78
  %65 = fadd <2 x double> %wide.load67, %wide.load79
  %66 = fadd <2 x double> %wide.load68, %wide.load80
  %67 = fadd <2 x double> %wide.load69, %wide.load81
  %68 = fadd <2 x double> %wide.load70, %wide.load82
  %69 = fadd <2 x double> %wide.load71, %wide.load83
  %70 = fadd <2 x double> %wide.load72, %wide.load84
  %71 = fadd <2 x double> %wide.load73, %wide.load85
  %72 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %74 = getelementptr inbounds double, double* %72, i64 2
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %76 = getelementptr inbounds double, double* %72, i64 4
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %78 = getelementptr inbounds double, double* %72, i64 6
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %80 = getelementptr inbounds double, double* %72, i64 8
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %82 = getelementptr inbounds double, double* %72, i64 10
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %84 = getelementptr inbounds double, double* %72, i64 12
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %86 = getelementptr inbounds double, double* %72, i64 14
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %88 = getelementptr inbounds double, double* %72, i64 16
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %90 = getelementptr inbounds double, double* %72, i64 18
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %92 = getelementptr inbounds double, double* %72, i64 20
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %94 = getelementptr inbounds double, double* %72, i64 22
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %96 = fadd <2 x double> %60, %wide.load86
  %97 = fadd <2 x double> %61, %wide.load87
  %98 = fadd <2 x double> %62, %wide.load88
  %99 = fadd <2 x double> %63, %wide.load89
  %100 = fadd <2 x double> %64, %wide.load90
  %101 = fadd <2 x double> %65, %wide.load91
  %102 = fadd <2 x double> %66, %wide.load92
  %103 = fadd <2 x double> %67, %wide.load93
  %104 = fadd <2 x double> %68, %wide.load94
  %105 = fadd <2 x double> %69, %wide.load95
  %106 = fadd <2 x double> %70, %wide.load96
  %107 = fadd <2 x double> %71, %wide.load97
  store <2 x double> %96, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  store <2 x double> %97, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  store <2 x double> %98, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  store <2 x double> %99, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  store <2 x double> %100, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  store <2 x double> %101, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  store <2 x double> %102, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  store <2 x double> %103, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  store <2 x double> %104, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  store <2 x double> %105, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  store <2 x double> %106, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  store <2 x double> %107, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !583, !noalias !585
  %index.next = add i64 %index, 24
  %108 = icmp eq i64 %index.next, %n.vec
  br i1 %108, label %middle.block, label %vector.body, !llvm.loop !586

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %7
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %7, %omp.inner.for.body.preheader ], [ %7, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %109 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx16 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %110 = load double, double* %arrayidx16, align 8, !tbaa !62
  %add17 = fadd double %109, %110
  %arrayidx19 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %111 = load double, double* %arrayidx19, align 8, !tbaa !62
  %add20 = fadd double %111, %add17
  store double %add20, double* %arrayidx19, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp12 = icmp slt i64 %indvars.iv, %8
  br i1 %cmp12, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !587

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.loop.exit, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..208(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub2 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre25 = bitcast i32* %.omp.is_last to i8*
  %.pre26 = bitcast i32* %.omp.stride to i8*
  %.pre28 = bitcast i32* %.omp.comb.ub to i8*
  %.pre30 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub2, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp6 = icmp sgt i32 %5, %sub2
  %cond = select i1 %cmp6, i32 %sub2, i32 %5
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp823 = icmp sgt i32 %6, %cond
  br i1 %cmp823, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.omp.inner.for.body_crit_edge, %omp.inner.for.body.lr.ph
  %7 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %.pre, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %8 = phi i32 [ %cond, %omp.inner.for.body.lr.ph ], [ %12, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %.omp.iv.024 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %add11, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %9 = zext i32 %7 to i64
  %10 = zext i32 %8 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 6, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*)* @.omp_outlined..209 to void (i32*, i32*, ...)*), i64 %9, i64 %10, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c) #6
  %11 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add11 = add nsw i32 %11, %.omp.iv.024
  %12 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %add11, %12
  br i1 %cmp8, label %omp.loop.exit, label %omp.inner.for.body.omp.inner.for.body_crit_edge

omp.inner.for.body.omp.inner.for.body_crit_edge:  ; preds = %omp.inner.for.body
  %.pre = load i32, i32* %.omp.comb.lb, align 4
  br label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.loop.exit
  %.pre-phi31 = phi i8* [ %.pre30, %entry.omp.precond.end_crit_edge ], [ %0, %omp.loop.exit ]
  %.pre-phi29 = phi i8* [ %.pre28, %entry.omp.precond.end_crit_edge ], [ %1, %omp.loop.exit ]
  %.pre-phi27 = phi i8* [ %.pre26, %entry.omp.precond.end_crit_edge ], [ %2, %omp.loop.exit ]
  %.pre-phi = phi i8* [ %.pre25, %entry.omp.precond.end_crit_edge ], [ %3, %omp.loop.exit ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi27) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi29) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi31) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..209(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub2 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv5 = trunc i64 %.previous.lb. to i32
  %conv6 = trunc i64 %.previous.ub. to i32
  store i32 %conv5, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv6, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %5, %sub2
  %cond = select i1 %cmp8, i32 %sub2, i32 %5
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp1033 = icmp sgt i32 %6, %cond
  br i1 %cmp1033, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %omp.precond.then
  %7 = sext i32 %6 to i64
  %8 = sext i32 %cond to i64
  %9 = icmp sgt i64 %7, %8
  %smax = select i1 %9, i64 %7, i64 %8
  %10 = add nsw i64 %smax, 1
  %11 = sub nsw i64 %10, %7
  %min.iters.check = icmp ult i64 %11, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr double, double* %a, i64 %7
  %scevgep37 = getelementptr double, double* %a, i64 %10
  %scevgep39 = getelementptr double, double* %b, i64 %7
  %scevgep41 = getelementptr double, double* %b, i64 %10
  %scevgep43 = getelementptr double, double* %c, i64 %7
  %scevgep45 = getelementptr double, double* %c, i64 %10
  %bound0 = icmp ult double* %scevgep, %scevgep41
  %bound1 = icmp ult double* %scevgep39, %scevgep37
  %found.conflict = and i1 %bound0, %bound1
  %bound047 = icmp ult double* %scevgep, %scevgep45
  %bound148 = icmp ult double* %scevgep43, %scevgep37
  %found.conflict49 = and i1 %bound047, %bound148
  %conflict.rdx = or i1 %found.conflict, %found.conflict49
  br i1 %conflict.rdx, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %11, 24
  %n.vec = sub nsw i64 %11, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %7
  %12 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %13 = bitcast double* %12 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %13, align 8, !tbaa !62, !alias.scope !588
  %14 = getelementptr inbounds double, double* %12, i64 2
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62, !alias.scope !588
  %16 = getelementptr inbounds double, double* %12, i64 4
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62, !alias.scope !588
  %18 = getelementptr inbounds double, double* %12, i64 6
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62, !alias.scope !588
  %20 = getelementptr inbounds double, double* %12, i64 8
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62, !alias.scope !588
  %22 = getelementptr inbounds double, double* %12, i64 10
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !588
  %24 = getelementptr inbounds double, double* %12, i64 12
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !588
  %26 = getelementptr inbounds double, double* %12, i64 14
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !588
  %28 = getelementptr inbounds double, double* %12, i64 16
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !588
  %30 = getelementptr inbounds double, double* %12, i64 18
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !588
  %32 = getelementptr inbounds double, double* %12, i64 20
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !588
  %34 = getelementptr inbounds double, double* %12, i64 22
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !588
  %36 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !591
  %38 = getelementptr inbounds double, double* %36, i64 2
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !591
  %40 = getelementptr inbounds double, double* %36, i64 4
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !591
  %42 = getelementptr inbounds double, double* %36, i64 6
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !591
  %44 = getelementptr inbounds double, double* %36, i64 8
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !591
  %46 = getelementptr inbounds double, double* %36, i64 10
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62, !alias.scope !591
  %48 = getelementptr inbounds double, double* %36, i64 12
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62, !alias.scope !591
  %50 = getelementptr inbounds double, double* %36, i64 14
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62, !alias.scope !591
  %52 = getelementptr inbounds double, double* %36, i64 16
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !591
  %54 = getelementptr inbounds double, double* %36, i64 18
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !591
  %56 = getelementptr inbounds double, double* %36, i64 20
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !591
  %58 = getelementptr inbounds double, double* %36, i64 22
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !591
  %60 = fadd <2 x double> %wide.load, %wide.load72
  %61 = fadd <2 x double> %wide.load61, %wide.load73
  %62 = fadd <2 x double> %wide.load62, %wide.load74
  %63 = fadd <2 x double> %wide.load63, %wide.load75
  %64 = fadd <2 x double> %wide.load64, %wide.load76
  %65 = fadd <2 x double> %wide.load65, %wide.load77
  %66 = fadd <2 x double> %wide.load66, %wide.load78
  %67 = fadd <2 x double> %wide.load67, %wide.load79
  %68 = fadd <2 x double> %wide.load68, %wide.load80
  %69 = fadd <2 x double> %wide.load69, %wide.load81
  %70 = fadd <2 x double> %wide.load70, %wide.load82
  %71 = fadd <2 x double> %wide.load71, %wide.load83
  %72 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %74 = getelementptr inbounds double, double* %72, i64 2
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %76 = getelementptr inbounds double, double* %72, i64 4
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %78 = getelementptr inbounds double, double* %72, i64 6
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %80 = getelementptr inbounds double, double* %72, i64 8
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %82 = getelementptr inbounds double, double* %72, i64 10
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %84 = getelementptr inbounds double, double* %72, i64 12
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %86 = getelementptr inbounds double, double* %72, i64 14
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %88 = getelementptr inbounds double, double* %72, i64 16
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %90 = getelementptr inbounds double, double* %72, i64 18
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %92 = getelementptr inbounds double, double* %72, i64 20
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %94 = getelementptr inbounds double, double* %72, i64 22
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %96 = fadd <2 x double> %60, %wide.load84
  %97 = fadd <2 x double> %61, %wide.load85
  %98 = fadd <2 x double> %62, %wide.load86
  %99 = fadd <2 x double> %63, %wide.load87
  %100 = fadd <2 x double> %64, %wide.load88
  %101 = fadd <2 x double> %65, %wide.load89
  %102 = fadd <2 x double> %66, %wide.load90
  %103 = fadd <2 x double> %67, %wide.load91
  %104 = fadd <2 x double> %68, %wide.load92
  %105 = fadd <2 x double> %69, %wide.load93
  %106 = fadd <2 x double> %70, %wide.load94
  %107 = fadd <2 x double> %71, %wide.load95
  store <2 x double> %96, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  store <2 x double> %97, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  store <2 x double> %98, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  store <2 x double> %99, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  store <2 x double> %100, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  store <2 x double> %101, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  store <2 x double> %102, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  store <2 x double> %103, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  store <2 x double> %104, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  store <2 x double> %105, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  store <2 x double> %106, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  store <2 x double> %107, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !593, !noalias !595
  %index.next = add i64 %index, 24
  %108 = icmp eq i64 %index.next, %n.vec
  br i1 %108, label %middle.block, label %vector.body, !llvm.loop !596

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %7
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %7, %omp.inner.for.body.preheader ], [ %7, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %109 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx14 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %110 = load double, double* %arrayidx14, align 8, !tbaa !62
  %add15 = fadd double %109, %110
  %arrayidx17 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %111 = load double, double* %arrayidx17, align 8, !tbaa !62
  %add18 = fadd double %111, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp10 = icmp slt i64 %indvars.iv, %8
  br i1 %cmp10, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !597

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.loop.exit, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..215(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub4 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre28 = bitcast i32* %.omp.is_last to i8*
  %.pre29 = bitcast i32* %.omp.stride to i8*
  %.pre31 = bitcast i32* %.omp.comb.ub to i8*
  %.pre33 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub4, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 92, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp8 = icmp sgt i32 %5, %sub4
  %cond = select i1 %cmp8, i32 %sub4, i32 %5
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1026 = icmp sgt i32 %6, %cond
  br i1 %cmp1026, label %omp.loop.exit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.omp.inner.for.body_crit_edge, %omp.inner.for.body.lr.ph
  %7 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %.pre, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %8 = phi i32 [ %cond, %omp.inner.for.body.lr.ph ], [ %12, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %.omp.iv.027 = phi i32 [ %6, %omp.inner.for.body.lr.ph ], [ %add14, %omp.inner.for.body.omp.inner.for.body_crit_edge ]
  %9 = zext i32 %7 to i64
  %10 = zext i32 %8 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64)* @.omp_outlined..216 to void (i32*, i32*, ...)*), i64 %9, i64 %10, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext) #6
  %11 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %add14 = add nsw i32 %11, %.omp.iv.027
  %12 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp10 = icmp sgt i32 %add14, %12
  br i1 %cmp10, label %omp.loop.exit, label %omp.inner.for.body.omp.inner.for.body_crit_edge

omp.inner.for.body.omp.inner.for.body_crit_edge:  ; preds = %omp.inner.for.body
  %.pre = load i32, i32* %.omp.comb.lb, align 4
  br label %omp.inner.for.body

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.loop.exit
  %.pre-phi34 = phi i8* [ %.pre33, %entry.omp.precond.end_crit_edge ], [ %0, %omp.loop.exit ]
  %.pre-phi32 = phi i8* [ %.pre31, %entry.omp.precond.end_crit_edge ], [ %1, %omp.loop.exit ]
  %.pre-phi30 = phi i8* [ %.pre29, %entry.omp.precond.end_crit_edge ], [ %2, %omp.loop.exit ]
  %.pre-phi = phi i8* [ %.pre28, %entry.omp.precond.end_crit_edge ], [ %3, %omp.loop.exit ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi30) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi32) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi34) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..216(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv7 = trunc i64 %.previous.lb. to i32
  %conv8 = trunc i64 %.previous.ub. to i32
  store i32 %conv7, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv8, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 33, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %conv1045 = sext i32 %5 to i64
  %cmp1146 = icmp ugt i64 %conv1045, %.previous.ub.
  %cond47 = select i1 %cmp1146, i64 %.previous.ub., i64 %conv1045
  %conv1448 = trunc i64 %cond47 to i32
  store i32 %conv1448, i32* %.omp.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp1549 = icmp sgt i32 %6, %conv1448
  br i1 %cmp1549, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %7 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %8 = sext i32 %6 to i64
  %9 = sext i32 %7 to i64
  %10 = sub nsw i64 1, %8
  %11 = sub nsw i64 0, %9
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %indvar = phi i64 [ 0, %omp.inner.for.cond.preheader.lr.ph ], [ %indvar.next, %omp.dispatch.inc ]
  %indvars.iv = phi i64 [ %8, %omp.inner.for.cond.preheader.lr.ph ], [ %indvars.iv.next, %omp.dispatch.inc ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add27, %omp.dispatch.inc ]
  %conv1450 = phi i32 [ %conv1448, %omp.inner.for.cond.preheader.lr.ph ], [ %conv14, %omp.dispatch.inc ]
  %13 = mul i64 %indvar, %9
  %14 = add i64 %13, %8
  %scevgep = getelementptr double, double* %a, i64 %14
  %15 = add i64 %14, 1
  %scevgep54 = getelementptr double, double* %a, i64 %15
  %16 = mul i64 %indvar, %11
  %17 = sub i64 %16, %8
  %scevgep58 = getelementptr double, double* %b, i64 %14
  %scevgep60 = getelementptr double, double* %b, i64 %15
  %scevgep63 = getelementptr double, double* %c, i64 %14
  %scevgep65 = getelementptr double, double* %c, i64 %15
  %cmp1743 = icmp sgt i32 %12, %conv1450
  br i1 %cmp1743, label %omp.dispatch.inc, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %omp.inner.for.cond.preheader
  %18 = add i64 %10, %16
  %19 = sext i32 %conv1450 to i64
  %20 = icmp sgt i64 %14, %19
  %smax = select i1 %20, i64 %14, i64 %19
  %21 = add i64 %18, %smax
  %min.iters.check = icmp ult i64 %21, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %22 = add i64 %smax, %17
  %scevgep56 = getelementptr double, double* %scevgep54, i64 %22
  %scevgep61 = getelementptr double, double* %scevgep60, i64 %22
  %scevgep66 = getelementptr double, double* %scevgep65, i64 %22
  %bound0 = icmp ult double* %scevgep, %scevgep61
  %bound1 = icmp ult double* %scevgep58, %scevgep56
  %found.conflict = and i1 %bound0, %bound1
  %bound068 = icmp ult double* %scevgep, %scevgep66
  %bound169 = icmp ult double* %scevgep63, %scevgep56
  %found.conflict70 = and i1 %bound068, %bound169
  %conflict.rdx = or i1 %found.conflict, %found.conflict70
  br i1 %conflict.rdx, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 24
  %n.vec = sub i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %indvars.iv
  %23 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !598
  %25 = getelementptr inbounds double, double* %23, i64 2
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !598
  %27 = getelementptr inbounds double, double* %23, i64 4
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !598
  %29 = getelementptr inbounds double, double* %23, i64 6
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !598
  %31 = getelementptr inbounds double, double* %23, i64 8
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !598
  %33 = getelementptr inbounds double, double* %23, i64 10
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !598
  %35 = getelementptr inbounds double, double* %23, i64 12
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !598
  %37 = getelementptr inbounds double, double* %23, i64 14
  %38 = bitcast double* %37 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %38, align 8, !tbaa !62, !alias.scope !598
  %39 = getelementptr inbounds double, double* %23, i64 16
  %40 = bitcast double* %39 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %40, align 8, !tbaa !62, !alias.scope !598
  %41 = getelementptr inbounds double, double* %23, i64 18
  %42 = bitcast double* %41 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %42, align 8, !tbaa !62, !alias.scope !598
  %43 = getelementptr inbounds double, double* %23, i64 20
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !598
  %45 = getelementptr inbounds double, double* %23, i64 22
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !598
  %47 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !601
  %49 = getelementptr inbounds double, double* %47, i64 2
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !601
  %51 = getelementptr inbounds double, double* %47, i64 4
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !601
  %53 = getelementptr inbounds double, double* %47, i64 6
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !601
  %55 = getelementptr inbounds double, double* %47, i64 8
  %56 = bitcast double* %55 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %56, align 8, !tbaa !62, !alias.scope !601
  %57 = getelementptr inbounds double, double* %47, i64 10
  %58 = bitcast double* %57 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %58, align 8, !tbaa !62, !alias.scope !601
  %59 = getelementptr inbounds double, double* %47, i64 12
  %60 = bitcast double* %59 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %60, align 8, !tbaa !62, !alias.scope !601
  %61 = getelementptr inbounds double, double* %47, i64 14
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !601
  %63 = getelementptr inbounds double, double* %47, i64 16
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !601
  %65 = getelementptr inbounds double, double* %47, i64 18
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !601
  %67 = getelementptr inbounds double, double* %47, i64 20
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !601
  %69 = getelementptr inbounds double, double* %47, i64 22
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !601
  %71 = fadd <2 x double> %wide.load, %wide.load93
  %72 = fadd <2 x double> %wide.load82, %wide.load94
  %73 = fadd <2 x double> %wide.load83, %wide.load95
  %74 = fadd <2 x double> %wide.load84, %wide.load96
  %75 = fadd <2 x double> %wide.load85, %wide.load97
  %76 = fadd <2 x double> %wide.load86, %wide.load98
  %77 = fadd <2 x double> %wide.load87, %wide.load99
  %78 = fadd <2 x double> %wide.load88, %wide.load100
  %79 = fadd <2 x double> %wide.load89, %wide.load101
  %80 = fadd <2 x double> %wide.load90, %wide.load102
  %81 = fadd <2 x double> %wide.load91, %wide.load103
  %82 = fadd <2 x double> %wide.load92, %wide.load104
  %83 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %85 = getelementptr inbounds double, double* %83, i64 2
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %87 = getelementptr inbounds double, double* %83, i64 4
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %89 = getelementptr inbounds double, double* %83, i64 6
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %91 = getelementptr inbounds double, double* %83, i64 8
  %92 = bitcast double* %91 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %92, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %93 = getelementptr inbounds double, double* %83, i64 10
  %94 = bitcast double* %93 to <2 x double>*
  %wide.load110 = load <2 x double>, <2 x double>* %94, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %95 = getelementptr inbounds double, double* %83, i64 12
  %96 = bitcast double* %95 to <2 x double>*
  %wide.load111 = load <2 x double>, <2 x double>* %96, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %97 = getelementptr inbounds double, double* %83, i64 14
  %98 = bitcast double* %97 to <2 x double>*
  %wide.load112 = load <2 x double>, <2 x double>* %98, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %99 = getelementptr inbounds double, double* %83, i64 16
  %100 = bitcast double* %99 to <2 x double>*
  %wide.load113 = load <2 x double>, <2 x double>* %100, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %101 = getelementptr inbounds double, double* %83, i64 18
  %102 = bitcast double* %101 to <2 x double>*
  %wide.load114 = load <2 x double>, <2 x double>* %102, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %103 = getelementptr inbounds double, double* %83, i64 20
  %104 = bitcast double* %103 to <2 x double>*
  %wide.load115 = load <2 x double>, <2 x double>* %104, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %105 = getelementptr inbounds double, double* %83, i64 22
  %106 = bitcast double* %105 to <2 x double>*
  %wide.load116 = load <2 x double>, <2 x double>* %106, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %107 = fadd <2 x double> %71, %wide.load105
  %108 = fadd <2 x double> %72, %wide.load106
  %109 = fadd <2 x double> %73, %wide.load107
  %110 = fadd <2 x double> %74, %wide.load108
  %111 = fadd <2 x double> %75, %wide.load109
  %112 = fadd <2 x double> %76, %wide.load110
  %113 = fadd <2 x double> %77, %wide.load111
  %114 = fadd <2 x double> %78, %wide.load112
  %115 = fadd <2 x double> %79, %wide.load113
  %116 = fadd <2 x double> %80, %wide.load114
  %117 = fadd <2 x double> %81, %wide.load115
  %118 = fadd <2 x double> %82, %wide.load116
  store <2 x double> %107, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  store <2 x double> %108, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  store <2 x double> %109, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  store <2 x double> %110, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  store <2 x double> %111, <2 x double>* %92, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  store <2 x double> %112, <2 x double>* %94, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  store <2 x double> %113, <2 x double>* %96, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  store <2 x double> %114, <2 x double>* %98, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  store <2 x double> %115, <2 x double>* %100, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  store <2 x double> %116, <2 x double>* %102, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  store <2 x double> %117, <2 x double>* %104, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  store <2 x double> %118, <2 x double>* %106, align 8, !tbaa !62, !alias.scope !603, !noalias !605
  %index.next = add i64 %index, 24
  %119 = icmp eq i64 %index.next, %n.vec
  br i1 %119, label %middle.block, label %vector.body, !llvm.loop !606

middle.block:                                     ; preds = %vector.body
  %ind.end = add i64 %n.vec, %indvars.iv
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.inc, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv51.ph = phi i64 [ %ind.end, %middle.block ], [ %indvars.iv, %omp.inner.for.body.preheader ], [ %indvars.iv, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv51 = phi i64 [ %indvars.iv.next52, %omp.inner.for.body ], [ %indvars.iv51.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv51
  %120 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx21 = getelementptr inbounds double, double* %c, i64 %indvars.iv51
  %121 = load double, double* %arrayidx21, align 8, !tbaa !62
  %add22 = fadd double %120, %121
  %arrayidx24 = getelementptr inbounds double, double* %a, i64 %indvars.iv51
  %122 = load double, double* %arrayidx24, align 8, !tbaa !62
  %add25 = fadd double %122, %add22
  store double %add25, double* %arrayidx24, align 8, !tbaa !62
  %indvars.iv.next52 = add i64 %indvars.iv51, 1
  %cmp17 = icmp slt i64 %indvars.iv51, %19
  br i1 %cmp17, label %omp.inner.for.body, label %omp.dispatch.inc, !llvm.loop !607

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %middle.block, %omp.inner.for.cond.preheader
  %add27 = add nsw i32 %12, %7
  %add28 = add nsw i32 %conv1450, %7
  %conv10 = sext i32 %add28 to i64
  %cmp11 = icmp ugt i64 %conv10, %.previous.ub.
  %cond = select i1 %cmp11, i64 %.previous.ub., i64 %conv10
  %conv14 = trunc i64 %cond to i32
  %cmp15 = icmp sgt i32 %add27, %conv14
  %indvars.iv.next = add i64 %indvars.iv, %9
  %indvar.next = add i64 %indvar, 1
  br i1 %cmp15, label %omp.dispatch.cond.omp.dispatch.end_crit_edge, label %omp.inner.for.cond.preheader

omp.dispatch.cond.omp.dispatch.end_crit_edge:     ; preds = %omp.dispatch.inc
  %conv14.le = trunc i64 %cond to i32
  store i32 %add27, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv14.le, i32* %.omp.ub, align 4, !tbaa !58
  br label %omp.dispatch.end

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.omp.dispatch.end_crit_edge, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.dispatch.end, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..222(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub4 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre41 = bitcast i32* %.omp.is_last to i8*
  %.pre42 = bitcast i32* %.omp.stride to i8*
  %.pre44 = bitcast i32* %.omp.comb.ub to i8*
  %.pre46 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub4, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 91, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp836 = icmp sgt i32 %5, %sub4
  %cond37 = select i1 %cmp836, i32 %sub4, i32 %5
  store i32 %cond37, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1038 = icmp sgt i32 %6, %cond37
  br i1 %cmp1038, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %7 = phi i32 [ %cond37, %omp.inner.for.cond.preheader.lr.ph ], [ %cond, %omp.dispatch.inc ]
  %8 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add17, %omp.dispatch.inc ]
  %cmp1233 = icmp sgt i32 %8, %7
  br i1 %cmp1233, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge: ; preds = %omp.inner.for.cond.preheader
  %.pre = load i32, i32* %.omp.stride, align 4, !tbaa !58
  br label %omp.dispatch.inc

omp.inner.for.body:                               ; preds = %omp.inner.for.cond.preheader, %omp.inner.for.body
  %9 = phi i32 [ %15, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %10 = phi i32 [ %14, %omp.inner.for.body ], [ %7, %omp.inner.for.cond.preheader ]
  %.omp.iv.034 = phi i32 [ %add16, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %11 = zext i32 %9 to i64
  %12 = zext i32 %10 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64)* @.omp_outlined..223 to void (i32*, i32*, ...)*), i64 %11, i64 %12, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext) #6, !llvm.mem.parallel_loop_access !608
  %13 = load i32, i32* %.omp.stride, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !608
  %add16 = add nsw i32 %13, %.omp.iv.034
  %14 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !608
  %cmp12 = icmp sgt i32 %add16, %14
  %15 = load i32, i32* %.omp.comb.lb, align 4
  br i1 %cmp12, label %omp.dispatch.inc, label %omp.inner.for.body, !llvm.loop !608

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge
  %16 = phi i32 [ %.pre, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %13, %omp.inner.for.body ]
  %.lcssa32 = phi i32 [ %7, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %14, %omp.inner.for.body ]
  %.lcssa = phi i32 [ %8, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %15, %omp.inner.for.body ]
  %add17 = add nsw i32 %.lcssa, %16
  store i32 %add17, i32* %.omp.comb.lb, align 4, !tbaa !58
  %add18 = add nsw i32 %.lcssa32, %16
  %cmp8 = icmp sgt i32 %add18, %sub4
  %cond = select i1 %cmp8, i32 %sub4, i32 %add18
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp10 = icmp sgt i32 %add17, %cond
  br i1 %cmp10, label %omp.dispatch.end, label %omp.inner.for.cond.preheader

omp.dispatch.end:                                 ; preds = %omp.dispatch.inc, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.dispatch.end
  %.pre-phi47 = phi i8* [ %.pre46, %entry.omp.precond.end_crit_edge ], [ %0, %omp.dispatch.end ]
  %.pre-phi45 = phi i8* [ %.pre44, %entry.omp.precond.end_crit_edge ], [ %1, %omp.dispatch.end ]
  %.pre-phi43 = phi i8* [ %.pre42, %entry.omp.precond.end_crit_edge ], [ %2, %omp.dispatch.end ]
  %.pre-phi = phi i8* [ %.pre41, %entry.omp.precond.end_crit_edge ], [ %3, %omp.dispatch.end ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi43) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi45) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi47) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..223(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub4 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv7 = trunc i64 %.previous.lb. to i32
  %conv8 = trunc i64 %.previous.ub. to i32
  store i32 %conv7, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv8, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 34, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 1) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %cmp10 = icmp sgt i32 %5, %sub4
  %cond = select i1 %cmp10, i32 %sub4, i32 %5
  store i32 %cond, i32* %.omp.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp1235 = icmp sgt i32 %6, %cond
  br i1 %cmp1235, label %omp.loop.exit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %omp.precond.then
  %7 = sext i32 %6 to i64
  %8 = sext i32 %cond to i64
  %9 = icmp sgt i64 %7, %8
  %smax = select i1 %9, i64 %7, i64 %8
  %10 = add nsw i64 %smax, 1
  %11 = sub nsw i64 %10, %7
  %min.iters.check = icmp ult i64 %11, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %scevgep = getelementptr double, double* %a, i64 %7
  %scevgep39 = getelementptr double, double* %a, i64 %10
  %scevgep41 = getelementptr double, double* %b, i64 %7
  %scevgep43 = getelementptr double, double* %b, i64 %10
  %scevgep45 = getelementptr double, double* %c, i64 %7
  %scevgep47 = getelementptr double, double* %c, i64 %10
  %bound0 = icmp ult double* %scevgep, %scevgep43
  %bound1 = icmp ult double* %scevgep41, %scevgep39
  %found.conflict = and i1 %bound0, %bound1
  %bound049 = icmp ult double* %scevgep, %scevgep47
  %bound150 = icmp ult double* %scevgep45, %scevgep39
  %found.conflict51 = and i1 %bound049, %bound150
  %conflict.rdx = or i1 %found.conflict, %found.conflict51
  br i1 %conflict.rdx, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %11, 24
  %n.vec = sub nsw i64 %11, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %7
  %12 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %13 = bitcast double* %12 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %13, align 8, !tbaa !62, !alias.scope !609
  %14 = getelementptr inbounds double, double* %12, i64 2
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62, !alias.scope !609
  %16 = getelementptr inbounds double, double* %12, i64 4
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62, !alias.scope !609
  %18 = getelementptr inbounds double, double* %12, i64 6
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62, !alias.scope !609
  %20 = getelementptr inbounds double, double* %12, i64 8
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62, !alias.scope !609
  %22 = getelementptr inbounds double, double* %12, i64 10
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62, !alias.scope !609
  %24 = getelementptr inbounds double, double* %12, i64 12
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62, !alias.scope !609
  %26 = getelementptr inbounds double, double* %12, i64 14
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62, !alias.scope !609
  %28 = getelementptr inbounds double, double* %12, i64 16
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62, !alias.scope !609
  %30 = getelementptr inbounds double, double* %12, i64 18
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62, !alias.scope !609
  %32 = getelementptr inbounds double, double* %12, i64 20
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62, !alias.scope !609
  %34 = getelementptr inbounds double, double* %12, i64 22
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62, !alias.scope !609
  %36 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62, !alias.scope !612
  %38 = getelementptr inbounds double, double* %36, i64 2
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62, !alias.scope !612
  %40 = getelementptr inbounds double, double* %36, i64 4
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62, !alias.scope !612
  %42 = getelementptr inbounds double, double* %36, i64 6
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62, !alias.scope !612
  %44 = getelementptr inbounds double, double* %36, i64 8
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62, !alias.scope !612
  %46 = getelementptr inbounds double, double* %36, i64 10
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62, !alias.scope !612
  %48 = getelementptr inbounds double, double* %36, i64 12
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62, !alias.scope !612
  %50 = getelementptr inbounds double, double* %36, i64 14
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62, !alias.scope !612
  %52 = getelementptr inbounds double, double* %36, i64 16
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62, !alias.scope !612
  %54 = getelementptr inbounds double, double* %36, i64 18
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load83 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62, !alias.scope !612
  %56 = getelementptr inbounds double, double* %36, i64 20
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load84 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62, !alias.scope !612
  %58 = getelementptr inbounds double, double* %36, i64 22
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62, !alias.scope !612
  %60 = fadd <2 x double> %wide.load, %wide.load74
  %61 = fadd <2 x double> %wide.load63, %wide.load75
  %62 = fadd <2 x double> %wide.load64, %wide.load76
  %63 = fadd <2 x double> %wide.load65, %wide.load77
  %64 = fadd <2 x double> %wide.load66, %wide.load78
  %65 = fadd <2 x double> %wide.load67, %wide.load79
  %66 = fadd <2 x double> %wide.load68, %wide.load80
  %67 = fadd <2 x double> %wide.load69, %wide.load81
  %68 = fadd <2 x double> %wide.load70, %wide.load82
  %69 = fadd <2 x double> %wide.load71, %wide.load83
  %70 = fadd <2 x double> %wide.load72, %wide.load84
  %71 = fadd <2 x double> %wide.load73, %wide.load85
  %72 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %73 = bitcast double* %72 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %74 = getelementptr inbounds double, double* %72, i64 2
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %76 = getelementptr inbounds double, double* %72, i64 4
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %78 = getelementptr inbounds double, double* %72, i64 6
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %80 = getelementptr inbounds double, double* %72, i64 8
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %82 = getelementptr inbounds double, double* %72, i64 10
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %84 = getelementptr inbounds double, double* %72, i64 12
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %86 = getelementptr inbounds double, double* %72, i64 14
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %88 = getelementptr inbounds double, double* %72, i64 16
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %90 = getelementptr inbounds double, double* %72, i64 18
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %92 = getelementptr inbounds double, double* %72, i64 20
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %94 = getelementptr inbounds double, double* %72, i64 22
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %96 = fadd <2 x double> %60, %wide.load86
  %97 = fadd <2 x double> %61, %wide.load87
  %98 = fadd <2 x double> %62, %wide.load88
  %99 = fadd <2 x double> %63, %wide.load89
  %100 = fadd <2 x double> %64, %wide.load90
  %101 = fadd <2 x double> %65, %wide.load91
  %102 = fadd <2 x double> %66, %wide.load92
  %103 = fadd <2 x double> %67, %wide.load93
  %104 = fadd <2 x double> %68, %wide.load94
  %105 = fadd <2 x double> %69, %wide.load95
  %106 = fadd <2 x double> %70, %wide.load96
  %107 = fadd <2 x double> %71, %wide.load97
  store <2 x double> %96, <2 x double>* %73, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  store <2 x double> %97, <2 x double>* %75, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  store <2 x double> %98, <2 x double>* %77, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  store <2 x double> %99, <2 x double>* %79, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  store <2 x double> %100, <2 x double>* %81, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  store <2 x double> %101, <2 x double>* %83, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  store <2 x double> %102, <2 x double>* %85, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  store <2 x double> %103, <2 x double>* %87, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  store <2 x double> %104, <2 x double>* %89, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  store <2 x double> %105, <2 x double>* %91, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  store <2 x double> %106, <2 x double>* %93, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  store <2 x double> %107, <2 x double>* %95, align 8, !tbaa !62, !alias.scope !614, !noalias !616
  %index.next = add i64 %index, 24
  %108 = icmp eq i64 %index.next, %n.vec
  br i1 %108, label %middle.block, label %vector.body, !llvm.loop !617

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %7
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.loop.exit, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %7, %omp.inner.for.body.preheader ], [ %7, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %109 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx16 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %110 = load double, double* %arrayidx16, align 8, !tbaa !62
  %add17 = fadd double %109, %110
  %arrayidx19 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %111 = load double, double* %arrayidx19, align 8, !tbaa !62
  %add20 = fadd double %111, %add17
  store double %add20, double* %arrayidx19, align 8, !tbaa !62
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp12 = icmp slt i64 %indvars.iv, %8
  br i1 %cmp12, label %omp.inner.for.body, label %omp.loop.exit, !llvm.loop !618

omp.loop.exit:                                    ; preds = %omp.inner.for.body, %middle.block, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.loop.exit, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..229(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub7 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre46 = bitcast i32* %.omp.is_last to i8*
  %.pre47 = bitcast i32* %.omp.stride to i8*
  %.pre49 = bitcast i32* %.omp.comb.ub to i8*
  %.pre51 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub7, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 91, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp1141 = icmp sgt i32 %5, %sub7
  %cond42 = select i1 %cmp1141, i32 %sub7, i32 %5
  store i32 %cond42, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1343 = icmp sgt i32 %6, %cond42
  br i1 %cmp1343, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  %.capture_expr..casted19.sroa.0.0.insert.ext = and i64 %.capture_expr.1, 4294967295
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %7 = phi i32 [ %cond42, %omp.inner.for.cond.preheader.lr.ph ], [ %cond, %omp.dispatch.inc ]
  %8 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add22, %omp.dispatch.inc ]
  %cmp1538 = icmp sgt i32 %8, %7
  br i1 %cmp1538, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge: ; preds = %omp.inner.for.cond.preheader
  %.pre = load i32, i32* %.omp.stride, align 4, !tbaa !58
  br label %omp.dispatch.inc

omp.inner.for.body:                               ; preds = %omp.inner.for.cond.preheader, %omp.inner.for.body
  %9 = phi i32 [ %15, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %10 = phi i32 [ %14, %omp.inner.for.body ], [ %7, %omp.inner.for.cond.preheader ]
  %.omp.iv.039 = phi i32 [ %add21, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %11 = zext i32 %9 to i64
  %12 = zext i32 %10 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 8, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64, i64)* @.omp_outlined..230 to void (i32*, i32*, ...)*), i64 %11, i64 %12, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext, i64 %.capture_expr..casted19.sroa.0.0.insert.ext) #6, !llvm.mem.parallel_loop_access !619
  %13 = load i32, i32* %.omp.stride, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !619
  %add21 = add nsw i32 %13, %.omp.iv.039
  %14 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !619
  %cmp15 = icmp sgt i32 %add21, %14
  %15 = load i32, i32* %.omp.comb.lb, align 4
  br i1 %cmp15, label %omp.dispatch.inc, label %omp.inner.for.body, !llvm.loop !619

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge
  %16 = phi i32 [ %.pre, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %13, %omp.inner.for.body ]
  %.lcssa37 = phi i32 [ %7, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %14, %omp.inner.for.body ]
  %.lcssa = phi i32 [ %8, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %15, %omp.inner.for.body ]
  %add22 = add nsw i32 %.lcssa, %16
  store i32 %add22, i32* %.omp.comb.lb, align 4, !tbaa !58
  %add23 = add nsw i32 %.lcssa37, %16
  %cmp11 = icmp sgt i32 %add23, %sub7
  %cond = select i1 %cmp11, i32 %sub7, i32 %add23
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp13 = icmp sgt i32 %add22, %cond
  br i1 %cmp13, label %omp.dispatch.end, label %omp.inner.for.cond.preheader

omp.dispatch.end:                                 ; preds = %omp.dispatch.inc, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.dispatch.end
  %.pre-phi52 = phi i8* [ %.pre51, %entry.omp.precond.end_crit_edge ], [ %0, %omp.dispatch.end ]
  %.pre-phi50 = phi i8* [ %.pre49, %entry.omp.precond.end_crit_edge ], [ %1, %omp.dispatch.end ]
  %.pre-phi48 = phi i8* [ %.pre47, %entry.omp.precond.end_crit_edge ], [ %2, %omp.dispatch.end ]
  %.pre-phi = phi i8* [ %.pre46, %entry.omp.precond.end_crit_edge ], [ %3, %omp.dispatch.end ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi48) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi50) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi52) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..230(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv10 = trunc i64 %.previous.lb. to i32
  %conv11 = trunc i64 %.previous.ub. to i32
  store i32 %conv10, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv11, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @1, i32 %4, i32 33, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.ub, align 4, !tbaa !58
  %conv1348 = sext i32 %5 to i64
  %cmp1449 = icmp ugt i64 %conv1348, %.previous.ub.
  %cond50 = select i1 %cmp1449, i64 %.previous.ub., i64 %conv1348
  %conv1751 = trunc i64 %cond50 to i32
  store i32 %conv1751, i32* %.omp.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %cmp1852 = icmp sgt i32 %6, %conv1751
  br i1 %cmp1852, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %7 = load i32, i32* %.omp.stride, align 4, !tbaa !58
  %8 = sext i32 %6 to i64
  %9 = sext i32 %7 to i64
  %10 = sub nsw i64 1, %8
  %11 = sub nsw i64 0, %9
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %indvar = phi i64 [ 0, %omp.inner.for.cond.preheader.lr.ph ], [ %indvar.next, %omp.dispatch.inc ]
  %indvars.iv = phi i64 [ %8, %omp.inner.for.cond.preheader.lr.ph ], [ %indvars.iv.next, %omp.dispatch.inc ]
  %12 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add30, %omp.dispatch.inc ]
  %conv1753 = phi i32 [ %conv1751, %omp.inner.for.cond.preheader.lr.ph ], [ %conv17, %omp.dispatch.inc ]
  %13 = mul i64 %indvar, %9
  %14 = add i64 %13, %8
  %scevgep = getelementptr double, double* %a, i64 %14
  %15 = add i64 %14, 1
  %scevgep57 = getelementptr double, double* %a, i64 %15
  %16 = mul i64 %indvar, %11
  %17 = sub i64 %16, %8
  %scevgep61 = getelementptr double, double* %b, i64 %14
  %scevgep63 = getelementptr double, double* %b, i64 %15
  %scevgep66 = getelementptr double, double* %c, i64 %14
  %scevgep68 = getelementptr double, double* %c, i64 %15
  %cmp2046 = icmp sgt i32 %12, %conv1753
  br i1 %cmp2046, label %omp.dispatch.inc, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %omp.inner.for.cond.preheader
  %18 = add i64 %10, %16
  %19 = sext i32 %conv1753 to i64
  %20 = icmp sgt i64 %14, %19
  %smax = select i1 %20, i64 %14, i64 %19
  %21 = add i64 %18, %smax
  %min.iters.check = icmp ult i64 %21, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader1, label %vector.memcheck

vector.memcheck:                                  ; preds = %omp.inner.for.body.preheader
  %22 = add i64 %smax, %17
  %scevgep59 = getelementptr double, double* %scevgep57, i64 %22
  %scevgep64 = getelementptr double, double* %scevgep63, i64 %22
  %scevgep69 = getelementptr double, double* %scevgep68, i64 %22
  %bound0 = icmp ult double* %scevgep, %scevgep64
  %bound1 = icmp ult double* %scevgep61, %scevgep59
  %found.conflict = and i1 %bound0, %bound1
  %bound071 = icmp ult double* %scevgep, %scevgep69
  %bound172 = icmp ult double* %scevgep66, %scevgep59
  %found.conflict73 = and i1 %bound071, %bound172
  %conflict.rdx = or i1 %found.conflict, %found.conflict73
  br i1 %conflict.rdx, label %omp.inner.for.body.preheader1, label %vector.ph

vector.ph:                                        ; preds = %vector.memcheck
  %n.mod.vf = urem i64 %21, 24
  %n.vec = sub i64 %21, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %indvars.iv
  %23 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %24 = bitcast double* %23 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %24, align 8, !tbaa !62, !alias.scope !620
  %25 = getelementptr inbounds double, double* %23, i64 2
  %26 = bitcast double* %25 to <2 x double>*
  %wide.load85 = load <2 x double>, <2 x double>* %26, align 8, !tbaa !62, !alias.scope !620
  %27 = getelementptr inbounds double, double* %23, i64 4
  %28 = bitcast double* %27 to <2 x double>*
  %wide.load86 = load <2 x double>, <2 x double>* %28, align 8, !tbaa !62, !alias.scope !620
  %29 = getelementptr inbounds double, double* %23, i64 6
  %30 = bitcast double* %29 to <2 x double>*
  %wide.load87 = load <2 x double>, <2 x double>* %30, align 8, !tbaa !62, !alias.scope !620
  %31 = getelementptr inbounds double, double* %23, i64 8
  %32 = bitcast double* %31 to <2 x double>*
  %wide.load88 = load <2 x double>, <2 x double>* %32, align 8, !tbaa !62, !alias.scope !620
  %33 = getelementptr inbounds double, double* %23, i64 10
  %34 = bitcast double* %33 to <2 x double>*
  %wide.load89 = load <2 x double>, <2 x double>* %34, align 8, !tbaa !62, !alias.scope !620
  %35 = getelementptr inbounds double, double* %23, i64 12
  %36 = bitcast double* %35 to <2 x double>*
  %wide.load90 = load <2 x double>, <2 x double>* %36, align 8, !tbaa !62, !alias.scope !620
  %37 = getelementptr inbounds double, double* %23, i64 14
  %38 = bitcast double* %37 to <2 x double>*
  %wide.load91 = load <2 x double>, <2 x double>* %38, align 8, !tbaa !62, !alias.scope !620
  %39 = getelementptr inbounds double, double* %23, i64 16
  %40 = bitcast double* %39 to <2 x double>*
  %wide.load92 = load <2 x double>, <2 x double>* %40, align 8, !tbaa !62, !alias.scope !620
  %41 = getelementptr inbounds double, double* %23, i64 18
  %42 = bitcast double* %41 to <2 x double>*
  %wide.load93 = load <2 x double>, <2 x double>* %42, align 8, !tbaa !62, !alias.scope !620
  %43 = getelementptr inbounds double, double* %23, i64 20
  %44 = bitcast double* %43 to <2 x double>*
  %wide.load94 = load <2 x double>, <2 x double>* %44, align 8, !tbaa !62, !alias.scope !620
  %45 = getelementptr inbounds double, double* %23, i64 22
  %46 = bitcast double* %45 to <2 x double>*
  %wide.load95 = load <2 x double>, <2 x double>* %46, align 8, !tbaa !62, !alias.scope !620
  %47 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %48 = bitcast double* %47 to <2 x double>*
  %wide.load96 = load <2 x double>, <2 x double>* %48, align 8, !tbaa !62, !alias.scope !623
  %49 = getelementptr inbounds double, double* %47, i64 2
  %50 = bitcast double* %49 to <2 x double>*
  %wide.load97 = load <2 x double>, <2 x double>* %50, align 8, !tbaa !62, !alias.scope !623
  %51 = getelementptr inbounds double, double* %47, i64 4
  %52 = bitcast double* %51 to <2 x double>*
  %wide.load98 = load <2 x double>, <2 x double>* %52, align 8, !tbaa !62, !alias.scope !623
  %53 = getelementptr inbounds double, double* %47, i64 6
  %54 = bitcast double* %53 to <2 x double>*
  %wide.load99 = load <2 x double>, <2 x double>* %54, align 8, !tbaa !62, !alias.scope !623
  %55 = getelementptr inbounds double, double* %47, i64 8
  %56 = bitcast double* %55 to <2 x double>*
  %wide.load100 = load <2 x double>, <2 x double>* %56, align 8, !tbaa !62, !alias.scope !623
  %57 = getelementptr inbounds double, double* %47, i64 10
  %58 = bitcast double* %57 to <2 x double>*
  %wide.load101 = load <2 x double>, <2 x double>* %58, align 8, !tbaa !62, !alias.scope !623
  %59 = getelementptr inbounds double, double* %47, i64 12
  %60 = bitcast double* %59 to <2 x double>*
  %wide.load102 = load <2 x double>, <2 x double>* %60, align 8, !tbaa !62, !alias.scope !623
  %61 = getelementptr inbounds double, double* %47, i64 14
  %62 = bitcast double* %61 to <2 x double>*
  %wide.load103 = load <2 x double>, <2 x double>* %62, align 8, !tbaa !62, !alias.scope !623
  %63 = getelementptr inbounds double, double* %47, i64 16
  %64 = bitcast double* %63 to <2 x double>*
  %wide.load104 = load <2 x double>, <2 x double>* %64, align 8, !tbaa !62, !alias.scope !623
  %65 = getelementptr inbounds double, double* %47, i64 18
  %66 = bitcast double* %65 to <2 x double>*
  %wide.load105 = load <2 x double>, <2 x double>* %66, align 8, !tbaa !62, !alias.scope !623
  %67 = getelementptr inbounds double, double* %47, i64 20
  %68 = bitcast double* %67 to <2 x double>*
  %wide.load106 = load <2 x double>, <2 x double>* %68, align 8, !tbaa !62, !alias.scope !623
  %69 = getelementptr inbounds double, double* %47, i64 22
  %70 = bitcast double* %69 to <2 x double>*
  %wide.load107 = load <2 x double>, <2 x double>* %70, align 8, !tbaa !62, !alias.scope !623
  %71 = fadd <2 x double> %wide.load, %wide.load96
  %72 = fadd <2 x double> %wide.load85, %wide.load97
  %73 = fadd <2 x double> %wide.load86, %wide.load98
  %74 = fadd <2 x double> %wide.load87, %wide.load99
  %75 = fadd <2 x double> %wide.load88, %wide.load100
  %76 = fadd <2 x double> %wide.load89, %wide.load101
  %77 = fadd <2 x double> %wide.load90, %wide.load102
  %78 = fadd <2 x double> %wide.load91, %wide.load103
  %79 = fadd <2 x double> %wide.load92, %wide.load104
  %80 = fadd <2 x double> %wide.load93, %wide.load105
  %81 = fadd <2 x double> %wide.load94, %wide.load106
  %82 = fadd <2 x double> %wide.load95, %wide.load107
  %83 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %84 = bitcast double* %83 to <2 x double>*
  %wide.load108 = load <2 x double>, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %85 = getelementptr inbounds double, double* %83, i64 2
  %86 = bitcast double* %85 to <2 x double>*
  %wide.load109 = load <2 x double>, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %87 = getelementptr inbounds double, double* %83, i64 4
  %88 = bitcast double* %87 to <2 x double>*
  %wide.load110 = load <2 x double>, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %89 = getelementptr inbounds double, double* %83, i64 6
  %90 = bitcast double* %89 to <2 x double>*
  %wide.load111 = load <2 x double>, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %91 = getelementptr inbounds double, double* %83, i64 8
  %92 = bitcast double* %91 to <2 x double>*
  %wide.load112 = load <2 x double>, <2 x double>* %92, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %93 = getelementptr inbounds double, double* %83, i64 10
  %94 = bitcast double* %93 to <2 x double>*
  %wide.load113 = load <2 x double>, <2 x double>* %94, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %95 = getelementptr inbounds double, double* %83, i64 12
  %96 = bitcast double* %95 to <2 x double>*
  %wide.load114 = load <2 x double>, <2 x double>* %96, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %97 = getelementptr inbounds double, double* %83, i64 14
  %98 = bitcast double* %97 to <2 x double>*
  %wide.load115 = load <2 x double>, <2 x double>* %98, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %99 = getelementptr inbounds double, double* %83, i64 16
  %100 = bitcast double* %99 to <2 x double>*
  %wide.load116 = load <2 x double>, <2 x double>* %100, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %101 = getelementptr inbounds double, double* %83, i64 18
  %102 = bitcast double* %101 to <2 x double>*
  %wide.load117 = load <2 x double>, <2 x double>* %102, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %103 = getelementptr inbounds double, double* %83, i64 20
  %104 = bitcast double* %103 to <2 x double>*
  %wide.load118 = load <2 x double>, <2 x double>* %104, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %105 = getelementptr inbounds double, double* %83, i64 22
  %106 = bitcast double* %105 to <2 x double>*
  %wide.load119 = load <2 x double>, <2 x double>* %106, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %107 = fadd <2 x double> %71, %wide.load108
  %108 = fadd <2 x double> %72, %wide.load109
  %109 = fadd <2 x double> %73, %wide.load110
  %110 = fadd <2 x double> %74, %wide.load111
  %111 = fadd <2 x double> %75, %wide.load112
  %112 = fadd <2 x double> %76, %wide.load113
  %113 = fadd <2 x double> %77, %wide.load114
  %114 = fadd <2 x double> %78, %wide.load115
  %115 = fadd <2 x double> %79, %wide.load116
  %116 = fadd <2 x double> %80, %wide.load117
  %117 = fadd <2 x double> %81, %wide.load118
  %118 = fadd <2 x double> %82, %wide.load119
  store <2 x double> %107, <2 x double>* %84, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  store <2 x double> %108, <2 x double>* %86, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  store <2 x double> %109, <2 x double>* %88, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  store <2 x double> %110, <2 x double>* %90, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  store <2 x double> %111, <2 x double>* %92, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  store <2 x double> %112, <2 x double>* %94, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  store <2 x double> %113, <2 x double>* %96, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  store <2 x double> %114, <2 x double>* %98, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  store <2 x double> %115, <2 x double>* %100, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  store <2 x double> %116, <2 x double>* %102, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  store <2 x double> %117, <2 x double>* %104, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  store <2 x double> %118, <2 x double>* %106, align 8, !tbaa !62, !alias.scope !625, !noalias !627
  %index.next = add i64 %index, 24
  %119 = icmp eq i64 %index.next, %n.vec
  br i1 %119, label %middle.block, label %vector.body, !llvm.loop !628

middle.block:                                     ; preds = %vector.body
  %ind.end = add i64 %n.vec, %indvars.iv
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.inc, label %omp.inner.for.body.preheader1

omp.inner.for.body.preheader1:                    ; preds = %middle.block, %vector.memcheck, %omp.inner.for.body.preheader
  %indvars.iv54.ph = phi i64 [ %ind.end, %middle.block ], [ %indvars.iv, %omp.inner.for.body.preheader ], [ %indvars.iv, %vector.memcheck ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader1, %omp.inner.for.body
  %indvars.iv54 = phi i64 [ %indvars.iv.next55, %omp.inner.for.body ], [ %indvars.iv54.ph, %omp.inner.for.body.preheader1 ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv54
  %120 = load double, double* %arrayidx, align 8, !tbaa !62
  %arrayidx24 = getelementptr inbounds double, double* %c, i64 %indvars.iv54
  %121 = load double, double* %arrayidx24, align 8, !tbaa !62
  %add25 = fadd double %120, %121
  %arrayidx27 = getelementptr inbounds double, double* %a, i64 %indvars.iv54
  %122 = load double, double* %arrayidx27, align 8, !tbaa !62
  %add28 = fadd double %122, %add25
  store double %add28, double* %arrayidx27, align 8, !tbaa !62
  %indvars.iv.next55 = add i64 %indvars.iv54, 1
  %cmp20 = icmp slt i64 %indvars.iv54, %19
  br i1 %cmp20, label %omp.inner.for.body, label %omp.dispatch.inc, !llvm.loop !629

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %middle.block, %omp.inner.for.cond.preheader
  %add30 = add nsw i32 %12, %7
  %add31 = add nsw i32 %conv1753, %7
  %conv13 = sext i32 %add31 to i64
  %cmp14 = icmp ugt i64 %conv13, %.previous.ub.
  %cond = select i1 %cmp14, i64 %.previous.ub., i64 %conv13
  %conv17 = trunc i64 %cond to i32
  %cmp18 = icmp sgt i32 %add30, %conv17
  %indvars.iv.next = add i64 %indvars.iv, %9
  %indvar.next = add i64 %indvar, 1
  br i1 %cmp18, label %omp.dispatch.cond.omp.dispatch.end_crit_edge, label %omp.inner.for.cond.preheader

omp.dispatch.cond.omp.dispatch.end_crit_edge:     ; preds = %omp.dispatch.inc
  %conv17.le = trunc i64 %cond to i32
  store i32 %add30, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv17.le, i32* %.omp.ub, align 4, !tbaa !58
  br label %omp.dispatch.end

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.omp.dispatch.end_crit_edge, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.dispatch.end, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..236(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub4 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre41 = bitcast i32* %.omp.is_last to i8*
  %.pre42 = bitcast i32* %.omp.stride to i8*
  %.pre44 = bitcast i32* %.omp.comb.ub to i8*
  %.pre46 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub4, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 91, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp836 = icmp sgt i32 %5, %sub4
  %cond37 = select i1 %cmp836, i32 %sub4, i32 %5
  store i32 %cond37, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1038 = icmp sgt i32 %6, %cond37
  br i1 %cmp1038, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %7 = phi i32 [ %cond37, %omp.inner.for.cond.preheader.lr.ph ], [ %cond, %omp.dispatch.inc ]
  %8 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add17, %omp.dispatch.inc ]
  %cmp1233 = icmp sgt i32 %8, %7
  br i1 %cmp1233, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge: ; preds = %omp.inner.for.cond.preheader
  %.pre = load i32, i32* %.omp.stride, align 4, !tbaa !58
  br label %omp.dispatch.inc

omp.inner.for.body:                               ; preds = %omp.inner.for.cond.preheader, %omp.inner.for.body
  %9 = phi i32 [ %15, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %10 = phi i32 [ %14, %omp.inner.for.body ], [ %7, %omp.inner.for.cond.preheader ]
  %.omp.iv.034 = phi i32 [ %add16, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %11 = zext i32 %9 to i64
  %12 = zext i32 %10 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64)* @.omp_outlined..237 to void (i32*, i32*, ...)*), i64 %11, i64 %12, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext) #6, !llvm.mem.parallel_loop_access !630
  %13 = load i32, i32* %.omp.stride, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !630
  %add16 = add nsw i32 %13, %.omp.iv.034
  %14 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !630
  %cmp12 = icmp sgt i32 %add16, %14
  %15 = load i32, i32* %.omp.comb.lb, align 4
  br i1 %cmp12, label %omp.dispatch.inc, label %omp.inner.for.body, !llvm.loop !630

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge
  %16 = phi i32 [ %.pre, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %13, %omp.inner.for.body ]
  %.lcssa32 = phi i32 [ %7, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %14, %omp.inner.for.body ]
  %.lcssa = phi i32 [ %8, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %15, %omp.inner.for.body ]
  %add17 = add nsw i32 %.lcssa, %16
  store i32 %add17, i32* %.omp.comb.lb, align 4, !tbaa !58
  %add18 = add nsw i32 %.lcssa32, %16
  %cmp8 = icmp sgt i32 %add18, %sub4
  %cond = select i1 %cmp8, i32 %sub4, i32 %add18
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp10 = icmp sgt i32 %add17, %cond
  br i1 %cmp10, label %omp.dispatch.end, label %omp.inner.for.cond.preheader

omp.dispatch.end:                                 ; preds = %omp.dispatch.inc, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.dispatch.end
  %.pre-phi47 = phi i8* [ %.pre46, %entry.omp.precond.end_crit_edge ], [ %0, %omp.dispatch.end ]
  %.pre-phi45 = phi i8* [ %.pre44, %entry.omp.precond.end_crit_edge ], [ %1, %omp.dispatch.end ]
  %.pre-phi43 = phi i8* [ %.pre42, %entry.omp.precond.end_crit_edge ], [ %2, %omp.dispatch.end ]
  %.pre-phi = phi i8* [ %.pre41, %entry.omp.precond.end_crit_edge ], [ %3, %omp.dispatch.end ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi43) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi45) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi47) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..237(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv7 = trunc i64 %.previous.lb. to i32
  %conv8 = trunc i64 %.previous.ub. to i32
  store i32 %conv7, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv8, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 35, i32 %conv7, i32 %conv8, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool33 = icmp eq i32 %5, 0
  br i1 %tobool33, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %omp.precond.then, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !631
  %cmp1031 = icmp sgt i32 %7, %8
  br i1 %cmp1031, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 24
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds double, double* %14, i64 12
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %14, i64 14
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %14, i64 16
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %14, i64 18
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %14, i64 20
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %14, i64 22
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62
  %40 = getelementptr inbounds double, double* %38, i64 2
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62
  %42 = getelementptr inbounds double, double* %38, i64 4
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62
  %44 = getelementptr inbounds double, double* %38, i64 6
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %38, i64 8
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %38, i64 10
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %38, i64 12
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %38, i64 14
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %38, i64 16
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = getelementptr inbounds double, double* %38, i64 18
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62
  %58 = getelementptr inbounds double, double* %38, i64 20
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62
  %60 = getelementptr inbounds double, double* %38, i64 22
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62
  %62 = fadd <2 x double> %wide.load, %wide.load56
  %63 = fadd <2 x double> %wide.load45, %wide.load57
  %64 = fadd <2 x double> %wide.load46, %wide.load58
  %65 = fadd <2 x double> %wide.load47, %wide.load59
  %66 = fadd <2 x double> %wide.load48, %wide.load60
  %67 = fadd <2 x double> %wide.load49, %wide.load61
  %68 = fadd <2 x double> %wide.load50, %wide.load62
  %69 = fadd <2 x double> %wide.load51, %wide.load63
  %70 = fadd <2 x double> %wide.load52, %wide.load64
  %71 = fadd <2 x double> %wide.load53, %wide.load65
  %72 = fadd <2 x double> %wide.load54, %wide.load66
  %73 = fadd <2 x double> %wide.load55, %wide.load67
  %74 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62
  %76 = getelementptr inbounds double, double* %74, i64 2
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62
  %78 = getelementptr inbounds double, double* %74, i64 4
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62
  %80 = getelementptr inbounds double, double* %74, i64 6
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %74, i64 8
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %74, i64 10
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %74, i64 12
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %74, i64 14
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %74, i64 16
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = getelementptr inbounds double, double* %74, i64 18
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62
  %94 = getelementptr inbounds double, double* %74, i64 20
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62
  %96 = getelementptr inbounds double, double* %74, i64 22
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62
  %98 = fadd <2 x double> %62, %wide.load68
  %99 = fadd <2 x double> %63, %wide.load69
  %100 = fadd <2 x double> %64, %wide.load70
  %101 = fadd <2 x double> %65, %wide.load71
  %102 = fadd <2 x double> %66, %wide.load72
  %103 = fadd <2 x double> %67, %wide.load73
  %104 = fadd <2 x double> %68, %wide.load74
  %105 = fadd <2 x double> %69, %wide.load75
  %106 = fadd <2 x double> %70, %wide.load76
  %107 = fadd <2 x double> %71, %wide.load77
  %108 = fadd <2 x double> %72, %wide.load78
  %109 = fadd <2 x double> %73, %wide.load79
  store <2 x double> %98, <2 x double>* %75, align 8, !tbaa !62
  store <2 x double> %99, <2 x double>* %77, align 8, !tbaa !62
  store <2 x double> %100, <2 x double>* %79, align 8, !tbaa !62
  store <2 x double> %101, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %102, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %103, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %104, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %105, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %106, <2 x double>* %91, align 8, !tbaa !62
  store <2 x double> %107, <2 x double>* %93, align 8, !tbaa !62
  store <2 x double> %108, <2 x double>* %95, align 8, !tbaa !62
  store <2 x double> %109, <2 x double>* %97, align 8, !tbaa !62
  %index.next = add i64 %index, 24
  %110 = icmp eq i64 %index.next, %n.vec
  br i1 %110, label %middle.block, label %vector.body, !llvm.loop !632

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %111 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !631
  %arrayidx14 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %112 = load double, double* %arrayidx14, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !631
  %add15 = fadd double %111, %112
  %arrayidx17 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %113 = load double, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !631
  %add18 = fadd double %113, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !631
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp10 = icmp slt i64 %indvars.iv, %10
  br i1 %cmp10, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !633

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %omp.precond.then
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.dispatch.end, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..243(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub7 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre46 = bitcast i32* %.omp.is_last to i8*
  %.pre47 = bitcast i32* %.omp.stride to i8*
  %.pre49 = bitcast i32* %.omp.comb.ub to i8*
  %.pre51 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub7, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 91, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp1141 = icmp sgt i32 %5, %sub7
  %cond42 = select i1 %cmp1141, i32 %sub7, i32 %5
  store i32 %cond42, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1343 = icmp sgt i32 %6, %cond42
  br i1 %cmp1343, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  %.capture_expr..casted19.sroa.0.0.insert.ext = and i64 %.capture_expr.1, 4294967295
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %7 = phi i32 [ %cond42, %omp.inner.for.cond.preheader.lr.ph ], [ %cond, %omp.dispatch.inc ]
  %8 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add22, %omp.dispatch.inc ]
  %cmp1538 = icmp sgt i32 %8, %7
  br i1 %cmp1538, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge: ; preds = %omp.inner.for.cond.preheader
  %.pre = load i32, i32* %.omp.stride, align 4, !tbaa !58
  br label %omp.dispatch.inc

omp.inner.for.body:                               ; preds = %omp.inner.for.cond.preheader, %omp.inner.for.body
  %9 = phi i32 [ %15, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %10 = phi i32 [ %14, %omp.inner.for.body ], [ %7, %omp.inner.for.cond.preheader ]
  %.omp.iv.039 = phi i32 [ %add21, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %11 = zext i32 %9 to i64
  %12 = zext i32 %10 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 8, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64, i64)* @.omp_outlined..244 to void (i32*, i32*, ...)*), i64 %11, i64 %12, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext, i64 %.capture_expr..casted19.sroa.0.0.insert.ext) #6, !llvm.mem.parallel_loop_access !634
  %13 = load i32, i32* %.omp.stride, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !634
  %add21 = add nsw i32 %13, %.omp.iv.039
  %14 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !634
  %cmp15 = icmp sgt i32 %add21, %14
  %15 = load i32, i32* %.omp.comb.lb, align 4
  br i1 %cmp15, label %omp.dispatch.inc, label %omp.inner.for.body, !llvm.loop !634

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge
  %16 = phi i32 [ %.pre, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %13, %omp.inner.for.body ]
  %.lcssa37 = phi i32 [ %7, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %14, %omp.inner.for.body ]
  %.lcssa = phi i32 [ %8, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %15, %omp.inner.for.body ]
  %add22 = add nsw i32 %.lcssa, %16
  store i32 %add22, i32* %.omp.comb.lb, align 4, !tbaa !58
  %add23 = add nsw i32 %.lcssa37, %16
  %cmp11 = icmp sgt i32 %add23, %sub7
  %cond = select i1 %cmp11, i32 %sub7, i32 %add23
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp13 = icmp sgt i32 %add22, %cond
  br i1 %cmp13, label %omp.dispatch.end, label %omp.inner.for.cond.preheader

omp.dispatch.end:                                 ; preds = %omp.dispatch.inc, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.dispatch.end
  %.pre-phi52 = phi i8* [ %.pre51, %entry.omp.precond.end_crit_edge ], [ %0, %omp.dispatch.end ]
  %.pre-phi50 = phi i8* [ %.pre49, %entry.omp.precond.end_crit_edge ], [ %1, %omp.dispatch.end ]
  %.pre-phi48 = phi i8* [ %.pre47, %entry.omp.precond.end_crit_edge ], [ %2, %omp.dispatch.end ]
  %.pre-phi = phi i8* [ %.pre46, %entry.omp.precond.end_crit_edge ], [ %3, %omp.dispatch.end ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi48) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi50) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi52) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..244(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv10 = trunc i64 %.previous.lb. to i32
  %conv11 = trunc i64 %.previous.ub. to i32
  store i32 %conv10, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv11, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 35, i32 %conv10, i32 %conv11, i32 1, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool36 = icmp eq i32 %5, 0
  br i1 %tobool36, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %omp.precond.then, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !635
  %cmp1334 = icmp sgt i32 %7, %8
  br i1 %cmp1334, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 24
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds double, double* %14, i64 12
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %14, i64 14
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %14, i64 16
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %14, i64 18
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %14, i64 20
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %14, i64 22
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62
  %40 = getelementptr inbounds double, double* %38, i64 2
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62
  %42 = getelementptr inbounds double, double* %38, i64 4
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62
  %44 = getelementptr inbounds double, double* %38, i64 6
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %38, i64 8
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %38, i64 10
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %38, i64 12
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %38, i64 14
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %38, i64 16
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = getelementptr inbounds double, double* %38, i64 18
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62
  %58 = getelementptr inbounds double, double* %38, i64 20
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62
  %60 = getelementptr inbounds double, double* %38, i64 22
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62
  %62 = fadd <2 x double> %wide.load, %wide.load59
  %63 = fadd <2 x double> %wide.load48, %wide.load60
  %64 = fadd <2 x double> %wide.load49, %wide.load61
  %65 = fadd <2 x double> %wide.load50, %wide.load62
  %66 = fadd <2 x double> %wide.load51, %wide.load63
  %67 = fadd <2 x double> %wide.load52, %wide.load64
  %68 = fadd <2 x double> %wide.load53, %wide.load65
  %69 = fadd <2 x double> %wide.load54, %wide.load66
  %70 = fadd <2 x double> %wide.load55, %wide.load67
  %71 = fadd <2 x double> %wide.load56, %wide.load68
  %72 = fadd <2 x double> %wide.load57, %wide.load69
  %73 = fadd <2 x double> %wide.load58, %wide.load70
  %74 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62
  %76 = getelementptr inbounds double, double* %74, i64 2
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62
  %78 = getelementptr inbounds double, double* %74, i64 4
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62
  %80 = getelementptr inbounds double, double* %74, i64 6
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %74, i64 8
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %74, i64 10
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %74, i64 12
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %74, i64 14
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %74, i64 16
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = getelementptr inbounds double, double* %74, i64 18
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62
  %94 = getelementptr inbounds double, double* %74, i64 20
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62
  %96 = getelementptr inbounds double, double* %74, i64 22
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62
  %98 = fadd <2 x double> %62, %wide.load71
  %99 = fadd <2 x double> %63, %wide.load72
  %100 = fadd <2 x double> %64, %wide.load73
  %101 = fadd <2 x double> %65, %wide.load74
  %102 = fadd <2 x double> %66, %wide.load75
  %103 = fadd <2 x double> %67, %wide.load76
  %104 = fadd <2 x double> %68, %wide.load77
  %105 = fadd <2 x double> %69, %wide.load78
  %106 = fadd <2 x double> %70, %wide.load79
  %107 = fadd <2 x double> %71, %wide.load80
  %108 = fadd <2 x double> %72, %wide.load81
  %109 = fadd <2 x double> %73, %wide.load82
  store <2 x double> %98, <2 x double>* %75, align 8, !tbaa !62
  store <2 x double> %99, <2 x double>* %77, align 8, !tbaa !62
  store <2 x double> %100, <2 x double>* %79, align 8, !tbaa !62
  store <2 x double> %101, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %102, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %103, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %104, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %105, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %106, <2 x double>* %91, align 8, !tbaa !62
  store <2 x double> %107, <2 x double>* %93, align 8, !tbaa !62
  store <2 x double> %108, <2 x double>* %95, align 8, !tbaa !62
  store <2 x double> %109, <2 x double>* %97, align 8, !tbaa !62
  %index.next = add i64 %index, 24
  %110 = icmp eq i64 %index.next, %n.vec
  br i1 %110, label %middle.block, label %vector.body, !llvm.loop !636

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %111 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !635
  %arrayidx17 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %112 = load double, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !635
  %add18 = fadd double %111, %112
  %arrayidx20 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %113 = load double, double* %arrayidx20, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !635
  %add21 = fadd double %113, %add18
  store double %add21, double* %arrayidx20, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !635
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp13 = icmp slt i64 %indvars.iv, %10
  br i1 %cmp13, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !637

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %omp.precond.then
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.dispatch.end, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..250(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub4 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre41 = bitcast i32* %.omp.is_last to i8*
  %.pre42 = bitcast i32* %.omp.stride to i8*
  %.pre44 = bitcast i32* %.omp.comb.ub to i8*
  %.pre46 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub4, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 91, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp836 = icmp sgt i32 %5, %sub4
  %cond37 = select i1 %cmp836, i32 %sub4, i32 %5
  store i32 %cond37, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1038 = icmp sgt i32 %6, %cond37
  br i1 %cmp1038, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %7 = phi i32 [ %cond37, %omp.inner.for.cond.preheader.lr.ph ], [ %cond, %omp.dispatch.inc ]
  %8 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add17, %omp.dispatch.inc ]
  %cmp1233 = icmp sgt i32 %8, %7
  br i1 %cmp1233, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge: ; preds = %omp.inner.for.cond.preheader
  %.pre = load i32, i32* %.omp.stride, align 4, !tbaa !58
  br label %omp.dispatch.inc

omp.inner.for.body:                               ; preds = %omp.inner.for.cond.preheader, %omp.inner.for.body
  %9 = phi i32 [ %15, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %10 = phi i32 [ %14, %omp.inner.for.body ], [ %7, %omp.inner.for.cond.preheader ]
  %.omp.iv.034 = phi i32 [ %add16, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %11 = zext i32 %9 to i64
  %12 = zext i32 %10 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64)* @.omp_outlined..251 to void (i32*, i32*, ...)*), i64 %11, i64 %12, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext) #6, !llvm.mem.parallel_loop_access !638
  %13 = load i32, i32* %.omp.stride, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !638
  %add16 = add nsw i32 %13, %.omp.iv.034
  %14 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !638
  %cmp12 = icmp sgt i32 %add16, %14
  %15 = load i32, i32* %.omp.comb.lb, align 4
  br i1 %cmp12, label %omp.dispatch.inc, label %omp.inner.for.body, !llvm.loop !638

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge
  %16 = phi i32 [ %.pre, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %13, %omp.inner.for.body ]
  %.lcssa32 = phi i32 [ %7, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %14, %omp.inner.for.body ]
  %.lcssa = phi i32 [ %8, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %15, %omp.inner.for.body ]
  %add17 = add nsw i32 %.lcssa, %16
  store i32 %add17, i32* %.omp.comb.lb, align 4, !tbaa !58
  %add18 = add nsw i32 %.lcssa32, %16
  %cmp8 = icmp sgt i32 %add18, %sub4
  %cond = select i1 %cmp8, i32 %sub4, i32 %add18
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp10 = icmp sgt i32 %add17, %cond
  br i1 %cmp10, label %omp.dispatch.end, label %omp.inner.for.cond.preheader

omp.dispatch.end:                                 ; preds = %omp.dispatch.inc, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.dispatch.end
  %.pre-phi47 = phi i8* [ %.pre46, %entry.omp.precond.end_crit_edge ], [ %0, %omp.dispatch.end ]
  %.pre-phi45 = phi i8* [ %.pre44, %entry.omp.precond.end_crit_edge ], [ %1, %omp.dispatch.end ]
  %.pre-phi43 = phi i8* [ %.pre42, %entry.omp.precond.end_crit_edge ], [ %2, %omp.dispatch.end ]
  %.pre-phi = phi i8* [ %.pre41, %entry.omp.precond.end_crit_edge ], [ %3, %omp.dispatch.end ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi43) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi45) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi47) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..251(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv7 = trunc i64 %.previous.lb. to i32
  %conv8 = trunc i64 %.previous.ub. to i32
  store i32 %conv7, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv8, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 36, i32 %conv7, i32 %conv8, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool33 = icmp eq i32 %5, 0
  br i1 %tobool33, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %omp.precond.then, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !639
  %cmp1031 = icmp sgt i32 %7, %8
  br i1 %cmp1031, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 24
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds double, double* %14, i64 12
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %14, i64 14
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %14, i64 16
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %14, i64 18
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %14, i64 20
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %14, i64 22
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62
  %40 = getelementptr inbounds double, double* %38, i64 2
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62
  %42 = getelementptr inbounds double, double* %38, i64 4
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62
  %44 = getelementptr inbounds double, double* %38, i64 6
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %38, i64 8
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %38, i64 10
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %38, i64 12
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %38, i64 14
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %38, i64 16
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = getelementptr inbounds double, double* %38, i64 18
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62
  %58 = getelementptr inbounds double, double* %38, i64 20
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62
  %60 = getelementptr inbounds double, double* %38, i64 22
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62
  %62 = fadd <2 x double> %wide.load, %wide.load56
  %63 = fadd <2 x double> %wide.load45, %wide.load57
  %64 = fadd <2 x double> %wide.load46, %wide.load58
  %65 = fadd <2 x double> %wide.load47, %wide.load59
  %66 = fadd <2 x double> %wide.load48, %wide.load60
  %67 = fadd <2 x double> %wide.load49, %wide.load61
  %68 = fadd <2 x double> %wide.load50, %wide.load62
  %69 = fadd <2 x double> %wide.load51, %wide.load63
  %70 = fadd <2 x double> %wide.load52, %wide.load64
  %71 = fadd <2 x double> %wide.load53, %wide.load65
  %72 = fadd <2 x double> %wide.load54, %wide.load66
  %73 = fadd <2 x double> %wide.load55, %wide.load67
  %74 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62
  %76 = getelementptr inbounds double, double* %74, i64 2
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62
  %78 = getelementptr inbounds double, double* %74, i64 4
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62
  %80 = getelementptr inbounds double, double* %74, i64 6
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %74, i64 8
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %74, i64 10
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %74, i64 12
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %74, i64 14
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %74, i64 16
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = getelementptr inbounds double, double* %74, i64 18
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62
  %94 = getelementptr inbounds double, double* %74, i64 20
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62
  %96 = getelementptr inbounds double, double* %74, i64 22
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62
  %98 = fadd <2 x double> %62, %wide.load68
  %99 = fadd <2 x double> %63, %wide.load69
  %100 = fadd <2 x double> %64, %wide.load70
  %101 = fadd <2 x double> %65, %wide.load71
  %102 = fadd <2 x double> %66, %wide.load72
  %103 = fadd <2 x double> %67, %wide.load73
  %104 = fadd <2 x double> %68, %wide.load74
  %105 = fadd <2 x double> %69, %wide.load75
  %106 = fadd <2 x double> %70, %wide.load76
  %107 = fadd <2 x double> %71, %wide.load77
  %108 = fadd <2 x double> %72, %wide.load78
  %109 = fadd <2 x double> %73, %wide.load79
  store <2 x double> %98, <2 x double>* %75, align 8, !tbaa !62
  store <2 x double> %99, <2 x double>* %77, align 8, !tbaa !62
  store <2 x double> %100, <2 x double>* %79, align 8, !tbaa !62
  store <2 x double> %101, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %102, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %103, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %104, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %105, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %106, <2 x double>* %91, align 8, !tbaa !62
  store <2 x double> %107, <2 x double>* %93, align 8, !tbaa !62
  store <2 x double> %108, <2 x double>* %95, align 8, !tbaa !62
  store <2 x double> %109, <2 x double>* %97, align 8, !tbaa !62
  %index.next = add i64 %index, 24
  %110 = icmp eq i64 %index.next, %n.vec
  br i1 %110, label %middle.block, label %vector.body, !llvm.loop !640

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %111 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !639
  %arrayidx14 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %112 = load double, double* %arrayidx14, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !639
  %add15 = fadd double %111, %112
  %arrayidx17 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %113 = load double, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !639
  %add18 = fadd double %113, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !639
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp10 = icmp slt i64 %indvars.iv, %10
  br i1 %cmp10, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !641

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %omp.precond.then
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.dispatch.end, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..257(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub7 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre46 = bitcast i32* %.omp.is_last to i8*
  %.pre47 = bitcast i32* %.omp.stride to i8*
  %.pre49 = bitcast i32* %.omp.comb.ub to i8*
  %.pre51 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub7, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 91, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp1141 = icmp sgt i32 %5, %sub7
  %cond42 = select i1 %cmp1141, i32 %sub7, i32 %5
  store i32 %cond42, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1343 = icmp sgt i32 %6, %cond42
  br i1 %cmp1343, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  %.capture_expr..casted19.sroa.0.0.insert.ext = and i64 %.capture_expr.1, 4294967295
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %7 = phi i32 [ %cond42, %omp.inner.for.cond.preheader.lr.ph ], [ %cond, %omp.dispatch.inc ]
  %8 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add22, %omp.dispatch.inc ]
  %cmp1538 = icmp sgt i32 %8, %7
  br i1 %cmp1538, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge: ; preds = %omp.inner.for.cond.preheader
  %.pre = load i32, i32* %.omp.stride, align 4, !tbaa !58
  br label %omp.dispatch.inc

omp.inner.for.body:                               ; preds = %omp.inner.for.cond.preheader, %omp.inner.for.body
  %9 = phi i32 [ %15, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %10 = phi i32 [ %14, %omp.inner.for.body ], [ %7, %omp.inner.for.cond.preheader ]
  %.omp.iv.039 = phi i32 [ %add21, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %11 = zext i32 %9 to i64
  %12 = zext i32 %10 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 8, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64, i64)* @.omp_outlined..258 to void (i32*, i32*, ...)*), i64 %11, i64 %12, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext, i64 %.capture_expr..casted19.sroa.0.0.insert.ext) #6, !llvm.mem.parallel_loop_access !642
  %13 = load i32, i32* %.omp.stride, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !642
  %add21 = add nsw i32 %13, %.omp.iv.039
  %14 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !642
  %cmp15 = icmp sgt i32 %add21, %14
  %15 = load i32, i32* %.omp.comb.lb, align 4
  br i1 %cmp15, label %omp.dispatch.inc, label %omp.inner.for.body, !llvm.loop !642

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge
  %16 = phi i32 [ %.pre, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %13, %omp.inner.for.body ]
  %.lcssa37 = phi i32 [ %7, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %14, %omp.inner.for.body ]
  %.lcssa = phi i32 [ %8, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %15, %omp.inner.for.body ]
  %add22 = add nsw i32 %.lcssa, %16
  store i32 %add22, i32* %.omp.comb.lb, align 4, !tbaa !58
  %add23 = add nsw i32 %.lcssa37, %16
  %cmp11 = icmp sgt i32 %add23, %sub7
  %cond = select i1 %cmp11, i32 %sub7, i32 %add23
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp13 = icmp sgt i32 %add22, %cond
  br i1 %cmp13, label %omp.dispatch.end, label %omp.inner.for.cond.preheader

omp.dispatch.end:                                 ; preds = %omp.dispatch.inc, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.dispatch.end
  %.pre-phi52 = phi i8* [ %.pre51, %entry.omp.precond.end_crit_edge ], [ %0, %omp.dispatch.end ]
  %.pre-phi50 = phi i8* [ %.pre49, %entry.omp.precond.end_crit_edge ], [ %1, %omp.dispatch.end ]
  %.pre-phi48 = phi i8* [ %.pre47, %entry.omp.precond.end_crit_edge ], [ %2, %omp.dispatch.end ]
  %.pre-phi = phi i8* [ %.pre46, %entry.omp.precond.end_crit_edge ], [ %3, %omp.dispatch.end ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi48) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi50) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi52) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..258(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr., i64 %.capture_expr.1) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr2.sroa.0.0.extract.trunc = trunc i64 %.capture_expr.1 to i32
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv10 = trunc i64 %.previous.lb. to i32
  %conv11 = trunc i64 %.previous.ub. to i32
  store i32 %conv10, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv11, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 36, i32 %conv10, i32 %conv11, i32 1, i32 %.capture_expr..addr2.sroa.0.0.extract.trunc) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool36 = icmp eq i32 %5, 0
  br i1 %tobool36, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %omp.precond.then, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !643
  %cmp1334 = icmp sgt i32 %7, %8
  br i1 %cmp1334, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 24
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds double, double* %14, i64 12
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %14, i64 14
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %14, i64 16
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %14, i64 18
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %14, i64 20
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %14, i64 22
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62
  %40 = getelementptr inbounds double, double* %38, i64 2
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62
  %42 = getelementptr inbounds double, double* %38, i64 4
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62
  %44 = getelementptr inbounds double, double* %38, i64 6
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %38, i64 8
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %38, i64 10
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %38, i64 12
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %38, i64 14
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %38, i64 16
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = getelementptr inbounds double, double* %38, i64 18
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62
  %58 = getelementptr inbounds double, double* %38, i64 20
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62
  %60 = getelementptr inbounds double, double* %38, i64 22
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62
  %62 = fadd <2 x double> %wide.load, %wide.load59
  %63 = fadd <2 x double> %wide.load48, %wide.load60
  %64 = fadd <2 x double> %wide.load49, %wide.load61
  %65 = fadd <2 x double> %wide.load50, %wide.load62
  %66 = fadd <2 x double> %wide.load51, %wide.load63
  %67 = fadd <2 x double> %wide.load52, %wide.load64
  %68 = fadd <2 x double> %wide.load53, %wide.load65
  %69 = fadd <2 x double> %wide.load54, %wide.load66
  %70 = fadd <2 x double> %wide.load55, %wide.load67
  %71 = fadd <2 x double> %wide.load56, %wide.load68
  %72 = fadd <2 x double> %wide.load57, %wide.load69
  %73 = fadd <2 x double> %wide.load58, %wide.load70
  %74 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62
  %76 = getelementptr inbounds double, double* %74, i64 2
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62
  %78 = getelementptr inbounds double, double* %74, i64 4
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62
  %80 = getelementptr inbounds double, double* %74, i64 6
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %74, i64 8
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %74, i64 10
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %74, i64 12
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %74, i64 14
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %74, i64 16
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = getelementptr inbounds double, double* %74, i64 18
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load80 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62
  %94 = getelementptr inbounds double, double* %74, i64 20
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load81 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62
  %96 = getelementptr inbounds double, double* %74, i64 22
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load82 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62
  %98 = fadd <2 x double> %62, %wide.load71
  %99 = fadd <2 x double> %63, %wide.load72
  %100 = fadd <2 x double> %64, %wide.load73
  %101 = fadd <2 x double> %65, %wide.load74
  %102 = fadd <2 x double> %66, %wide.load75
  %103 = fadd <2 x double> %67, %wide.load76
  %104 = fadd <2 x double> %68, %wide.load77
  %105 = fadd <2 x double> %69, %wide.load78
  %106 = fadd <2 x double> %70, %wide.load79
  %107 = fadd <2 x double> %71, %wide.load80
  %108 = fadd <2 x double> %72, %wide.load81
  %109 = fadd <2 x double> %73, %wide.load82
  store <2 x double> %98, <2 x double>* %75, align 8, !tbaa !62
  store <2 x double> %99, <2 x double>* %77, align 8, !tbaa !62
  store <2 x double> %100, <2 x double>* %79, align 8, !tbaa !62
  store <2 x double> %101, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %102, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %103, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %104, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %105, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %106, <2 x double>* %91, align 8, !tbaa !62
  store <2 x double> %107, <2 x double>* %93, align 8, !tbaa !62
  store <2 x double> %108, <2 x double>* %95, align 8, !tbaa !62
  store <2 x double> %109, <2 x double>* %97, align 8, !tbaa !62
  %index.next = add i64 %index, 24
  %110 = icmp eq i64 %index.next, %n.vec
  br i1 %110, label %middle.block, label %vector.body, !llvm.loop !644

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %111 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !643
  %arrayidx17 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %112 = load double, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !643
  %add18 = fadd double %111, %112
  %arrayidx20 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %113 = load double, double* %arrayidx20, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !643
  %add21 = fadd double %113, %add18
  store double %add21, double* %arrayidx20, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !643
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp13 = icmp slt i64 %indvars.iv, %10
  br i1 %cmp13, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !645

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %omp.precond.then
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.dispatch.end, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..264(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub4 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre41 = bitcast i32* %.omp.is_last to i8*
  %.pre42 = bitcast i32* %.omp.stride to i8*
  %.pre44 = bitcast i32* %.omp.comb.ub to i8*
  %.pre46 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub4, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 91, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp836 = icmp sgt i32 %5, %sub4
  %cond37 = select i1 %cmp836, i32 %sub4, i32 %5
  store i32 %cond37, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1038 = icmp sgt i32 %6, %cond37
  br i1 %cmp1038, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %7 = phi i32 [ %cond37, %omp.inner.for.cond.preheader.lr.ph ], [ %cond, %omp.dispatch.inc ]
  %8 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add17, %omp.dispatch.inc ]
  %cmp1233 = icmp sgt i32 %8, %7
  br i1 %cmp1233, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge: ; preds = %omp.inner.for.cond.preheader
  %.pre = load i32, i32* %.omp.stride, align 4, !tbaa !58
  br label %omp.dispatch.inc

omp.inner.for.body:                               ; preds = %omp.inner.for.cond.preheader, %omp.inner.for.body
  %9 = phi i32 [ %15, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %10 = phi i32 [ %14, %omp.inner.for.body ], [ %7, %omp.inner.for.cond.preheader ]
  %.omp.iv.034 = phi i32 [ %add16, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %11 = zext i32 %9 to i64
  %12 = zext i32 %10 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64)* @.omp_outlined..265 to void (i32*, i32*, ...)*), i64 %11, i64 %12, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext) #6, !llvm.mem.parallel_loop_access !646
  %13 = load i32, i32* %.omp.stride, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !646
  %add16 = add nsw i32 %13, %.omp.iv.034
  %14 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !646
  %cmp12 = icmp sgt i32 %add16, %14
  %15 = load i32, i32* %.omp.comb.lb, align 4
  br i1 %cmp12, label %omp.dispatch.inc, label %omp.inner.for.body, !llvm.loop !646

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge
  %16 = phi i32 [ %.pre, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %13, %omp.inner.for.body ]
  %.lcssa32 = phi i32 [ %7, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %14, %omp.inner.for.body ]
  %.lcssa = phi i32 [ %8, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %15, %omp.inner.for.body ]
  %add17 = add nsw i32 %.lcssa, %16
  store i32 %add17, i32* %.omp.comb.lb, align 4, !tbaa !58
  %add18 = add nsw i32 %.lcssa32, %16
  %cmp8 = icmp sgt i32 %add18, %sub4
  %cond = select i1 %cmp8, i32 %sub4, i32 %add18
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp10 = icmp sgt i32 %add17, %cond
  br i1 %cmp10, label %omp.dispatch.end, label %omp.inner.for.cond.preheader

omp.dispatch.end:                                 ; preds = %omp.dispatch.inc, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.dispatch.end
  %.pre-phi47 = phi i8* [ %.pre46, %entry.omp.precond.end_crit_edge ], [ %0, %omp.dispatch.end ]
  %.pre-phi45 = phi i8* [ %.pre44, %entry.omp.precond.end_crit_edge ], [ %1, %omp.dispatch.end ]
  %.pre-phi43 = phi i8* [ %.pre42, %entry.omp.precond.end_crit_edge ], [ %2, %omp.dispatch.end ]
  %.pre-phi = phi i8* [ %.pre41, %entry.omp.precond.end_crit_edge ], [ %3, %omp.dispatch.end ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi43) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi45) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi47) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..265(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv7 = trunc i64 %.previous.lb. to i32
  %conv8 = trunc i64 %.previous.ub. to i32
  store i32 %conv7, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv8, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 38, i32 %conv7, i32 %conv8, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool33 = icmp eq i32 %5, 0
  br i1 %tobool33, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %omp.precond.then, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !647
  %cmp1031 = icmp sgt i32 %7, %8
  br i1 %cmp1031, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 24
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds double, double* %14, i64 12
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %14, i64 14
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %14, i64 16
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %14, i64 18
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %14, i64 20
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %14, i64 22
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62
  %40 = getelementptr inbounds double, double* %38, i64 2
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62
  %42 = getelementptr inbounds double, double* %38, i64 4
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62
  %44 = getelementptr inbounds double, double* %38, i64 6
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %38, i64 8
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %38, i64 10
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %38, i64 12
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %38, i64 14
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %38, i64 16
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = getelementptr inbounds double, double* %38, i64 18
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62
  %58 = getelementptr inbounds double, double* %38, i64 20
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62
  %60 = getelementptr inbounds double, double* %38, i64 22
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62
  %62 = fadd <2 x double> %wide.load, %wide.load56
  %63 = fadd <2 x double> %wide.load45, %wide.load57
  %64 = fadd <2 x double> %wide.load46, %wide.load58
  %65 = fadd <2 x double> %wide.load47, %wide.load59
  %66 = fadd <2 x double> %wide.load48, %wide.load60
  %67 = fadd <2 x double> %wide.load49, %wide.load61
  %68 = fadd <2 x double> %wide.load50, %wide.load62
  %69 = fadd <2 x double> %wide.load51, %wide.load63
  %70 = fadd <2 x double> %wide.load52, %wide.load64
  %71 = fadd <2 x double> %wide.load53, %wide.load65
  %72 = fadd <2 x double> %wide.load54, %wide.load66
  %73 = fadd <2 x double> %wide.load55, %wide.load67
  %74 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62
  %76 = getelementptr inbounds double, double* %74, i64 2
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62
  %78 = getelementptr inbounds double, double* %74, i64 4
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62
  %80 = getelementptr inbounds double, double* %74, i64 6
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %74, i64 8
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %74, i64 10
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %74, i64 12
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %74, i64 14
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %74, i64 16
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = getelementptr inbounds double, double* %74, i64 18
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62
  %94 = getelementptr inbounds double, double* %74, i64 20
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62
  %96 = getelementptr inbounds double, double* %74, i64 22
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62
  %98 = fadd <2 x double> %62, %wide.load68
  %99 = fadd <2 x double> %63, %wide.load69
  %100 = fadd <2 x double> %64, %wide.load70
  %101 = fadd <2 x double> %65, %wide.load71
  %102 = fadd <2 x double> %66, %wide.load72
  %103 = fadd <2 x double> %67, %wide.load73
  %104 = fadd <2 x double> %68, %wide.load74
  %105 = fadd <2 x double> %69, %wide.load75
  %106 = fadd <2 x double> %70, %wide.load76
  %107 = fadd <2 x double> %71, %wide.load77
  %108 = fadd <2 x double> %72, %wide.load78
  %109 = fadd <2 x double> %73, %wide.load79
  store <2 x double> %98, <2 x double>* %75, align 8, !tbaa !62
  store <2 x double> %99, <2 x double>* %77, align 8, !tbaa !62
  store <2 x double> %100, <2 x double>* %79, align 8, !tbaa !62
  store <2 x double> %101, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %102, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %103, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %104, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %105, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %106, <2 x double>* %91, align 8, !tbaa !62
  store <2 x double> %107, <2 x double>* %93, align 8, !tbaa !62
  store <2 x double> %108, <2 x double>* %95, align 8, !tbaa !62
  store <2 x double> %109, <2 x double>* %97, align 8, !tbaa !62
  %index.next = add i64 %index, 24
  %110 = icmp eq i64 %index.next, %n.vec
  br i1 %110, label %middle.block, label %vector.body, !llvm.loop !648

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %111 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !647
  %arrayidx14 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %112 = load double, double* %arrayidx14, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !647
  %add15 = fadd double %111, %112
  %arrayidx17 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %113 = load double, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !647
  %add18 = fadd double %113, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !647
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp10 = icmp slt i64 %indvars.iv, %10
  br i1 %cmp10, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !649

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %omp.precond.then
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.dispatch.end, %entry
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..271(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %n, double* %a, double* %b, double* %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.comb.lb = alloca i32, align 4
  %.omp.comb.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %sub4 = add nsw i32 %n.addr.sroa.0.0.extract.trunc, -1
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %entry.omp.precond.end_crit_edge

entry.omp.precond.end_crit_edge:                  ; preds = %entry
  %.pre41 = bitcast i32* %.omp.is_last to i8*
  %.pre42 = bitcast i32* %.omp.stride to i8*
  %.pre44 = bitcast i32* %.omp.comb.ub to i8*
  %.pre46 = bitcast i32* %.omp.comb.lb to i8*
  br label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %.capture_expr..addr.sroa.0.0.extract.trunc = trunc i64 %.capture_expr. to i32
  %0 = bitcast i32* %.omp.comb.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  store i32 0, i32* %.omp.comb.lb, align 4, !tbaa !58
  %1 = bitcast i32* %.omp.comb.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  store i32 %sub4, i32* %.omp.comb.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  call void @__kmpc_for_static_init_4(%struct.ident_t* nonnull @0, i32 %4, i32 91, i32* nonnull %.omp.is_last, i32* nonnull %.omp.comb.lb, i32* nonnull %.omp.comb.ub, i32* nonnull %.omp.stride, i32 1, i32 %.capture_expr..addr.sroa.0.0.extract.trunc) #6
  %5 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp836 = icmp sgt i32 %5, %sub4
  %cond37 = select i1 %cmp836, i32 %sub4, i32 %5
  store i32 %cond37, i32* %.omp.comb.ub, align 4, !tbaa !58
  %6 = load i32, i32* %.omp.comb.lb, align 4, !tbaa !58
  %cmp1038 = icmp sgt i32 %6, %cond37
  br i1 %cmp1038, label %omp.dispatch.end, label %omp.inner.for.cond.preheader.lr.ph

omp.inner.for.cond.preheader.lr.ph:               ; preds = %omp.precond.then
  %n.casted.sroa.0.0.insert.ext = and i64 %n, 4294967295
  %.capture_expr..casted.sroa.0.0.insert.ext = and i64 %.capture_expr., 4294967295
  br label %omp.inner.for.cond.preheader

omp.inner.for.cond.preheader:                     ; preds = %omp.inner.for.cond.preheader.lr.ph, %omp.dispatch.inc
  %7 = phi i32 [ %cond37, %omp.inner.for.cond.preheader.lr.ph ], [ %cond, %omp.dispatch.inc ]
  %8 = phi i32 [ %6, %omp.inner.for.cond.preheader.lr.ph ], [ %add17, %omp.dispatch.inc ]
  %cmp1233 = icmp sgt i32 %8, %7
  br i1 %cmp1233, label %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge, label %omp.inner.for.body

omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge: ; preds = %omp.inner.for.cond.preheader
  %.pre = load i32, i32* %.omp.stride, align 4, !tbaa !58
  br label %omp.dispatch.inc

omp.inner.for.body:                               ; preds = %omp.inner.for.cond.preheader, %omp.inner.for.body
  %9 = phi i32 [ %15, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %10 = phi i32 [ %14, %omp.inner.for.body ], [ %7, %omp.inner.for.cond.preheader ]
  %.omp.iv.034 = phi i32 [ %add16, %omp.inner.for.body ], [ %8, %omp.inner.for.cond.preheader ]
  %11 = zext i32 %9 to i64
  %12 = zext i32 %10 to i64
  call void (%struct.ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%struct.ident_t* nonnull @2, i32 7, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i64, i64, double*, double*, double*, i64)* @.omp_outlined..272 to void (i32*, i32*, ...)*), i64 %11, i64 %12, i64 %n.casted.sroa.0.0.insert.ext, double* %a, double* %b, double* %c, i64 %.capture_expr..casted.sroa.0.0.insert.ext) #6, !llvm.mem.parallel_loop_access !650
  %13 = load i32, i32* %.omp.stride, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !650
  %add16 = add nsw i32 %13, %.omp.iv.034
  %14 = load i32, i32* %.omp.comb.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !650
  %cmp12 = icmp sgt i32 %add16, %14
  %15 = load i32, i32* %.omp.comb.lb, align 4
  br i1 %cmp12, label %omp.dispatch.inc, label %omp.inner.for.body, !llvm.loop !650

omp.dispatch.inc:                                 ; preds = %omp.inner.for.body, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge
  %16 = phi i32 [ %.pre, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %13, %omp.inner.for.body ]
  %.lcssa32 = phi i32 [ %7, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %14, %omp.inner.for.body ]
  %.lcssa = phi i32 [ %8, %omp.inner.for.cond.preheader.omp.dispatch.inc_crit_edge ], [ %15, %omp.inner.for.body ]
  %add17 = add nsw i32 %.lcssa, %16
  store i32 %add17, i32* %.omp.comb.lb, align 4, !tbaa !58
  %add18 = add nsw i32 %.lcssa32, %16
  %cmp8 = icmp sgt i32 %add18, %sub4
  %cond = select i1 %cmp8, i32 %sub4, i32 %add18
  store i32 %cond, i32* %.omp.comb.ub, align 4, !tbaa !58
  %cmp10 = icmp sgt i32 %add17, %cond
  br i1 %cmp10, label %omp.dispatch.end, label %omp.inner.for.cond.preheader

omp.dispatch.end:                                 ; preds = %omp.dispatch.inc, %omp.precond.then
  call void @__kmpc_for_static_fini(%struct.ident_t* nonnull @0, i32 %4) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %entry.omp.precond.end_crit_edge, %omp.dispatch.end
  %.pre-phi47 = phi i8* [ %.pre46, %entry.omp.precond.end_crit_edge ], [ %0, %omp.dispatch.end ]
  %.pre-phi45 = phi i8* [ %.pre44, %entry.omp.precond.end_crit_edge ], [ %1, %omp.dispatch.end ]
  %.pre-phi43 = phi i8* [ %.pre42, %entry.omp.precond.end_crit_edge ], [ %2, %omp.dispatch.end ]
  %.pre-phi = phi i8* [ %.pre41, %entry.omp.precond.end_crit_edge ], [ %3, %omp.dispatch.end ]
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi43) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi45) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %.pre-phi47) #6
  ret void
}

; Function Attrs: norecurse nounwind
define internal void @.omp_outlined..272(i32* noalias nocapture readonly %.global_tid., i32* noalias nocapture readnone %.bound_tid., i64 %.previous.lb., i64 %.previous.ub., i64 %n, double* nocapture %a, double* nocapture readonly %b, double* nocapture readonly %c, i64 %.capture_expr.) #5 {
entry:
  %.omp.lb = alloca i32, align 4
  %.omp.ub = alloca i32, align 4
  %.omp.stride = alloca i32, align 4
  %.omp.is_last = alloca i32, align 4
  %n.addr.sroa.0.0.extract.trunc = trunc i64 %n to i32
  %cmp = icmp sgt i32 %n.addr.sroa.0.0.extract.trunc, 0
  br i1 %cmp, label %omp.precond.then, label %omp.precond.end

omp.precond.then:                                 ; preds = %entry
  %0 = bitcast i32* %.omp.lb to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %0) #6
  %1 = bitcast i32* %.omp.ub to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %1) #6
  %conv7 = trunc i64 %.previous.lb. to i32
  %conv8 = trunc i64 %.previous.ub. to i32
  store i32 %conv7, i32* %.omp.lb, align 4, !tbaa !58
  store i32 %conv8, i32* %.omp.ub, align 4, !tbaa !58
  %2 = bitcast i32* %.omp.stride to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %2) #6
  store i32 1, i32* %.omp.stride, align 4, !tbaa !58
  %3 = bitcast i32* %.omp.is_last to i8*
  call void @llvm.lifetime.start.p0i8(i64 4, i8* nonnull %3) #6
  store i32 0, i32* %.omp.is_last, align 4, !tbaa !58
  %4 = load i32, i32* %.global_tid., align 4, !tbaa !58
  tail call void @__kmpc_dispatch_init_4(%struct.ident_t* nonnull @2, i32 %4, i32 37, i32 %conv7, i32 %conv8, i32 1, i32 1) #6
  %5 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool33 = icmp eq i32 %5, 0
  br i1 %tobool33, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.cond.loopexit:                       ; preds = %omp.inner.for.body, %middle.block, %omp.dispatch.body
  %6 = call i32 @__kmpc_dispatch_next_4(%struct.ident_t* nonnull @2, i32 %4, i32* nonnull %.omp.is_last, i32* nonnull %.omp.lb, i32* nonnull %.omp.ub, i32* nonnull %.omp.stride) #6
  %tobool = icmp eq i32 %6, 0
  br i1 %tobool, label %omp.dispatch.end, label %omp.dispatch.body

omp.dispatch.body:                                ; preds = %omp.precond.then, %omp.dispatch.cond.loopexit
  %7 = load i32, i32* %.omp.lb, align 4, !tbaa !58
  %8 = load i32, i32* %.omp.ub, align 4, !tbaa !58, !llvm.mem.parallel_loop_access !651
  %cmp1031 = icmp sgt i32 %7, %8
  br i1 %cmp1031, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.lr.ph

omp.inner.for.body.lr.ph:                         ; preds = %omp.dispatch.body
  %9 = sext i32 %7 to i64
  %10 = sext i32 %8 to i64
  %11 = icmp sgt i64 %9, %10
  %smax = select i1 %11, i64 %9, i64 %10
  %12 = sub nsw i64 1, %9
  %13 = add nsw i64 %12, %smax
  %min.iters.check = icmp ult i64 %13, 24
  br i1 %min.iters.check, label %omp.inner.for.body.preheader, label %vector.ph

vector.ph:                                        ; preds = %omp.inner.for.body.lr.ph
  %n.mod.vf = urem i64 %13, 24
  %n.vec = sub nsw i64 %13, %n.mod.vf
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %offset.idx = add i64 %index, %9
  %14 = getelementptr inbounds double, double* %b, i64 %offset.idx
  %15 = bitcast double* %14 to <2 x double>*
  %wide.load = load <2 x double>, <2 x double>* %15, align 8, !tbaa !62
  %16 = getelementptr inbounds double, double* %14, i64 2
  %17 = bitcast double* %16 to <2 x double>*
  %wide.load45 = load <2 x double>, <2 x double>* %17, align 8, !tbaa !62
  %18 = getelementptr inbounds double, double* %14, i64 4
  %19 = bitcast double* %18 to <2 x double>*
  %wide.load46 = load <2 x double>, <2 x double>* %19, align 8, !tbaa !62
  %20 = getelementptr inbounds double, double* %14, i64 6
  %21 = bitcast double* %20 to <2 x double>*
  %wide.load47 = load <2 x double>, <2 x double>* %21, align 8, !tbaa !62
  %22 = getelementptr inbounds double, double* %14, i64 8
  %23 = bitcast double* %22 to <2 x double>*
  %wide.load48 = load <2 x double>, <2 x double>* %23, align 8, !tbaa !62
  %24 = getelementptr inbounds double, double* %14, i64 10
  %25 = bitcast double* %24 to <2 x double>*
  %wide.load49 = load <2 x double>, <2 x double>* %25, align 8, !tbaa !62
  %26 = getelementptr inbounds double, double* %14, i64 12
  %27 = bitcast double* %26 to <2 x double>*
  %wide.load50 = load <2 x double>, <2 x double>* %27, align 8, !tbaa !62
  %28 = getelementptr inbounds double, double* %14, i64 14
  %29 = bitcast double* %28 to <2 x double>*
  %wide.load51 = load <2 x double>, <2 x double>* %29, align 8, !tbaa !62
  %30 = getelementptr inbounds double, double* %14, i64 16
  %31 = bitcast double* %30 to <2 x double>*
  %wide.load52 = load <2 x double>, <2 x double>* %31, align 8, !tbaa !62
  %32 = getelementptr inbounds double, double* %14, i64 18
  %33 = bitcast double* %32 to <2 x double>*
  %wide.load53 = load <2 x double>, <2 x double>* %33, align 8, !tbaa !62
  %34 = getelementptr inbounds double, double* %14, i64 20
  %35 = bitcast double* %34 to <2 x double>*
  %wide.load54 = load <2 x double>, <2 x double>* %35, align 8, !tbaa !62
  %36 = getelementptr inbounds double, double* %14, i64 22
  %37 = bitcast double* %36 to <2 x double>*
  %wide.load55 = load <2 x double>, <2 x double>* %37, align 8, !tbaa !62
  %38 = getelementptr inbounds double, double* %c, i64 %offset.idx
  %39 = bitcast double* %38 to <2 x double>*
  %wide.load56 = load <2 x double>, <2 x double>* %39, align 8, !tbaa !62
  %40 = getelementptr inbounds double, double* %38, i64 2
  %41 = bitcast double* %40 to <2 x double>*
  %wide.load57 = load <2 x double>, <2 x double>* %41, align 8, !tbaa !62
  %42 = getelementptr inbounds double, double* %38, i64 4
  %43 = bitcast double* %42 to <2 x double>*
  %wide.load58 = load <2 x double>, <2 x double>* %43, align 8, !tbaa !62
  %44 = getelementptr inbounds double, double* %38, i64 6
  %45 = bitcast double* %44 to <2 x double>*
  %wide.load59 = load <2 x double>, <2 x double>* %45, align 8, !tbaa !62
  %46 = getelementptr inbounds double, double* %38, i64 8
  %47 = bitcast double* %46 to <2 x double>*
  %wide.load60 = load <2 x double>, <2 x double>* %47, align 8, !tbaa !62
  %48 = getelementptr inbounds double, double* %38, i64 10
  %49 = bitcast double* %48 to <2 x double>*
  %wide.load61 = load <2 x double>, <2 x double>* %49, align 8, !tbaa !62
  %50 = getelementptr inbounds double, double* %38, i64 12
  %51 = bitcast double* %50 to <2 x double>*
  %wide.load62 = load <2 x double>, <2 x double>* %51, align 8, !tbaa !62
  %52 = getelementptr inbounds double, double* %38, i64 14
  %53 = bitcast double* %52 to <2 x double>*
  %wide.load63 = load <2 x double>, <2 x double>* %53, align 8, !tbaa !62
  %54 = getelementptr inbounds double, double* %38, i64 16
  %55 = bitcast double* %54 to <2 x double>*
  %wide.load64 = load <2 x double>, <2 x double>* %55, align 8, !tbaa !62
  %56 = getelementptr inbounds double, double* %38, i64 18
  %57 = bitcast double* %56 to <2 x double>*
  %wide.load65 = load <2 x double>, <2 x double>* %57, align 8, !tbaa !62
  %58 = getelementptr inbounds double, double* %38, i64 20
  %59 = bitcast double* %58 to <2 x double>*
  %wide.load66 = load <2 x double>, <2 x double>* %59, align 8, !tbaa !62
  %60 = getelementptr inbounds double, double* %38, i64 22
  %61 = bitcast double* %60 to <2 x double>*
  %wide.load67 = load <2 x double>, <2 x double>* %61, align 8, !tbaa !62
  %62 = fadd <2 x double> %wide.load, %wide.load56
  %63 = fadd <2 x double> %wide.load45, %wide.load57
  %64 = fadd <2 x double> %wide.load46, %wide.load58
  %65 = fadd <2 x double> %wide.load47, %wide.load59
  %66 = fadd <2 x double> %wide.load48, %wide.load60
  %67 = fadd <2 x double> %wide.load49, %wide.load61
  %68 = fadd <2 x double> %wide.load50, %wide.load62
  %69 = fadd <2 x double> %wide.load51, %wide.load63
  %70 = fadd <2 x double> %wide.load52, %wide.load64
  %71 = fadd <2 x double> %wide.load53, %wide.load65
  %72 = fadd <2 x double> %wide.load54, %wide.load66
  %73 = fadd <2 x double> %wide.load55, %wide.load67
  %74 = getelementptr inbounds double, double* %a, i64 %offset.idx
  %75 = bitcast double* %74 to <2 x double>*
  %wide.load68 = load <2 x double>, <2 x double>* %75, align 8, !tbaa !62
  %76 = getelementptr inbounds double, double* %74, i64 2
  %77 = bitcast double* %76 to <2 x double>*
  %wide.load69 = load <2 x double>, <2 x double>* %77, align 8, !tbaa !62
  %78 = getelementptr inbounds double, double* %74, i64 4
  %79 = bitcast double* %78 to <2 x double>*
  %wide.load70 = load <2 x double>, <2 x double>* %79, align 8, !tbaa !62
  %80 = getelementptr inbounds double, double* %74, i64 6
  %81 = bitcast double* %80 to <2 x double>*
  %wide.load71 = load <2 x double>, <2 x double>* %81, align 8, !tbaa !62
  %82 = getelementptr inbounds double, double* %74, i64 8
  %83 = bitcast double* %82 to <2 x double>*
  %wide.load72 = load <2 x double>, <2 x double>* %83, align 8, !tbaa !62
  %84 = getelementptr inbounds double, double* %74, i64 10
  %85 = bitcast double* %84 to <2 x double>*
  %wide.load73 = load <2 x double>, <2 x double>* %85, align 8, !tbaa !62
  %86 = getelementptr inbounds double, double* %74, i64 12
  %87 = bitcast double* %86 to <2 x double>*
  %wide.load74 = load <2 x double>, <2 x double>* %87, align 8, !tbaa !62
  %88 = getelementptr inbounds double, double* %74, i64 14
  %89 = bitcast double* %88 to <2 x double>*
  %wide.load75 = load <2 x double>, <2 x double>* %89, align 8, !tbaa !62
  %90 = getelementptr inbounds double, double* %74, i64 16
  %91 = bitcast double* %90 to <2 x double>*
  %wide.load76 = load <2 x double>, <2 x double>* %91, align 8, !tbaa !62
  %92 = getelementptr inbounds double, double* %74, i64 18
  %93 = bitcast double* %92 to <2 x double>*
  %wide.load77 = load <2 x double>, <2 x double>* %93, align 8, !tbaa !62
  %94 = getelementptr inbounds double, double* %74, i64 20
  %95 = bitcast double* %94 to <2 x double>*
  %wide.load78 = load <2 x double>, <2 x double>* %95, align 8, !tbaa !62
  %96 = getelementptr inbounds double, double* %74, i64 22
  %97 = bitcast double* %96 to <2 x double>*
  %wide.load79 = load <2 x double>, <2 x double>* %97, align 8, !tbaa !62
  %98 = fadd <2 x double> %62, %wide.load68
  %99 = fadd <2 x double> %63, %wide.load69
  %100 = fadd <2 x double> %64, %wide.load70
  %101 = fadd <2 x double> %65, %wide.load71
  %102 = fadd <2 x double> %66, %wide.load72
  %103 = fadd <2 x double> %67, %wide.load73
  %104 = fadd <2 x double> %68, %wide.load74
  %105 = fadd <2 x double> %69, %wide.load75
  %106 = fadd <2 x double> %70, %wide.load76
  %107 = fadd <2 x double> %71, %wide.load77
  %108 = fadd <2 x double> %72, %wide.load78
  %109 = fadd <2 x double> %73, %wide.load79
  store <2 x double> %98, <2 x double>* %75, align 8, !tbaa !62
  store <2 x double> %99, <2 x double>* %77, align 8, !tbaa !62
  store <2 x double> %100, <2 x double>* %79, align 8, !tbaa !62
  store <2 x double> %101, <2 x double>* %81, align 8, !tbaa !62
  store <2 x double> %102, <2 x double>* %83, align 8, !tbaa !62
  store <2 x double> %103, <2 x double>* %85, align 8, !tbaa !62
  store <2 x double> %104, <2 x double>* %87, align 8, !tbaa !62
  store <2 x double> %105, <2 x double>* %89, align 8, !tbaa !62
  store <2 x double> %106, <2 x double>* %91, align 8, !tbaa !62
  store <2 x double> %107, <2 x double>* %93, align 8, !tbaa !62
  store <2 x double> %108, <2 x double>* %95, align 8, !tbaa !62
  store <2 x double> %109, <2 x double>* %97, align 8, !tbaa !62
  %index.next = add i64 %index, 24
  %110 = icmp eq i64 %index.next, %n.vec
  br i1 %110, label %middle.block, label %vector.body, !llvm.loop !652

middle.block:                                     ; preds = %vector.body
  %ind.end = add nsw i64 %n.vec, %9
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %omp.dispatch.cond.loopexit, label %omp.inner.for.body.preheader

omp.inner.for.body.preheader:                     ; preds = %middle.block, %omp.inner.for.body.lr.ph
  %indvars.iv.ph = phi i64 [ %ind.end, %middle.block ], [ %9, %omp.inner.for.body.lr.ph ]
  br label %omp.inner.for.body

omp.inner.for.body:                               ; preds = %omp.inner.for.body.preheader, %omp.inner.for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %omp.inner.for.body ], [ %indvars.iv.ph, %omp.inner.for.body.preheader ]
  %arrayidx = getelementptr inbounds double, double* %b, i64 %indvars.iv
  %111 = load double, double* %arrayidx, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !651
  %arrayidx14 = getelementptr inbounds double, double* %c, i64 %indvars.iv
  %112 = load double, double* %arrayidx14, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !651
  %add15 = fadd double %111, %112
  %arrayidx17 = getelementptr inbounds double, double* %a, i64 %indvars.iv
  %113 = load double, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !651
  %add18 = fadd double %113, %add15
  store double %add18, double* %arrayidx17, align 8, !tbaa !62, !llvm.mem.parallel_loop_access !651
  %indvars.iv.next = add nsw i64 %indvars.iv, 1
  %cmp10 = icmp slt i64 %indvars.iv, %10
  br i1 %cmp10, label %omp.inner.for.body, label %omp.dispatch.cond.loopexit, !llvm.loop !653

omp.dispatch.end:                                 ; preds = %omp.dispatch.cond.loopexit, %omp.precond.then
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %3) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %2) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %1) #6
  call void @llvm.lifetime.end.p0i8(i64 4, i8* nonnull %0) #6
  br label %omp.precond.end

omp.precond.end:                                  ; preds = %omp.dispatch.end, %entry
  ret void
}

declare void @__tgt_target_data_end(i64, i32, i8**, i8**, i64*, i64*) local_unnamed_addr

; Function Attrs: nounwind
define internal void @.omp_offloading.descriptor_unreg(i8* nocapture readnone) #3 section ".text.startup" comdat($.omp_offloading.descriptor_reg) {
entry:
  %1 = tail call i32 @__tgt_unregister_lib(%struct.__tgt_bin_desc* nonnull @.omp_offloading.descriptor) #6
  ret void
}

declare i32 @__tgt_unregister_lib(%struct.__tgt_bin_desc*) local_unnamed_addr

; Function Attrs: nounwind
define linkonce hidden void @.omp_offloading.descriptor_reg() #3 section ".text.startup" comdat {
entry:
  %0 = tail call i32 @__tgt_register_lib(%struct.__tgt_bin_desc* nonnull @.omp_offloading.descriptor) #6
  %1 = tail call i32 @__cxa_atexit(void (i8*)* nonnull @.omp_offloading.descriptor_unreg, i8* bitcast (%struct.__tgt_bin_desc* @.omp_offloading.descriptor to i8*), i8* nonnull @__dso_handle) #6
  ret void
}

declare i32 @__tgt_register_lib(%struct.__tgt_bin_desc*) local_unnamed_addr

; Function Attrs: nounwind
declare i32 @__cxa_atexit(void (i8*)*, i8*, i8*) local_unnamed_addr #6

; Function Attrs: nounwind
declare i32 @puts(i8* nocapture readonly) local_unnamed_addr #6

; Function Attrs: argmemonly nounwind
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1) #1

attributes #0 = { nounwind readonly "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="pwr8" "target-features"="+altivec,+bpermd,+crypto,+direct-move,+extdiv,+htm,+power8-vector,+vsx,-power9-vector,-qpx" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #1 = { argmemonly nounwind }
attributes #2 = { nounwind readonly "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="pwr8" "target-features"="+altivec,+bpermd,+crypto,+direct-move,+extdiv,+htm,+power8-vector,+vsx,-power9-vector,-qpx" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #3 = { nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="pwr8" "target-features"="+altivec,+bpermd,+crypto,+direct-move,+extdiv,+htm,+power8-vector,+vsx,-power9-vector,-qpx" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #4 = { nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="pwr8" "target-features"="+altivec,+bpermd,+crypto,+direct-move,+extdiv,+htm,+power8-vector,+vsx,-power9-vector,-qpx" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #5 = { norecurse nounwind "correctly-rounded-divide-sqrt-fp-math"="false" "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="false" "no-infs-fp-math"="false" "no-jump-tables"="false" "no-nans-fp-math"="false" "no-signed-zeros-fp-math"="false" "no-trapping-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="pwr8" "target-features"="+altivec,+bpermd,+crypto,+direct-move,+extdiv,+htm,+power8-vector,+vsx,-power9-vector,-qpx" "unsafe-fp-math"="false" "use-soft-float"="false" }
attributes #6 = { nounwind }

!omp_offload.info = !{!0, !1, !2, !3, !4, !5, !6, !7, !8, !9, !10, !11, !12, !13, !14, !15, !16, !17, !18, !19, !20, !21, !22, !23, !24, !25, !26, !27, !28, !29, !30, !31, !32, !33, !34, !35, !36, !37, !38, !39, !40, !41, !42, !43, !44, !45, !46, !47, !48, !49, !50, !51, !52, !53, !54}
!llvm.module.flags = !{!55, !56}
!llvm.ident = !{!57}

!0 = !{i32 0, i32 2050, i32 21764393, !"check_offloading", i32 22, i32 0}
!1 = !{i32 0, i32 2050, i32 21764274, !"main", i32 256, i32 11}
!2 = !{i32 0, i32 2050, i32 21764274, !"main", i32 45, i32 1}
!3 = !{i32 0, i32 2050, i32 21764274, !"main", i32 346, i32 15}
!4 = !{i32 0, i32 2050, i32 21764274, !"main", i32 737, i32 31}
!5 = !{i32 0, i32 2050, i32 21764274, !"main", i32 993, i32 37}
!6 = !{i32 0, i32 2050, i32 21764274, !"main", i32 66, i32 2}
!7 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1391, i32 48}
!8 = !{i32 0, i32 2050, i32 21764274, !"main", i32 637, i32 27}
!9 = !{i32 0, i32 2050, i32 21764274, !"main", i32 426, i32 19}
!10 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1502, i32 51}
!11 = !{i32 0, i32 2050, i32 21764274, !"main", i32 146, i32 6}
!12 = !{i32 0, i32 2050, i32 21764274, !"main", i32 530, i32 23}
!13 = !{i32 0, i32 2050, i32 21764274, !"main", i32 236, i32 10}
!14 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1177, i32 42}
!15 = !{i32 0, i32 2050, i32 21764274, !"main", i32 326, i32 14}
!16 = !{i32 0, i32 2050, i32 21764274, !"main", i32 883, i32 32}
!17 = !{i32 0, i32 2050, i32 21764274, !"main", i32 461, i32 20}
!18 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1613, i32 54}
!19 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1070, i32 39}
!20 = !{i32 0, i32 2050, i32 21764274, !"main", i32 949, i32 35}
!21 = !{i32 0, i32 2050, i32 21764274, !"main", i32 406, i32 18}
!22 = !{i32 0, i32 2050, i32 21764274, !"main", i32 662, i32 28}
!23 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1354, i32 47}
!24 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1105, i32 40}
!25 = !{i32 0, i32 2050, i32 21764274, !"main", i32 126, i32 5}
!26 = !{i32 0, i32 2050, i32 21764274, !"main", i32 216, i32 9}
!27 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1036, i32 38}
!28 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1465, i32 50}
!29 = !{i32 0, i32 2050, i32 21764274, !"main", i32 576, i32 25}
!30 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1140, i32 41}
!31 = !{i32 0, i32 2050, i32 21764274, !"main", i32 507, i32 22}
!32 = !{i32 0, i32 2050, i32 21764274, !"main", i32 296, i32 13}
!33 = !{i32 0, i32 2050, i32 21764274, !"main", i32 386, i32 17}
!34 = !{i32 0, i32 2050, i32 21764274, !"main", i32 687, i32 29}
!35 = !{i32 0, i32 2050, i32 21764274, !"main", i32 905, i32 33}
!36 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1213, i32 43}
!37 = !{i32 0, i32 2050, i32 21764274, !"main", i32 106, i32 4}
!38 = !{i32 0, i32 2050, i32 21764274, !"main", i32 196, i32 8}
!39 = !{i32 0, i32 2050, i32 21764274, !"main", i32 971, i32 36}
!40 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1248, i32 44}
!41 = !{i32 0, i32 2050, i32 21764274, !"main", i32 276, i32 12}
!42 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1428, i32 49}
!43 = !{i32 0, i32 2050, i32 21764274, !"main", i32 366, i32 16}
!44 = !{i32 0, i32 2050, i32 21764274, !"main", i32 712, i32 30}
!45 = !{i32 0, i32 2050, i32 21764274, !"main", i32 553, i32 24}
!46 = !{i32 0, i32 2050, i32 21764274, !"main", i32 86, i32 3}
!47 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1283, i32 45}
!48 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1539, i32 52}
!49 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1577, i32 53}
!50 = !{i32 0, i32 2050, i32 21764274, !"main", i32 484, i32 21}
!51 = !{i32 0, i32 2050, i32 21764274, !"main", i32 612, i32 26}
!52 = !{i32 0, i32 2050, i32 21764274, !"main", i32 927, i32 34}
!53 = !{i32 0, i32 2050, i32 21764274, !"main", i32 1318, i32 46}
!54 = !{i32 0, i32 2050, i32 21764274, !"main", i32 166, i32 7}
!55 = !{i32 1, !"wchar_size", i32 4}
!56 = !{i32 7, !"PIC Level", i32 2}
!57 = !{!"clang version 7.0.0 (trunk 336676) (llvm/trunk 336675)"}
!58 = !{!59, !59, i64 0}
!59 = !{!"int", !60, i64 0}
!60 = !{!"omnipotent char", !61, i64 0}
!61 = !{!"Simple C/C++ TBAA"}
!62 = !{!63, !63, i64 0}
!63 = !{!"double", !60, i64 0}
!64 = !{!65}
!65 = distinct !{!65, !66}
!66 = distinct !{!66, !"LVerDomain"}
!67 = !{!68, !69, !70}
!68 = distinct !{!68, !66}
!69 = distinct !{!69, !66}
!70 = distinct !{!70, !66}
!71 = !{!68}
!72 = !{!69, !70}
!73 = !{!69}
!74 = !{!70}
!75 = distinct !{!75, !76}
!76 = !{!"llvm.loop.isvectorized", i32 1}
!77 = distinct !{!77, !76}
!78 = distinct !{!78, !76}
!79 = distinct !{!79, !76}
!80 = distinct !{!80, !76}
!81 = distinct !{!81, !82, !76}
!82 = !{!"llvm.loop.unroll.runtime.disable"}
!83 = distinct !{!83, !76}
!84 = distinct !{!84, !76}
!85 = distinct !{!85, !82, !76}
!86 = distinct !{!86, !76}
!87 = distinct !{!87, !76}
!88 = distinct !{!88, !82, !76}
!89 = distinct !{!89, !76}
!90 = distinct !{!90, !76}
!91 = distinct !{!91, !82, !76}
!92 = distinct !{!92, !76}
!93 = distinct !{!93, !76}
!94 = distinct !{!94, !82, !76}
!95 = distinct !{!95, !76}
!96 = distinct !{!96, !76}
!97 = distinct !{!97, !82, !76}
!98 = distinct !{!98, !76}
!99 = distinct !{!99, !76}
!100 = distinct !{!100, !82, !76}
!101 = distinct !{!101, !76}
!102 = distinct !{!102, !76}
!103 = distinct !{!103, !82, !76}
!104 = distinct !{!104, !76}
!105 = distinct !{!105, !76}
!106 = distinct !{!106, !82, !76}
!107 = distinct !{!107, !76}
!108 = distinct !{!108, !76}
!109 = distinct !{!109, !82, !76}
!110 = distinct !{!110, !76}
!111 = distinct !{!111, !76}
!112 = distinct !{!112, !82, !76}
!113 = distinct !{!113, !76}
!114 = distinct !{!114, !76}
!115 = distinct !{!115, !82, !76}
!116 = distinct !{!116, !76}
!117 = distinct !{!117, !76}
!118 = distinct !{!118, !82, !76}
!119 = distinct !{!119, !76}
!120 = distinct !{!120, !76}
!121 = distinct !{!121, !82, !76}
!122 = distinct !{!122, !76}
!123 = distinct !{!123, !76}
!124 = distinct !{!124, !82, !76}
!125 = distinct !{!125, !76}
!126 = distinct !{!126, !76}
!127 = distinct !{!127, !82, !76}
!128 = distinct !{!128, !76}
!129 = distinct !{!129, !76}
!130 = distinct !{!130, !82, !76}
!131 = !{!132}
!132 = distinct !{!132, !133, !".omp_outlined..7: %.global_tid."}
!133 = distinct !{!133, !".omp_outlined..7"}
!134 = !{!135}
!135 = distinct !{!135, !136}
!136 = distinct !{!136, !"LVerDomain"}
!137 = !{!138}
!138 = distinct !{!138, !136}
!139 = !{!140}
!140 = distinct !{!140, !136}
!141 = !{!132, !142, !135, !138, !143}
!142 = distinct !{!142, !136}
!143 = distinct !{!143, !136}
!144 = !{!143}
!145 = !{!142}
!146 = !{!132, !135, !138, !143}
!147 = distinct !{!147, !76}
!148 = distinct !{!148, !76}
!149 = !{!150}
!150 = distinct !{!150, !151}
!151 = distinct !{!151, !"LVerDomain"}
!152 = !{!153}
!153 = distinct !{!153, !151}
!154 = !{!155}
!155 = distinct !{!155, !151}
!156 = !{!157, !150, !153, !158}
!157 = distinct !{!157, !151}
!158 = distinct !{!158, !151}
!159 = !{!158}
!160 = !{!157}
!161 = !{!150, !153, !158}
!162 = distinct !{!162, !76}
!163 = distinct !{!163, !76}
!164 = distinct !{!164}
!165 = distinct !{!165, !76}
!166 = distinct !{!166, !82, !76}
!167 = distinct !{!167}
!168 = distinct !{!168, !76}
!169 = distinct !{!169, !82, !76}
!170 = distinct !{!170}
!171 = distinct !{!171, !76}
!172 = distinct !{!172, !82, !76}
!173 = distinct !{!173}
!174 = distinct !{!174, !76}
!175 = distinct !{!175, !82, !76}
!176 = !{!177}
!177 = distinct !{!177, !178, !".omp_outlined..31: %.global_tid."}
!178 = distinct !{!178, !".omp_outlined..31"}
!179 = !{!180}
!180 = distinct !{!180, !181}
!181 = distinct !{!181, !"LVerDomain"}
!182 = !{!183}
!183 = distinct !{!183, !181}
!184 = !{!185}
!185 = distinct !{!185, !181}
!186 = !{!177, !187, !180, !183, !188}
!187 = distinct !{!187, !181}
!188 = distinct !{!188, !181}
!189 = !{!188}
!190 = !{!187}
!191 = !{!177, !180, !183, !188}
!192 = distinct !{!192, !76}
!193 = distinct !{!193, !76}
!194 = !{!195}
!195 = distinct !{!195, !196}
!196 = distinct !{!196, !"LVerDomain"}
!197 = !{!198}
!198 = distinct !{!198, !196}
!199 = !{!200}
!200 = distinct !{!200, !196}
!201 = !{!202, !195, !198, !203}
!202 = distinct !{!202, !196}
!203 = distinct !{!203, !196}
!204 = !{!203}
!205 = !{!202}
!206 = !{!195, !198, !203}
!207 = distinct !{!207, !76}
!208 = distinct !{!208, !76}
!209 = !{!210}
!210 = distinct !{!210, !211, !".omp_outlined..35: %.global_tid."}
!211 = distinct !{!211, !".omp_outlined..35"}
!212 = !{!213}
!213 = distinct !{!213, !214}
!214 = distinct !{!214, !"LVerDomain"}
!215 = !{!216}
!216 = distinct !{!216, !214}
!217 = !{!218}
!218 = distinct !{!218, !214}
!219 = !{!210, !220, !213, !216, !221}
!220 = distinct !{!220, !214}
!221 = distinct !{!221, !214}
!222 = !{!221}
!223 = !{!220}
!224 = !{!210, !213, !216, !221}
!225 = distinct !{!225, !76}
!226 = distinct !{!226, !76}
!227 = !{!228}
!228 = distinct !{!228, !229}
!229 = distinct !{!229, !"LVerDomain"}
!230 = !{!231}
!231 = distinct !{!231, !229}
!232 = !{!233}
!233 = distinct !{!233, !229}
!234 = !{!235, !228, !231, !236}
!235 = distinct !{!235, !229}
!236 = distinct !{!236, !229}
!237 = !{!236}
!238 = !{!235}
!239 = !{!228, !231, !236}
!240 = distinct !{!240, !76}
!241 = distinct !{!241, !76}
!242 = distinct !{!242}
!243 = distinct !{!243, !76}
!244 = distinct !{!244, !82, !76}
!245 = distinct !{!245}
!246 = distinct !{!246, !76}
!247 = distinct !{!247, !82, !76}
!248 = distinct !{!248}
!249 = distinct !{!249, !76}
!250 = distinct !{!250, !82, !76}
!251 = distinct !{!251}
!252 = distinct !{!252, !76}
!253 = distinct !{!253, !82, !76}
!254 = !{!255}
!255 = distinct !{!255, !256, !".omp_outlined..55: %.global_tid."}
!256 = distinct !{!256, !".omp_outlined..55"}
!257 = !{!258}
!258 = distinct !{!258, !259}
!259 = distinct !{!259, !"LVerDomain"}
!260 = !{!261}
!261 = distinct !{!261, !259}
!262 = !{!263}
!263 = distinct !{!263, !259}
!264 = !{!255, !265, !258, !261, !266}
!265 = distinct !{!265, !259}
!266 = distinct !{!266, !259}
!267 = !{!266}
!268 = !{!265}
!269 = !{!255, !258, !261, !266}
!270 = distinct !{!270, !76}
!271 = distinct !{!271, !76}
!272 = !{!273}
!273 = distinct !{!273, !274}
!274 = distinct !{!274, !"LVerDomain"}
!275 = !{!276}
!276 = distinct !{!276, !274}
!277 = !{!278}
!278 = distinct !{!278, !274}
!279 = !{!280, !273, !276, !281}
!280 = distinct !{!280, !274}
!281 = distinct !{!281, !274}
!282 = !{!281}
!283 = !{!280}
!284 = !{!273, !276, !281}
!285 = distinct !{!285, !76}
!286 = distinct !{!286, !76}
!287 = !{!288}
!288 = distinct !{!288, !289, !".omp_outlined..59: %.global_tid."}
!289 = distinct !{!289, !".omp_outlined..59"}
!290 = !{!291}
!291 = distinct !{!291, !292}
!292 = distinct !{!292, !"LVerDomain"}
!293 = !{!294}
!294 = distinct !{!294, !292}
!295 = !{!296}
!296 = distinct !{!296, !292}
!297 = !{!288, !298, !291, !294, !299}
!298 = distinct !{!298, !292}
!299 = distinct !{!299, !292}
!300 = !{!299}
!301 = !{!298}
!302 = !{!288, !291, !294, !299}
!303 = distinct !{!303, !76}
!304 = distinct !{!304, !76}
!305 = !{!306}
!306 = distinct !{!306, !307}
!307 = distinct !{!307, !"LVerDomain"}
!308 = !{!309}
!309 = distinct !{!309, !307}
!310 = !{!311}
!311 = distinct !{!311, !307}
!312 = !{!313, !306, !309, !314}
!313 = distinct !{!313, !307}
!314 = distinct !{!314, !307}
!315 = !{!314}
!316 = !{!313}
!317 = !{!306, !309, !314}
!318 = distinct !{!318, !76}
!319 = distinct !{!319, !76}
!320 = distinct !{!320}
!321 = distinct !{!321, !76}
!322 = distinct !{!322, !82, !76}
!323 = distinct !{!323}
!324 = distinct !{!324, !76}
!325 = distinct !{!325, !82, !76}
!326 = distinct !{!326}
!327 = distinct !{!327, !76}
!328 = distinct !{!328, !82, !76}
!329 = distinct !{!329}
!330 = distinct !{!330, !76}
!331 = distinct !{!331, !82, !76}
!332 = !{!333}
!333 = distinct !{!333, !334, !".omp_outlined..79: %.global_tid."}
!334 = distinct !{!334, !".omp_outlined..79"}
!335 = !{!336}
!336 = distinct !{!336, !337}
!337 = distinct !{!337, !"LVerDomain"}
!338 = !{!339}
!339 = distinct !{!339, !337}
!340 = !{!341}
!341 = distinct !{!341, !337}
!342 = !{!333, !343, !336, !339, !344}
!343 = distinct !{!343, !337}
!344 = distinct !{!344, !337}
!345 = !{!344}
!346 = !{!343}
!347 = !{!333, !336, !339, !344}
!348 = distinct !{!348, !76}
!349 = distinct !{!349, !76}
!350 = !{!351}
!351 = distinct !{!351, !352}
!352 = distinct !{!352, !"LVerDomain"}
!353 = !{!354}
!354 = distinct !{!354, !352}
!355 = !{!356}
!356 = distinct !{!356, !352}
!357 = !{!358, !351, !354, !359}
!358 = distinct !{!358, !352}
!359 = distinct !{!359, !352}
!360 = !{!359}
!361 = !{!358}
!362 = !{!351, !354, !359}
!363 = distinct !{!363, !76}
!364 = distinct !{!364, !76}
!365 = !{!366}
!366 = distinct !{!366, !367, !".omp_outlined..83: %.global_tid."}
!367 = distinct !{!367, !".omp_outlined..83"}
!368 = !{!369}
!369 = distinct !{!369, !370}
!370 = distinct !{!370, !"LVerDomain"}
!371 = !{!372}
!372 = distinct !{!372, !370}
!373 = !{!374}
!374 = distinct !{!374, !370}
!375 = !{!376}
!376 = distinct !{!376, !370}
!377 = !{!366, !378, !369, !372, !374}
!378 = distinct !{!378, !370}
!379 = !{!378}
!380 = !{!366, !369, !372, !374}
!381 = distinct !{!381, !76}
!382 = distinct !{!382, !76}
!383 = !{!384}
!384 = distinct !{!384, !385}
!385 = distinct !{!385, !"LVerDomain"}
!386 = !{!387}
!387 = distinct !{!387, !385}
!388 = !{!389}
!389 = distinct !{!389, !385}
!390 = !{!391}
!391 = distinct !{!391, !385}
!392 = !{!393, !384, !387, !389}
!393 = distinct !{!393, !385}
!394 = !{!393}
!395 = !{!384, !387, !389}
!396 = distinct !{!396, !76}
!397 = distinct !{!397, !76}
!398 = distinct !{!398}
!399 = distinct !{!399, !76}
!400 = distinct !{!400, !82, !76}
!401 = distinct !{!401}
!402 = distinct !{!402, !76}
!403 = distinct !{!403, !82, !76}
!404 = distinct !{!404}
!405 = distinct !{!405, !76}
!406 = distinct !{!406, !82, !76}
!407 = distinct !{!407}
!408 = distinct !{!408, !76}
!409 = distinct !{!409, !82, !76}
!410 = !{!411}
!411 = distinct !{!411, !412, !".omp_outlined..103: %.global_tid."}
!412 = distinct !{!412, !".omp_outlined..103"}
!413 = !{!414}
!414 = distinct !{!414, !415}
!415 = distinct !{!415, !"LVerDomain"}
!416 = !{!417}
!417 = distinct !{!417, !415}
!418 = !{!419}
!419 = distinct !{!419, !415}
!420 = !{!421}
!421 = distinct !{!421, !415}
!422 = !{!411, !423, !414, !417, !419}
!423 = distinct !{!423, !415}
!424 = !{!423}
!425 = !{!411, !414, !417, !419}
!426 = distinct !{!426, !76}
!427 = distinct !{!427, !76}
!428 = !{!429}
!429 = distinct !{!429, !430}
!430 = distinct !{!430, !"LVerDomain"}
!431 = !{!432}
!432 = distinct !{!432, !430}
!433 = !{!434}
!434 = distinct !{!434, !430}
!435 = !{!436}
!436 = distinct !{!436, !430}
!437 = !{!438, !429, !432, !434}
!438 = distinct !{!438, !430}
!439 = !{!438}
!440 = !{!429, !432, !434}
!441 = distinct !{!441, !76}
!442 = distinct !{!442, !76}
!443 = !{!444}
!444 = distinct !{!444, !445, !".omp_outlined..107: %.global_tid."}
!445 = distinct !{!445, !".omp_outlined..107"}
!446 = distinct !{!446}
!447 = distinct !{!447}
!448 = distinct !{!448}
!449 = distinct !{!449}
!450 = !{!451}
!451 = distinct !{!451, !452, !".omp_outlined..127: %.global_tid."}
!452 = distinct !{!452, !".omp_outlined..127"}
!453 = !{!454}
!454 = distinct !{!454, !455, !".omp_outlined..131: %.global_tid."}
!455 = distinct !{!455, !".omp_outlined..131"}
!456 = !{!457}
!457 = distinct !{!457, !458}
!458 = distinct !{!458, !"LVerDomain"}
!459 = !{!460}
!460 = distinct !{!460, !458}
!461 = !{!462}
!462 = distinct !{!462, !458}
!463 = !{!454, !464, !457, !460, !465}
!464 = distinct !{!464, !458}
!465 = distinct !{!465, !458}
!466 = !{!465}
!467 = !{!464}
!468 = !{!454, !457, !460, !465}
!469 = distinct !{!469, !76}
!470 = distinct !{!470, !76}
!471 = !{!472}
!472 = distinct !{!472, !473}
!473 = distinct !{!473, !"LVerDomain"}
!474 = !{!475}
!475 = distinct !{!475, !473}
!476 = !{!477}
!477 = distinct !{!477, !473}
!478 = !{!479, !472, !475, !480}
!479 = distinct !{!479, !473}
!480 = distinct !{!480, !473}
!481 = !{!480}
!482 = !{!479}
!483 = !{!472, !475, !480}
!484 = distinct !{!484, !76}
!485 = distinct !{!485, !76}
!486 = distinct !{!486}
!487 = distinct !{!487, !76}
!488 = distinct !{!488, !82, !76}
!489 = distinct !{!489}
!490 = distinct !{!490, !76}
!491 = distinct !{!491, !82, !76}
!492 = distinct !{!492}
!493 = distinct !{!493, !76}
!494 = distinct !{!494, !82, !76}
!495 = distinct !{!495}
!496 = distinct !{!496, !76}
!497 = distinct !{!497, !82, !76}
!498 = !{!499}
!499 = distinct !{!499, !500, !".omp_outlined..151: %.global_tid."}
!500 = distinct !{!500, !".omp_outlined..151"}
!501 = !{!502}
!502 = distinct !{!502, !503}
!503 = distinct !{!503, !"LVerDomain"}
!504 = !{!505}
!505 = distinct !{!505, !503}
!506 = !{!507}
!507 = distinct !{!507, !503}
!508 = !{!499, !509, !502, !505, !510}
!509 = distinct !{!509, !503}
!510 = distinct !{!510, !503}
!511 = !{!510}
!512 = !{!509}
!513 = !{!499, !502, !505, !510}
!514 = distinct !{!514, !76}
!515 = distinct !{!515, !76}
!516 = !{!517}
!517 = distinct !{!517, !518}
!518 = distinct !{!518, !"LVerDomain"}
!519 = !{!520}
!520 = distinct !{!520, !518}
!521 = !{!522}
!522 = distinct !{!522, !518}
!523 = !{!524, !517, !520, !525}
!524 = distinct !{!524, !518}
!525 = distinct !{!525, !518}
!526 = !{!525}
!527 = !{!524}
!528 = !{!517, !520, !525}
!529 = distinct !{!529, !76}
!530 = distinct !{!530, !76}
!531 = !{!532}
!532 = distinct !{!532, !533}
!533 = distinct !{!533, !"LVerDomain"}
!534 = !{!535}
!535 = distinct !{!535, !533}
!536 = !{!537}
!537 = distinct !{!537, !533}
!538 = !{!532, !535}
!539 = distinct !{!539, !76}
!540 = distinct !{!540, !76}
!541 = !{!542}
!542 = distinct !{!542, !543}
!543 = distinct !{!543, !"LVerDomain"}
!544 = !{!545}
!545 = distinct !{!545, !543}
!546 = !{!547}
!547 = distinct !{!547, !543}
!548 = !{!542, !545}
!549 = distinct !{!549, !76}
!550 = distinct !{!550, !76}
!551 = !{!552}
!552 = distinct !{!552, !553}
!553 = distinct !{!553, !"LVerDomain"}
!554 = !{!555}
!555 = distinct !{!555, !553}
!556 = !{!557}
!557 = distinct !{!557, !553}
!558 = !{!552, !555}
!559 = distinct !{!559, !76}
!560 = distinct !{!560, !76}
!561 = distinct !{!561}
!562 = distinct !{!562, !76}
!563 = distinct !{!563, !82, !76}
!564 = distinct !{!564}
!565 = distinct !{!565, !76}
!566 = distinct !{!566, !82, !76}
!567 = !{!568}
!568 = distinct !{!568, !569}
!569 = distinct !{!569, !"LVerDomain"}
!570 = !{!571}
!571 = distinct !{!571, !569}
!572 = !{!573}
!573 = distinct !{!573, !569}
!574 = !{!568, !571}
!575 = distinct !{!575, !76}
!576 = distinct !{!576, !76}
!577 = distinct !{!577}
!578 = !{!579}
!579 = distinct !{!579, !580}
!580 = distinct !{!580, !"LVerDomain"}
!581 = !{!582}
!582 = distinct !{!582, !580}
!583 = !{!584}
!584 = distinct !{!584, !580}
!585 = !{!579, !582}
!586 = distinct !{!586, !76}
!587 = distinct !{!587, !76}
!588 = !{!589}
!589 = distinct !{!589, !590}
!590 = distinct !{!590, !"LVerDomain"}
!591 = !{!592}
!592 = distinct !{!592, !590}
!593 = !{!594}
!594 = distinct !{!594, !590}
!595 = !{!589, !592}
!596 = distinct !{!596, !76}
!597 = distinct !{!597, !76}
!598 = !{!599}
!599 = distinct !{!599, !600}
!600 = distinct !{!600, !"LVerDomain"}
!601 = !{!602}
!602 = distinct !{!602, !600}
!603 = !{!604}
!604 = distinct !{!604, !600}
!605 = !{!599, !602}
!606 = distinct !{!606, !76}
!607 = distinct !{!607, !76}
!608 = distinct !{!608}
!609 = !{!610}
!610 = distinct !{!610, !611}
!611 = distinct !{!611, !"LVerDomain"}
!612 = !{!613}
!613 = distinct !{!613, !611}
!614 = !{!615}
!615 = distinct !{!615, !611}
!616 = !{!610, !613}
!617 = distinct !{!617, !76}
!618 = distinct !{!618, !76}
!619 = distinct !{!619}
!620 = !{!621}
!621 = distinct !{!621, !622}
!622 = distinct !{!622, !"LVerDomain"}
!623 = !{!624}
!624 = distinct !{!624, !622}
!625 = !{!626}
!626 = distinct !{!626, !622}
!627 = !{!621, !624}
!628 = distinct !{!628, !76}
!629 = distinct !{!629, !76}
!630 = distinct !{!630}
!631 = distinct !{!631}
!632 = distinct !{!632, !76}
!633 = distinct !{!633, !82, !76}
!634 = distinct !{!634}
!635 = distinct !{!635}
!636 = distinct !{!636, !76}
!637 = distinct !{!637, !82, !76}
!638 = distinct !{!638}
!639 = distinct !{!639}
!640 = distinct !{!640, !76}
!641 = distinct !{!641, !82, !76}
!642 = distinct !{!642}
!643 = distinct !{!643}
!644 = distinct !{!644, !76}
!645 = distinct !{!645, !82, !76}
!646 = distinct !{!646}
!647 = distinct !{!647}
!648 = distinct !{!648, !76}
!649 = distinct !{!649, !82, !76}
!650 = distinct !{!650}
!651 = distinct !{!651}
!652 = distinct !{!652, !76}
!653 = distinct !{!653, !82, !76}

; __CLANG_OFFLOAD_BUNDLE____END__ host-powerpc64le-ibm-linux-gnu
